# Optimized Adaptive Pipeline RS

A **production-grade**, **high-performance** file processing system built with Rust, featuring advanced concurrency patterns, adaptive performance optimization, and enterprise-level reliability. This project demonstrates professional Rust development with Channel-based Architecture, Domain-Driven Design, and comprehensive error handling.

[![License](https://img.shields.io/badge/License-BSD_3--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause)
[![Rust](https://img.shields.io/badge/rust-1.70%2B-orange.svg)](https://www.rust-lang.org/)

## ğŸš€ What Makes This Different

This isn't just another file processor - it's a **showcase of advanced Rust patterns** and **production engineering**:

- **ğŸ”„ Channel-Based Concurrency**: Reader â†’ CPU Workers â†’ Direct Writer pattern eliminates bottlenecks
- **âš¡ Hybrid Parallelism**: Rayon for CPU-bound ops + Tokio for async I/O = optimal resource utilization
- **ğŸ¯ Adaptive Performance**: Dynamic chunk sizing and worker scaling based on file characteristics
- **ğŸ›¡ï¸ Zero-Panic Production Code**: 297 unwrap/expect/panic patterns eliminated through systematic remediation
- **ğŸ” Security First**: AES-256-GCM, ChaCha20-Poly1305 with Argon2 key derivation
- **ğŸ“Š Observable**: Prometheus metrics, structured tracing, performance dashboards

## ğŸ“‹ Table of Contents

- [Architecture](#architecture)
- [Performance](#performance)
- [Quick Start](#quick-start)
- [Features](#features)
- [Development](#development)
- [Advanced Usage](#advanced-usage)
- [Contributing](#contributing)

## ğŸ—ï¸ Architecture

### Workspace Structure

The project uses a **3-crate workspace** for clean separation of concerns:

```
optimized_adaptive_pipeline_rs/
â”œâ”€â”€ pipeline-domain/          # Pure domain logic (no async, no I/O)
â”‚   â”œâ”€â”€ entities/             # Business entities with identity
â”‚   â”œâ”€â”€ value_objects/        # Immutable domain values
â”‚   â”œâ”€â”€ services/             # Core business logic (sync)
â”‚   â””â”€â”€ Cargo.toml           # Zero infrastructure deps
â”‚
â”œâ”€â”€ pipeline/                 # Application & Infrastructure
â”‚   â”œâ”€â”€ application/          # Use cases, orchestration
â”‚   â”œâ”€â”€ infrastructure/       # I/O, persistence, adapters
â”‚   â”œâ”€â”€ presentation/         # CLI interface
â”‚   â””â”€â”€ Cargo.toml           # Full feature set
â”‚
â”œâ”€â”€ bootstrap/                # Entry point & platform layer
â”‚   â”œâ”€â”€ config.rs            # DI container, service registry
â”‚   â”œâ”€â”€ signals.rs           # SIGTERM/SIGINT handling
â”‚   â””â”€â”€ platform/            # Cross-platform abstractions
â”‚
â””â”€â”€ Cargo.toml               # Workspace config
```

### Concurrency Model

**Channel-Based Execution Pipeline** (Week 2 Architecture):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Channel     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Direct Write    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Reader    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚ CPU Workers  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚   Writer   â”‚
â”‚   Task      â”‚  Backpressure  â”‚  (Parallel)  â”‚  Random Access     â”‚  (.adapipe)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“                              â†“ â†“ â†“                              â†“
  File I/O                      Rayon Threads                    Concurrent Seeks
  (Streaming)                   (CPU-bound)                      (No Mutex!)
```

**Key Design Decisions:**

1. **Reader Task**: Streams file chunks with backpressure - prevents memory overload
2. **CPU Workers**: Rayon thread pool for parallel stage execution (compress/encrypt)
3. **Direct Writes**: Workers write directly to calculated positions - **no writer bottleneck**
4. **Resource Manager**: Global semaphores prevent CPU/IO oversubscription

### Layer Responsibilities

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Bootstrap Layer                     â”‚
â”‚  (DI, Platform Detection, Signal Handling)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Presentation Layer                  â”‚
â”‚  (CLI, API endpoints, DTOs)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Application Layer                   â”‚
â”‚  (Use cases, orchestration, async services) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Domain Layer                      â”‚
â”‚  (Pure business logic - SYNC only)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Infrastructure Layer                 â”‚
â”‚  (Database, File I/O, External Systems)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Architecture Principles:**

- **Domain Layer**: Pure Rust, no async, no I/O - just business logic
- **Infrastructure Layer**: All I/O, all async, all external dependencies
- **Dependency Inversion**: Domain defines interfaces, infrastructure implements
- **Hexagonal Ports**: `FileIOService`, `CompressionService` are domain ports

## âš¡ Performance

### Benchmarks (M1 Pro, 10-core CPU)

| File Size | Throughput | Worker Count | Chunk Size | Memory |
|-----------|-----------|--------------|------------|--------|
| 100 MB    | 520 MB/s  | 8 workers    | 4 MB       | 128 MB |
| 1 GB      | 580 MB/s  | 10 workers   | 8 MB       | 256 MB |
| 10 GB     | 610 MB/s  | 10 workers   | 16 MB      | 512 MB |

### Optimizations Implemented

âœ… **Memory Efficiency**
- Streaming I/O (no full-file read)
- Memory-mapped files for large data
- Zero-copy operations where possible
- Adaptive chunk sizing (1MB-64MB)

âœ… **CPU Optimization**
- Rayon work-stealing for CPU-bound ops
- SIMD acceleration (where available)
- Lock-free metrics collection
- Parallel chunk processing

âœ… **I/O Optimization**
- Async I/O with Tokio
- Direct concurrent writes (no mutex!)
- Read-ahead buffering
- Write coalescing

âœ… **Concurrency Patterns**
- Channel backpressure prevents overload
- Resource tokens prevent oversubscription
- Graceful cancellation (CancellationToken)
- Supervision tree for task management

## ğŸš€ Quick Start

### Prerequisites

- **Rust**: 1.70 or later (install via [rustup](https://rustup.rs/))
- **SQLite**: For pipeline persistence
- **OS**: macOS, Linux, or Windows (WSL recommended)

### Installation

```bash
# Clone the repository
git clone https://github.com/abitofhelp/optimized_adaptive_pipeline_rs.git
cd optimized_adaptive_pipeline_rs

# Build optimized binary
make build-release

# Run tests to verify
make test

# Binary location
./target/release/pipeline --help
```

### First Run

```bash
# Create a test file
dd if=/dev/urandom of=test.dat bs=1M count=100

# Process with compression + encryption
./target/release/pipeline process \
  --input test.dat \
  --output test.adapipe \
  --compress \
  --encrypt

# Check output
ls -lh test.adapipe

# Restore original
./target/release/pipeline restore \
  --input test.adapipe \
  --output restored.dat

# Verify integrity
diff test.dat restored.dat
```

## âœ¨ Features

### Core Capabilities

ğŸ”„ **Multi-Stage Processing Pipeline**
- Configurable stages: compression, encryption, validation
- Dynamic stage ordering and parallelization
- Plugin architecture for custom stages

ğŸ” **Enterprise Security**
- **Encryption**: AES-256-GCM (hardware accelerated), ChaCha20-Poly1305
- **Key Derivation**: Argon2id, scrypt, PBKDF2
- **Memory Safety**: Automatic key zeroing, secure memory handling
- **Integrity**: SHA-256 checksums, chunk-level verification

âš¡ **Adaptive Performance**
- Auto-detects optimal chunk size (1MB-64MB)
- Dynamic worker count (1-32 based on cores)
- File size-based optimization profiles
- Resource-aware backpressure

ğŸ“Š **Observability**
- Prometheus metrics endpoint
- Structured logging (tracing)
- Performance dashboards (Grafana)
- Queue depth monitoring

ğŸ›¡ï¸ **Production Reliability**
- Zero panic in production code (297 patterns eliminated)
- Comprehensive error handling
- Graceful shutdown (SIGTERM/SIGINT)
- Supervision tree for task recovery

### Supported Algorithms

**Compression:**
- Brotli (best ratio, good speed)
- Zstd (balanced)
- Gzip (widely compatible)
- LZ4 (fastest)

**Encryption:**
- AES-256-GCM (default, hardware accelerated)
- ChaCha20-Poly1305 (constant-time)
- AES-128/192-GCM variants

**Key Derivation:**
- Argon2id (memory-hard, GPU-resistant)
- scrypt (alternative memory-hard)
- PBKDF2 (legacy compatibility)

## ğŸ’» Development

### Makefile Targets

```bash
# Development
make check              # Fast syntax check
make build             # Debug build
make build-release     # Optimized build

# Quality
make lint              # Development linting
make lint-strict       # Production linting (no unwrap/panic)
make format            # Auto-format code
make test              # Run all tests

# CI/CD
make ci-local          # Full CI pipeline locally
make pre-commit        # Pre-commit checks

# Performance
make bench             # Run benchmarks
make flamegraph        # Generate flamegraph

# Documentation
make doc-open          # Generate & open docs
```

### Running Tests

```bash
# All tests
cargo test --workspace

# Unit tests only (fast)
cargo test --lib

# Integration tests
cargo test --test '*'

# With logging
RUST_LOG=debug cargo test -- --nocapture

# Specific test
cargo test test_channel_pipeline
```

### Code Quality Standards

**Zero Tolerance in Production:**
```rust
// âŒ Never in production code
.unwrap()
.expect("...")
panic!("...")
todo!()
unimplemented!()

// âœ… Always use
.map_err(|e| PipelineError::...)?
.ok_or_else(|| PipelineError::...)?
Result<T, PipelineError>
```

**Enforced by CI:**
```bash
make lint-strict  # Must pass for merge

# Denies:
# - clippy::unwrap_used
# - clippy::expect_used
# - clippy::panic
# - clippy::todo
# - clippy::unimplemented
```

### Project Structure

```
pipeline/src/
â”œâ”€â”€ application/
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ pipeline_service.rs          # Channel-based orchestration
â”‚   â”‚   â”œâ”€â”€ file_processor_service.rs    # File processing logic
â”‚   â”‚   â””â”€â”€ transactional_chunk_writer.rs # Concurrent writes
â”‚   â””â”€â”€ use_cases/
â”‚       â””â”€â”€ restore_file.rs               # File restoration
â”‚
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ adapters/
â”‚   â”‚   â”œâ”€â”€ compression_service_adapter.rs
â”‚   â”‚   â”œâ”€â”€ encryption_service_adapter.rs
â”‚   â”‚   â””â”€â”€ file_io_service_adapter.rs
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ rayon_config.rs               # CPU pool management
â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â”œâ”€â”€ metrics_service.rs            # Prometheus integration
â”‚   â”‚   â””â”€â”€ concurrency_metrics.rs        # Queue depth tracking
â”‚   â”œâ”€â”€ runtime/
â”‚   â”‚   â”œâ”€â”€ resource_manager.rs           # CPU/IO tokens
â”‚   â”‚   â””â”€â”€ supervisor.rs                 # Task supervision
â”‚   â””â”€â”€ repositories/
â”‚       â””â”€â”€ sqlite_pipeline_repository.rs # Pipeline persistence
â”‚
â”œâ”€â”€ presentation/
â”‚   â””â”€â”€ cli/
â”‚       â””â”€â”€ commands.rs                    # CLI interface
â”‚
â””â”€â”€ main.rs                                # Entry point

bootstrap/src/
â”œâ”€â”€ config.rs           # DI container
â”œâ”€â”€ signals.rs          # Signal handling
â””â”€â”€ platform/           # Platform abstractions

pipeline-domain/src/
â”œâ”€â”€ entities/           # Business entities
â”œâ”€â”€ value_objects/      # Domain values
â””â”€â”€ services/           # Domain services (sync)
```

## ğŸ¯ Advanced Usage

### Custom Pipeline Configuration

```rust
use pipeline::PipelineBuilder;

// Create custom pipeline
let pipeline = PipelineBuilder::new("secure-backup")
    .add_compression("zstd", 9)
    .add_encryption("aes256gcm")
    .add_integrity_check()
    .chunk_size_mb(16)
    .workers(8)
    .build()?;

// Execute
pipeline.process("input.dat", "output.adapipe").await?;
```

### Benchmarking

```bash
# Auto-benchmark all file sizes
./target/release/pipeline benchmark

# Specific size with iterations
./target/release/pipeline benchmark \
  --size-mb 1000 \
  --iterations 5 \
  --output-report bench_results.md

# Compare configurations
./target/release/pipeline compare \
  --configs baseline.toml,optimized.toml \
  --size-mb 500
```

### Monitoring

```bash
# Start with metrics
./target/release/pipeline serve --metrics-port 9090

# Query Prometheus
curl http://localhost:9090/metrics

# Key metrics:
# - pipeline_throughput_bytes_per_second
# - pipeline_cpu_queue_depth
# - pipeline_worker_utilization
# - pipeline_chunk_processing_duration_ms
```

### Platform-Specific Builds

```bash
# Cross-compilation (requires cross)
make install-cross-targets

# Linux x86_64
make build-linux-x86_64

# macOS Apple Silicon
make build-macos-aarch64

# Windows x64
make build-windows-x86_64

# All platforms
make build-all-platforms
```

## ğŸ”§ Configuration

### Example: `pipeline.toml`

```toml
[pipeline]
name = "production-pipeline"
chunk_size_mb = 8        # Adaptive default
parallel_workers = 0     # 0 = auto-detect

[compression]
algorithm = "zstd"
level = "balanced"       # fast | balanced | best
parallel_processing = true

[encryption]
algorithm = "aes256gcm"
key_derivation = "argon2id"
memory_cost = 65536     # 64 MB
iterations = 3

[performance]
memory_limit_mb = 2048
use_memory_mapping = true
cpu_throttle = false
simd_enabled = true

[observability]
metrics_enabled = true
metrics_port = 9090
trace_level = "info"    # trace | debug | info | warn | error
```

### Environment Variables

```bash
# Database
export ADAPIPE_SQLITE_PATH="pipeline.db"

# Logging
export RUST_LOG="pipeline=debug,tower_http=warn"

# Performance
export RAYON_NUM_THREADS=8
export TOKIO_WORKER_THREADS=4
```

## ğŸ¤ Contributing

We welcome contributions! This project showcases production-grade Rust patterns.

### Getting Started

```bash
# Fork the repository
git clone https://github.com/YOUR_USERNAME/optimized_adaptive_pipeline_rs.git

# Create feature branch
git checkout -b feature/amazing-feature

# Make changes, add tests
make test

# Ensure quality
make lint-strict
make format

# Commit with conventional commits
git commit -m "feat: add amazing feature"

# Push and create PR
git push origin feature/amazing-feature
```

### Development Guidelines

1. **All production code must be panic-free** (`make lint-strict` must pass)
2. **Domain layer stays pure** (no async, no I/O dependencies)
3. **Test coverage** for new features (unit + integration)
4. **Documentation** for public APIs (rustdoc comments)
5. **Performance** - benchmark before/after for critical paths

### Code Review Checklist

- [ ] No `unwrap/expect/panic` in production code
- [ ] Domain layer remains pure (no async)
- [ ] Tests added and passing
- [ ] Benchmarks show no regression
- [ ] Documentation updated
- [ ] `make lint-strict` passes
- [ ] Conventional commit messages

## ğŸ“š Resources

### Documentation

- [Channel-Based Architecture](docs/EXECUTION_VS_PROCESSING_PIPELINES.md)
- [Database Setup](docs/DATABASE_SETUP.md)
- [Performance Tuning](docs/adaptive-performance-optimization.md)
- [API Documentation](https://docs.rs/optimized-adaptive-pipeline-rs)

### Recent Improvements

- âœ… **Week 2 Concurrency**: Channel-based pipeline with direct writes
- âœ… **Error Remediation**: 297 unwrap/expect/panic eliminated
- âœ… **Rayon Integration**: Parallel CPU-bound processing
- âœ… **Resource Manager**: Global CPU/IO token management
- âœ… **Signal Handling**: Graceful shutdown on SIGTERM/SIGINT
- âœ… **Streaming I/O**: Memory-efficient file processing
- âœ… **License Change**: MIT â†’ BSD 3-Clause

## ğŸ“„ License

This project is licensed under the **BSD 3-Clause License** - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Architecture inspired by production systems at scale
- Concurrency patterns from Tokio/Rayon ecosystems
- Security design following NIST Cybersecurity Framework
- Domain-Driven Design principles from Eric Evans
- Built with â¤ï¸ by the Rust community

---

**Built with Rust ğŸ¦€ | Production-Ready âœ¨ | Performance-First âš¡**
