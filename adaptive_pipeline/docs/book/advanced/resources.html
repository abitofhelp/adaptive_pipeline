<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Resource Management - Pipeline Developer Guide</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Pipeline Developer Guide</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="resource-management"><a class="header" href="#resource-management">Resource Management</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> 2025-01-04
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>This chapter explores the pipeline's resource management system, including CPU and I/O token governance, memory tracking, and resource optimization strategies for different workload types.</p>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>The pipeline employs a <strong>two-level resource governance</strong> architecture that prevents system oversubscription when processing multiple files concurrently:</p>
<ol>
<li><strong>Global Resource Manager</strong>: Caps total system resources (CPU tokens, I/O tokens, memory)</li>
<li><strong>Local Resource Limits</strong>: Caps per-file concurrency (semaphores within each file's processing)</li>
</ol>
<p>This two-level approach ensures optimal resource utilization without overwhelming the system.</p>
<h3 id="why-resource-governance"><a class="header" href="#why-resource-governance">Why Resource Governance?</a></h3>
<p><strong>Problem without limits:</strong></p>
<pre><code class="language-text">10 files × 8 workers/file = 80 concurrent tasks on an 8-core machine
Result: CPU oversubscription, cache thrashing, poor throughput
</code></pre>
<p><strong>Solution with resource governance:</strong></p>
<pre><code class="language-text">Global CPU tokens: 7 (cores - 1)
Maximum 7 concurrent CPU-intensive tasks across all files
Result: Optimal CPU utilization, consistent performance
</code></pre>
<h2 id="resource-management-architecture"><a class="header" href="#resource-management-architecture">Resource Management Architecture</a></h2>
<pre><code class="language-text">┌─────────────────────────────────────────────────────────────┐
│              Global Resource Manager                        │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  CPU Tokens (Semaphore)                              │   │
│  │  - Default: cores - 1                                │   │
│  │  - Purpose: Prevent CPU oversubscription             │   │
│  │  - Used by: Rayon workers, CPU-bound operations      │   │
│  └──────────────────────────────────────────────────────┘   │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  I/O Tokens (Semaphore)                              │   │
│  │  - Default: Device-specific (NVMe:24, SSD:12, HDD:4) │   │
│  │  - Purpose: Prevent I/O queue overrun                │   │
│  │  - Used by: File reads/writes                        │   │
│  └──────────────────────────────────────────────────────┘   │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  Memory Tracking (AtomicUsize)                       │   │
│  │  - Default: 40 GB capacity                           │   │
│  │  - Purpose: Monitor memory pressure (gauge only)     │   │
│  │  - Used by: Memory allocation/deallocation           │   │
│  └──────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
                                │
                                │ acquire_cpu() / acquire_io()
                                ▼
┌─────────────────────────────────────────────────────────────┐
│              File-Level Processing                          │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  Per-File Semaphore                                  │   │
│  │  - Local concurrency limit (e.g., 8 workers/file)    │   │
│  │  - Prevents single file from saturating system       │   │
│  └──────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
</code></pre>
<h2 id="cpu-token-management"><a class="header" href="#cpu-token-management">CPU Token Management</a></h2>
<h3 id="overview-1"><a class="header" href="#overview-1">Overview</a></h3>
<p><strong>CPU tokens</strong> limit the total number of concurrent CPU-bound operations across all files being processed. This prevents CPU oversubscription and cache thrashing.</p>
<p><strong>Implementation:</strong> Semaphore-based with RAII (Resource Acquisition Is Initialization)</p>
<h3 id="configuration"><a class="header" href="#configuration">Configuration</a></h3>
<p><strong>Default CPU Token Count:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Automatic detection: cores - 1
let available_cores = std::thread::available_parallelism().map(|n| n.get()).unwrap_or(4);
let cpu_token_count = (available_cores - 1).max(1);

// Example on 8-core machine: 7 CPU tokens
<span class="boring">}</span></code></pre></pre>
<p><strong>Why <code>cores - 1</code>?</strong></p>
<ul>
<li>Leave one core for OS, I/O threads, and system tasks</li>
<li>Prevents complete CPU saturation</li>
<li>Improves overall system responsiveness</li>
<li>Reduces context switching overhead</li>
</ul>
<p><strong>Custom Configuration:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use adaptive_pipeline::infrastructure::runtime::{init_resource_manager, ResourceConfig};

// Initialize with custom CPU token count
let config = ResourceConfig {
    cpu_tokens: Some(6),  // Override: use 6 CPU workers
    ..Default::default()
};

init_resource_manager(config)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="usage-pattern"><a class="header" href="#usage-pattern">Usage Pattern</a></h3>
<p><strong>Acquire CPU token before CPU-intensive work:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use adaptive_pipeline::infrastructure::runtime::RESOURCE_MANAGER;

async fn process_chunk(chunk: FileChunk) -&gt; Result&lt;FileChunk, PipelineError&gt; {
    // 1. Acquire global CPU token (waits if system is saturated)
    let _cpu_permit = RESOURCE_MANAGER.acquire_cpu().await?;

    // 2. Do CPU-intensive work (compression, encryption, hashing)
    tokio::task::spawn_blocking(move || {
        RAYON_POOLS.cpu_bound_pool().install(|| {
            compress_and_encrypt(chunk)
        })
    })
    .await?

    // 3. Permit released automatically when _cpu_permit goes out of scope (RAII)
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Key Points:</strong></p>
<ul>
<li><code>acquire_cpu()</code> returns a <code>SemaphorePermit</code> guard</li>
<li>If all CPU tokens are in use, the call <strong>waits</strong> (backpressure)</li>
<li>Permit is auto-released when dropped (RAII pattern)</li>
<li>This creates natural flow control and prevents oversubscription</li>
</ul>
<h3 id="backpressure-mechanism"><a class="header" href="#backpressure-mechanism">Backpressure Mechanism</a></h3>
<p>When all CPU tokens are in use:</p>
<pre><code class="language-text">Timeline:
─────────────────────────────────────────────────────────────
Task 1: [===CPU work===]         (acquired token)
Task 2:   [===CPU work===]       (acquired token)
Task 3:     [===CPU work===]     (acquired token)
...
Task 7:             [===CPU===]  (acquired token - last one!)
Task 8:               ⏳⏳⏳     (waiting for token...)
Task 9:                 ⏳⏳⏳   (waiting for token...)
─────────────────────────────────────────────────────────────

When Task 1 completes → Task 8 acquires the released token
When Task 2 completes → Task 9 acquires the released token
</code></pre>
<p>This backpressure prevents overwhelming the CPU with too many concurrent tasks.</p>
<h3 id="monitoring-cpu-saturation"><a class="header" href="#monitoring-cpu-saturation">Monitoring CPU Saturation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use adaptive_pipeline::infrastructure::metrics::CONCURRENCY_METRICS;

// Check CPU saturation percentage
let saturation = CONCURRENCY_METRICS.cpu_saturation_percent();

if saturation &gt; 80.0 {
    println!("CPU-saturated: {}%", saturation);
    println!("Available tokens: {}", RESOURCE_MANAGER.cpu_tokens_available());
    println!("Total tokens: {}", RESOURCE_MANAGER.cpu_tokens_total());
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Saturation Interpretation:</strong></p>
<ul>
<li><strong>0-50%</strong>: Underutilized, could increase worker count</li>
<li><strong>50-80%</strong>: Good utilization, healthy balance</li>
<li><strong>80-95%</strong>: High utilization, approaching saturation</li>
<li><strong>95-100%</strong>: Fully saturated, tasks frequently waiting</li>
</ul>
<h2 id="io-token-management"><a class="header" href="#io-token-management">I/O Token Management</a></h2>
<h3 id="overview-2"><a class="header" href="#overview-2">Overview</a></h3>
<p><strong>I/O tokens</strong> limit the total number of concurrent I/O operations to prevent overwhelming the storage device's queue depth.</p>
<p><strong>Why separate from CPU tokens?</strong></p>
<ul>
<li>I/O and CPU have different characteristics</li>
<li>Storage devices have specific optimal queue depths</li>
<li>Prevents I/O queue saturation independent of CPU usage</li>
</ul>
<h3 id="device-specific-optimization"><a class="header" href="#device-specific-optimization">Device-Specific Optimization</a></h3>
<p>Different storage devices have different optimal I/O queue depths:</p>
<div class="table-wrapper"><table><thead><tr><th>Storage Type</th><th>Queue Depth</th><th>Rationale</th></tr></thead><tbody>
<tr><td><strong>NVMe</strong></td><td>24</td><td>Multiple parallel channels, low latency</td></tr>
<tr><td><strong>SSD</strong></td><td>12</td><td>Medium parallelism, good random access</td></tr>
<tr><td><strong>HDD</strong></td><td>4</td><td>Sequential access preferred, high seek latency</td></tr>
<tr><td><strong>Auto</strong></td><td>12</td><td>Conservative default (assumes SSD)</td></tr>
</tbody></table>
</div>
<p><strong>Implementation</strong> (<code>pipeline/src/infrastructure/runtime/resource_manager.rs</code>):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub enum StorageType {
    NVMe,   // 24 tokens
    SSD,    // 12 tokens
    HDD,    // 4 tokens
    Auto,   // 12 tokens (SSD default)
    Custom(usize),
}

fn detect_optimal_io_tokens(storage_type: StorageType) -&gt; usize {
    match storage_type {
        StorageType::NVMe =&gt; 24,
        StorageType::SSD =&gt; 12,
        StorageType::HDD =&gt; 4,
        StorageType::Auto =&gt; 12,  // Conservative default
        StorageType::Custom(n) =&gt; n,
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="configuration-1"><a class="header" href="#configuration-1">Configuration</a></h3>
<p><strong>Custom I/O token count:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let config = ResourceConfig {
    io_tokens: Some(24),              // Override: NVMe-optimized
    storage_type: StorageType::NVMe,
    ..Default::default()
};

init_resource_manager(config)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="usage-pattern-1"><a class="header" href="#usage-pattern-1">Usage Pattern</a></h3>
<p><strong>Acquire I/O token before file operations:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn read_file_chunk(path: &amp;Path, offset: u64, size: usize)
    -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt;
{
    // 1. Acquire global I/O token (waits if I/O queue is full)
    let _io_permit = RESOURCE_MANAGER.acquire_io().await?;

    // 2. Perform I/O operation
    let mut file = tokio::fs::File::open(path).await?;
    file.seek(SeekFrom::Start(offset)).await?;

    let mut buffer = vec![0u8; size];
    file.read_exact(&amp;mut buffer).await?;

    Ok(buffer)

    // 3. Permit auto-released when _io_permit goes out of scope
}
<span class="boring">}</span></code></pre></pre>
<h3 id="monitoring-io-saturation"><a class="header" href="#monitoring-io-saturation">Monitoring I/O Saturation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Check I/O saturation
let io_saturation = CONCURRENCY_METRICS.io_saturation_percent();

if io_saturation &gt; 80.0 {
    println!("I/O-saturated: {}%", io_saturation);
    println!("Available tokens: {}", RESOURCE_MANAGER.io_tokens_available());
    println!("Consider reducing concurrent I/O or optimizing chunk size");
}
<span class="boring">}</span></code></pre></pre>
<h2 id="memory-management"><a class="header" href="#memory-management">Memory Management</a></h2>
<h3 id="overview-3"><a class="header" href="#overview-3">Overview</a></h3>
<p>The pipeline uses <strong>memory tracking</strong> (not enforcement) to monitor memory pressure and provide visibility into resource usage.</p>
<p><strong>Design Philosophy:</strong></p>
<ul>
<li><strong>Phase 1</strong>: Monitor memory usage (current implementation)</li>
<li><strong>Phase 2</strong>: Soft limits with warnings</li>
<li><strong>Phase 3</strong>: Hard limits with enforcement (future)</li>
</ul>
<p><strong>Why start with monitoring only?</strong></p>
<ul>
<li>Memory is harder to predict and control than CPU/I/O</li>
<li>Avoids complexity in initial implementation</li>
<li>Allows gathering real-world usage data before adding limits</li>
</ul>
<h3 id="memory-tracking"><a class="header" href="#memory-tracking">Memory Tracking</a></h3>
<p><strong>Atomic counter</strong> (<code>AtomicUsize</code>) tracks allocated memory:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use adaptive_pipeline::infrastructure::runtime::RESOURCE_MANAGER;

// Track memory allocation
let chunk_size = 1024 * 1024;  // 1 MB
RESOURCE_MANAGER.allocate_memory(chunk_size);

// ... use memory ...

// Track memory deallocation
RESOURCE_MANAGER.deallocate_memory(chunk_size);
<span class="boring">}</span></code></pre></pre>
<p><strong>Usage with RAII guard:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct MemoryGuard {
    size: usize,
}

impl MemoryGuard {
    pub fn new(size: usize) -&gt; Self {
        RESOURCE_MANAGER.allocate_memory(size);
        Self { size }
    }
}

impl Drop for MemoryGuard {
    fn drop(&amp;mut self) {
        RESOURCE_MANAGER.deallocate_memory(self.size);
    }
}

// Usage
let _guard = MemoryGuard::new(chunk_size);
// Memory automatically tracked on allocation and deallocation
<span class="boring">}</span></code></pre></pre>
<h3 id="configuration-2"><a class="header" href="#configuration-2">Configuration</a></h3>
<p><strong>Set memory capacity:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let config = ResourceConfig {
    memory_limit: Some(40 * 1024 * 1024 * 1024),  // 40 GB capacity
    ..Default::default()
};

init_resource_manager(config)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="monitoring-memory-usage"><a class="header" href="#monitoring-memory-usage">Monitoring Memory Usage</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use adaptive_pipeline::infrastructure::metrics::CONCURRENCY_METRICS;

// Check current memory usage
let used_bytes = RESOURCE_MANAGER.memory_used();
let used_mb = CONCURRENCY_METRICS.memory_used_mb();
let utilization = CONCURRENCY_METRICS.memory_utilization_percent();

println!("Memory: {:.2} MB / {} MB ({:.1}%)",
    used_mb,
    RESOURCE_MANAGER.memory_capacity() / (1024 * 1024),
    utilization
);

// Alert on high memory usage
if utilization &gt; 80.0 {
    println!("⚠️  High memory usage: {:.1}%", utilization);
    println!("Consider reducing chunk size or worker count");
}
<span class="boring">}</span></code></pre></pre>
<h2 id="concurrency-metrics"><a class="header" href="#concurrency-metrics">Concurrency Metrics</a></h2>
<h3 id="overview-4"><a class="header" href="#overview-4">Overview</a></h3>
<p>The pipeline provides comprehensive metrics for monitoring resource utilization, wait times, and saturation levels.</p>
<p><strong>Metric Types:</strong></p>
<ul>
<li><strong>Gauges</strong>: Instant values (e.g., CPU tokens available)</li>
<li><strong>Counters</strong>: Cumulative values (e.g., total wait time)</li>
<li><strong>Histograms</strong>: Distribution of values (e.g., P50/P95/P99 wait times)</li>
</ul>
<h3 id="cpu-metrics"><a class="header" href="#cpu-metrics">CPU Metrics</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use adaptive_pipeline::infrastructure::metrics::CONCURRENCY_METRICS;

// Instant metrics (gauges)
let cpu_available = CONCURRENCY_METRICS.cpu_tokens_available();
let cpu_saturation = CONCURRENCY_METRICS.cpu_saturation_percent();

// Wait time percentiles (histograms)
let p50 = CONCURRENCY_METRICS.cpu_wait_p50();  // Median wait time (ms)
let p95 = CONCURRENCY_METRICS.cpu_wait_p95();  // 95th percentile (ms)
let p99 = CONCURRENCY_METRICS.cpu_wait_p99();  // 99th percentile (ms)

println!("CPU Saturation: {:.1}%", cpu_saturation);
println!("Wait times: P50={} ms, P95={} ms, P99={} ms", p50, p95, p99);
<span class="boring">}</span></code></pre></pre>
<p><strong>Why percentiles matter:</strong></p>
<p>Averages hide problems:</p>
<ul>
<li>Average wait: 10 ms (looks fine)</li>
<li>P99 wait: 500 ms (users experience terrible latency!)</li>
</ul>
<p>Histograms show the full distribution and reveal tail latencies.</p>
<h3 id="io-metrics"><a class="header" href="#io-metrics">I/O Metrics</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// I/O saturation and wait times
let io_saturation = CONCURRENCY_METRICS.io_saturation_percent();
let io_p95 = CONCURRENCY_METRICS.io_wait_p95();

if io_saturation &gt; 80.0 &amp;&amp; io_p95 &gt; 50 {
    println!("⚠️  I/O bottleneck detected:");
    println!("  Saturation: {:.1}%", io_saturation);
    println!("  P95 wait time: {} ms", io_p95);
}
<span class="boring">}</span></code></pre></pre>
<h3 id="worker-metrics"><a class="header" href="#worker-metrics">Worker Metrics</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Worker tracking
let active = CONCURRENCY_METRICS.active_workers();
let spawned = CONCURRENCY_METRICS.tasks_spawned();
let completed = CONCURRENCY_METRICS.tasks_completed();

println!("Workers: {} active, {} spawned, {} completed",
    active, spawned, completed);

// Queue depth (backpressure indicator)
let queue_depth = CONCURRENCY_METRICS.cpu_queue_depth();
let queue_max = CONCURRENCY_METRICS.cpu_queue_depth_max();

if queue_depth &gt; 100 {
    println!("⚠️  High queue depth: {} (max: {})", queue_depth, queue_max);
    println!("Workers can't keep up with reader - increase workers or optimize stages");
}
<span class="boring">}</span></code></pre></pre>
<h3 id="queue-metrics"><a class="header" href="#queue-metrics">Queue Metrics</a></h3>
<p><strong>Queue depth</strong> reveals whether workers can keep up with the reader:</p>
<ul>
<li><strong>Depth near 0</strong>: Workers are faster than reader (good!)</li>
<li><strong>Depth near capacity</strong>: Workers are bottleneck (increase workers or optimize stages)</li>
<li><strong>Depth at capacity</strong>: Reader is blocked (severe backpressure)</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Queue wait time distribution
let queue_p50 = CONCURRENCY_METRICS.cpu_queue_wait_p50();
let queue_p95 = CONCURRENCY_METRICS.cpu_queue_wait_p95();
let queue_p99 = CONCURRENCY_METRICS.cpu_queue_wait_p99();

println!("Queue wait: P50={} ms, P95={} ms, P99={} ms",
    queue_p50, queue_p95, queue_p99);
<span class="boring">}</span></code></pre></pre>
<h2 id="resource-configuration"><a class="header" href="#resource-configuration">Resource Configuration</a></h2>
<h3 id="resourceconfig-structure"><a class="header" href="#resourceconfig-structure">ResourceConfig Structure</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct ResourceConfig {
    /// Number of CPU worker tokens (default: cores - 1)
    pub cpu_tokens: Option&lt;usize&gt;,

    /// Number of I/O tokens (default: device-specific)
    pub io_tokens: Option&lt;usize&gt;,

    /// Storage device type for I/O optimization
    pub storage_type: StorageType,

    /// Soft memory limit in bytes (gauge only, no enforcement)
    pub memory_limit: Option&lt;usize&gt;,
}
<span class="boring">}</span></code></pre></pre>
<h3 id="initialization-pattern"><a class="header" href="#initialization-pattern">Initialization Pattern</a></h3>
<p><strong>In <code>main()</code>, before any operations:</strong></p>
<pre><pre class="playground"><code class="language-rust">use adaptive_pipeline::infrastructure::runtime::{init_resource_manager, ResourceConfig, StorageType};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    // 1. Initialize resource manager with configuration
    let config = ResourceConfig {
        cpu_tokens: Some(6),              // 6 CPU workers
        io_tokens: Some(24),              // NVMe-optimized
        storage_type: StorageType::NVMe,
        memory_limit: Some(40 * 1024 * 1024 * 1024),  // 40 GB
    };

    init_resource_manager(config)
        .map_err(|e| anyhow!("Failed to initialize resources: {}", e))?;

    // 2. Now safe to use RESOURCE_MANAGER anywhere
    run_pipeline().await
}</code></pre></pre>
<p><strong>Global access pattern:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn my_function() {
    // Access global resource manager
    let _cpu_permit = RESOURCE_MANAGER.acquire_cpu().await?;
    // ... CPU work ...
}
<span class="boring">}</span></code></pre></pre>
<h2 id="tuning-guidelines"><a class="header" href="#tuning-guidelines">Tuning Guidelines</a></h2>
<h3 id="1-cpu-bound-workloads"><a class="header" href="#1-cpu-bound-workloads">1. CPU-Bound Workloads</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>High CPU saturation (&gt; 80%)</li>
<li>Low CPU wait times (&lt; 10 ms P95)</li>
<li>Heavy compression, encryption, or hashing</li>
</ul>
<p><strong>Tuning:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Increase CPU tokens to match cores (remove safety margin)
let config = ResourceConfig {
    cpu_tokens: Some(available_cores),  // Use all cores
    ..Default::default()
};
<span class="boring">}</span></code></pre></pre>
<p><strong>Trade-offs:</strong></p>
<ul>
<li>✅ Higher throughput for CPU-bound work</li>
<li>❌ Reduced system responsiveness</li>
<li>❌ Higher context switching overhead</li>
</ul>
<h3 id="2-io-bound-workloads"><a class="header" href="#2-io-bound-workloads">2. I/O-Bound Workloads</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>High I/O saturation (&gt; 80%)</li>
<li>High I/O wait times (&gt; 50 ms P95)</li>
<li>Many concurrent file operations</li>
</ul>
<p><strong>Tuning:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Increase I/O tokens for high-throughput storage
let config = ResourceConfig {
    io_tokens: Some(32),              // Higher than default
    storage_type: StorageType::Custom(32),
    ..Default::default()
};
<span class="boring">}</span></code></pre></pre>
<p><strong>Monitoring:</strong></p>
<pre><code class="language-bash"># Check I/O queue depth on Linux
iostat -x 1

# Look for:
# - avgqu-sz (average queue size) - should be &lt; I/O tokens
# - %util (device utilization) - should be 70-90%
</code></pre>
<h3 id="3-memory-constrained-systems"><a class="header" href="#3-memory-constrained-systems">3. Memory-Constrained Systems</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>High memory utilization (&gt; 80%)</li>
<li>Swapping or OOM errors</li>
<li>Processing large files</li>
</ul>
<p><strong>Tuning:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Reduce chunk size to lower memory usage
let chunk_size = ChunkSize::new(16 * 1024 * 1024);  // 16 MB (smaller)

// Reduce worker count to limit concurrent chunks
let config = ResourceConfig {
    cpu_tokens: Some(3),  // Fewer workers = less memory
    ..Default::default()
};
<span class="boring">}</span></code></pre></pre>
<p><strong>Formula:</strong></p>
<pre><code class="language-text">Peak Memory ≈ chunk_size × cpu_tokens × files_processed_concurrently

Example:
  chunk_size = 64 MB
  cpu_tokens = 7
  files = 3
  Peak Memory ≈ 64 MB × 7 × 3 = 1.3 GB
</code></pre>
<h3 id="4-mixed-workloads"><a class="header" href="#4-mixed-workloads">4. Mixed Workloads</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>Both CPU and I/O saturation</li>
<li>Variable chunk processing times</li>
<li>Compression + file I/O operations</li>
</ul>
<p><strong>Tuning:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Balanced configuration
let config = ResourceConfig {
    cpu_tokens: Some(available_cores - 1),  // Leave headroom
    io_tokens: Some(12),                    // Moderate I/O concurrency
    storage_type: StorageType::SSD,
    ..Default::default()
};
<span class="boring">}</span></code></pre></pre>
<p><strong>Best practices:</strong></p>
<ul>
<li>Monitor both CPU and I/O saturation</li>
<li>Adjust based on bottleneck (CPU vs I/O)</li>
<li>Use Rayon's mixed workload pool for hybrid operations</li>
</ul>
<h3 id="5-multi-file-processing"><a class="header" href="#5-multi-file-processing">5. Multi-File Processing</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>Processing many files concurrently</li>
<li>High queue depths</li>
<li>Resource contention between files</li>
</ul>
<p><strong>Tuning:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Global limits prevent oversubscription
let config = ResourceConfig {
    cpu_tokens: Some(7),   // Total across ALL files
    io_tokens: Some(12),   // Total across ALL files
    ..Default::default()
};

// Per-file semaphores limit individual file's concurrency
async fn process_file(path: &amp;Path) -&gt; Result&lt;()&gt; {
    let file_semaphore = Arc::new(Semaphore::new(4));  // Max 4 workers/file

    for chunk in chunks {
        let _global_cpu = RESOURCE_MANAGER.acquire_cpu().await?;  // Global limit
        let _local = file_semaphore.acquire().await?;             // Local limit

        // Process chunk...
    }

    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Two-level governance:</strong></p>
<ul>
<li><strong>Global</strong>: Prevents system oversubscription (7 CPU tokens total)</li>
<li><strong>Local</strong>: Prevents single file from monopolizing resources (4 workers/file)</li>
</ul>
<h2 id="performance-characteristics"><a class="header" href="#performance-characteristics">Performance Characteristics</a></h2>
<h3 id="resource-acquisition-overhead"><a class="header" href="#resource-acquisition-overhead">Resource Acquisition Overhead</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Time</th><th>Notes</th></tr></thead><tbody>
<tr><td>acquire_cpu() (fast path)</td><td>~100 ns</td><td>Token immediately available</td></tr>
<tr><td>acquire_cpu() (slow path)</td><td>~1-50 ms</td><td>Must wait for token</td></tr>
<tr><td>acquire_io() (fast path)</td><td>~100 ns</td><td>Token immediately available</td></tr>
<tr><td>allocate_memory() tracking</td><td>~10 ns</td><td>Atomic increment (Relaxed)</td></tr>
<tr><td>Metrics update</td><td>~50 ns</td><td>Atomic operations</td></tr>
</tbody></table>
</div>
<p><strong>Guidelines:</strong></p>
<ul>
<li>Fast path is negligible overhead</li>
<li>Slow path (waiting) creates backpressure (intentional)</li>
<li>Memory tracking is extremely low overhead</li>
</ul>
<h3 id="scalability"><a class="header" href="#scalability">Scalability</a></h3>
<p><strong>Linear scaling</strong> (ideal):</p>
<ul>
<li>CPU tokens = available cores</li>
<li>I/O tokens matched to device queue depth</li>
<li>Minimal waiting for resources</li>
</ul>
<p><strong>Sub-linear scaling</strong> (common with oversubscription):</p>
<ul>
<li>Too many CPU tokens (&gt; 2x cores)</li>
<li>Excessive context switching</li>
<li>Cache thrashing</li>
</ul>
<p><strong>Performance cliff</strong> (avoid):</p>
<ul>
<li>No resource limits</li>
<li>Uncontrolled parallelism</li>
<li>System thrashing (swapping, CPU saturation)</li>
</ul>
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<h3 id="1-always-acquire-resources-before-work"><a class="header" href="#1-always-acquire-resources-before-work">1. Always Acquire Resources Before Work</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Acquire global resource token before work
async fn process_chunk(chunk: FileChunk) -&gt; Result&lt;FileChunk&gt; {
    let _cpu_permit = RESOURCE_MANAGER.acquire_cpu().await?;

    tokio::task::spawn_blocking(move || {
        // CPU-intensive work
    }).await?
}

// ❌ Bad: No resource governance
async fn process_chunk(chunk: FileChunk) -&gt; Result&lt;FileChunk&gt; {
    tokio::task::spawn_blocking(move || {
        // Uncontrolled parallelism!
    }).await?
}
<span class="boring">}</span></code></pre></pre>
<h3 id="2-use-raii-for-automatic-release"><a class="header" href="#2-use-raii-for-automatic-release">2. Use RAII for Automatic Release</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: RAII guard auto-releases
let _permit = RESOURCE_MANAGER.acquire_cpu().await?;
// Work here...
// Permit released automatically when _permit goes out of scope

// ❌ Bad: Manual release (error-prone)
// Don't do this - no manual release API
<span class="boring">}</span></code></pre></pre>
<h3 id="3-monitor-saturation-regularly"><a class="header" href="#3-monitor-saturation-regularly">3. Monitor Saturation Regularly</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Periodic monitoring
tokio::spawn(async {
    let mut interval = tokio::time::interval(Duration::from_secs(10));
    loop {
        interval.tick().await;

        let cpu_sat = CONCURRENCY_METRICS.cpu_saturation_percent();
        let io_sat = CONCURRENCY_METRICS.io_saturation_percent();
        let mem_util = CONCURRENCY_METRICS.memory_utilization_percent();

        info!("Resources: CPU={:.1}%, I/O={:.1}%, Mem={:.1}%",
            cpu_sat, io_sat, mem_util);
    }
});
<span class="boring">}</span></code></pre></pre>
<h3 id="4-configure-based-on-workload-type"><a class="header" href="#4-configure-based-on-workload-type">4. Configure Based on Workload Type</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Workload-specific configuration
let config = if is_cpu_intensive {
    ResourceConfig {
        cpu_tokens: Some(available_cores),
        io_tokens: Some(8),  // Lower I/O concurrency
        ..Default::default()
    }
} else {
    ResourceConfig {
        cpu_tokens: Some(available_cores / 2),
        io_tokens: Some(24),  // Higher I/O concurrency
        ..Default::default()
    }
};
<span class="boring">}</span></code></pre></pre>
<h3 id="5-track-memory-with-guards"><a class="header" href="#5-track-memory-with-guards">5. Track Memory with Guards</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: RAII memory guard
pub struct ChunkBuffer {
    data: Vec&lt;u8&gt;,
    _memory_guard: MemoryGuard,
}

impl ChunkBuffer {
    pub fn new(size: usize) -&gt; Self {
        let data = vec![0u8; size];
        let _memory_guard = MemoryGuard::new(size);
        Self { data, _memory_guard }
    }
}
// Memory automatically tracked on allocation and freed on drop
<span class="boring">}</span></code></pre></pre>
<h2 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h2>
<h3 id="issue-1-high-cpu-wait-times"><a class="header" href="#issue-1-high-cpu-wait-times">Issue 1: High CPU Wait Times</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>P95 CPU wait time &gt; 50 ms</li>
<li>Low CPU saturation (&lt; 50%)</li>
<li>Many tasks waiting for CPU tokens</li>
</ul>
<p><strong>Causes:</strong></p>
<ul>
<li>Too few CPU tokens configured</li>
<li>CPU tokens not matching actual cores</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Check current configuration
println!("CPU tokens: {}", RESOURCE_MANAGER.cpu_tokens_total());
println!("CPU saturation: {:.1}%", CONCURRENCY_METRICS.cpu_saturation_percent());
println!("CPU wait P95: {} ms", CONCURRENCY_METRICS.cpu_wait_p95());

// Increase CPU tokens
let config = ResourceConfig {
    cpu_tokens: Some(available_cores),  // Increase from cores-1
    ..Default::default()
};
init_resource_manager(config)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-2-io-queue-saturation"><a class="header" href="#issue-2-io-queue-saturation">Issue 2: I/O Queue Saturation</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>I/O saturation &gt; 90%</li>
<li>High I/O wait times (&gt; 100 ms P95)</li>
<li><code>iostat</code> shows high <code>avgqu-sz</code></li>
</ul>
<p><strong>Causes:</strong></p>
<ul>
<li>Too many I/O tokens for storage device</li>
<li>Storage device can't handle queue depth</li>
<li>Sequential I/O on HDD with high concurrency</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Reduce I/O tokens for HDD
let config = ResourceConfig {
    storage_type: StorageType::HDD,  // Sets I/O tokens = 4
    ..Default::default()
};

// Or manually configure
let config = ResourceConfig {
    io_tokens: Some(4),  // Lower concurrency
    ..Default::default()
};
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-3-memory-pressure"><a class="header" href="#issue-3-memory-pressure">Issue 3: Memory Pressure</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>Memory utilization &gt; 80%</li>
<li>Swapping (check <code>vmstat</code>)</li>
<li>OOM killer activated</li>
</ul>
<p><strong>Causes:</strong></p>
<ul>
<li>Too many concurrent chunks allocated</li>
<li>Large chunk size × high worker count</li>
<li>Memory leaks (not tracked properly)</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Reduce memory usage
let chunk_size = ChunkSize::new(16 * 1024 * 1024);  // Smaller chunks

let config = ResourceConfig {
    cpu_tokens: Some(3),  // Fewer concurrent chunks
    ..Default::default()
};

// Monitor memory closely
let mem_mb = CONCURRENCY_METRICS.memory_used_mb();
let mem_pct = CONCURRENCY_METRICS.memory_utilization_percent();

if mem_pct &gt; 80.0 {
    error!("⚠️  High memory usage: {:.1}% ({:.2} MB)", mem_pct, mem_mb);
}
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-4-high-queue-depth"><a class="header" href="#issue-4-high-queue-depth">Issue 4: High Queue Depth</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>CPU queue depth &gt; 100</li>
<li>High queue wait times (&gt; 50 ms P95)</li>
<li>Reader blocked waiting for workers</li>
</ul>
<p><strong>Causes:</strong></p>
<ul>
<li>Workers can't keep up with reader</li>
<li>Slow compression/encryption stages</li>
<li>Insufficient worker count</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Check queue metrics
let depth = CONCURRENCY_METRICS.cpu_queue_depth();
let max_depth = CONCURRENCY_METRICS.cpu_queue_depth_max();
let wait_p95 = CONCURRENCY_METRICS.cpu_queue_wait_p95();

println!("Queue depth: {} (max: {})", depth, max_depth);
println!("Queue wait P95: {} ms", wait_p95);

// Increase workers to drain queue faster
let config = ResourceConfig {
    cpu_tokens: Some(available_cores),  // More workers
    ..Default::default()
};

// Or optimize stages (faster compression, fewer stages)
<span class="boring">}</span></code></pre></pre>
<h2 id="related-topics"><a class="header" href="#related-topics">Related Topics</a></h2>
<ul>
<li>See <a href="thread-pooling.html">Thread Pooling</a> for worker pool configuration</li>
<li>See <a href="concurrency.html">Concurrency Model</a> for async/await patterns</li>
<li>See <a href="../advanced/performance.html">Performance Optimization</a> for benchmarking</li>
<li>See <a href="../advanced/profiling.html">Profiling</a> for performance analysis</li>
</ul>
<h2 id="summary"><a class="header" href="#summary">Summary</a></h2>
<p>The pipeline's resource management system provides:</p>
<ol>
<li><strong>CPU Token Management</strong>: Prevent CPU oversubscription with semaphore-based limits</li>
<li><strong>I/O Token Management</strong>: Device-specific I/O queue depth optimization</li>
<li><strong>Memory Tracking</strong>: Monitor memory usage with atomic counters (gauge only)</li>
<li><strong>Concurrency Metrics</strong>: Comprehensive observability (gauges, counters, histograms)</li>
<li><strong>Two-Level Governance</strong>: Global + local limits prevent system saturation</li>
</ol>
<p><strong>Key Takeaways:</strong></p>
<ul>
<li>Always acquire resource tokens before CPU/I/O work</li>
<li>Use RAII guards for automatic resource release</li>
<li>Monitor saturation percentages and wait time percentiles</li>
<li>Configure based on workload type (CPU-intensive vs I/O-intensive)</li>
<li>Start with defaults, tune based on actual measurements</li>
<li>Memory tracking is informational only (no enforcement yet)</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../advanced/thread-pooling.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../advanced/performance.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../advanced/thread-pooling.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../advanced/performance.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
