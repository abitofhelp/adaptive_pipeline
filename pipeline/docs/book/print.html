<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Pipeline Developer Guide</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Pipeline Developer Guide</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="pipeline-developer-guide"><a class="header" href="#pipeline-developer-guide">Pipeline Developer Guide</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<h2 id="welcome"><a class="header" href="#welcome">Welcome</a></h2>
<p>This is the comprehensive technical guide for the Optimized Adaptive Pipeline. Whether you're learning advanced Rust patterns, contributing to the project, or using the pipeline in production, this guide provides the depth you need.</p>
<h2 id="how-to-use-this-guide"><a class="header" href="#how-to-use-this-guide">How to Use This Guide</a></h2>
<p>This guide follows a <strong>progressive disclosure</strong> approach - each section builds on previous ones:</p>
<h3 id="start-here-fundamentals"><a class="header" href="#start-here-fundamentals">Start Here: Fundamentals</a></h3>
<p>If you're new to the pipeline, start with <strong>Fundamentals</strong>. This section introduces core concepts in an accessible way:</p>
<ul>
<li>What pipelines do and why they're useful</li>
<li>Key terminology and concepts</li>
<li>How stages work together</li>
<li>Basic configuration</li>
<li>Running your first pipeline</li>
</ul>
<p><strong>Time commitment:</strong> 30-45 minutes</p>
<h3 id="building-understanding-architecture"><a class="header" href="#building-understanding-architecture">Building Understanding: Architecture</a></h3>
<p>Once you understand the basics, explore the <strong>Architecture</strong> section. This explains <em>how</em> the pipeline is designed:</p>
<ul>
<li>Layered architecture (Domain, Application, Infrastructure)</li>
<li>Domain-Driven Design concepts</li>
<li>Design patterns in use (Repository, Service, Adapter, Observer)</li>
<li>Dependency management</li>
</ul>
<p>This section bridges the gap between basic usage and implementation details.</p>
<p><strong>Time commitment:</strong> 1-2 hours</p>
<h3 id="going-deeper-implementation"><a class="header" href="#going-deeper-implementation">Going Deeper: Implementation</a></h3>
<p>The <strong>Implementation</strong> section covers how specific features work:</p>
<ul>
<li>Stage processing details</li>
<li>Compression and encryption</li>
<li>Data persistence and schema management</li>
<li>File I/O and chunking</li>
<li>Metrics and observability</li>
</ul>
<p>Perfect for contributors or those adapting the pipeline for specific needs.</p>
<p><strong>Time commitment:</strong> 2-3 hours</p>
<h3 id="expert-level-advanced-topics"><a class="header" href="#expert-level-advanced-topics">Expert Level: Advanced Topics</a></h3>
<p>For optimization and extension, the <strong>Advanced Topics</strong> section covers:</p>
<ul>
<li>Concurrency model and thread pooling</li>
<li>Performance optimization techniques</li>
<li>Creating custom stages and algorithms</li>
</ul>
<p><strong>Time commitment:</strong> 2-4 hours depending on depth</p>
<h3 id="reference-formal-documentation"><a class="header" href="#reference-formal-documentation">Reference: Formal Documentation</a></h3>
<p>The <strong>Formal Documentation</strong> section contains:</p>
<ul>
<li>Software Requirements Specification (SRS)</li>
<li>Software Design Document (SDD)</li>
<li>Test Strategy (STP)</li>
</ul>
<p>These are comprehensive reference documents.</p>
<h2 id="documentation-scope"><a class="header" href="#documentation-scope">Documentation Scope</a></h2>
<p>Following our <strong>"reasonable" principle</strong>, this guide focuses on:</p>
<p>✅ <strong>What you need to know</strong> to use, contribute to, or extend the pipeline
✅ <strong>Why decisions were made</strong> with just enough context
✅ <strong>How to accomplish tasks</strong> with practical examples
✅ <strong>Advanced Rust patterns</strong> demonstrated in real code</p>
<p>We intentionally <strong>do not</strong> include:</p>
<p>❌ Rust language tutorials (see <a href="https://doc.rust-lang.org/book/">The Rust Book</a>)
❌ General programming concepts
❌ Third-party library documentation (links provided instead)
❌ Exhaustive algorithm details (high-level explanations with references)</p>
<h2 id="learning-path-recommendations"><a class="header" href="#learning-path-recommendations">Learning Path Recommendations</a></h2>
<h3 id="i-want-to-use-the-pipeline"><a class="header" href="#i-want-to-use-the-pipeline">I want to use the pipeline</a></h3>
<p>→ Read <a href="fundamentals/what-is-a-pipeline.html">Fundamentals</a>
→ Skip to <a href="implementation/stages.html">Implementation</a> for specific features</p>
<h3 id="i-want-to-contribute"><a class="header" href="#i-want-to-contribute">I want to contribute</a></h3>
<p>→ Read Fundamentals + Architecture (full sections)
→ Review relevant Implementation chapters
→ Check <a href="../../docs/book/contributing/guidelines.html">Contributing Guide</a></p>
<h3 id="i-want-to-learn-advanced-rust-patterns"><a class="header" href="#i-want-to-learn-advanced-rust-patterns">I want to learn advanced Rust patterns</a></h3>
<p>→ Focus on Architecture section (patterns)
→ Review Implementation for real-world examples
→ Study Advanced Topics for concurrency/performance</p>
<h3 id="im-building-something-similar"><a class="header" href="#im-building-something-similar">I'm building something similar</a></h3>
<p>→ Read Architecture + Implementation
→ Study formal documentation (SRS/SDD)
→ Review source code with this guide as reference</p>
<h2 id="conventions-used"><a class="header" href="#conventions-used">Conventions Used</a></h2>
<p>Throughout this guide:</p>
<ul>
<li><strong>Code examples</strong> are complete and runnable unless marked otherwise</li>
<li><strong>File paths</strong> use format <code>module/file.rs:line</code> for source references</li>
<li><strong>Diagrams</strong> are in PlantUML (SVG rendered in book)</li>
<li><strong>Callouts</strong> highlight important information:</li>
</ul>
<blockquote>
<p><strong>Note:</strong> Additional helpful information</p>
</blockquote>
<blockquote>
<p><strong>Warning:</strong> Important caveats or gotchas</p>
</blockquote>
<blockquote>
<p><strong>Example:</strong> Practical code demonstration</p>
</blockquote>
<h2 id="ready-to-start"><a class="header" href="#ready-to-start">Ready to Start?</a></h2>
<p>Choose your path:</p>
<ul>
<li><strong>New users:</strong> <a href="fundamentals/what-is-a-pipeline.html">What is a Pipeline?</a></li>
<li><strong>Contributors:</strong> <a href="architecture/overview.html">Architecture Overview</a></li>
<li><strong>Specific feature:</strong> Use search (press 's') or browse <a href="introduction.html#">table of contents</a></li>
</ul>
<p>Let's dive in!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="what-is-a-pipeline"><a class="header" href="#what-is-a-pipeline">What is a Pipeline?</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Introduction to pipelines and their purpose.</p>
<h2 id="what-is-a-pipeline-1"><a class="header" href="#what-is-a-pipeline-1">What is a Pipeline?</a></h2>
<p>A <strong>pipeline</strong> is a series of connected processing stages that transform data from input to output. Each stage performs a specific operation, and data flows through the stages sequentially or in parallel.</p>
<p>Think of it like a factory assembly line:</p>
<ul>
<li>Raw materials (input file) enter at one end</li>
<li>Each station (stage) performs a specific task</li>
<li>The finished product (processed file) exits at the other end</li>
</ul>
<h2 id="real-world-analogy"><a class="header" href="#real-world-analogy">Real-World Analogy</a></h2>
<h3 id="assembly-line"><a class="header" href="#assembly-line">Assembly Line</a></h3>
<p>Imagine an automobile assembly line:</p>
<pre><code>Raw Materials → Welding → Painting → Assembly → Quality Check → Finished Car
</code></pre>
<p>In our pipeline system:</p>
<pre><code>Input File → Compression → Encryption → Validation → Output File
</code></pre>
<p>Each stage:</p>
<ul>
<li>Receives data from the previous stage</li>
<li>Performs its specific transformation</li>
<li>Passes the result to the next stage</li>
</ul>
<h2 id="why-use-a-pipeline"><a class="header" href="#why-use-a-pipeline">Why Use a Pipeline?</a></h2>
<h3 id="modularity"><a class="header" href="#modularity">Modularity</a></h3>
<p>Each stage does one thing well. You can:</p>
<ul>
<li>Add new stages easily</li>
<li>Remove stages you don't need</li>
<li>Reorder stages as needed</li>
</ul>
<p><strong>Example</strong>: Need encryption? Add an encryption stage. Don't need compression? Remove the compression stage.</p>
<h3 id="reusability"><a class="header" href="#reusability">Reusability</a></h3>
<p>Stages can be used in multiple pipelines:</p>
<ul>
<li>Use the same compression stage in different workflows</li>
<li>Share validation logic across projects</li>
<li>Build libraries of reusable components</li>
</ul>
<h3 id="testability"><a class="header" href="#testability">Testability</a></h3>
<p>Each stage can be tested independently:</p>
<ul>
<li>Unit test individual stages</li>
<li>Mock stage inputs/outputs</li>
<li>Verify stage behavior in isolation</li>
</ul>
<h3 id="scalability"><a class="header" href="#scalability">Scalability</a></h3>
<p>Pipelines can process data efficiently:</p>
<ul>
<li>Process file chunks in parallel</li>
<li>Distribute work across CPU cores</li>
<li>Handle files of any size</li>
</ul>
<h2 id="our-pipeline-system"><a class="header" href="#our-pipeline-system">Our Pipeline System</a></h2>
<p>The Optimized Adaptive Pipeline provides:</p>
<p><strong>File Processing</strong>: Transform files through configurable stages</p>
<ul>
<li>Input: Any file type</li>
<li>Stages: Compression, encryption, validation</li>
<li>Output: Processed <code>.adapipe</code> file</li>
<li>Memory-mapped files for efficient processing of huge files</li>
</ul>
<p><strong>Flexibility</strong>: Configure stages for your needs</p>
<ul>
<li>Enable/disable stages</li>
<li>Choose algorithms (Brotli, LZ4, Zstandard for compression)</li>
<li>Set security levels (Public → Top Secret)</li>
</ul>
<p><strong>Performance</strong>: Handle large files efficiently</p>
<ul>
<li>Stream processing (low memory usage)</li>
<li>Parallel chunk processing</li>
<li>Optimized algorithms</li>
</ul>
<p><strong>Security</strong>: Protect sensitive data</p>
<ul>
<li>AES-256-GCM encryption</li>
<li>Argon2 key derivation</li>
<li>Integrity verification with checksums</li>
</ul>
<h2 id="pipeline-flow"><a class="header" href="#pipeline-flow">Pipeline Flow</a></h2>
<p>Here's how data flows through the pipeline:</p>
<p><img src="fundamentals/../diagrams/pipeline-flow.svg" alt="Pipeline Flow" /></p>
<ol>
<li><strong>Input</strong>: Read file from disk</li>
<li><strong>Chunk</strong>: Split into manageable pieces (default 1MB)</li>
<li><strong>Process</strong>: Apply stages to each chunk
<ul>
<li>Compress (optional)</li>
<li>Encrypt (optional)</li>
<li>Calculate checksum (always)</li>
</ul>
</li>
<li><strong>Store</strong>: Write processed data and metadata</li>
<li><strong>Verify</strong>: Confirm integrity of output</li>
</ol>
<h2 id="what-you-can-do"><a class="header" href="#what-you-can-do">What You Can Do</a></h2>
<p>With this pipeline, you can:</p>
<p>✅ <strong>Compress files</strong> to save storage space
✅ <strong>Encrypt files</strong> to protect sensitive data
✅ <strong>Validate integrity</strong> to detect corruption
✅ <strong>Process large files</strong> without running out of memory
✅ <strong>Customize workflows</strong> with configurable stages
✅ <strong>Track metrics</strong> to monitor performance</p>
<h2 id="next-steps"><a class="header" href="#next-steps">Next Steps</a></h2>
<p>Continue to:</p>
<ul>
<li><a href="fundamentals/core-concepts.html">Core Concepts</a> - Key terminology and ideas</li>
<li><a href="fundamentals/stages.html">Pipeline Stages</a> - Understanding stage types</li>
<li><a href="fundamentals/configuration.html">Configuration Basics</a> - How to configure pipelines</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="core-concepts"><a class="header" href="#core-concepts">Core Concepts</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Essential concepts for understanding the pipeline.</p>
<h2 id="key-terminology"><a class="header" href="#key-terminology">Key Terminology</a></h2>
<h3 id="pipeline"><a class="header" href="#pipeline">Pipeline</a></h3>
<p>A complete file processing workflow with:</p>
<ul>
<li><strong>Unique ID</strong>: Every pipeline has a ULID identifier</li>
<li><strong>Input path</strong>: Source file to process</li>
<li><strong>Output path</strong>: Destination for processed data</li>
<li><strong>Stages</strong>: Ordered list of processing steps</li>
<li><strong>Status</strong>: Created → Running → Completed (or Failed)</li>
</ul>
<h3 id="stage"><a class="header" href="#stage">Stage</a></h3>
<p>An individual processing operation within a pipeline:</p>
<ul>
<li><strong>Type</strong>: Compression, Encryption, or Integrity Check</li>
<li><strong>Algorithm</strong>: Specific implementation (e.g., Brotli, AES-256-GCM)</li>
<li><strong>Sequence</strong>: Order in the pipeline (1, 2, 3, ...)</li>
<li><strong>Configuration</strong>: Stage-specific settings</li>
</ul>
<h3 id="file-chunk"><a class="header" href="#file-chunk">File Chunk</a></h3>
<p>A portion of a file processed independently:</p>
<ul>
<li><strong>Size</strong>: Configurable (default 1MB)</li>
<li><strong>Sequence</strong>: Chunk number (0, 1, 2, ...)</li>
<li><strong>Checksum</strong>: Integrity verification value</li>
<li><strong>Offset</strong>: Position in original file</li>
</ul>
<h2 id="core-components"><a class="header" href="#core-components">Core Components</a></h2>
<h3 id="entities"><a class="header" href="#entities">Entities</a></h3>
<p><strong>Pipeline Entity</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Pipeline {
    id: PipelineId,
    input_file_path: FilePath,
    output_file_path: FilePath,
    stages: Vec&lt;PipelineStage&gt;,
    status: PipelineStatus,
    created_at: DateTime,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>PipelineStage Entity</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>PipelineStage {
    id: StageId,
    pipeline_id: PipelineId,
    stage_type: StageType,
    algorithm: Algorithm,
    sequence_number: u32,
}
<span class="boring">}</span></code></pre></pre>
<h3 id="value-objects"><a class="header" href="#value-objects">Value Objects</a></h3>
<p><strong>FilePath</strong> - Validated file system path</p>
<ul>
<li>Must exist (for input) or be writable (for output)</li>
<li>Supports absolute and relative paths</li>
<li>Cross-platform compatibility</li>
</ul>
<p><strong>FileSize</strong> - File size in bytes</p>
<ul>
<li>Human-readable display (KB, MB, GB)</li>
<li>Validation for reasonable limits</li>
<li>Efficient storage representation</li>
</ul>
<p><strong>Algorithm</strong> - Processing algorithm specification</p>
<ul>
<li>Compression: Brotli, LZ4, Zstandard</li>
<li>Encryption: AES-256-GCM, ChaCha20-Poly1305</li>
<li>Checksum: Blake3, SHA-256</li>
</ul>
<h2 id="data-flow"><a class="header" href="#data-flow">Data Flow</a></h2>
<h3 id="sequential-processing"><a class="header" href="#sequential-processing">Sequential Processing</a></h3>
<p>Stages execute in order:</p>
<pre><code>Input → Stage 1 → Stage 2 → Stage 3 → Output
</code></pre>
<h3 id="parallel-chunk-processing"><a class="header" href="#parallel-chunk-processing">Parallel Chunk Processing</a></h3>
<p>Chunks process independently:</p>
<pre><code>Chunk 0 ──┐
Chunk 1 ──┼→ All go through stages → Reassemble
Chunk 2 ──┘
</code></pre>
<p>This enables:</p>
<ul>
<li><strong>Concurrency</strong>: Multiple chunks processed simultaneously</li>
<li><strong>Memory efficiency</strong>: Only active chunks in memory</li>
<li><strong>Scalability</strong>: Leverage multiple CPU cores</li>
</ul>
<h3 id="pipeline-execution-sequence"><a class="header" href="#pipeline-execution-sequence">Pipeline Execution Sequence</a></h3>
<p><img src="fundamentals/../diagrams/stage-execution.svg" alt="Stage Execution" /></p>
<ol>
<li><strong>CLI</strong> receives command</li>
<li><strong>Pipeline Service</strong> creates pipeline</li>
<li><strong>File Processor</strong> reads input file</li>
<li>For each chunk:
<ul>
<li>Apply compression (if enabled)</li>
<li>Apply encryption (if enabled)</li>
<li>Calculate checksum (always)</li>
<li>Store chunk metadata</li>
<li>Write processed chunk</li>
</ul>
</li>
<li>Update pipeline status</li>
<li>Return result to user</li>
</ol>
<h2 id="domain-model"><a class="header" href="#domain-model">Domain Model</a></h2>
<p>Our domain model follows Domain-Driven Design principles:</p>
<p><img src="fundamentals/../diagrams/domain-model.svg" alt="Domain Model" /></p>
<h3 id="aggregates"><a class="header" href="#aggregates">Aggregates</a></h3>
<p><strong>Pipeline Aggregate</strong> - The root entity</p>
<ul>
<li>Contains Pipeline entity</li>
<li>Manages associated FileChunks</li>
<li>Enforces business rules</li>
<li>Ensures consistency</li>
</ul>
<h3 id="relationships"><a class="header" href="#relationships">Relationships</a></h3>
<ul>
<li>Pipeline <strong>has many</strong> PipelineStages (1:N)</li>
<li>Pipeline <strong>processes</strong> FileChunks (1:N)</li>
<li>FileChunk <strong>belongs to</strong> Pipeline (N:1)</li>
<li>PipelineStage <strong>uses</strong> Algorithm (N:1)</li>
</ul>
<h2 id="processing-guarantees"><a class="header" href="#processing-guarantees">Processing Guarantees</a></h2>
<h3 id="integrity"><a class="header" href="#integrity">Integrity</a></h3>
<p>Every chunk has a checksum:</p>
<ul>
<li>Calculated after processing</li>
<li>Verified on read/restore</li>
<li>Detects any corruption</li>
</ul>
<h3 id="atomicity"><a class="header" href="#atomicity">Atomicity</a></h3>
<p>Pipeline operations are transactional:</p>
<ul>
<li>All stages complete, or none do</li>
<li>Metadata stored consistently</li>
<li>No partial outputs on failure</li>
</ul>
<h3 id="durability"><a class="header" href="#durability">Durability</a></h3>
<p>Processed data is persisted:</p>
<ul>
<li>SQLite database for metadata</li>
<li>File system for binary data</li>
<li>Recoverable after crashes</li>
</ul>
<h2 id="next-steps-1"><a class="header" href="#next-steps-1">Next Steps</a></h2>
<p>Continue to:</p>
<ul>
<li><a href="fundamentals/stages.html">Pipeline Stages</a> - Types of stages available</li>
<li><a href="fundamentals/configuration.html">Configuration Basics</a> - How to configure pipelines</li>
<li><a href="fundamentals/first-run.html">Running Your First Pipeline</a> - Hands-on tutorial</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pipeline-stages"><a class="header" href="#pipeline-stages">Pipeline Stages</a></h1>
<p><strong>Version:</strong> 1.0
<strong>Date:</strong> 2025-01-04
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner, Claude Code
<strong>Status:</strong> Active</p>
<h2 id="what-is-a-stage"><a class="header" href="#what-is-a-stage">What is a Stage?</a></h2>
<p>A <strong>pipeline stage</strong> is a single processing operation that transforms data in a specific way. Each stage performs one well-defined task, like compressing data, encrypting it, or verifying its integrity.</p>
<p>Think of stages like workstations on an assembly line. Each workstation has specialized tools and performs one specific operation. The product moves from one workstation to the next until it's complete.</p>
<h2 id="stage-types"><a class="header" href="#stage-types">Stage Types</a></h2>
<p>Our pipeline supports three main categories of stages:</p>
<h3 id="1-compression-stages"><a class="header" href="#1-compression-stages">1. Compression Stages</a></h3>
<p>Compression stages reduce the size of your data. This is useful for:</p>
<ul>
<li>Saving disk space</li>
<li>Reducing network bandwidth</li>
<li>Faster file transfers</li>
<li>Lower storage costs</li>
</ul>
<p><strong>Available Compression Algorithms:</strong></p>
<ul>
<li>
<p><strong>Brotli</strong> - Best compression ratio, slower speed</p>
<ul>
<li>Best for: Text files, web content, logs</li>
<li>Performance: Excellent compression, moderate speed</li>
<li>Memory: Higher memory usage</li>
</ul>
</li>
<li>
<p><strong>Gzip</strong> - General-purpose compression</p>
<ul>
<li>Best for: General files, wide compatibility</li>
<li>Performance: Good balance of speed and ratio</li>
<li>Memory: Moderate memory usage</li>
</ul>
</li>
<li>
<p><strong>Zstandard (zstd)</strong> - Modern, fast compression</p>
<ul>
<li>Best for: Large files, real-time compression</li>
<li>Performance: Excellent speed and ratio</li>
<li>Memory: Efficient memory usage</li>
</ul>
</li>
<li>
<p><strong>LZ4</strong> - Extremely fast compression</p>
<ul>
<li>Best for: Real-time applications, live data streams</li>
<li>Performance: Fastest compression, moderate ratio</li>
<li>Memory: Low memory usage</li>
</ul>
</li>
</ul>
<h3 id="2-encryption-stages"><a class="header" href="#2-encryption-stages">2. Encryption Stages</a></h3>
<p>Encryption stages protect your data by making it unreadable without the correct key. This is essential for:</p>
<ul>
<li>Protecting sensitive information</li>
<li>Compliance with security regulations</li>
<li>Secure data transmission</li>
<li>Privacy protection</li>
</ul>
<p><strong>Available Encryption Algorithms:</strong></p>
<ul>
<li>
<p><strong>AES-256-GCM</strong> - Industry standard encryption</p>
<ul>
<li>Key Size: 256 bits (32 bytes)</li>
<li>Security: FIPS approved, very strong</li>
<li>Performance: Excellent with AES-NI hardware support</li>
<li>Authentication: Built-in integrity verification</li>
</ul>
</li>
<li>
<p><strong>ChaCha20-Poly1305</strong> - Modern stream cipher</p>
<ul>
<li>Key Size: 256 bits (32 bytes)</li>
<li>Security: Strong, constant-time implementation</li>
<li>Performance: Consistent across all platforms</li>
<li>Authentication: Built-in integrity verification</li>
</ul>
</li>
<li>
<p><strong>AES-128-GCM</strong> - Faster AES variant</p>
<ul>
<li>Key Size: 128 bits (16 bytes)</li>
<li>Security: Still very secure, slightly faster</li>
<li>Performance: Faster than AES-256</li>
<li>Authentication: Built-in integrity verification</li>
</ul>
</li>
</ul>
<h3 id="3-integrity-verification-stages"><a class="header" href="#3-integrity-verification-stages">3. Integrity Verification Stages</a></h3>
<p>Integrity stages ensure your data hasn't been corrupted or tampered with. They create a unique "fingerprint" of your data called a checksum or hash.</p>
<p><strong>Available Hashing Algorithms:</strong></p>
<ul>
<li>
<p><strong>SHA-256</strong> - Industry standard hashing</p>
<ul>
<li>Output: 256 bits (32 bytes)</li>
<li>Security: Cryptographically secure</li>
<li>Performance: Good balance</li>
<li>Use Case: General integrity verification</li>
</ul>
</li>
<li>
<p><strong>SHA-512</strong> - Stronger SHA variant</p>
<ul>
<li>Output: 512 bits (64 bytes)</li>
<li>Security: Stronger than SHA-256</li>
<li>Performance: Good on 64-bit systems</li>
<li>Use Case: High-security applications</li>
</ul>
</li>
<li>
<p><strong>BLAKE3</strong> - Modern, high-performance hashing</p>
<ul>
<li>Output: 256 bits (32 bytes)</li>
<li>Security: Strong security properties</li>
<li>Performance: Very fast</li>
<li>Use Case: High-performance applications</li>
</ul>
</li>
</ul>
<h2 id="stage-configuration"><a class="header" href="#stage-configuration">Stage Configuration</a></h2>
<p>Each stage has a configuration that specifies how it should process data:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::entities::{PipelineStage, StageType, StageConfiguration};
use std::collections::HashMap;

// Example: Compression stage
let compression_stage = PipelineStage::new(
    "compress".to_string(),
    StageType::Compression,
    StageConfiguration::new(
        "zstd".to_string(),  // algorithm name
        HashMap::new(),      // parameters
        false,               // parallel processing
    ),
    0, // stage order
)?;

// Example: Encryption stage
let encryption_stage = PipelineStage::new(
    "encrypt".to_string(),
    StageType::Encryption,
    StageConfiguration::new(
        "aes256gcm".to_string(),
        HashMap::new(),
        false,
    ),
    1, // stage order
)?;

// Example: Integrity verification stage
let integrity_stage = PipelineStage::new(
    "verify".to_string(),
    StageType::Checksum,
    StageConfiguration::new(
        "sha256".to_string(),
        HashMap::new(),
        false,
    ),
    2, // stage order
)?;
<span class="boring">}</span></code></pre></pre>
<h2 id="stage-execution-order"><a class="header" href="#stage-execution-order">Stage Execution Order</a></h2>
<p>Stages execute in the order you define them. The output of one stage becomes the input to the next stage.</p>
<p><strong>Recommended Order for Processing:</strong></p>
<ol>
<li>Compress (reduce size first)</li>
<li>Encrypt (protect compressed data)</li>
<li>Verify integrity (create checksum of encrypted data)</li>
</ol>
<p><strong>For Restoration (reverse order):</strong></p>
<ol>
<li>Verify integrity (check encrypted data)</li>
<li>Decrypt (recover compressed data)</li>
<li>Decompress (restore original file)</li>
</ol>
<pre><code class="language-text">Processing Pipeline:
Input File → Compress → Encrypt → Verify → Output File

Restoration Pipeline:
Input File → Verify → Decrypt → Decompress → Output File
</code></pre>
<h2 id="combining-stages"><a class="header" href="#combining-stages">Combining Stages</a></h2>
<p>You can combine stages in different ways depending on your needs:</p>
<h3 id="maximum-security"><a class="header" href="#maximum-security">Maximum Security</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>vec![
    PipelineStage::new(
        "compress".to_string(),
        StageType::Compression,
        StageConfiguration::new("brotli".to_string(), HashMap::new(), false),
        0,
    )?,
    PipelineStage::new(
        "encrypt".to_string(),
        StageType::Encryption,
        StageConfiguration::new("aes256gcm".to_string(), HashMap::new(), false),
        1,
    )?,
    PipelineStage::new(
        "verify".to_string(),
        StageType::Checksum,
        StageConfiguration::new("blake3".to_string(), HashMap::new(), false),
        2,
    )?,
]
<span class="boring">}</span></code></pre></pre>
<h3 id="maximum-speed"><a class="header" href="#maximum-speed">Maximum Speed</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>vec![
    PipelineStage::new(
        "compress".to_string(),
        StageType::Compression,
        StageConfiguration::new("lz4".to_string(), HashMap::new(), false),
        0,
    )?,
    PipelineStage::new(
        "encrypt".to_string(),
        StageType::Encryption,
        StageConfiguration::new("chacha20poly1305".to_string(), HashMap::new(), false),
        1,
    )?,
]
<span class="boring">}</span></code></pre></pre>
<h3 id="balanced-approach"><a class="header" href="#balanced-approach">Balanced Approach</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>vec![
    PipelineStage::new(
        "compress".to_string(),
        StageType::Compression,
        StageConfiguration::new("zstd".to_string(), HashMap::new(), false),
        0,
    )?,
    PipelineStage::new(
        "encrypt".to_string(),
        StageType::Encryption,
        StageConfiguration::new("aes256gcm".to_string(), HashMap::new(), false),
        1,
    )?,
    PipelineStage::new(
        "verify".to_string(),
        StageType::Checksum,
        StageConfiguration::new("sha256".to_string(), HashMap::new(), false),
        2,
    )?,
]
<span class="boring">}</span></code></pre></pre>
<h2 id="parallel-processing"><a class="header" href="#parallel-processing">Parallel Processing</a></h2>
<p>Stages process file chunks in parallel for better performance:</p>
<pre><code class="language-text">File Split into Chunks:
┌──────┬──────┬──────┬──────┐
│Chunk1│Chunk2│Chunk3│Chunk4│
└──┬───┴──┬───┴──┬───┴──┬───┘
   │      │      │      │
   ▼      ▼      ▼      ▼
   ┌──────┬──────┬──────┬──────┐
   │Stage1│Stage1│Stage1│Stage1│ (Parallel)
   └──┬───┴──┬───┴──┬───┴──┬───┘
      ▼      ▼      ▼      ▼
   ┌──────┬──────┬──────┬──────┐
   │Stage2│Stage2│Stage2│Stage2│ (Parallel)
   └──┬───┴──┬───┴──┬───┴──┬───┘
      │      │      │      │
      ▼      ▼      ▼      ▼
   Combined Output File
</code></pre>
<p>This parallel processing allows the pipeline to utilize multiple CPU cores for faster throughput.</p>
<h2 id="stage-validation"><a class="header" href="#stage-validation">Stage Validation</a></h2>
<p>The pipeline validates stages at creation time:</p>
<ul>
<li><strong>Algorithm compatibility</strong>: Ensures compression algorithms are only used in compression stages</li>
<li><strong>Stage order</strong>: Verifies stages have unique, sequential order numbers</li>
<li><strong>Configuration validity</strong>: Checks all stage parameters are valid</li>
<li><strong>Dependency checks</strong>: Ensures restoration pipelines match processing pipelines</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// This will fail - wrong algorithm for stage type
PipelineStage::new(
    "compress".to_string(),
    StageType::Compression,
    StageConfiguration::new(
        "aes256gcm".to_string(), // ❌ Encryption algorithm in compression stage!
        HashMap::new(),
        false,
    ),
    0,
) // ❌ Error: Algorithm not compatible with stage type
<span class="boring">}</span></code></pre></pre>
<h2 id="extending-with-custom-stages"><a class="header" href="#extending-with-custom-stages">Extending with Custom Stages</a></h2>
<p>The pipeline can be easily extended through custom stages to meet your specific requirements. You can create custom stages that implement your own processing logic, integrate third-party tools, or add specialized transformations.</p>
<p>For detailed information on implementing custom stages, see <a href="fundamentals/../advanced/custom-stages.html">Custom Stages</a> in the Advanced Topics section.</p>
<h2 id="next-steps-2"><a class="header" href="#next-steps-2">Next Steps</a></h2>
<p>Now that you understand pipeline stages, you can learn about:</p>
<ul>
<li><a href="fundamentals/configuration.html">Configuration</a> - How to configure pipelines and stages</li>
<li><a href="fundamentals/first-run.html">Your First Pipeline</a> - Run your first pipeline</li>
<li><a href="fundamentals/../architecture/overview.html">Architecture Overview</a> - Deeper dive into the architecture</li>
<li><a href="fundamentals/../advanced/custom-stages.html">Custom Stages</a> - Create your own custom stage implementations</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="configuration-basics"><a class="header" href="#configuration-basics">Configuration Basics</a></h1>
<p><strong>Version:</strong> 1.0
<strong>Date:</strong> 2025-01-04
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner, Claude Code
<strong>Status:</strong> Active</p>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>The pipeline system provides flexible configuration through command-line options, environment variables, and configuration files. This chapter covers the basics of configuring your pipelines.</p>
<h2 id="command-line-interface"><a class="header" href="#command-line-interface">Command-Line Interface</a></h2>
<p>The pipeline CLI provides several commands for managing and running pipelines.</p>
<h3 id="basic-commands"><a class="header" href="#basic-commands">Basic Commands</a></h3>
<h4 id="process-a-file"><a class="header" href="#process-a-file">Process a File</a></h4>
<pre><code class="language-bash">pipeline process \
  --input /path/to/input.txt \
  --output /path/to/output.bin \
  --pipeline my-pipeline
</code></pre>
<h4 id="create-a-pipeline"><a class="header" href="#create-a-pipeline">Create a Pipeline</a></h4>
<pre><code class="language-bash">pipeline create \
  --name my-pipeline \
  --stages compression,encryption,integrity
</code></pre>
<h4 id="list-pipelines"><a class="header" href="#list-pipelines">List Pipelines</a></h4>
<pre><code class="language-bash">pipeline list
</code></pre>
<h4 id="show-pipeline-details"><a class="header" href="#show-pipeline-details">Show Pipeline Details</a></h4>
<pre><code class="language-bash">pipeline show my-pipeline
</code></pre>
<h4 id="delete-a-pipeline"><a class="header" href="#delete-a-pipeline">Delete a Pipeline</a></h4>
<pre><code class="language-bash">pipeline delete my-pipeline --force
</code></pre>
<h3 id="performance-options"><a class="header" href="#performance-options">Performance Options</a></h3>
<h4 id="cpu-threads"><a class="header" href="#cpu-threads">CPU Threads</a></h4>
<p>Control the number of worker threads for CPU-bound operations (compression, encryption):</p>
<pre><code class="language-bash">pipeline process \
  --input file.txt \
  --output file.bin \
  --pipeline my-pipeline \
  --cpu-threads 8
</code></pre>
<p><strong>Default:</strong> Number of CPU cores - 1 (reserves one core for I/O)</p>
<p><strong>Tips:</strong></p>
<ul>
<li>Too high: CPU thrashing, context switching overhead</li>
<li>Too low: Underutilized cores, slower processing</li>
<li>Monitor CPU saturation metrics to tune</li>
</ul>
<h4 id="io-threads"><a class="header" href="#io-threads">I/O Threads</a></h4>
<p>Control the number of concurrent I/O operations:</p>
<pre><code class="language-bash">pipeline process \
  --input file.txt \
  --output file.bin \
  --pipeline my-pipeline \
  --io-threads 24
</code></pre>
<p><strong>Default:</strong> Device-specific (NVMe: 24, SSD: 12, HDD: 4)</p>
<p><strong>Storage Type Detection:</strong></p>
<pre><code class="language-bash">pipeline process \
  --input file.txt \
  --output file.bin \
  --pipeline my-pipeline \
  --storage-type nvme  # or ssd, hdd
</code></pre>
<h4 id="channel-depth"><a class="header" href="#channel-depth">Channel Depth</a></h4>
<p>Control backpressure in the pipeline stages:</p>
<pre><code class="language-bash">pipeline process \
  --input file.txt \
  --output file.bin \
  --pipeline my-pipeline \
  --channel-depth 8
</code></pre>
<p><strong>Default:</strong> 4</p>
<p><strong>Tips:</strong></p>
<ul>
<li>Lower values: Less memory, may cause pipeline stalls</li>
<li>Higher values: More buffering, higher memory usage</li>
<li>Optimal value depends on chunk processing time and I/O latency</li>
</ul>
<h4 id="chunk-size"><a class="header" href="#chunk-size">Chunk Size</a></h4>
<p>Configure the size of file chunks for parallel processing:</p>
<pre><code class="language-bash">pipeline process \
  --input file.txt \
  --output file.bin \
  --pipeline my-pipeline \
  --chunk-size-mb 10
</code></pre>
<p><strong>Default:</strong> Automatically determined based on file size and available resources</p>
<h3 id="global-options"><a class="header" href="#global-options">Global Options</a></h3>
<h4 id="verbose-logging"><a class="header" href="#verbose-logging">Verbose Logging</a></h4>
<p>Enable detailed logging output:</p>
<pre><code class="language-bash">pipeline --verbose process \
  --input file.txt \
  --output file.bin \
  --pipeline my-pipeline
</code></pre>
<h4 id="configuration-file"><a class="header" href="#configuration-file">Configuration File</a></h4>
<p>Use a custom configuration file:</p>
<pre><code class="language-bash">pipeline --config /path/to/config.toml process \
  --input file.txt \
  --output file.bin \
  --pipeline my-pipeline
</code></pre>
<h2 id="configuration-files"><a class="header" href="#configuration-files">Configuration Files</a></h2>
<p>Configuration files use TOML format and allow you to save pipeline settings for reuse.</p>
<h3 id="basic-configuration"><a class="header" href="#basic-configuration">Basic Configuration</a></h3>
<pre><code class="language-toml">[pipeline]
name = "my-pipeline"
stages = ["compression", "encryption", "integrity"]

[performance]
cpu_threads = 8
io_threads = 24
channel_depth = 4

[processing]
chunk_size_mb = 10
</code></pre>
<h3 id="algorithm-configuration"><a class="header" href="#algorithm-configuration">Algorithm Configuration</a></h3>
<pre><code class="language-toml">[stages.compression]
algorithm = "zstd"

[stages.encryption]
algorithm = "aes-256-gcm"
key_file = "/path/to/keyfile"

[stages.integrity]
algorithm = "sha256"
</code></pre>
<h3 id="complete-example"><a class="header" href="#complete-example">Complete Example</a></h3>
<pre><code class="language-toml"># Pipeline configuration example
[pipeline]
name = "secure-archival"
description = "High compression with encryption for archival"

[stages.compression]
algorithm = "brotli"
level = 11  # Maximum compression

[stages.encryption]
algorithm = "aes-256-gcm"
key_derivation = "argon2"

[stages.integrity]
algorithm = "blake3"

[performance]
cpu_threads = 16
io_threads = 24
channel_depth = 8
storage_type = "nvme"

[processing]
chunk_size_mb = 64
parallel_workers = 16
</code></pre>
<h3 id="using-configuration-files"><a class="header" href="#using-configuration-files">Using Configuration Files</a></h3>
<pre><code class="language-bash"># Use a configuration file
pipeline --config secure-archival.toml process \
  --input large-dataset.tar \
  --output large-dataset.bin

# Override configuration file settings
pipeline --config secure-archival.toml \
  --cpu-threads 8 \
  process --input file.txt --output file.bin
</code></pre>
<h2 id="environment-variables"><a class="header" href="#environment-variables">Environment Variables</a></h2>
<p>Environment variables provide another way to configure the pipeline:</p>
<pre><code class="language-bash"># Set performance defaults
export PIPELINE_CPU_THREADS=8
export PIPELINE_IO_THREADS=24
export PIPELINE_CHANNEL_DEPTH=8

# Set default chunk size
export PIPELINE_CHUNK_SIZE_MB=10

# Enable verbose logging
export PIPELINE_VERBOSE=true

# Run pipeline
pipeline process --input file.txt --output file.bin --pipeline my-pipeline
</code></pre>
<h2 id="configuration-priority"><a class="header" href="#configuration-priority">Configuration Priority</a></h2>
<p>When the same setting is configured in multiple places, the following priority applies (highest to lowest):</p>
<ol>
<li><strong>Command-line arguments</strong> - Explicit flags like <code>--cpu-threads</code></li>
<li><strong>Environment variables</strong> - <code>PIPELINE_*</code> variables</li>
<li><strong>Configuration file</strong> - Settings from <code>--config</code> file</li>
<li><strong>Default values</strong> - Built-in intelligent defaults</li>
</ol>
<p>Example:</p>
<pre><code class="language-bash"># Config file says cpu_threads = 8
# Environment says PIPELINE_CPU_THREADS=12
# Command line says --cpu-threads=16

# Result: Uses 16 (command-line wins)
</code></pre>
<h2 id="performance-tuning-guidelines"><a class="header" href="#performance-tuning-guidelines">Performance Tuning Guidelines</a></h2>
<h3 id="for-maximum-speed"><a class="header" href="#for-maximum-speed">For Maximum Speed</a></h3>
<ul>
<li>Use LZ4 compression</li>
<li>Use ChaCha20-Poly1305 encryption</li>
<li>Increase CPU threads to match cores</li>
<li>Use large chunks (32-64 MB)</li>
<li>Higher channel depth (8-16)</li>
</ul>
<pre><code class="language-bash">pipeline process \
  --input file.txt \
  --output file.bin \
  --pipeline speed-pipeline \
  --cpu-threads 16 \
  --chunk-size-mb 64 \
  --channel-depth 16
</code></pre>
<h3 id="for-maximum-compression"><a class="header" href="#for-maximum-compression">For Maximum Compression</a></h3>
<ul>
<li>Use Brotli compression</li>
<li>Smaller chunks for better compression ratio</li>
<li>More CPU threads for parallel compression</li>
</ul>
<pre><code class="language-bash">pipeline process \
  --input file.txt \
  --output file.bin \
  --pipeline compression-pipeline \
  --cpu-threads 16 \
  --chunk-size-mb 4
</code></pre>
<h3 id="for-resource-constrained-systems"><a class="header" href="#for-resource-constrained-systems">For Resource-Constrained Systems</a></h3>
<ul>
<li>Reduce CPU and I/O threads</li>
<li>Smaller chunks</li>
<li>Lower channel depth</li>
</ul>
<pre><code class="language-bash">pipeline process \
  --input file.txt \
  --output file.bin \
  --pipeline minimal-pipeline \
  --cpu-threads 2 \
  --io-threads 4 \
  --chunk-size-mb 2 \
  --channel-depth 2
</code></pre>
<h2 id="next-steps-3"><a class="header" href="#next-steps-3">Next Steps</a></h2>
<p>Now that you understand configuration, you're ready to:</p>
<ul>
<li><a href="fundamentals/first-run.html">Run Your First Pipeline</a> - Step-by-step tutorial</li>
<li><a href="fundamentals/stages.html">Learn About Stages</a> - Deep dive into pipeline stages</li>
<li><a href="fundamentals/../architecture/overview.html">Explore Architecture</a> - Understand the system design</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="running-your-first-pipeline"><a class="header" href="#running-your-first-pipeline">Running Your First Pipeline</a></h1>
<p><strong>Version:</strong> 1.0
<strong>Date:</strong> 2025-01-04
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner, Claude Code
<strong>Status:</strong> Active</p>
<h2 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h2>
<p>Before running your first pipeline, ensure you have:</p>
<ul>
<li>
<p><strong>Pipeline binary</strong> - Built and available in your PATH</p>
<pre><code class="language-bash">cargo build --release
cp target/release/pipeline /usr/local/bin/  # or add to PATH
</code></pre>
</li>
<li>
<p><strong>Test file</strong> - A sample file to process</p>
<pre><code class="language-bash">echo "Hello, Pipeline World!" &gt; test.txt
</code></pre>
</li>
<li>
<p><strong>Permissions</strong> - Read/write access to input and output directories</p>
</li>
</ul>
<h2 id="quick-start-5-minutes"><a class="header" href="#quick-start-5-minutes">Quick Start (5 minutes)</a></h2>
<p>Let's run a simple compression and encryption pipeline in 3 steps:</p>
<h3 id="step-1-create-a-pipeline"><a class="header" href="#step-1-create-a-pipeline">Step 1: Create a Pipeline</a></h3>
<pre><code class="language-bash">pipeline create \
  --name my-first-pipeline \
  --stages compression,encryption
</code></pre>
<p>You should see output like:</p>
<pre><code>✓ Created pipeline: my-first-pipeline
  Stages: compression (zstd), encryption (aes-256-gcm)
</code></pre>
<h3 id="step-2-process-a-file"><a class="header" href="#step-2-process-a-file">Step 2: Process a File</a></h3>
<pre><code class="language-bash">pipeline process \
  --input test.txt \
  --output test.bin \
  --pipeline my-first-pipeline
</code></pre>
<p>You should see progress output:</p>
<pre><code>Processing: test.txt
Pipeline: my-first-pipeline
  Stage 1/2: Compression (zstd)... ✓
  Stage 2/2: Encryption (aes-256-gcm)... ✓
Output: test.bin (24 bytes)
Time: 0.05s
</code></pre>
<h3 id="step-3-restore-the-file"><a class="header" href="#step-3-restore-the-file">Step 3: Restore the File</a></h3>
<pre><code class="language-bash">pipeline restore \
  --input test.bin \
  --output restored.txt
</code></pre>
<p>Verify the restoration:</p>
<pre><code class="language-bash">diff test.txt restored.txt
# No output = files are identical ✓
</code></pre>
<h2 id="detailed-walkthrough"><a class="header" href="#detailed-walkthrough">Detailed Walkthrough</a></h2>
<p>Let's explore each step in more detail.</p>
<h3 id="creating-pipelines"><a class="header" href="#creating-pipelines">Creating Pipelines</a></h3>
<h4 id="basic-pipeline"><a class="header" href="#basic-pipeline">Basic Pipeline</a></h4>
<pre><code class="language-bash">pipeline create \
  --name basic \
  --stages compression
</code></pre>
<p>This creates a simple compression-only pipeline using default settings (zstd compression).</p>
<h4 id="secure-pipeline"><a class="header" href="#secure-pipeline">Secure Pipeline</a></h4>
<pre><code class="language-bash">pipeline create \
  --name secure \
  --stages compression,encryption,integrity
</code></pre>
<p>This creates a complete security pipeline with:</p>
<ul>
<li>Compression (reduces size)</li>
<li>Encryption (protects data)</li>
<li>Integrity verification (detects tampering)</li>
</ul>
<h4 id="save-pipeline-configuration"><a class="header" href="#save-pipeline-configuration">Save Pipeline Configuration</a></h4>
<pre><code class="language-bash">pipeline create \
  --name archival \
  --stages compression,encryption \
  --output archival-pipeline.toml
</code></pre>
<p>This saves the pipeline configuration to a file for reuse.</p>
<h3 id="processing-files"><a class="header" href="#processing-files">Processing Files</a></h3>
<h4 id="basic-processing"><a class="header" href="#basic-processing">Basic Processing</a></h4>
<pre><code class="language-bash"># Process a file
pipeline process \
  --input large-file.log \
  --output large-file.bin \
  --pipeline secure
</code></pre>
<h4 id="with-performance-options"><a class="header" href="#with-performance-options">With Performance Options</a></h4>
<pre><code class="language-bash"># Process with custom settings
pipeline process \
  --input large-file.log \
  --output large-file.bin \
  --pipeline secure \
  --cpu-threads 8 \
  --chunk-size-mb 32
</code></pre>
<h4 id="with-verbose-logging"><a class="header" href="#with-verbose-logging">With Verbose Logging</a></h4>
<pre><code class="language-bash"># See detailed progress
pipeline --verbose process \
  --input large-file.log \
  --output large-file.bin \
  --pipeline secure
</code></pre>
<h3 id="restoring-files"><a class="header" href="#restoring-files">Restoring Files</a></h3>
<p>The pipeline automatically detects the processing stages from the output file's metadata:</p>
<pre><code class="language-bash"># Restore automatically reverses all stages
pipeline restore \
  --input large-file.bin \
  --output restored-file.log
</code></pre>
<p>The system will:</p>
<ol>
<li>Read metadata from the file header</li>
<li>Apply stages in reverse order</li>
<li>Verify integrity if available</li>
<li>Restore original file</li>
</ol>
<h3 id="managing-pipelines"><a class="header" href="#managing-pipelines">Managing Pipelines</a></h3>
<h4 id="list-all-pipelines"><a class="header" href="#list-all-pipelines">List All Pipelines</a></h4>
<pre><code class="language-bash">pipeline list
</code></pre>
<p>Output:</p>
<pre><code>Available Pipelines:
  - my-first-pipeline (compression, encryption)
  - secure (compression, encryption, integrity)
  - archival (compression, encryption)
</code></pre>
<h4 id="show-pipeline-details-1"><a class="header" href="#show-pipeline-details-1">Show Pipeline Details</a></h4>
<pre><code class="language-bash">pipeline show secure
</code></pre>
<p>Output:</p>
<pre><code>Pipeline: secure
  Stage 1: Compression (zstd)
  Stage 2: Encryption (aes-256-gcm)
  Stage 3: Integrity (sha256)
Created: 2025-01-04 10:30:00
</code></pre>
<h4 id="delete-a-pipeline-1"><a class="header" href="#delete-a-pipeline-1">Delete a Pipeline</a></h4>
<pre><code class="language-bash">pipeline delete my-first-pipeline --force
</code></pre>
<h2 id="understanding-output"><a class="header" href="#understanding-output">Understanding Output</a></h2>
<h3 id="successful-processing"><a class="header" href="#successful-processing">Successful Processing</a></h3>
<p>When processing completes successfully:</p>
<pre><code>Processing: test.txt
Pipeline: my-first-pipeline
  Stage 1/2: Compression (zstd)... ✓
  Stage 2/2: Encryption (aes-256-gcm)... ✓

Statistics:
  Input size:  1,024 KB
  Output size: 512 KB
  Compression ratio: 50%
  Processing time: 0.15s
  Throughput: 6.8 MB/s

Output: test.bin
</code></pre>
<h3 id="performance-metrics"><a class="header" href="#performance-metrics">Performance Metrics</a></h3>
<p>With <code>--verbose</code> flag, you'll see detailed metrics:</p>
<pre><code>Pipeline Execution Metrics:
  Chunks processed: 64
  Parallel workers: 8
  Average chunk time: 2.3ms
  CPU utilization: 87%
  I/O wait: 3%

Stage Breakdown:
  Compression: 0.08s (53%)
  Encryption: 0.05s (33%)
  I/O: 0.02s (14%)
</code></pre>
<h3 id="error-messages"><a class="header" href="#error-messages">Error Messages</a></h3>
<h4 id="file-not-found"><a class="header" href="#file-not-found">File Not Found</a></h4>
<pre><code>Error: Input file not found: test.txt
  Check the file path and try again
</code></pre>
<h4 id="permission-denied"><a class="header" href="#permission-denied">Permission Denied</a></h4>
<pre><code>Error: Permission denied: /protected/output.bin
  Ensure you have write access to the output directory
</code></pre>
<h4 id="invalid-pipeline"><a class="header" href="#invalid-pipeline">Invalid Pipeline</a></h4>
<pre><code>Error: Pipeline not found: nonexistent
  Use 'pipeline list' to see available pipelines
</code></pre>
<h2 id="common-scenarios"><a class="header" href="#common-scenarios">Common Scenarios</a></h2>
<h3 id="scenario-1-compress-large-log-files"><a class="header" href="#scenario-1-compress-large-log-files">Scenario 1: Compress Large Log Files</a></h3>
<pre><code class="language-bash"># Create compression pipeline
pipeline create --name logs --stages compression

# Process log files
pipeline process \
  --input app.log \
  --output app.log.bin \
  --pipeline logs \
  --chunk-size-mb 64

# Compression ratio is typically 70-90% for text logs
</code></pre>
<h3 id="scenario-2-secure-sensitive-files"><a class="header" href="#scenario-2-secure-sensitive-files">Scenario 2: Secure Sensitive Files</a></h3>
<pre><code class="language-bash"># Create secure pipeline with all protections
pipeline create --name sensitive --stages compression,encryption,integrity

# Process sensitive file
pipeline process \
  --input customer-data.csv \
  --output customer-data.bin \
  --pipeline sensitive

# File is now compressed, encrypted, and tamper-evident
</code></pre>
<h3 id="scenario-3-high-performance-batch-processing"><a class="header" href="#scenario-3-high-performance-batch-processing">Scenario 3: High-Performance Batch Processing</a></h3>
<pre><code class="language-bash"># Process multiple files with optimized settings
for file in data/*.csv; do
  pipeline process \
    --input "$file" \
    --output "processed/$(basename $file).bin" \
    --pipeline fast \
    --cpu-threads 16 \
    --chunk-size-mb 128 \
    --channel-depth 16
done
</code></pre>
<h3 id="scenario-4-restore-and-verify"><a class="header" href="#scenario-4-restore-and-verify">Scenario 4: Restore and Verify</a></h3>
<pre><code class="language-bash"># Restore file
pipeline restore \
  --input customer-data.bin \
  --output customer-data-restored.csv

# Verify restoration
sha256sum customer-data.csv customer-data-restored.csv
# Both checksums should match
</code></pre>
<h2 id="testing-your-pipeline"><a class="header" href="#testing-your-pipeline">Testing Your Pipeline</a></h2>
<h3 id="create-test-data"><a class="header" href="#create-test-data">Create Test Data</a></h3>
<pre><code class="language-bash"># Create a test file
dd if=/dev/urandom of=test-10mb.bin bs=1M count=10

# Calculate original checksum
sha256sum test-10mb.bin &gt; original.sha256
</code></pre>
<h3 id="process-and-restore"><a class="header" href="#process-and-restore">Process and Restore</a></h3>
<pre><code class="language-bash"># Process the file
pipeline process \
  --input test-10mb.bin \
  --output test-10mb.processed \
  --pipeline my-first-pipeline

# Restore the file
pipeline restore \
  --input test-10mb.processed \
  --output test-10mb.restored
</code></pre>
<h3 id="verify-integrity"><a class="header" href="#verify-integrity">Verify Integrity</a></h3>
<pre><code class="language-bash"># Verify restored file matches original
sha256sum -c original.sha256
# Should output: test-10mb.bin: OK
</code></pre>
<h2 id="next-steps-4"><a class="header" href="#next-steps-4">Next Steps</a></h2>
<p>Congratulations! You've run your first pipeline. Now you can:</p>
<ul>
<li>
<p><strong>Explore Advanced Features</strong></p>
<ul>
<li><a href="fundamentals/../architecture/overview.html">Architecture Overview</a> - Understand the system design</li>
<li><a href="fundamentals/../implementation/compression.html">Implementation Details</a> - Learn about algorithms</li>
<li><a href="fundamentals/../advanced/performance.html">Performance Tuning</a> - Optimize for your use case</li>
</ul>
</li>
<li>
<p><strong>Learn More About Configuration</strong></p>
<ul>
<li><a href="fundamentals/configuration.html">Configuration Guide</a> - Detailed configuration options</li>
<li><a href="fundamentals/stages.html">Stage Types</a> - Available processing stages</li>
</ul>
</li>
<li>
<p><strong>Build Custom Pipelines</strong></p>
<ul>
<li>Experiment with different stage combinations</li>
<li>Test different algorithms for your workload</li>
<li>Benchmark performance with your data</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="architecture-overview"><a class="header" href="#architecture-overview">Architecture Overview</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>High-level architectural overview of the pipeline system.</p>
<h2 id="design-philosophy"><a class="header" href="#design-philosophy">Design Philosophy</a></h2>
<p>The Optimized Adaptive Pipeline is built on three foundational architectural patterns:</p>
<ol>
<li><strong>Clean Architecture</strong> - Organizing code by dependency direction</li>
<li><strong>Domain-Driven Design (DDD)</strong> - Modeling the business domain</li>
<li><strong>Hexagonal Architecture</strong> - Isolating business logic from infrastructure</li>
</ol>
<p>These patterns work together to create a maintainable, testable, and flexible system.</p>
<h2 id="layered-architecture"><a class="header" href="#layered-architecture">Layered Architecture</a></h2>
<p>The pipeline follows a strict layered architecture where dependencies flow inward:</p>
<p><img src="architecture/../diagrams/layered-architecture.svg" alt="Layered Architecture" /></p>
<h3 id="layer-overview"><a class="header" href="#layer-overview">Layer Overview</a></h3>
<p><strong>Presentation Layer</strong> (Outermost)</p>
<ul>
<li>CLI interface for user interaction</li>
<li>Configuration management</li>
<li>Request/response handling</li>
</ul>
<p><strong>Application Layer</strong></p>
<ul>
<li>Use cases and application services</li>
<li>Pipeline orchestration</li>
<li>File processing coordination</li>
</ul>
<p><strong>Domain Layer</strong> (Core)</p>
<ul>
<li>Business logic and rules</li>
<li>Entities (Pipeline, PipelineStage)</li>
<li>Value objects (FilePath, FileSize, Algorithm)</li>
<li>Domain services</li>
</ul>
<p><strong>Infrastructure Layer</strong> (Outermost)</p>
<ul>
<li>Database implementations (SQLite)</li>
<li>File system operations</li>
<li>External system adapters</li>
<li>Metrics collection</li>
</ul>
<h2 id="clean-architecture"><a class="header" href="#clean-architecture">Clean Architecture</a></h2>
<p>Clean Architecture ensures that business logic doesn't depend on implementation details:</p>
<p><img src="architecture/../diagrams/dependency-flow.svg" alt="Dependency Flow" /></p>
<h3 id="key-principles"><a class="header" href="#key-principles">Key Principles</a></h3>
<p><strong>Dependency Rule</strong>: Source code dependencies point only inward, toward higher-level policies.</p>
<ul>
<li><strong>High-level policy</strong> (Application layer) defines what the system does</li>
<li><strong>Abstractions</strong> (Traits) define how components interact</li>
<li><strong>Low-level details</strong> (Infrastructure) implements the abstractions</li>
</ul>
<p>This means:</p>
<ul>
<li>Domain layer has <strong>zero external dependencies</strong></li>
<li>Application layer depends only on domain traits</li>
<li>Infrastructure implements domain interfaces</li>
</ul>
<h3 id="benefits"><a class="header" href="#benefits">Benefits</a></h3>
<p>✅ <strong>Testability</strong>: Business logic can be tested without database or file system
✅ <strong>Flexibility</strong>: Swap implementations (SQLite → PostgreSQL) without changing business logic
✅ <strong>Independence</strong>: Domain logic doesn't know about HTTP, databases, or file formats</p>
<h2 id="hexagonal-architecture-ports-and-adapters"><a class="header" href="#hexagonal-architecture-ports-and-adapters">Hexagonal Architecture (Ports and Adapters)</a></h2>
<p>The pipeline uses Hexagonal Architecture to isolate the core business logic:</p>
<p><img src="architecture/../diagrams/hexagonal-architecture.svg" alt="Hexagonal Architecture" /></p>
<h3 id="core-components-1"><a class="header" href="#core-components-1">Core Components</a></h3>
<p><strong>Application Core</strong></p>
<ul>
<li>Domain model (entities, value objects)</li>
<li>Business logic (pipeline orchestration)</li>
<li>Ports (trait definitions)</li>
</ul>
<p><strong>Primary Adapters</strong> (Driving)</p>
<ul>
<li>CLI adapter - drives the application</li>
<li>HTTP adapter - future API endpoints</li>
</ul>
<p><strong>Secondary Adapters</strong> (Driven)</p>
<ul>
<li>SQLite repository adapter - driven by the application</li>
<li>File system adapter - driven by the application</li>
<li>Prometheus metrics adapter - driven by the application</li>
</ul>
<h3 id="how-it-works"><a class="header" href="#how-it-works">How It Works</a></h3>
<ol>
<li><strong>User</strong> interacts with <strong>Primary Adapter</strong> (CLI)</li>
<li><strong>Primary Adapter</strong> calls <strong>Application Core</strong> through defined ports</li>
<li><strong>Application Core</strong> uses <strong>Ports</strong> (traits) to interact with infrastructure</li>
<li><strong>Secondary Adapters</strong> implement these ports</li>
<li><strong>Adapters</strong> connect to external systems (database, files)</li>
</ol>
<p><strong>Example Flow</strong>:</p>
<pre><code>CLI → Pipeline Service → Repository Port → SQLite Adapter → Database
</code></pre>
<p>The application core never knows it's using SQLite - it only knows the <code>Repository</code> trait.</p>
<h2 id="architecture-integration"><a class="header" href="#architecture-integration">Architecture Integration</a></h2>
<p>These three patterns work together:</p>
<pre><code class="language-text">Clean Architecture:    Layers with dependency direction
Domain-Driven Design:  Business modeling within layers
Hexagonal Architecture: Ports/Adapters at layer boundaries
</code></pre>
<p><strong>In Practice</strong>:</p>
<ul>
<li><strong>Domain layer</strong> contains pure business logic (DDD entities)</li>
<li><strong>Application layer</strong> orchestrates use cases (Clean Architecture)</li>
<li><strong>Infrastructure</strong> implements ports (Hexagonal Architecture)</li>
</ul>
<p>This combination provides:</p>
<ul>
<li>Clear separation of concerns</li>
<li>Testable business logic</li>
<li>Flexible infrastructure</li>
<li>Maintainable codebase</li>
</ul>
<h2 id="next-steps-5"><a class="header" href="#next-steps-5">Next Steps</a></h2>
<p>Continue to:</p>
<ul>
<li><a href="architecture/layers.html">Layered Architecture Details</a> - Deep dive into each layer</li>
<li><a href="architecture/domain-model.html">Domain Model</a> - Understanding entities and value objects</li>
<li><a href="architecture/patterns.html">Design Patterns</a> - Patterns used throughout the codebase</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="layered-architecture-1"><a class="header" href="#layered-architecture-1">Layered Architecture</a></h1>
<p><strong>Version:</strong> 1.0
<strong>Date:</strong> 2025-01-04
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner, Claude Code
<strong>Status:</strong> Active</p>
<h2 id="overview-1"><a class="header" href="#overview-1">Overview</a></h2>
<p>The pipeline system is organized into four distinct layers, each with specific responsibilities and clear boundaries. This layered architecture provides separation of concerns, testability, and maintainability.</p>
<p><img src="architecture/../diagrams/layered-architecture.svg" alt="Layered Architecture" /></p>
<h2 id="the-four-layers"><a class="header" href="#the-four-layers">The Four Layers</a></h2>
<pre><code class="language-text">┌─────────────────────────────────────────┐
│         Presentation Layer              │  ← User interface (CLI)
│  - CLI commands                         │
│  - User interaction                     │
└─────────────────┬───────────────────────┘
                  │ depends on
┌─────────────────▼───────────────────────┐
│         Application Layer               │  ← Use cases, orchestration
│  - Use cases                            │
│  - Application services                 │
│  - Commands/Queries                     │
└─────────────────┬───────────────────────┘
                  │ depends on
┌─────────────────▼───────────────────────┐
│           Domain Layer                  │  ← Core business logic
│  - Entities                             │
│  - Value objects                        │
│  - Domain services (interfaces)         │
│  - Business rules                       │
└─────────────────△───────────────────────┘
                  │ implements interfaces
┌─────────────────┴───────────────────────┐
│        Infrastructure Layer             │  ← External dependencies
│  - Database repositories                │
│  - File I/O                             │
│  - External services                    │
│  - Encryption/Compression               │
└─────────────────────────────────────────┘
</code></pre>
<h2 id="dependency-rule"><a class="header" href="#dependency-rule">Dependency Rule</a></h2>
<p>The <strong>dependency rule</strong> is the most important principle in layered architecture:</p>
<blockquote>
<p><strong>Dependencies flow inward toward the domain layer.</strong></p>
</blockquote>
<ul>
<li>Presentation depends on Application</li>
<li>Application depends on Domain</li>
<li>Infrastructure depends on Domain (via interfaces)</li>
<li><strong>Domain depends on nothing</strong> (pure business logic)</li>
</ul>
<p>This means:</p>
<ul>
<li>✅ Application can use Domain types</li>
<li>✅ Infrastructure implements Domain interfaces</li>
<li>❌ Domain cannot use Application types</li>
<li>❌ Domain cannot use Infrastructure types</li>
</ul>
<h2 id="domain-layer"><a class="header" href="#domain-layer">Domain Layer</a></h2>
<h3 id="purpose"><a class="header" href="#purpose">Purpose</a></h3>
<p>The domain layer contains the <strong>core business logic</strong> and is the heart of the application. It's completely independent of external concerns like databases, user interfaces, or frameworks.</p>
<h3 id="responsibilities"><a class="header" href="#responsibilities">Responsibilities</a></h3>
<ul>
<li>Define business entities and value objects</li>
<li>Enforce business rules and invariants</li>
<li>Provide domain service interfaces</li>
<li>Emit domain events</li>
<li>Define repository interfaces</li>
</ul>
<h3 id="structure"><a class="header" href="#structure">Structure</a></h3>
<pre><code>pipeline-domain/
├── entities/
│   ├── pipeline.rs           # Pipeline entity
│   ├── pipeline_stage.rs     # Stage entity
│   ├── processing_context.rs # Processing state
│   └── security_context.rs   # Security management
├── value_objects/
│   ├── algorithm.rs          # Algorithm value object
│   ├── chunk_size.rs         # Chunk size validation
│   ├── file_path.rs          # Type-safe paths
│   └── pipeline_id.rs        # Type-safe IDs
├── services/
│   ├── compression_service.rs    # Compression interface
│   ├── encryption_service.rs     # Encryption interface
│   └── checksum_service.rs       # Checksum interface
├── repositories/
│   └── pipeline_repository.rs    # Repository interface
├── events/
│   └── domain_events.rs      # Business events
└── error/
    └── pipeline_error.rs     # Domain errors
</code></pre>
<h3 id="example"><a class="header" href="#example">Example</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Domain layer - pure business logic
pub struct Pipeline {
    id: PipelineId,
    name: String,
    stages: Vec&lt;PipelineStage&gt;,
    // ... no database or UI dependencies
}

impl Pipeline {
    pub fn new(name: String, stages: Vec&lt;PipelineStage&gt;) -&gt; Result&lt;Self, PipelineError&gt; {
        // Business rule: must have at least one stage
        if stages.is_empty() {
            return Err(PipelineError::InvalidConfiguration(
                "Pipeline must have at least one stage".to_string()
            ));
        }

        // Create pipeline with validated business rules
        Ok(Self {
            id: PipelineId::new(),
            name,
            stages,
            // ...
        })
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="key-characteristics"><a class="header" href="#key-characteristics">Key Characteristics</a></h3>
<ul>
<li><strong>No external dependencies</strong> - Only standard library and domain types</li>
<li><strong>Highly testable</strong> - Can test without databases or files</li>
<li><strong>Portable</strong> - Can be used in any context (web, CLI, embedded)</li>
<li><strong>Stable</strong> - Rarely changes except for business requirement changes</li>
</ul>
<h2 id="application-layer"><a class="header" href="#application-layer">Application Layer</a></h2>
<h3 id="purpose-1"><a class="header" href="#purpose-1">Purpose</a></h3>
<p>The application layer orchestrates the execution of business use cases. It coordinates domain objects and delegates to domain services to accomplish specific tasks.</p>
<h3 id="responsibilities-1"><a class="header" href="#responsibilities-1">Responsibilities</a></h3>
<ul>
<li>Implement use cases (user actions)</li>
<li>Coordinate domain objects</li>
<li>Manage transactions</li>
<li>Handle application-specific workflows</li>
<li>Emit application events</li>
</ul>
<h3 id="structure-1"><a class="header" href="#structure-1">Structure</a></h3>
<pre><code>pipeline/src/application/
├── use_cases/
│   ├── process_file.rs       # File processing use case
│   ├── restore_file.rs       # File restoration use case
│   └── create_pipeline.rs    # Pipeline creation
├── services/
│   ├── pipeline_service.rs   # Pipeline orchestration
│   ├── file_processor_service.rs  # File processing
│   └── transactional_chunk_writer.rs  # Chunk writing
├── commands/
│   └── commands.rs           # CQRS commands
└── utilities/
    └── generic_service_base.rs  # Service helpers
</code></pre>
<h3 id="example-1"><a class="header" href="#example-1">Example</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Application layer - orchestrates domain objects
pub struct FileProcessorService {
    pipeline_repo: Arc&lt;dyn PipelineRepository&gt;,
    compression: Arc&lt;dyn CompressionService&gt;,
    encryption: Arc&lt;dyn EncryptionService&gt;,
}

impl FileProcessorService {
    pub async fn process_file(
        &amp;self,
        pipeline_id: &amp;PipelineId,
        input_path: &amp;FilePath,
        output_path: &amp;FilePath,
    ) -&gt; Result&lt;ProcessingMetrics, PipelineError&gt; {
        // 1. Fetch pipeline from repository
        let pipeline = self.pipeline_repo
            .find_by_id(pipeline_id)
            .await?
            .ok_or(PipelineError::NotFound)?;

        // 2. Create processing context
        let context = ProcessingContext::new(
            pipeline.id().clone(),
            input_path.clone(),
            output_path.clone(),
        );

        // 3. Process each stage
        for stage in pipeline.stages() {
            match stage.stage_type() {
                StageType::Compression =&gt; {
                    self.compression.compress(/* ... */).await?;
                }
                StageType::Encryption =&gt; {
                    self.encryption.encrypt(/* ... */).await?;
                }
                // ... more stages
            }
        }

        // 4. Return metrics
        Ok(context.metrics().clone())
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="key-characteristics-1"><a class="header" href="#key-characteristics-1">Key Characteristics</a></h3>
<ul>
<li><strong>Thin layer</strong> - Delegates to domain for business logic</li>
<li><strong>Workflow coordination</strong> - Orchestrates multiple domain operations</li>
<li><strong>Transaction management</strong> - Ensures atomic operations</li>
<li><strong>No business logic</strong> - Business rules belong in domain layer</li>
</ul>
<h2 id="infrastructure-layer"><a class="header" href="#infrastructure-layer">Infrastructure Layer</a></h2>
<h3 id="purpose-2"><a class="header" href="#purpose-2">Purpose</a></h3>
<p>The infrastructure layer provides concrete implementations of interfaces defined in the domain layer. It handles all external concerns like databases, file systems, and third-party services.</p>
<h3 id="responsibilities-2"><a class="header" href="#responsibilities-2">Responsibilities</a></h3>
<ul>
<li>Implement repository interfaces</li>
<li>Provide database access</li>
<li>Handle file I/O operations</li>
<li>Implement compression/encryption services</li>
<li>Integrate with external systems</li>
<li>Provide logging and metrics</li>
</ul>
<h3 id="structure-2"><a class="header" href="#structure-2">Structure</a></h3>
<pre><code>pipeline/src/infrastructure/
├── repositories/
│   ├── sqlite_pipeline_repository.rs  # SQLite implementation
│   └── stage_executor.rs              # Stage execution
├── adapters/
│   ├── compression_service_adapter.rs # Compression implementation
│   ├── encryption_service_adapter.rs  # Encryption implementation
│   └── repositories/
│       └── sqlite_repository_adapter.rs  # Repository adapter
├── services/
│   └── binary_format_service.rs       # File format handling
├── metrics/
│   ├── metrics_service.rs             # Prometheus metrics
│   └── metrics_observer.rs            # Metrics collection
├── logging/
│   └── observability_service.rs       # Logging setup
└── config/
    └── config_service.rs              # Configuration management
</code></pre>
<h3 id="example-2"><a class="header" href="#example-2">Example</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Infrastructure layer - implements domain interfaces
pub struct SQLitePipelineRepository {
    pool: SqlitePool,
}

#[async_trait]
impl PipelineRepository for SQLitePipelineRepository {
    async fn find_by_id(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;Option&lt;Pipeline&gt;, PipelineError&gt; {
        // Database-specific code
        let row = sqlx::query_as::&lt;_, PipelineRow&gt;(
            "SELECT * FROM pipelines WHERE id = ?"
        )
        .bind(id.to_string())
        .fetch_optional(&amp;self.pool)
        .await
        .map_err(|e| PipelineError::RepositoryError(e.to_string()))?;

        // Map database row to domain entity
        row.map(|r| self.to_domain_entity(r)).transpose()
    }

    async fn create(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt; {
        // Convert domain entity to database row
        let row = self.to_persistence_model(pipeline);

        // Insert into database
        sqlx::query(
            "INSERT INTO pipelines (id, name, ...) VALUES (?, ?, ...)"
        )
        .bind(&amp;row.id)
        .bind(&amp;row.name)
        .execute(&amp;self.pool)
        .await
        .map_err(|e| PipelineError::RepositoryError(e.to_string()))?;

        Ok(())
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="key-characteristics-2"><a class="header" href="#key-characteristics-2">Key Characteristics</a></h3>
<ul>
<li><strong>Implements domain interfaces</strong> - Provides concrete implementations</li>
<li><strong>Database access</strong> - Handles all persistence operations</li>
<li><strong>External integrations</strong> - Communicates with external systems</li>
<li><strong>Technology-specific</strong> - Uses specific libraries and frameworks</li>
<li><strong>Replaceable</strong> - Can swap implementations without changing domain</li>
</ul>
<h2 id="presentation-layer"><a class="header" href="#presentation-layer">Presentation Layer</a></h2>
<h3 id="purpose-3"><a class="header" href="#purpose-3">Purpose</a></h3>
<p>The presentation layer handles user interaction and input/output. It translates user commands into application use cases and presents results back to the user.</p>
<h3 id="responsibilities-3"><a class="header" href="#responsibilities-3">Responsibilities</a></h3>
<ul>
<li>Parse and validate user input</li>
<li>Execute application use cases</li>
<li>Format and display output</li>
<li>Handle user interaction</li>
<li>Map errors to user-friendly messages</li>
</ul>
<h3 id="structure-3"><a class="header" href="#structure-3">Structure</a></h3>
<pre><code>pipeline/src/presentation/
├── mod.rs                    # Presentation module
└── (CLI is in main.rs)
</code></pre>
<h3 id="example-3"><a class="header" href="#example-3">Example</a></h3>
<pre><pre class="playground"><code class="language-rust">// Presentation layer - CLI interaction
#[tokio::main]
async fn main() -&gt; std::process::ExitCode {
    // 1. Parse CLI arguments
    let cli = bootstrap::bootstrap_cli()
        .unwrap_or_else(|e| {
            eprintln!("Error: {}", e);
            std::process::exit(65);
        });

    // 2. Set up dependencies (infrastructure)
    let db_pool = create_database_pool().await?;
    let pipeline_repo = Arc::new(SQLitePipelineRepository::new(db_pool));
    let compression = Arc::new(CompressionServiceAdapter::new());
    let file_processor = FileProcessorService::new(pipeline_repo, compression);

    // 3. Execute use case based on command
    let result = match cli.command {
        Commands::Process { input, output, pipeline } =&gt; {
            // Call application service
            file_processor.process_file(&amp;pipeline, &amp;input, &amp;output).await
        }
        Commands::Create { name, stages } =&gt; {
            // Call application service
            create_pipeline_service.create(&amp;name, stages).await
        }
        // ... more commands
    };

    // 4. Handle result and display to user
    match result {
        Ok(_) =&gt; {
            println!("✓ Processing completed successfully");
            ExitCode::SUCCESS
        }
        Err(e) =&gt; {
            eprintln!("✗ Error: {}", e);
            ExitCode::FAILURE
        }
    }
}</code></pre></pre>
<h3 id="key-characteristics-3"><a class="header" href="#key-characteristics-3">Key Characteristics</a></h3>
<ul>
<li><strong>Thin layer</strong> - Minimal logic, delegates to application</li>
<li><strong>User-facing</strong> - Handles all user interaction</li>
<li><strong>Input validation</strong> - Validates user input before processing</li>
<li><strong>Error formatting</strong> - Converts technical errors to user-friendly messages</li>
</ul>
<h2 id="layer-interactions"><a class="header" href="#layer-interactions">Layer Interactions</a></h2>
<h3 id="example-processing-a-file"><a class="header" href="#example-processing-a-file">Example: Processing a File</a></h3>
<p>Here's how the layers work together to process a file:</p>
<pre><code class="language-text">1. Presentation Layer (CLI)
   ↓ User runs: pipeline process --input file.txt --output file.bin --pipeline my-pipeline
   ├─ Parse command-line arguments
   ├─ Validate input parameters
   └─ Call Application Service

2. Application Layer (FileProcessorService)
   ↓ process_file(pipeline_id, input_path, output_path)
   ├─ Fetch Pipeline from Repository (Infrastructure)
   ├─ Create ProcessingContext (Domain)
   ├─ For each stage:
   │  ├─ Call CompressionService (Infrastructure)
   │  ├─ Call EncryptionService (Infrastructure)
   │  └─ Update metrics (Domain)
   └─ Return ProcessingMetrics (Domain)

3. Domain Layer (Pipeline, ProcessingContext)
   ↓ Enforce business rules
   ├─ Validate stage compatibility
   ├─ Enforce chunk sequencing
   └─ Calculate metrics

4. Infrastructure Layer (Repositories, Services)
   ↓ Handle external operations
   ├─ Query SQLite database
   ├─ Read/write files
   ├─ Compress data (brotli, zstd, etc.)
   └─ Encrypt data (AES, ChaCha20)
</code></pre>
<h2 id="benefits-of-layered-architecture"><a class="header" href="#benefits-of-layered-architecture">Benefits of Layered Architecture</a></h2>
<h3 id="separation-of-concerns"><a class="header" href="#separation-of-concerns">Separation of Concerns</a></h3>
<p>Each layer has a single, well-defined responsibility. This makes the code easier to understand and maintain.</p>
<h3 id="testability-1"><a class="header" href="#testability-1">Testability</a></h3>
<ul>
<li><strong>Domain Layer</strong>: Test business logic without any infrastructure</li>
<li><strong>Application Layer</strong>: Test workflows with mock repositories</li>
<li><strong>Infrastructure Layer</strong>: Test database operations independently</li>
<li><strong>Presentation Layer</strong>: Test user interaction separately</li>
</ul>
<h3 id="flexibility"><a class="header" href="#flexibility">Flexibility</a></h3>
<p>You can change infrastructure (e.g., swap SQLite for PostgreSQL) without touching domain or application layers.</p>
<h3 id="maintainability"><a class="header" href="#maintainability">Maintainability</a></h3>
<p>Changes in one layer typically don't affect other layers, reducing the risk of breaking existing functionality.</p>
<h3 id="parallel-development"><a class="header" href="#parallel-development">Parallel Development</a></h3>
<p>Teams can work on different layers simultaneously without conflicts.</p>
<h2 id="common-pitfalls"><a class="header" href="#common-pitfalls">Common Pitfalls</a></h2>
<h3 id="-breaking-the-dependency-rule"><a class="header" href="#-breaking-the-dependency-rule">❌ Breaking the Dependency Rule</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// WRONG: Domain depending on infrastructure
pub struct Pipeline {
    id: PipelineId,
    db_connection: SqlitePool,  // ❌ Database dependency in domain!
}
<span class="boring">}</span></code></pre></pre>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// CORRECT: Domain independent of infrastructure
pub struct Pipeline {
    id: PipelineId,
    name: String,
    stages: Vec&lt;PipelineStage&gt;,  // ✅ Pure domain types
}
<span class="boring">}</span></code></pre></pre>
<h3 id="-business-logic-in-application-layer"><a class="header" href="#-business-logic-in-application-layer">❌ Business Logic in Application Layer</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// WRONG: Business logic in application
impl FileProcessorService {
    pub async fn process_file(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), Error&gt; {
        // ❌ Business rule in application layer!
        if pipeline.stages().is_empty() {
            return Err(Error::InvalidPipeline);
        }
        // ...
    }
}
<span class="boring">}</span></code></pre></pre>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// CORRECT: Business logic in domain
impl Pipeline {
    pub fn new(name: String, stages: Vec&lt;PipelineStage&gt;) -&gt; Result&lt;Self, PipelineError&gt; {
        // ✅ Business rule in domain layer
        if stages.is_empty() {
            return Err(PipelineError::InvalidConfiguration(
                "Pipeline must have at least one stage".to_string()
            ));
        }
        Ok(Self { /* ... */ })
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="-direct-infrastructure-access-from-presentation"><a class="header" href="#-direct-infrastructure-access-from-presentation">❌ Direct Infrastructure Access from Presentation</a></h3>
<pre><pre class="playground"><code class="language-rust">// WRONG: Presentation accessing infrastructure directly
async fn main() {
    let db_pool = create_database_pool().await?;
    // ❌ CLI directly using repository!
    let pipeline = db_pool.query("SELECT * FROM pipelines").await?;
}</code></pre></pre>
<pre><pre class="playground"><code class="language-rust">// CORRECT: Presentation using application services
async fn main() {
    let file_processor = create_file_processor().await?;
    // ✅ CLI using application service
    let result = file_processor.process_file(&amp;pipeline_id, &amp;input, &amp;output).await?;
}</code></pre></pre>
<h2 id="next-steps-6"><a class="header" href="#next-steps-6">Next Steps</a></h2>
<p>Now that you understand the layered architecture:</p>
<ul>
<li><a href="architecture/adapter-pattern.html">Hexagonal Architecture</a> - Ports and adapters pattern</li>
<li><a href="architecture/dependencies.html">Dependency Inversion</a> - Managing dependencies</li>
<li><a href="architecture/domain-model.html">Domain Model</a> - Deep dive into the domain layer</li>
<li><a href="architecture/repository-pattern.html">Repository Pattern</a> - Data persistence abstraction</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dependency-flow"><a class="header" href="#dependency-flow">Dependency Flow</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Understanding dependency direction and inversion.</p>
<h2 id="dependency-rule-1"><a class="header" href="#dependency-rule-1">Dependency Rule</a></h2>
<p>TODO: Explain dependency direction</p>
<h2 id="dependency-inversion"><a class="header" href="#dependency-inversion">Dependency Inversion</a></h2>
<p>TODO: Explain DIP application</p>
<h2 id="trait-abstractions"><a class="header" href="#trait-abstractions">Trait Abstractions</a></h2>
<p>TODO: Explain trait usage</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="domain-model-1"><a class="header" href="#domain-model-1">Domain Model</a></h1>
<p><strong>Version:</strong> 1.0
<strong>Date:</strong> 2025-01-04
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner, Claude Code
<strong>Status:</strong> Active</p>
<h2 id="overview-2"><a class="header" href="#overview-2">Overview</a></h2>
<p>The domain model is the heart of the pipeline system. It captures the core business concepts, rules, and behaviors using Domain-Driven Design (DDD) principles. This chapter explains how the domain model is structured and why it's designed this way.</p>
<p><img src="architecture/../diagrams/domain-model.svg" alt="Domain Model" /></p>
<h2 id="domain-driven-design-principles"><a class="header" href="#domain-driven-design-principles">Domain-Driven Design Principles</a></h2>
<p>Domain-Driven Design (DDD) is a software development approach that emphasizes:</p>
<ol>
<li><strong>Focus on the core domain</strong> - The business logic is the most important part</li>
<li><strong>Model-driven design</strong> - The domain model drives the software design</li>
<li><strong>Ubiquitous language</strong> - Shared vocabulary between developers and domain experts</li>
<li><strong>Bounded contexts</strong> - Clear boundaries between different parts of the system</li>
</ol>
<h3 id="why-ddd"><a class="header" href="#why-ddd">Why DDD?</a></h3>
<p>For a pipeline processing system, DDD provides:</p>
<ul>
<li><strong>Clear separation</strong> between business logic and infrastructure</li>
<li><strong>Testable code</strong> - Domain logic can be tested without databases or files</li>
<li><strong>Flexibility</strong> - Easy to change infrastructure without touching business rules</li>
<li><strong>Maintainability</strong> - Business rules are explicit and well-organized</li>
</ul>
<h2 id="core-domain-concepts"><a class="header" href="#core-domain-concepts">Core Domain Concepts</a></h2>
<h3 id="entities-1"><a class="header" href="#entities-1">Entities</a></h3>
<p><strong>Entities</strong> are objects with a unique identity that persists through time. Two entities are equal if they have the same ID, even if all their other attributes differ.</p>
<h4 id="pipeline-entity"><a class="header" href="#pipeline-entity">Pipeline Entity</a></h4>
<p>The central entity representing a file processing workflow.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct Pipeline {
    id: PipelineId,                    // Unique identity
    name: String,                      // Human-readable name
    stages: Vec&lt;PipelineStage&gt;,        // Ordered processing stages
    configuration: HashMap&lt;String, String&gt;,  // Custom settings
    metrics: ProcessingMetrics,        // Performance data
    archived: bool,                    // Lifecycle state
    created_at: DateTime&lt;Utc&gt;,         // Creation timestamp
    updated_at: DateTime&lt;Utc&gt;,         // Last modification
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Key characteristics:</strong></p>
<ul>
<li>Has unique <code>PipelineId</code></li>
<li>Can be modified while maintaining identity</li>
<li>Enforces business rules (e.g., must have at least one stage)</li>
<li>Automatically adds integrity verification stages</li>
</ul>
<p><strong>Example:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::Pipeline;

// Two pipelines with same ID are equal, even if names differ
let pipeline1 = Pipeline::new("Original Name", stages.clone())?;
let pipeline2 = pipeline1.clone();
pipeline2.set_name("Different Name");

assert_eq!(pipeline1.id(), pipeline2.id());  // Same identity
<span class="boring">}</span></code></pre></pre>
<h4 id="pipelinestage-entity"><a class="header" href="#pipelinestage-entity">PipelineStage Entity</a></h4>
<p>Represents a single processing operation within a pipeline.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct PipelineStage {
    id: StageId,                       // Unique identity
    name: String,                      // Stage name
    stage_type: StageType,             // Compression, Encryption, etc.
    configuration: StageConfiguration, // Algorithm and parameters
    order: usize,                      // Execution order
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Stage Types:</strong></p>
<ul>
<li><code>Compression</code> - Data compression</li>
<li><code>Encryption</code> - Data encryption</li>
<li><code>Integrity</code> - Checksum verification</li>
<li><code>Custom</code> - User-defined operations</li>
</ul>
<h4 id="processingcontext-entity"><a class="header" href="#processingcontext-entity">ProcessingContext Entity</a></h4>
<p>Manages the runtime execution state of a pipeline.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct ProcessingContext {
    id: ProcessingContextId,           // Unique identity
    pipeline_id: PipelineId,           // Associated pipeline
    input_path: FilePath,              // Input file
    output_path: FilePath,             // Output file
    current_stage: usize,              // Current stage index
    status: ProcessingStatus,          // Running, Completed, Failed
    metrics: ProcessingMetrics,        // Runtime metrics
}
<span class="boring">}</span></code></pre></pre>
<h4 id="securitycontext-entity"><a class="header" href="#securitycontext-entity">SecurityContext Entity</a></h4>
<p>Manages security and permissions for pipeline operations.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct SecurityContext {
    id: SecurityContextId,             // Unique identity
    user_id: UserId,                   // User performing operation
    security_level: SecurityLevel,     // Required security level
    permissions: Vec&lt;Permission&gt;,      // Granted permissions
    encryption_key_id: Option&lt;EncryptionKeyId&gt;,  // Key for encryption
}
<span class="boring">}</span></code></pre></pre>
<h3 id="value-objects-1"><a class="header" href="#value-objects-1">Value Objects</a></h3>
<p><strong>Value Objects</strong> are immutable objects defined by their attributes. Two value objects with the same attributes are considered equal.</p>
<h4 id="algorithm-value-object"><a class="header" href="#algorithm-value-object">Algorithm Value Object</a></h4>
<p>Type-safe representation of processing algorithms.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct Algorithm(String);

impl Algorithm {
    // Predefined compression algorithms
    pub fn brotli() -&gt; Self { /* ... */ }
    pub fn gzip() -&gt; Self { /* ... */ }
    pub fn zstd() -&gt; Self { /* ... */ }
    pub fn lz4() -&gt; Self { /* ... */ }

    // Predefined encryption algorithms
    pub fn aes_256_gcm() -&gt; Self { /* ... */ }
    pub fn chacha20_poly1305() -&gt; Self { /* ... */ }

    // Predefined hashing algorithms
    pub fn sha256() -&gt; Self { /* ... */ }
    pub fn blake3() -&gt; Self { /* ... */ }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Key characteristics:</strong></p>
<ul>
<li>Immutable after creation</li>
<li>Self-validating (enforces format rules)</li>
<li>Category detection (is_compression(), is_encryption())</li>
<li>Type-safe (can't accidentally use wrong algorithm)</li>
</ul>
<h4 id="chunksize-value-object"><a class="header" href="#chunksize-value-object">ChunkSize Value Object</a></h4>
<p>Represents validated chunk sizes for file processing.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct ChunkSize(usize);

impl ChunkSize {
    pub fn new(bytes: usize) -&gt; Result&lt;Self, PipelineError&gt; {
        // Validates size is within acceptable range
        if bytes &lt; MIN_CHUNK_SIZE || bytes &gt; MAX_CHUNK_SIZE {
            return Err(PipelineError::InvalidConfiguration(/* ... */));
        }
        Ok(Self(bytes))
    }

    pub fn from_megabytes(mb: usize) -&gt; Result&lt;Self, PipelineError&gt; {
        Self::new(mb * 1024 * 1024)
    }
}
<span class="boring">}</span></code></pre></pre>
<h4 id="filechunk-value-object"><a class="header" href="#filechunk-value-object">FileChunk Value Object</a></h4>
<p>Immutable representation of a piece of file data.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct FileChunk {
    id: FileChunkId,                   // Unique chunk identifier
    sequence: usize,                   // Position in file
    data: Vec&lt;u8&gt;,                     // Chunk data
    is_final: bool,                    // Last chunk flag
    checksum: Option&lt;String&gt;,          // Integrity verification
}
<span class="boring">}</span></code></pre></pre>
<h4 id="filepath-value-object"><a class="header" href="#filepath-value-object">FilePath Value Object</a></h4>
<p>Type-safe, validated file paths.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct FilePath(PathBuf);

impl FilePath {
    pub fn new(path: impl Into&lt;PathBuf&gt;) -&gt; Result&lt;Self, PipelineError&gt; {
        let path = path.into();
        // Validation:
        // - Path traversal prevention
        // - Null byte checks
        // - Length limits
        // - Encoding validation
        Ok(Self(path))
    }
}
<span class="boring">}</span></code></pre></pre>
<h4 id="pipelineid-stageid-userid-type-safe-ids"><a class="header" href="#pipelineid-stageid-userid-type-safe-ids">PipelineId, StageId, UserId (Type-Safe IDs)</a></h4>
<p>All identifiers are wrapped in newtype value objects for type safety:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct PipelineId(Ulid);  // Can't accidentally use StageId as PipelineId
pub struct StageId(Ulid);
pub struct UserId(Ulid);
pub struct ProcessingContextId(Ulid);
pub struct SecurityContextId(Ulid);
<span class="boring">}</span></code></pre></pre>
<p>This prevents common bugs like passing the wrong ID to a function.</p>
<h3 id="domain-services"><a class="header" href="#domain-services">Domain Services</a></h3>
<p><strong>Domain Services</strong> contain business logic that doesn't naturally fit in an entity or value object. They are stateless and operate on domain objects.</p>
<p>Domain services in our system are defined as traits (interfaces) in the domain layer and implemented in the infrastructure layer.</p>
<h4 id="compressionservice"><a class="header" href="#compressionservice">CompressionService</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[async_trait]
pub trait CompressionService: Send + Sync {
    async fn compress(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt;;

    async fn decompress(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt;;
}
<span class="boring">}</span></code></pre></pre>
<h4 id="encryptionservice"><a class="header" href="#encryptionservice">EncryptionService</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[async_trait]
pub trait EncryptionService: Send + Sync {
    async fn encrypt(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
        key: &amp;EncryptionKey,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt;;

    async fn decrypt(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
        key: &amp;EncryptionKey,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt;;
}
<span class="boring">}</span></code></pre></pre>
<h4 id="checksumservice"><a class="header" href="#checksumservice">ChecksumService</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub trait ChecksumService: Send + Sync {
    fn calculate(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
    ) -&gt; Result&lt;String, PipelineError&gt;;

    fn verify(
        &amp;self,
        data: &amp;[u8],
        expected: &amp;str,
        algorithm: &amp;Algorithm,
    ) -&gt; Result&lt;bool, PipelineError&gt;;
}
<span class="boring">}</span></code></pre></pre>
<h3 id="repositories"><a class="header" href="#repositories">Repositories</a></h3>
<p><strong>Repositories</strong> abstract data persistence, allowing the domain to work with collections without knowing about storage details.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[async_trait]
pub trait PipelineRepository: Send + Sync {
    async fn create(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt;;
    async fn find_by_id(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;Option&lt;Pipeline&gt;, PipelineError&gt;;
    async fn find_by_name(&amp;self, name: &amp;str) -&gt; Result&lt;Option&lt;Pipeline&gt;, PipelineError&gt;;
    async fn update(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt;;
    async fn delete(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;(), PipelineError&gt;;
    async fn list_all(&amp;self) -&gt; Result&lt;Vec&lt;Pipeline&gt;, PipelineError&gt;;
}
<span class="boring">}</span></code></pre></pre>
<p>The repository interface is defined in the domain layer, but implementations live in the infrastructure layer. This follows the Dependency Inversion Principle.</p>
<h3 id="domain-events"><a class="header" href="#domain-events">Domain Events</a></h3>
<p><strong>Domain Events</strong> represent significant business occurrences that other parts of the system might care about.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub enum DomainEvent {
    PipelineCreated {
        pipeline_id: PipelineId,
        name: String,
        created_at: DateTime&lt;Utc&gt;,
    },
    ProcessingStarted {
        pipeline_id: PipelineId,
        context_id: ProcessingContextId,
        input_path: FilePath,
    },
    ProcessingCompleted {
        pipeline_id: PipelineId,
        context_id: ProcessingContextId,
        metrics: ProcessingMetrics,
    },
    ProcessingFailed {
        pipeline_id: PipelineId,
        context_id: ProcessingContextId,
        error: String,
    },
}
<span class="boring">}</span></code></pre></pre>
<p>Events enable:</p>
<ul>
<li><strong>Loose coupling</strong> - Components don't need direct references</li>
<li><strong>Audit trails</strong> - Track all significant operations</li>
<li><strong>Integration</strong> - External systems can react to events</li>
<li><strong>Event sourcing</strong> - Reconstruct state from event history</li>
</ul>
<h2 id="business-rules-and-invariants"><a class="header" href="#business-rules-and-invariants">Business Rules and Invariants</a></h2>
<p>The domain model enforces critical business rules:</p>
<h3 id="pipeline-rules"><a class="header" href="#pipeline-rules">Pipeline Rules</a></h3>
<ol>
<li>
<p><strong>Pipelines must have at least one user-defined stage</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>if user_stages.is_empty() {
    return Err(PipelineError::InvalidConfiguration(
        "Pipeline must have at least one stage".to_string()
    ));
}
<span class="boring">}</span></code></pre></pre>
</li>
<li>
<p><strong>Stage order must be sequential and valid</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Stages are automatically reordered: 0, 1, 2, 3...
// Input checksum = 0
// User stages = 1, 2, 3...
// Output checksum = final
<span class="boring">}</span></code></pre></pre>
</li>
<li>
<p><strong>Pipeline names must be unique</strong> (enforced by repository)</p>
</li>
</ol>
<h3 id="chunk-processing-rules"><a class="header" href="#chunk-processing-rules">Chunk Processing Rules</a></h3>
<ol>
<li>
<p><strong>Chunks must have non-zero size</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>if size == 0 {
    return Err(PipelineError::InvalidChunkSize);
}
<span class="boring">}</span></code></pre></pre>
</li>
<li>
<p><strong>Chunk sequence numbers must be sequential</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Chunks are numbered 0, 1, 2, 3...
// Missing sequences cause processing to fail
<span class="boring">}</span></code></pre></pre>
</li>
<li>
<p><strong>Final chunks must be properly marked</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>if chunk.is_final() {
    // No more chunks should follow
}
<span class="boring">}</span></code></pre></pre>
</li>
</ol>
<h3 id="security-rules"><a class="header" href="#security-rules">Security Rules</a></h3>
<ol>
<li>
<p><strong>Security contexts must be validated</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>security_context.validate()?;
<span class="boring">}</span></code></pre></pre>
</li>
<li>
<p><strong>Encryption keys must meet strength requirements</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>if key.len() &lt; MIN_KEY_LENGTH {
    return Err(PipelineError::WeakEncryptionKey);
}
<span class="boring">}</span></code></pre></pre>
</li>
<li>
<p><strong>Access permissions must be checked</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>if !security_context.has_permission(Permission::ProcessFile) {
    return Err(PipelineError::PermissionDenied);
}
<span class="boring">}</span></code></pre></pre>
</li>
</ol>
<h2 id="ubiquitous-language"><a class="header" href="#ubiquitous-language">Ubiquitous Language</a></h2>
<p>The domain model uses consistent terminology shared between developers and domain experts:</p>
<div class="table-wrapper"><table><thead><tr><th>Term</th><th>Meaning</th></tr></thead><tbody>
<tr><td><strong>Pipeline</strong></td><td>An ordered sequence of processing stages</td></tr>
<tr><td><strong>Stage</strong></td><td>A single processing operation (compress, encrypt, etc.)</td></tr>
<tr><td><strong>Chunk</strong></td><td>A piece of a file processed in parallel</td></tr>
<tr><td><strong>Algorithm</strong></td><td>A specific processing method (zstd, aes-256-gcm, etc.)</td></tr>
<tr><td><strong>Repository</strong></td><td>Storage abstraction for domain objects</td></tr>
<tr><td><strong>Context</strong></td><td>Runtime execution state</td></tr>
<tr><td><strong>Metrics</strong></td><td>Performance and operational measurements</td></tr>
<tr><td><strong>Integrity</strong></td><td>Data verification through checksums</td></tr>
<tr><td><strong>Security Level</strong></td><td>Required protection level (Public, Confidential, Secret)</td></tr>
</tbody></table>
</div>
<h2 id="testing-domain-logic"><a class="header" href="#testing-domain-logic">Testing Domain Logic</a></h2>
<p>Domain objects are designed for easy testing:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn pipeline_enforces_minimum_stages() {
        // Domain logic can be tested without any infrastructure
        let result = Pipeline::new("test".to_string(), vec![]);
        assert!(result.is_err());
    }

    #[test]
    fn algorithm_validates_format() {
        // Value objects self-validate
        let result = Algorithm::new("INVALID-NAME".to_string());
        assert!(result.is_err());

        let result = Algorithm::new("valid-name".to_string());
        assert!(result.is_ok());
    }

    #[test]
    fn chunk_size_enforces_limits() {
        // Business rules are explicit and testable
        let too_small = ChunkSize::new(1);
        assert!(too_small.is_err());

        let valid = ChunkSize::from_megabytes(10);
        assert!(valid.is_ok());
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="benefits-of-this-domain-model"><a class="header" href="#benefits-of-this-domain-model">Benefits of This Domain Model</a></h2>
<ol>
<li><strong>Pure Business Logic</strong> - No infrastructure dependencies</li>
<li><strong>Highly Testable</strong> - Can test without databases, files, or networks</li>
<li><strong>Type Safety</strong> - Strong typing prevents many bugs at compile time</li>
<li><strong>Self-Documenting</strong> - Code structure reflects business concepts</li>
<li><strong>Flexible</strong> - Easy to change infrastructure without touching domain</li>
<li><strong>Maintainable</strong> - Business rules are explicit and centralized</li>
</ol>
<h2 id="next-steps-7"><a class="header" href="#next-steps-7">Next Steps</a></h2>
<p>Now that you understand the domain model:</p>
<ul>
<li><a href="architecture/layered-architecture.html">Layered Architecture</a> - How the domain fits into the overall architecture</li>
<li><a href="architecture/hexagonal-architecture.html">Hexagonal Architecture</a> - Ports and adapters pattern</li>
<li><a href="architecture/repository-pattern.html">Repository Pattern</a> - Data persistence abstraction</li>
<li><a href="architecture/event-driven.html">Domain Events</a> - Event-driven communication</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="entities-2"><a class="header" href="#entities-2">Entities</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Understanding entities in the domain model.</p>
<h2 id="what-are-entities"><a class="header" href="#what-are-entities">What are Entities?</a></h2>
<p>TODO: Define entities</p>
<h2 id="pipeline-entity-1"><a class="header" href="#pipeline-entity-1">Pipeline Entity</a></h2>
<p>TODO: Extract from entities/pipeline.rs</p>
<h2 id="stage-entity"><a class="header" href="#stage-entity">Stage Entity</a></h2>
<p>TODO: Extract from entities/pipeline_stage.rs</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="value-objects-2"><a class="header" href="#value-objects-2">Value Objects</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Understanding value objects in the domain model.</p>
<h2 id="what-are-value-objects"><a class="header" href="#what-are-value-objects">What are Value Objects?</a></h2>
<p>TODO: Define value objects</p>
<h2 id="immutability"><a class="header" href="#immutability">Immutability</a></h2>
<p>TODO: Explain immutability</p>
<h2 id="examples"><a class="header" href="#examples">Examples</a></h2>
<p>TODO: List key value objects</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="aggregates-1"><a class="header" href="#aggregates-1">Aggregates</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Understanding aggregates and aggregate roots.</p>
<h2 id="what-are-aggregates"><a class="header" href="#what-are-aggregates">What are Aggregates?</a></h2>
<p>TODO: Define aggregates</p>
<h2 id="aggregate-boundaries"><a class="header" href="#aggregate-boundaries">Aggregate Boundaries</a></h2>
<p>TODO: Explain boundaries</p>
<h2 id="pipeline-aggregate"><a class="header" href="#pipeline-aggregate">Pipeline Aggregate</a></h2>
<p>TODO: Explain pipeline aggregate</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="design-patterns"><a class="header" href="#design-patterns">Design Patterns</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Design patterns used throughout the pipeline.</p>
<h2 id="pattern-overview"><a class="header" href="#pattern-overview">Pattern Overview</a></h2>
<p>TODO: List patterns</p>
<h2 id="when-to-use-each-pattern"><a class="header" href="#when-to-use-each-pattern">When to Use Each Pattern</a></h2>
<p>TODO: Add guidance</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="repository-pattern"><a class="header" href="#repository-pattern">Repository Pattern</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>The Repository pattern for data persistence.</p>
<h2 id="pattern-overview-1"><a class="header" href="#pattern-overview-1">Pattern Overview</a></h2>
<p>The Repository pattern provides an abstraction layer between the domain and data mapping layers. It acts like an in-memory collection of domain objects, hiding the complexities of database operations.</p>
<p><strong>Key Idea</strong>: Your business logic shouldn't know whether data comes from SQLite, PostgreSQL, or a file. It just uses a <code>Repository</code> trait.</p>
<h2 id="architecture"><a class="header" href="#architecture">Architecture</a></h2>
<p><img src="architecture/../diagrams/repository-pattern.svg" alt="Repository Pattern" /></p>
<h3 id="components"><a class="header" href="#components">Components</a></h3>
<p><strong>Repository Trait</strong> (Domain Layer)</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>trait PipelineRepository {
    fn create(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;()&gt;;
    fn find_by_id(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;Option&lt;Pipeline&gt;&gt;;
    fn update(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;()&gt;;
    fn delete(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;()&gt;;
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Repository Adapter</strong> (Infrastructure Layer)</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct PipelineRepositoryAdapter {
    repository: SQLitePipelineRepository,
}

impl PipelineRepository for PipelineRepositoryAdapter {
    // Implements trait methods
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Concrete Repository</strong> (Infrastructure Layer)</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct SQLitePipelineRepository {
    pool: SqlitePool,
    mapper: PipelineMapper,
}
<span class="boring">}</span></code></pre></pre>
<h2 id="layer-responsibilities"><a class="header" href="#layer-responsibilities">Layer Responsibilities</a></h2>
<h3 id="domain-layer-1"><a class="header" href="#domain-layer-1">Domain Layer</a></h3>
<p>Defines <strong>what</strong> operations are needed:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Domain defines the interface
pub trait PipelineRepository: Send + Sync {
    async fn create(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;()&gt;;
    async fn find_by_id(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;Option&lt;Pipeline&gt;&gt;;
    // ... more methods
}
<span class="boring">}</span></code></pre></pre>
<p>Domain knows:</p>
<ul>
<li>What operations it needs</li>
<li>What domain entities look like</li>
<li>Business rules and validations</li>
</ul>
<p>Domain <strong>doesn't know</strong>:</p>
<ul>
<li>SQL syntax</li>
<li>Database technology</li>
<li>Connection pooling</li>
</ul>
<h3 id="infrastructure-layer-1"><a class="header" href="#infrastructure-layer-1">Infrastructure Layer</a></h3>
<p>Implements <strong>how</strong> to persist data:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl PipelineRepository for PipelineRepositoryAdapter {
    async fn create(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;()&gt; {
        // Convert domain entity to database row
        let row = self.mapper.to_persistence(pipeline);

        // Execute SQL
        sqlx::query("INSERT INTO pipelines ...")
            .execute(&amp;self.pool)
            .await?;

        Ok(())
    }
}
<span class="boring">}</span></code></pre></pre>
<p>Infrastructure knows:</p>
<ul>
<li>SQL syntax and queries</li>
<li>Database schema</li>
<li>Connection management</li>
<li>Error handling</li>
</ul>
<h2 id="data-mapping"><a class="header" href="#data-mapping">Data Mapping</a></h2>
<p>The <strong>Mapper</strong> separates domain models from database schema:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct PipelineMapper;

impl PipelineMapper {
    // Domain → Database
    fn to_persistence(&amp;self, pipeline: &amp;Pipeline) -&gt; PipelineRow {
        PipelineRow {
            id: pipeline.id().to_string(),
            input_path: pipeline.input_path().to_string(),
            // ... map all fields
        }
    }

    // Database → Domain
    fn to_domain(&amp;self, row: SqliteRow) -&gt; Result&lt;Pipeline&gt; {
        Pipeline::new(
            PipelineId::from_string(&amp;row.id)?,
            FilePath::new(&amp;row.input_path)?,
            FilePath::new(&amp;row.output_path)?,
        )
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Why mapping?</strong></p>
<ul>
<li>Domain entities stay pure (no database annotations)</li>
<li>Database schema can change independently</li>
<li>Different databases can use different schemas</li>
<li>Validation happens in domain layer</li>
</ul>
<h2 id="benefits-1"><a class="header" href="#benefits-1">Benefits</a></h2>
<h3 id="1-testability"><a class="header" href="#1-testability">1. Testability</a></h3>
<p>Business logic can be tested without a database:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(test)]
mod tests {
    use mockall::mock;

    mock! {
        PipelineRepo {}

        impl PipelineRepository for PipelineRepo {
            async fn create(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;()&gt;;
            // ... mock other methods
        }
    }

    #[tokio::test]
    async fn test_pipeline_service() {
        let mut mock_repo = MockPipelineRepo::new();
        mock_repo.expect_create()
            .returning(|_| Ok(()));

        let service = PipelineService::new(Arc::new(mock_repo));
        // Test business logic without database
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="2-flexibility"><a class="header" href="#2-flexibility">2. Flexibility</a></h3>
<p>Swap implementations without changing business logic:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Start with SQLite
let repo = SQLitePipelineRepositoryAdapter::new(pool);
let service = PipelineService::new(Arc::new(repo));

// Later, switch to PostgreSQL
let repo = PostgresPipelineRepositoryAdapter::new(pool);
let service = PipelineService::new(Arc::new(repo));
// Business logic unchanged!
<span class="boring">}</span></code></pre></pre>
<h3 id="3-centralized-data-access"><a class="header" href="#3-centralized-data-access">3. Centralized Data Access</a></h3>
<p>All database queries in one place:</p>
<ul>
<li>Easier to optimize</li>
<li>Easier to audit</li>
<li>Easier to cache</li>
<li>Easier to add logging</li>
</ul>
<h3 id="4-domain-purity"><a class="header" href="#4-domain-purity">4. Domain Purity</a></h3>
<p>Domain layer stays technology-agnostic:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Domain doesn't import sqlx, postgres, etc.
// Only depends on standard Rust types
pub struct Pipeline {
    id: PipelineId,           // Not i64 or UUID from database
    input_path: FilePath,     // Not String from database
    status: PipelineStatus,   // Not database enum
}
<span class="boring">}</span></code></pre></pre>
<h2 id="usage-example"><a class="header" href="#usage-example">Usage Example</a></h2>
<h3 id="application-layer-1"><a class="header" href="#application-layer-1">Application Layer</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct PipelineService {
    repository: Arc&lt;dyn PipelineRepository&gt;,
}

impl PipelineService {
    pub async fn create_pipeline(
        &amp;self,
        input: FilePath,
        output: FilePath,
    ) -&gt; Result&lt;Pipeline&gt; {
        // Create domain entity
        let pipeline = Pipeline::new(
            PipelineId::new(),
            input,
            output,
        )?;

        // Persist using repository
        self.repository.create(&amp;pipeline).await?;

        Ok(pipeline)
    }

    pub async fn get_pipeline(
        &amp;self,
        id: PipelineId,
    ) -&gt; Result&lt;Option&lt;Pipeline&gt;&gt; {
        self.repository.find_by_id(&amp;id).await
    }
}
<span class="boring">}</span></code></pre></pre>
<p>The service doesn't know or care:</p>
<ul>
<li>Which database is used</li>
<li>How data is stored</li>
<li>What the SQL looks like</li>
</ul>
<p>It just uses the <code>Repository</code> trait!</p>
<h2 id="implementation-in-pipeline"><a class="header" href="#implementation-in-pipeline">Implementation in Pipeline</a></h2>
<p>Our pipeline uses this pattern for:</p>
<p><strong>PipelineRepository</strong> - Stores pipeline metadata</p>
<ul>
<li><code>pipeline/domain/src/repositories/pipeline_repository.rs</code> (trait)</li>
<li><code>pipeline/src/infrastructure/repositories/sqlite_pipeline_repository.rs</code> (impl)</li>
</ul>
<p><strong>FileChunkRepository</strong> - Stores chunk metadata</p>
<ul>
<li><code>pipeline/domain/src/repositories/file_chunk_repository.rs</code> (trait)</li>
<li><code>pipeline/src/infrastructure/repositories/sqlite_file_chunk_repository.rs</code> (impl)</li>
</ul>
<h2 id="next-steps-8"><a class="header" href="#next-steps-8">Next Steps</a></h2>
<p>Continue to:</p>
<ul>
<li><a href="architecture/service-pattern.html">Service Pattern</a> - Business logic organization</li>
<li><a href="architecture/adapter-pattern.html">Adapter Pattern</a> - Infrastructure integration</li>
<li><a href="architecture/../implementation/repositories.html">Implementation: Repositories</a> - Concrete implementations</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="service-pattern"><a class="header" href="#service-pattern">Service Pattern</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>The Service pattern for domain and application logic.</p>
<h2 id="pattern-overview-2"><a class="header" href="#pattern-overview-2">Pattern Overview</a></h2>
<p>TODO: Extract from service files</p>
<h2 id="domain-vs-application-services"><a class="header" href="#domain-vs-application-services">Domain vs Application Services</a></h2>
<p>TODO: Explain distinction</p>
<h2 id="implementation"><a class="header" href="#implementation">Implementation</a></h2>
<p>TODO: Show examples</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hexagonal-architecture-ports-and-adapters-1"><a class="header" href="#hexagonal-architecture-ports-and-adapters-1">Hexagonal Architecture (Ports and Adapters)</a></h1>
<p><strong>Version:</strong> 1.0
<strong>Date:</strong> 2025-01-04
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner, Claude Code
<strong>Status:</strong> Active</p>
<h2 id="overview-3"><a class="header" href="#overview-3">Overview</a></h2>
<p>Hexagonal Architecture, also known as <strong>Ports and Adapters</strong>, is a pattern that isolates the core business logic (domain) from external concerns. The pipeline system uses this pattern to keep the domain pure and infrastructure replaceable.</p>
<p><img src="architecture/../diagrams/hexagonal-architecture.svg" alt="Hexagonal Architecture" /></p>
<h2 id="the-hexagon-metaphor"><a class="header" href="#the-hexagon-metaphor">The Hexagon Metaphor</a></h2>
<p>Think of your application as a hexagon:</p>
<pre><code class="language-text">                     ┌─────────────────┐
                     │   Primary       │
                     │   Adapters      │
                     │  (Drivers)      │
                     └────────┬────────┘
                              │
        ┌─────────────────────┼─────────────────────┐
        │                     │                     │
        │              ┌──────▼──────┐              │
        │              │             │              │
        │              │   Domain    │              │
        │              │    (Core)   │              │
        │              │             │              │
        │              └──────┬──────┘              │
        │                     │                     │
        └─────────────────────┼─────────────────────┘
                              │
                     ┌────────▼────────┐
                     │   Secondary     │
                     │   Adapters      │
                     │  (Driven)       │
                     └─────────────────┘
</code></pre>
<ul>
<li><strong>The Hexagon (Core)</strong>: Your domain logic - completely independent</li>
<li><strong>Ports</strong>: Interfaces that define how to interact with the core</li>
<li><strong>Adapters</strong>: Implementations that connect the core to the outside world</li>
</ul>
<h2 id="ports-the-interfaces"><a class="header" href="#ports-the-interfaces">Ports: The Interfaces</a></h2>
<p><strong>Ports</strong> are interfaces defined by the domain layer. They specify what the domain needs without caring about implementation details.</p>
<h3 id="primary-ports-driving"><a class="header" href="#primary-ports-driving">Primary Ports (Driving)</a></h3>
<p>Primary ports define <strong>use cases</strong> - what the application can do. External systems drive the application through these ports.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Domain layer defines the interface (port)
#[async_trait]
pub trait FileProcessorService: Send + Sync {
    async fn process_file(
        &amp;self,
        pipeline_id: &amp;PipelineId,
        input_path: &amp;FilePath,
        output_path: &amp;FilePath,
    ) -&gt; Result&lt;ProcessingMetrics, PipelineError&gt;;
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Examples in our system:</strong></p>
<ul>
<li><code>FileProcessorService</code> - File processing operations</li>
<li><code>PipelineService</code> - Pipeline management operations</li>
</ul>
<h3 id="secondary-ports-driven"><a class="header" href="#secondary-ports-driven">Secondary Ports (Driven)</a></h3>
<p>Secondary ports define <strong>dependencies</strong> - what the domain needs from the outside world. The application drives these external systems.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Domain layer defines what it needs (port)
#[async_trait]
pub trait PipelineRepository: Send + Sync {
    async fn create(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt;;
    async fn find_by_id(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;Option&lt;Pipeline&gt;, PipelineError&gt;;
    async fn update(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt;;
    async fn delete(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;(), PipelineError&gt;;
}

#[async_trait]
pub trait CompressionService: Send + Sync {
    async fn compress(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt;;

    async fn decompress(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt;;
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Examples in our system:</strong></p>
<ul>
<li><code>PipelineRepository</code> - Data persistence</li>
<li><code>CompressionService</code> - Data compression</li>
<li><code>EncryptionService</code> - Data encryption</li>
<li><code>ChecksumService</code> - Integrity verification</li>
</ul>
<h2 id="adapters-the-implementations"><a class="header" href="#adapters-the-implementations">Adapters: The Implementations</a></h2>
<p><strong>Adapters</strong> are concrete implementations of ports. They translate between the domain and external systems.</p>
<h3 id="primary-adapters-driving"><a class="header" href="#primary-adapters-driving">Primary Adapters (Driving)</a></h3>
<p>Primary adapters <strong>drive</strong> the application. They take input from the outside world and call the domain.</p>
<h4 id="cli-adapter-mainrs"><a class="header" href="#cli-adapter-mainrs">CLI Adapter (main.rs)</a></h4>
<pre><pre class="playground"><code class="language-rust">// Primary adapter - drives the application
#[tokio::main]
async fn main() -&gt; std::process::ExitCode {
    // 1. Parse user input
    let cli = bootstrap::bootstrap_cli()?;

    // 2. Set up infrastructure (dependency injection)
    let services = setup_services().await?;

    // 3. Drive the domain through primary port
    match cli.command {
        Commands::Process { input, output, pipeline } =&gt; {
            // Call domain through FileProcessorService port
            services.file_processor
                .process_file(&amp;pipeline, &amp;input, &amp;output)
                .await?
        }
        Commands::Create { name, stages } =&gt; {
            // Call domain through PipelineService port
            services.pipeline_service
                .create_pipeline(&amp;name, stages)
                .await?
        }
        // ... more commands
    }
}</code></pre></pre>
<p><strong>Key characteristics:</strong></p>
<ul>
<li>Translates user input to domain operations</li>
<li>Handles presentation concerns (formatting, errors)</li>
<li>Drives the application core</li>
</ul>
<h4 id="http-api-adapter-future"><a class="header" href="#http-api-adapter-future">HTTP API Adapter (future)</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Another primary adapter for HTTP API
async fn handle_process_request(
    req: HttpRequest,
    services: Arc&lt;Services&gt;,
) -&gt; HttpResponse {
    // Parse HTTP request
    let body: ProcessFileRequest = req.json().await?;

    // Drive domain through the same port
    let result = services.file_processor
        .process_file(&amp;body.pipeline_id, &amp;body.input, &amp;body.output)
        .await;

    // Convert result to HTTP response
    match result {
        Ok(metrics) =&gt; HttpResponse::Ok().json(metrics),
        Err(e) =&gt; HttpResponse::BadRequest().json(e),
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Notice:</strong> Both CLI and HTTP adapters use the <strong>same domain ports</strong>. The domain doesn't know or care which adapter is calling it.</p>
<h3 id="secondary-adapters-driven"><a class="header" href="#secondary-adapters-driven">Secondary Adapters (Driven)</a></h3>
<p>Secondary adapters are <strong>driven by</strong> the application. They implement the interfaces the domain needs.</p>
<h4 id="sqlite-repository-adapter"><a class="header" href="#sqlite-repository-adapter">SQLite Repository Adapter</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Infrastructure layer - implements domain port
pub struct SQLitePipelineRepository {
    pool: SqlitePool,
}

#[async_trait]
impl PipelineRepository for SQLitePipelineRepository {
    async fn create(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt; {
        // Convert domain entity to database row
        let row = PipelineRow::from_domain(pipeline);

        // Persist to SQLite
        sqlx::query(
            "INSERT INTO pipelines (id, name, archived, created_at, updated_at)
             VALUES (?, ?, ?, ?, ?)"
        )
        .bind(&amp;row.id)
        .bind(&amp;row.name)
        .bind(row.archived)
        .bind(&amp;row.created_at)
        .bind(&amp;row.updated_at)
        .execute(&amp;self.pool)
        .await
        .map_err(|e| PipelineError::RepositoryError(e.to_string()))?;

        Ok(())
    }

    async fn find_by_id(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;Option&lt;Pipeline&gt;, PipelineError&gt; {
        // Query SQLite
        let row = sqlx::query_as::&lt;_, PipelineRow&gt;(
            "SELECT * FROM pipelines WHERE id = ?"
        )
        .bind(id.to_string())
        .fetch_optional(&amp;self.pool)
        .await
        .map_err(|e| PipelineError::RepositoryError(e.to_string()))?;

        // Convert database row to domain entity
        row.map(|r| Pipeline::from_database(r)).transpose()
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Key characteristics:</strong></p>
<ul>
<li>Implements domain-defined interface</li>
<li>Handles database-specific operations</li>
<li>Translates between domain models and database rows</li>
<li>Can be swapped without changing domain</li>
</ul>
<h4 id="compression-service-adapter"><a class="header" href="#compression-service-adapter">Compression Service Adapter</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Infrastructure layer - implements domain port
pub struct CompressionServiceAdapter {
    // Internal state for compression libraries
}

#[async_trait]
impl CompressionService for CompressionServiceAdapter {
    async fn compress(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        // Route to appropriate compression library
        match algorithm.name() {
            "brotli" =&gt; {
                let mut compressed = Vec::new();
                brotli::BrotliCompress(
                    &amp;mut Cursor::new(data),
                    &amp;mut compressed,
                    &amp;Default::default(),
                )?;
                Ok(compressed)
            }
            "zstd" =&gt; {
                let compressed = zstd::encode_all(data, 3)?;
                Ok(compressed)
            }
            "lz4" =&gt; {
                let compressed = lz4::block::compress(data, None, false)?;
                Ok(compressed)
            }
            _ =&gt; Err(PipelineError::UnsupportedAlgorithm(
                algorithm.name().to_string()
            )),
        }
    }

    async fn decompress(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        // Similar implementation for decompression
        // ...
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Key characteristics:</strong></p>
<ul>
<li>Wraps external libraries (brotli, zstd, lz4)</li>
<li>Implements domain interface</li>
<li>Handles library-specific details</li>
<li>Can be swapped for different implementations</li>
</ul>
<h2 id="benefits-of-hexagonal-architecture"><a class="header" href="#benefits-of-hexagonal-architecture">Benefits of Hexagonal Architecture</a></h2>
<h3 id="1-testability-1"><a class="header" href="#1-testability-1">1. Testability</a></h3>
<p>You can test the domain in isolation using mock adapters:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Mock adapter for testing
struct MockPipelineRepository {
    pipelines: Mutex&lt;HashMap&lt;PipelineId, Pipeline&gt;&gt;,
}

#[async_trait]
impl PipelineRepository for MockPipelineRepository {
    async fn create(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt; {
        self.pipelines.lock().unwrap()
            .insert(pipeline.id().clone(), pipeline.clone());
        Ok(())
    }

    async fn find_by_id(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;Option&lt;Pipeline&gt;, PipelineError&gt; {
        Ok(self.pipelines.lock().unwrap().get(id).cloned())
    }
}

#[tokio::test]
async fn test_file_processor_service() {
    // Use mock adapter instead of real database
    let repo = Arc::new(MockPipelineRepository::new());
    let service = FileProcessorService::new(repo);

    // Test domain logic without database
    let result = service.process_file(/* ... */).await;
    assert!(result.is_ok());
}
<span class="boring">}</span></code></pre></pre>
<h3 id="2-flexibility-1"><a class="header" href="#2-flexibility-1">2. Flexibility</a></h3>
<p>Swap implementations without changing the domain:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Start with SQLite
let repo: Arc&lt;dyn PipelineRepository&gt; =
    Arc::new(SQLitePipelineRepository::new(pool));

// Later, switch to PostgreSQL
let repo: Arc&lt;dyn PipelineRepository&gt; =
    Arc::new(PostgresPipelineRepository::new(pool));

// Domain doesn't change - same interface!
let service = FileProcessorService::new(repo);
<span class="boring">}</span></code></pre></pre>
<h3 id="3-multiple-interfaces"><a class="header" href="#3-multiple-interfaces">3. Multiple Interfaces</a></h3>
<p>Support multiple input sources using the same domain:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// CLI adapter
async fn cli_handler(cli: Cli, services: Arc&lt;Services&gt;) {
    services.file_processor.process_file(/* ... */).await?;
}

// HTTP adapter
async fn http_handler(req: HttpRequest, services: Arc&lt;Services&gt;) {
    services.file_processor.process_file(/* ... */).await?;
}

// gRPC adapter
async fn grpc_handler(req: GrpcRequest, services: Arc&lt;Services&gt;) {
    services.file_processor.process_file(/* ... */).await?;
}
<span class="boring">}</span></code></pre></pre>
<p>All three adapters use the <strong>same domain logic</strong> through the <strong>same port</strong>.</p>
<h3 id="4-technology-independence"><a class="header" href="#4-technology-independence">4. Technology Independence</a></h3>
<p>The domain doesn't depend on specific technologies:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Domain doesn't know about:
// - SQLite, PostgreSQL, or MongoDB
// - HTTP, gRPC, or CLI
// - Brotli, Zstd, or LZ4
// - Any specific framework or library

// It only knows about:
// - Business concepts (Pipeline, Stage, Chunk)
// - Business rules (validation, ordering)
// - Interfaces it needs (Repository, CompressionService)
<span class="boring">}</span></code></pre></pre>
<h2 id="dependency-inversion-1"><a class="header" href="#dependency-inversion-1">Dependency Inversion</a></h2>
<p>Hexagonal Architecture relies on <strong>Dependency Inversion Principle</strong>:</p>
<pre><code class="language-text">Traditional:                    Hexagonal:

┌──────────┐                   ┌──────────┐
│   CLI    │                   │   CLI    │
└────┬─────┘                   └────┬─────┘
     │ depends on                   │ depends on
     ▼                              ▼
┌──────────┐                   ┌──────────┐
│ Domain   │                   │  Port    │ ← Interface
└────┬─────┘                   │ (trait)  │
     │ depends on               └────△─────┘
     ▼                               │ implements
┌──────────┐                   ┌────┴─────┐
│ Database │                   │  Domain  │
└──────────┘                   └──────────┘
                                     △
                                     │ implements
                               ┌─────┴─────┐
                               │ Database  │
                               │ Adapter   │
                               └───────────┘
</code></pre>
<p><strong>Traditional:</strong> Domain depends on Database (tight coupling)
<strong>Hexagonal:</strong> Database depends on Domain interface (loose coupling)</p>
<h2 id="our-adapter-structure"><a class="header" href="#our-adapter-structure">Our Adapter Structure</a></h2>
<pre><code class="language-text">pipeline/src/
├── infrastructure/
│   └── adapters/
│       ├── compression_service_adapter.rs    # Implements CompressionService
│       ├── encryption_service_adapter.rs     # Implements EncryptionService
│       ├── async_compression_adapter.rs      # Async wrapper
│       ├── async_encryption_adapter.rs       # Async wrapper
│       └── repositories/
│           ├── sqlite_repository_adapter.rs  # Implements PipelineRepository
│           └── sqlite_base_repository.rs     # Base repository utilities
</code></pre>
<h2 id="adapter-responsibilities"><a class="header" href="#adapter-responsibilities">Adapter Responsibilities</a></h2>
<h3 id="what-adapters-should-do"><a class="header" href="#what-adapters-should-do">What Adapters Should Do</a></h3>
<p>✅ <strong>Translate</strong> between domain and external systems
✅ <strong>Handle</strong> technology-specific details
✅ <strong>Implement</strong> domain-defined interfaces
✅ <strong>Convert</strong> data formats (domain ↔ database, domain ↔ API)
✅ <strong>Manage</strong> external resources (connections, files, etc.)</p>
<h3 id="what-adapters-should-not-do"><a class="header" href="#what-adapters-should-not-do">What Adapters Should NOT Do</a></h3>
<p>❌ <strong>Contain business logic</strong> - belongs in domain
❌ <strong>Make business decisions</strong> - belongs in domain
❌ <strong>Validate business rules</strong> - belongs in domain
❌ <strong>Know about other adapters</strong> - should be independent
❌ <strong>Expose infrastructure details</strong> to domain</p>
<h2 id="example-complete-flow"><a class="header" href="#example-complete-flow">Example: Complete Flow</a></h2>
<p>Let's trace a complete request through the hexagonal architecture:</p>
<pre><code class="language-text">1. Primary Adapter (CLI)
   ↓ User types: pipeline process --input file.txt --output file.bin

2. Parse and validate input
   ↓ Create FilePath("/path/to/file.txt")

3. Call Primary Port (FileProcessorService)
   ↓ process_file(pipeline_id, input_path, output_path)

4. Domain Logic
   ├─ Fetch Pipeline (via PipelineRepository port)
   │  └─ Secondary Adapter queries SQLite
   ├─ Process each stage
   │  ├─ Compress (via CompressionService port)
   │  │  └─ Secondary Adapter uses brotli library
   │  ├─ Encrypt (via EncryptionService port)
   │  │  └─ Secondary Adapter uses aes-gcm library
   │  └─ Calculate checksum (via ChecksumService port)
   │     └─ Secondary Adapter uses sha2 library
   └─ Return ProcessingMetrics

5. Primary Adapter formats output
   ↓ Display metrics to user
</code></pre>
<h2 id="common-adapter-patterns"><a class="header" href="#common-adapter-patterns">Common Adapter Patterns</a></h2>
<h3 id="repository-adapter-pattern"><a class="header" href="#repository-adapter-pattern">Repository Adapter Pattern</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 1. Domain defines interface (port)
pub trait PipelineRepository: Send + Sync {
    async fn find_by_id(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;Option&lt;Pipeline&gt;&gt;;
}

// 2. Infrastructure implements adapter
pub struct SQLitePipelineRepository { /* ... */ }

impl PipelineRepository for SQLitePipelineRepository {
    async fn find_by_id(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;Option&lt;Pipeline&gt;&gt; {
        // Database-specific implementation
    }
}

// 3. Application uses through interface
pub struct FileProcessorService {
    repository: Arc&lt;dyn PipelineRepository&gt;,  // Uses interface, not concrete type
}
<span class="boring">}</span></code></pre></pre>
<h3 id="service-adapter-pattern"><a class="header" href="#service-adapter-pattern">Service Adapter Pattern</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 1. Domain defines interface
pub trait CompressionService: Send + Sync {
    async fn compress(&amp;self, data: &amp;[u8], algo: &amp;Algorithm) -&gt; Result&lt;Vec&lt;u8&gt;&gt;;
}

// 2. Infrastructure implements adapter
pub struct CompressionServiceAdapter { /* ... */ }

impl CompressionService for CompressionServiceAdapter {
    async fn compress(&amp;self, data: &amp;[u8], algo: &amp;Algorithm) -&gt; Result&lt;Vec&lt;u8&gt;&gt; {
        // Library-specific implementation
    }
}

// 3. Application uses through interface
pub struct StageExecutor {
    compression: Arc&lt;dyn CompressionService&gt;,  // Uses interface
}
<span class="boring">}</span></code></pre></pre>
<h2 id="testing-with-adapters"><a class="header" href="#testing-with-adapters">Testing with Adapters</a></h2>
<h3 id="unit-tests-domain-layer"><a class="header" href="#unit-tests-domain-layer">Unit Tests (Domain Layer)</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Test domain logic without any adapters
#[test]
fn test_pipeline_validation() {
    // Pure domain logic - no infrastructure needed
    let result = Pipeline::new("test", vec![]);
    assert!(result.is_err());
}
<span class="boring">}</span></code></pre></pre>
<h3 id="integration-tests-with-mock-adapters"><a class="header" href="#integration-tests-with-mock-adapters">Integration Tests (With Mock Adapters)</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_file_processing() {
    // Use mock adapters
    let mock_repo = Arc::new(MockPipelineRepository::new());
    let mock_compression = Arc::new(MockCompressionService::new());

    let service = FileProcessorService::new(mock_repo, mock_compression);

    // Test without real database or compression
    let result = service.process_file(/* ... */).await;
    assert!(result.is_ok());
}
<span class="boring">}</span></code></pre></pre>
<h3 id="end-to-end-tests-with-real-adapters"><a class="header" href="#end-to-end-tests-with-real-adapters">End-to-End Tests (With Real Adapters)</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_real_file_processing() {
    // Use real adapters
    let db_pool = create_test_database().await;
    let real_repo = Arc::new(SQLitePipelineRepository::new(db_pool));
    let real_compression = Arc::new(CompressionServiceAdapter::new());

    let service = FileProcessorService::new(real_repo, real_compression);

    // Test with real infrastructure
    let result = service.process_file(/* ... */).await;
    assert!(result.is_ok());
}
<span class="boring">}</span></code></pre></pre>
<h2 id="next-steps-9"><a class="header" href="#next-steps-9">Next Steps</a></h2>
<p>Now that you understand hexagonal architecture:</p>
<ul>
<li><a href="architecture/dependencies.html">Dependency Inversion</a> - Managing dependencies properly</li>
<li><a href="architecture/layers.html">Layered Architecture</a> - How layers relate to ports/adapters</li>
<li><a href="architecture/repository-pattern.html">Repository Pattern</a> - Detailed repository implementation</li>
<li><a href="architecture/domain-model.html">Domain Model</a> - Understanding the core domain</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="observer-pattern"><a class="header" href="#observer-pattern">Observer Pattern</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>The Observer pattern for metrics and events.</p>
<h2 id="pattern-overview-3"><a class="header" href="#pattern-overview-3">Pattern Overview</a></h2>
<p>TODO: Extract from metrics observer</p>
<h2 id="implementation-1"><a class="header" href="#implementation-1">Implementation</a></h2>
<p>TODO: Show implementation</p>
<h2 id="use-cases"><a class="header" href="#use-cases">Use Cases</a></h2>
<p>TODO: List use cases</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="stage-processing"><a class="header" href="#stage-processing">Stage Processing</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>This chapter provides a comprehensive overview of the stage processing architecture in the adaptive pipeline system. Stages are the fundamental building blocks that transform data as it flows through a pipeline.</p>
<hr />
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ul>
<li><a href="implementation/stages.html#overview">Overview</a></li>
<li><a href="implementation/stages.html#stage-types">Stage Types</a></li>
<li><a href="implementation/stages.html#stage-entity">Stage Entity</a></li>
<li><a href="implementation/stages.html#stage-configuration">Stage Configuration</a></li>
<li><a href="implementation/stages.html#stage-lifecycle">Stage Lifecycle</a></li>
<li><a href="implementation/stages.html#stage-execution-model">Stage Execution Model</a></li>
<li><a href="implementation/stages.html#stage-executor-interface">Stage Executor Interface</a></li>
<li><a href="implementation/stages.html#compatibility-and-ordering">Compatibility and Ordering</a></li>
<li><a href="implementation/stages.html#resource-management">Resource Management</a></li>
<li><a href="implementation/stages.html#usage-examples">Usage Examples</a></li>
<li><a href="implementation/stages.html#performance-considerations">Performance Considerations</a></li>
<li><a href="implementation/stages.html#best-practices">Best Practices</a></li>
<li><a href="implementation/stages.html#troubleshooting">Troubleshooting</a></li>
<li><a href="implementation/stages.html#testing-strategies">Testing Strategies</a></li>
<li><a href="implementation/stages.html#next-steps">Next Steps</a></li>
</ul>
<hr />
<h2 id="overview-4"><a class="header" href="#overview-4">Overview</a></h2>
<p><strong>Stages</strong> are individual processing steps within a pipeline that transform file chunks as data flows from input to output. Each stage performs a specific operation such as compression, encryption, or integrity checking.</p>
<h3 id="key-characteristics-4"><a class="header" href="#key-characteristics-4">Key Characteristics</a></h3>
<ul>
<li><strong>Type Safety</strong>: Strongly-typed stage operations prevent configuration errors</li>
<li><strong>Ordering</strong>: Explicit ordering ensures predictable execution sequence</li>
<li><strong>Lifecycle Management</strong>: Stages track creation and modification timestamps</li>
<li><strong>State Management</strong>: Stages can be enabled/disabled without removal</li>
<li><strong>Resource Awareness</strong>: Stages provide resource estimation and management</li>
</ul>
<h3 id="stage-processing-architecture"><a class="header" href="#stage-processing-architecture">Stage Processing Architecture</a></h3>
<pre><code class="language-text">┌─────────────────────────────────────────────────────────────┐
│                        Pipeline                             │
│                                                             │
│  ┌────────────┐  ┌────────────┐  ┌────────────┐           │
│  │  Stage 1   │  │  Stage 2   │  │  Stage 3   │           │
│  │ Checksum   │→ │ Compress   │→ │  Encrypt   │→ Output  │
│  │ (Order 0)  │  │ (Order 1)  │  │ (Order 2)  │           │
│  └────────────┘  └────────────┘  └────────────┘           │
│        ↑               ↑               ↑                    │
│        └───────────────┴───────────────┘                    │
│              Stage Executor                                 │
└─────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="design-principles"><a class="header" href="#design-principles">Design Principles</a></h3>
<ol>
<li><strong>Domain-Driven Design</strong>: Stages are domain entities with identity</li>
<li><strong>Separation of Concerns</strong>: Configuration separated from execution</li>
<li><strong>Async-First</strong>: All operations are asynchronous for scalability</li>
<li><strong>Extensibility</strong>: New stage types can be added through configuration</li>
</ol>
<hr />
<h2 id="stage-types-1"><a class="header" href="#stage-types-1">Stage Types</a></h2>
<p>The pipeline supports five distinct stage types, each optimized for different data transformation operations.</p>
<h3 id="stagetype-enum"><a class="header" href="#stagetype-enum">StageType Enum</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum StageType {
    /// Compression or decompression operations
    Compression,

    /// Encryption or decryption operations
    Encryption,

    /// Data transformation operations
    Transform,

    /// Checksum calculation and verification
    Checksum,

    /// Pass-through stage that doesn't modify data
    PassThrough,
}
<span class="boring">}</span></code></pre></pre>
<h3 id="stage-type-details"><a class="header" href="#stage-type-details">Stage Type Details</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Stage Type</th><th>Purpose</th><th>Examples</th><th>Typical Use Case</th></tr></thead><tbody>
<tr><td><strong>Compression</strong></td><td>Reduce data size</td><td>Brotli, Gzip, Zstd, Lz4</td><td>Minimize storage/bandwidth</td></tr>
<tr><td><strong>Encryption</strong></td><td>Secure data</td><td>AES-256-GCM, ChaCha20</td><td>Data protection</td></tr>
<tr><td><strong>Transform</strong></td><td>Modify structure</td><td>Format conversion</td><td>Data reshaping</td></tr>
<tr><td><strong>Checksum</strong></td><td>Verify integrity</td><td>SHA-256, SHA-512, Blake3</td><td>Data validation</td></tr>
<tr><td><strong>PassThrough</strong></td><td>No modification</td><td>Identity transform</td><td>Testing/debugging</td></tr>
</tbody></table>
</div>
<h3 id="parsing-stage-types"><a class="header" href="#parsing-stage-types">Parsing Stage Types</a></h3>
<p>Stage types support case-insensitive parsing from strings:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::entities::pipeline_stage::StageType;
use std::str::FromStr;

// Parse from lowercase
let compression = StageType::from_str("compression").unwrap();
assert_eq!(compression, StageType::Compression);

// Case-insensitive parsing
let encryption = StageType::from_str("ENCRYPTION").unwrap();
assert_eq!(encryption, StageType::Encryption);

// Display format
assert_eq!(format!("{}", StageType::Checksum), "checksum");
<span class="boring">}</span></code></pre></pre>
<h3 id="pattern-matching"><a class="header" href="#pattern-matching">Pattern Matching</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn describe_stage(stage_type: StageType) -&gt; &amp;'static str {
    match stage_type {
        StageType::Compression =&gt; "Reduces data size",
        StageType::Encryption =&gt; "Secures data",
        StageType::Transform =&gt; "Modifies data structure",
        StageType::Checksum =&gt; "Verifies data integrity",
        StageType::PassThrough =&gt; "No modification",
    }
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="stage-entity-1"><a class="header" href="#stage-entity-1">Stage Entity</a></h2>
<p>The <code>PipelineStage</code> is a domain entity that encapsulates a specific data transformation operation within a pipeline.</p>
<h3 id="entity-structure"><a class="header" href="#entity-structure">Entity Structure</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PipelineStage {
    id: StageId,
    name: String,
    stage_type: StageType,
    configuration: StageConfiguration,
    enabled: bool,
    order: u32,
    created_at: chrono::DateTime&lt;chrono::Utc&gt;,
    updated_at: chrono::DateTime&lt;chrono::Utc&gt;,
}
<span class="boring">}</span></code></pre></pre>
<h3 id="entity-characteristics"><a class="header" href="#entity-characteristics">Entity Characteristics</a></h3>
<ul>
<li><strong>Identity</strong>: Unique <code>StageId</code> persists through configuration changes</li>
<li><strong>Name</strong>: Human-readable identifier (must not be empty)</li>
<li><strong>Type</strong>: Strongly-typed operation (Compression, Encryption, etc.)</li>
<li><strong>Configuration</strong>: Algorithm-specific parameters</li>
<li><strong>Enabled Flag</strong>: Controls execution without removal</li>
<li><strong>Order</strong>: Determines execution sequence (0-based)</li>
<li><strong>Timestamps</strong>: Track creation and modification times</li>
</ul>
<h3 id="creating-a-stage"><a class="header" href="#creating-a-stage">Creating a Stage</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::entities::pipeline_stage::{PipelineStage, StageConfiguration, StageType};
use std::collections::HashMap;

let mut params = HashMap::new();
params.insert("level".to_string(), "6".to_string());

let config = StageConfiguration::new("brotli".to_string(), params, true);
let stage = PipelineStage::new(
    "compression".to_string(),
    StageType::Compression,
    config,
    0  // Order: execute first
).unwrap();

assert_eq!(stage.name(), "compression");
assert_eq!(stage.stage_type(), &amp;StageType::Compression);
assert_eq!(stage.algorithm(), "brotli");
assert!(stage.is_enabled());
<span class="boring">}</span></code></pre></pre>
<h3 id="modifying-stage-state"><a class="header" href="#modifying-stage-state">Modifying Stage State</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut stage = PipelineStage::new(
    "checksum".to_string(),
    StageType::Checksum,
    StageConfiguration::default(),
    0,
).unwrap();

// Disable the stage temporarily
stage.set_enabled(false);
assert!(!stage.is_enabled());

// Update configuration
let mut new_params = HashMap::new();
new_params.insert("algorithm".to_string(), "sha512".to_string());
let new_config = StageConfiguration::new("sha512".to_string(), new_params, true);
stage.update_configuration(new_config);

// Change execution order
stage.update_order(2);
assert_eq!(stage.order(), 2);

// Re-enable the stage
stage.set_enabled(true);
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="stage-configuration-1"><a class="header" href="#stage-configuration-1">Stage Configuration</a></h2>
<p>Each stage has a configuration that specifies how data should be transformed.</p>
<h3 id="configuration-structure"><a class="header" href="#configuration-structure">Configuration Structure</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StageConfiguration {
    pub algorithm: String,
    pub parameters: HashMap&lt;String, String&gt;,
    pub parallel_processing: bool,
    pub chunk_size: Option&lt;usize&gt;,
}
<span class="boring">}</span></code></pre></pre>
<h3 id="configuration-parameters"><a class="header" href="#configuration-parameters">Configuration Parameters</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Description</th><th>Default</th></tr></thead><tbody>
<tr><td><code>algorithm</code></td><td>String</td><td>Algorithm name (e.g., "brotli", "aes256gcm")</td><td>"default"</td></tr>
<tr><td><code>parameters</code></td><td>HashMap</td><td>Algorithm-specific key-value parameters</td><td>{}</td></tr>
<tr><td><code>parallel_processing</code></td><td>bool</td><td>Enable parallel chunk processing</td><td>true</td></tr>
<tr><td><code>chunk_size</code></td><td>Option&lt;usize&gt;</td><td>Custom chunk size (1KB - 100MB)</td><td>None</td></tr>
</tbody></table>
</div>
<h3 id="compression-configuration"><a class="header" href="#compression-configuration">Compression Configuration</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut params = HashMap::new();
params.insert("level".to_string(), "9".to_string());

let config = StageConfiguration::new(
    "zstd".to_string(),
    params,
    true,  // Enable parallel processing
);
<span class="boring">}</span></code></pre></pre>
<h3 id="encryption-configuration"><a class="header" href="#encryption-configuration">Encryption Configuration</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut params = HashMap::new();
params.insert("key_size".to_string(), "256".to_string());

let config = StageConfiguration::new(
    "aes256gcm".to_string(),
    params,
    false,  // Sequential processing for encryption
);
<span class="boring">}</span></code></pre></pre>
<h3 id="default-configuration"><a class="header" href="#default-configuration">Default Configuration</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let config = StageConfiguration::default();
// algorithm: "default"
// parameters: {}
// parallel_processing: true
// chunk_size: None
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="stage-lifecycle"><a class="header" href="#stage-lifecycle">Stage Lifecycle</a></h2>
<p>Stages progress through several lifecycle phases from creation to execution.</p>
<h3 id="lifecycle-phases"><a class="header" href="#lifecycle-phases">Lifecycle Phases</a></h3>
<pre><code class="language-text">1. Creation
   ↓
2. Configuration
   ↓
3. Ordering
   ↓
4. Execution
   ↓
5. Monitoring
</code></pre>
<h3 id="1-creation-phase"><a class="header" href="#1-creation-phase">1. Creation Phase</a></h3>
<p>Stages are created with initial configuration:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let stage = PipelineStage::new(
    "compression".to_string(),
    StageType::Compression,
    StageConfiguration::default(),
    0,
)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="2-configuration-phase"><a class="header" href="#2-configuration-phase">2. Configuration Phase</a></h3>
<p>Parameters can be updated as needed:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>stage.update_configuration(new_config);
// updated_at timestamp is automatically updated
<span class="boring">}</span></code></pre></pre>
<h3 id="3-ordering-phase"><a class="header" href="#3-ordering-phase">3. Ordering Phase</a></h3>
<p>Position in pipeline can be adjusted:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>stage.update_order(1);
// Stage now executes second instead of first
<span class="boring">}</span></code></pre></pre>
<h3 id="4-execution-phase"><a class="header" href="#4-execution-phase">4. Execution Phase</a></h3>
<p>Stage processes data according to its configuration:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let executor: Arc&lt;dyn StageExecutor&gt; = /* ... */;
let result = executor.execute(&amp;stage, chunk, &amp;mut context).await?;
<span class="boring">}</span></code></pre></pre>
<h3 id="5-monitoring-phase"><a class="header" href="#5-monitoring-phase">5. Monitoring Phase</a></h3>
<p>Timestamps track when changes occur:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>println!("Created: {}", stage.created_at());
println!("Last modified: {}", stage.updated_at());
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="stage-execution-model"><a class="header" href="#stage-execution-model">Stage Execution Model</a></h2>
<p>The stage executor processes file chunks through configured stages using two primary execution modes.</p>
<h3 id="single-chunk-processing"><a class="header" href="#single-chunk-processing">Single Chunk Processing</a></h3>
<p>Process individual chunks sequentially:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn execute(
    &amp;self,
    stage: &amp;PipelineStage,
    chunk: FileChunk,
    context: &amp;mut ProcessingContext,
) -&gt; Result&lt;FileChunk, PipelineError&gt;
<span class="boring">}</span></code></pre></pre>
<p><strong>Execution Flow:</strong></p>
<pre><code class="language-text">Input Chunk → Validate → Process → Update Context → Output Chunk
</code></pre>
<h3 id="parallel-processing-1"><a class="header" href="#parallel-processing-1">Parallel Processing</a></h3>
<p>Process multiple chunks concurrently:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn execute_parallel(
    &amp;self,
    stage: &amp;PipelineStage,
    chunks: Vec&lt;FileChunk&gt;,
    context: &amp;mut ProcessingContext,
) -&gt; Result&lt;Vec&lt;FileChunk&gt;, PipelineError&gt;
<span class="boring">}</span></code></pre></pre>
<p><strong>Execution Flow:</strong></p>
<pre><code class="language-text">Chunks: [1, 2, 3, 4]
         ↓  ↓  ↓  ↓
      ┌────┬───┬───┬────┐
      │ T1 │T2 │T3 │ T4 │  (Parallel threads)
      └────┴───┴───┴────┘
         ↓  ↓  ↓  ↓
Results: [1, 2, 3, 4]
</code></pre>
<h3 id="processing-context"><a class="header" href="#processing-context">Processing Context</a></h3>
<p>The <code>ProcessingContext</code> maintains state during execution:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct ProcessingContext {
    pub pipeline_id: String,
    pub stage_metrics: HashMap&lt;String, StageMetrics&gt;,
    pub checksums: HashMap&lt;String, Vec&lt;u8&gt;&gt;,
    // ... other context fields
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="stage-executor-interface"><a class="header" href="#stage-executor-interface">Stage Executor Interface</a></h2>
<p>The <code>StageExecutor</code> trait defines the contract for stage execution engines.</p>
<h3 id="trait-definition"><a class="header" href="#trait-definition">Trait Definition</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[async_trait]
pub trait StageExecutor: Send + Sync {
    /// Execute a stage on a single chunk
    async fn execute(
        &amp;self,
        stage: &amp;PipelineStage,
        chunk: FileChunk,
        context: &amp;mut ProcessingContext,
    ) -&gt; Result&lt;FileChunk, PipelineError&gt;;

    /// Execute a stage on multiple chunks in parallel
    async fn execute_parallel(
        &amp;self,
        stage: &amp;PipelineStage,
        chunks: Vec&lt;FileChunk&gt;,
        context: &amp;mut ProcessingContext,
    ) -&gt; Result&lt;Vec&lt;FileChunk&gt;, PipelineError&gt;;

    /// Validate if a stage can be executed
    async fn can_execute(&amp;self, stage: &amp;PipelineStage) -&gt; Result&lt;bool, PipelineError&gt;;

    /// Get supported stage types
    fn supported_stage_types(&amp;self) -&gt; Vec&lt;String&gt;;

    /// Estimate processing time for a stage
    async fn estimate_processing_time(
        &amp;self,
        stage: &amp;PipelineStage,
        data_size: u64,
    ) -&gt; Result&lt;std::time::Duration, PipelineError&gt;;

    /// Get resource requirements for a stage
    async fn get_resource_requirements(
        &amp;self,
        stage: &amp;PipelineStage,
        data_size: u64,
    ) -&gt; Result&lt;ResourceRequirements, PipelineError&gt;;
}
<span class="boring">}</span></code></pre></pre>
<h3 id="basicstageexecutor-implementation"><a class="header" href="#basicstageexecutor-implementation">BasicStageExecutor Implementation</a></h3>
<p>The infrastructure layer provides a concrete implementation:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct BasicStageExecutor {
    checksums: Arc&lt;RwLock&lt;HashMap&lt;String, Sha256&gt;&gt;&gt;,
    compression_service: Arc&lt;dyn CompressionService&gt;,
    encryption_service: Arc&lt;dyn EncryptionService&gt;,
}

impl BasicStageExecutor {
    pub fn new(
        compression_service: Arc&lt;dyn CompressionService&gt;,
        encryption_service: Arc&lt;dyn EncryptionService&gt;,
    ) -&gt; Self {
        Self {
            checksums: Arc::new(RwLock::new(HashMap::new())),
            compression_service,
            encryption_service,
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="supported-stage-types"><a class="header" href="#supported-stage-types">Supported Stage Types</a></h3>
<p>The <code>BasicStageExecutor</code> supports:</p>
<ul>
<li><strong>Compression</strong>: Via <code>CompressionService</code> (Brotli, Gzip, Zstd, Lz4)</li>
<li><strong>Encryption</strong>: Via <code>EncryptionService</code> (AES-256-GCM, ChaCha20-Poly1305)</li>
<li><strong>Checksum</strong>: Via internal SHA-256 implementation</li>
</ul>
<hr />
<h2 id="compatibility-and-ordering"><a class="header" href="#compatibility-and-ordering">Compatibility and Ordering</a></h2>
<p>Stages have compatibility rules that ensure optimal pipeline performance.</p>
<h3 id="recommended-ordering"><a class="header" href="#recommended-ordering">Recommended Ordering</a></h3>
<pre><code class="language-text">1. Input Checksum (automatic)
   ↓
2. Compression (reduces data size)
   ↓
3. Encryption (secures compressed data)
   ↓
4. Output Checksum (automatic)
</code></pre>
<p><strong>Rationale:</strong></p>
<ul>
<li>Compress before encrypting to reduce encrypted payload size</li>
<li>Checksum before compression to detect input corruption early</li>
<li>Checksum after encryption to verify output integrity</li>
</ul>
<h3 id="compatibility-matrix"><a class="header" href="#compatibility-matrix">Compatibility Matrix</a></h3>
<pre><code class="language-text">From \ To      | Compression | Encryption | Checksum | PassThrough | Transform
---------------|-------------|------------|----------|-------------|----------
Compression    | ❌ No       | ✅ Yes     | ✅ Yes   | ✅ Yes      | ⚠️ Rare
Encryption     | ❌ No       | ❌ No      | ✅ Yes   | ✅ Yes      | ❌ No
Checksum       | ✅ Yes      | ✅ Yes     | ✅ Yes   | ✅ Yes      | ✅ Yes
PassThrough    | ✅ Yes      | ✅ Yes     | ✅ Yes   | ✅ Yes      | ✅ Yes
Transform      | ✅ Yes      | ✅ Yes     | ✅ Yes   | ✅ Yes      | ⚠️ Depends
</code></pre>
<p><strong>Legend:</strong></p>
<ul>
<li>✅ Yes: Recommended combination</li>
<li>❌ No: Not recommended (avoid duplication or inefficiency)</li>
<li>⚠️ Rare/Depends: Context-dependent</li>
</ul>
<h3 id="checking-compatibility"><a class="header" href="#checking-compatibility">Checking Compatibility</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let compression = PipelineStage::new(
    "compression".to_string(),
    StageType::Compression,
    StageConfiguration::default(),
    0,
).unwrap();

let encryption = PipelineStage::new(
    "encryption".to_string(),
    StageType::Encryption,
    StageConfiguration::default(),
    1,
).unwrap();

// Compression should come before encryption
assert!(compression.is_compatible_with(&amp;encryption));
<span class="boring">}</span></code></pre></pre>
<h3 id="compatibility-rules"><a class="header" href="#compatibility-rules">Compatibility Rules</a></h3>
<p>The <code>is_compatible_with</code> method implements these rules:</p>
<ol>
<li><strong>Compression → Encryption</strong>: ✅ Compress first, then encrypt</li>
<li><strong>Compression → Compression</strong>: ❌ Avoid double compression</li>
<li><strong>Encryption → Encryption</strong>: ❌ Avoid double encryption</li>
<li><strong>Encryption → Compression</strong>: ❌ Cannot compress encrypted data effectively</li>
<li><strong>PassThrough → Any</strong>: ✅ No restrictions</li>
<li><strong>Checksum → Any</strong>: ✅ Checksums compatible with everything</li>
</ol>
<hr />
<h2 id="resource-management"><a class="header" href="#resource-management">Resource Management</a></h2>
<p>Stages provide resource estimation and requirements to enable efficient execution planning.</p>
<h3 id="resource-requirements"><a class="header" href="#resource-requirements">Resource Requirements</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Debug, Clone)]
pub struct ResourceRequirements {
    pub memory_bytes: u64,
    pub cpu_cores: u32,
    pub disk_space_bytes: u64,
    pub network_bandwidth_bps: Option&lt;u64&gt;,
    pub gpu_memory_bytes: Option&lt;u64&gt;,
    pub estimated_duration: std::time::Duration,
}
<span class="boring">}</span></code></pre></pre>
<h3 id="default-requirements"><a class="header" href="#default-requirements">Default Requirements</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>ResourceRequirements::default()
// memory_bytes: 64 MB
// cpu_cores: 1
// disk_space_bytes: 0
// network_bandwidth_bps: None
// gpu_memory_bytes: None
// estimated_duration: 1 second
<span class="boring">}</span></code></pre></pre>
<h3 id="custom-requirements"><a class="header" href="#custom-requirements">Custom Requirements</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let requirements = ResourceRequirements::new(
    128 * 1024 * 1024,  // 128 MB memory
    4,                   // 4 CPU cores
    1024 * 1024 * 1024, // 1 GB disk space
)
.with_duration(Duration::from_secs(30))
.with_network_bandwidth(100_000_000); // 100 Mbps
<span class="boring">}</span></code></pre></pre>
<h3 id="estimating-resources"><a class="header" href="#estimating-resources">Estimating Resources</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let executor: Arc&lt;dyn StageExecutor&gt; = /* ... */;
let requirements = executor.get_resource_requirements(
    &amp;stage,
    10 * 1024 * 1024,  // 10 MB data size
).await?;

println!("Memory required: {}", Byte::from_bytes(requirements.memory_bytes));
println!("CPU cores: {}", requirements.cpu_cores);
println!("Estimated time: {:?}", requirements.estimated_duration);
<span class="boring">}</span></code></pre></pre>
<h3 id="scaling-requirements"><a class="header" href="#scaling-requirements">Scaling Requirements</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut requirements = ResourceRequirements::default();
requirements.scale(2.0);  // Double all requirements
<span class="boring">}</span></code></pre></pre>
<h3 id="merging-requirements"><a class="header" href="#merging-requirements">Merging Requirements</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut req1 = ResourceRequirements::default();
let req2 = ResourceRequirements::new(256_000_000, 2, 0);
req1.merge(&amp;req2);  // Takes maximum of each field
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="usage-examples"><a class="header" href="#usage-examples">Usage Examples</a></h2>
<h3 id="example-1-creating-a-compression-stage"><a class="header" href="#example-1-creating-a-compression-stage">Example 1: Creating a Compression Stage</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::entities::pipeline_stage::{PipelineStage, StageConfiguration, StageType};
use std::collections::HashMap;

let mut params = HashMap::new();
params.insert("level".to_string(), "9".to_string());

let config = StageConfiguration::new(
    "zstd".to_string(),
    params,
    true,  // Enable parallel processing
);

let compression_stage = PipelineStage::new(
    "fast-compression".to_string(),
    StageType::Compression,
    config,
    1,  // Execute after input checksum (order 0)
)?;

println!("Created stage: {}", compression_stage.name());
println!("Algorithm: {}", compression_stage.algorithm());
<span class="boring">}</span></code></pre></pre>
<h3 id="example-2-creating-an-encryption-stage"><a class="header" href="#example-2-creating-an-encryption-stage">Example 2: Creating an Encryption Stage</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut params = HashMap::new();
params.insert("key_size".to_string(), "256".to_string());

let config = StageConfiguration::new(
    "aes256gcm".to_string(),
    params,
    false,  // Sequential processing for security
);

let encryption_stage = PipelineStage::new(
    "secure-encryption".to_string(),
    StageType::Encryption,
    config,
    2,  // Execute after compression
)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="example-3-building-a-complete-pipeline"><a class="header" href="#example-3-building-a-complete-pipeline">Example 3: Building a Complete Pipeline</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut stages = Vec::new();

// Stage 0: Input checksum
let checksum_in = PipelineStage::new(
    "input-checksum".to_string(),
    StageType::Checksum,
    StageConfiguration::new("sha256".to_string(), HashMap::new(), true),
    0,
)?;
stages.push(checksum_in);

// Stage 1: Compression
let mut compress_params = HashMap::new();
compress_params.insert("level".to_string(), "6".to_string());
let compression = PipelineStage::new(
    "compression".to_string(),
    StageType::Compression,
    StageConfiguration::new("brotli".to_string(), compress_params, true),
    1,
)?;
stages.push(compression);

// Stage 2: Encryption
let mut encrypt_params = HashMap::new();
encrypt_params.insert("key_size".to_string(), "256".to_string());
let encryption = PipelineStage::new(
    "encryption".to_string(),
    StageType::Encryption,
    StageConfiguration::new("aes256gcm".to_string(), encrypt_params, false),
    2,
)?;
stages.push(encryption);

// Stage 3: Output checksum
let checksum_out = PipelineStage::new(
    "output-checksum".to_string(),
    StageType::Checksum,
    StageConfiguration::new("sha256".to_string(), HashMap::new(), true),
    3,
)?;
stages.push(checksum_out);

// Validate compatibility
for i in 0..stages.len() - 1 {
    assert!(stages[i].is_compatible_with(&amp;stages[i + 1]));
}
<span class="boring">}</span></code></pre></pre>
<h3 id="example-4-executing-a-stage"><a class="header" href="#example-4-executing-a-stage">Example 4: Executing a Stage</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::repositories::stage_executor::StageExecutor;

let executor: Arc&lt;dyn StageExecutor&gt; = /* ... */;
let stage = /* ... */;
let chunk = FileChunk::new(0, vec![1, 2, 3, 4, 5]);
let mut context = ProcessingContext::new("pipeline-123");

// Execute single chunk
let result = executor.execute(&amp;stage, chunk, &amp;mut context).await?;

println!("Processed {} bytes", result.data().len());
<span class="boring">}</span></code></pre></pre>
<h3 id="example-5-parallel-execution"><a class="header" href="#example-5-parallel-execution">Example 5: Parallel Execution</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let chunks = vec![
    FileChunk::new(0, vec![1, 2, 3]),
    FileChunk::new(1, vec![4, 5, 6]),
    FileChunk::new(2, vec![7, 8, 9]),
];

let results = executor.execute_parallel(&amp;stage, chunks, &amp;mut context).await?;

println!("Processed {} chunks", results.len());
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="performance-considerations"><a class="header" href="#performance-considerations">Performance Considerations</a></h2>
<h3 id="chunk-size-selection"><a class="header" href="#chunk-size-selection">Chunk Size Selection</a></h3>
<p>Chunk size significantly impacts stage performance:</p>
<div class="table-wrapper"><table><thead><tr><th>Data Size</th><th>Recommended Chunk Size</th><th>Rationale</th></tr></thead><tbody>
<tr><td>&lt; 10 MB</td><td>1 MB</td><td>Minimize overhead</td></tr>
<tr><td>10-100 MB</td><td>2-4 MB</td><td>Balance memory/IO</td></tr>
<tr><td>100 MB - 1 GB</td><td>4-8 MB</td><td>Optimize parallelization</td></tr>
<tr><td>&gt; 1 GB</td><td>8-16 MB</td><td>Maximize throughput</td></tr>
</tbody></table>
</div>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut config = StageConfiguration::default();
config.chunk_size = Some(4 * 1024 * 1024);  // 4 MB chunks
<span class="boring">}</span></code></pre></pre>
<h3 id="parallel-processing-2"><a class="header" href="#parallel-processing-2">Parallel Processing</a></h3>
<p>Enable parallel processing for CPU-bound operations:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Compression: parallel processing beneficial
let compress_config = StageConfiguration::new(
    "zstd".to_string(),
    HashMap::new(),
    true,  // Enable parallel
);

// Encryption: sequential often better for security
let encrypt_config = StageConfiguration::new(
    "aes256gcm".to_string(),
    HashMap::new(),
    false,  // Disable parallel
);
<span class="boring">}</span></code></pre></pre>
<h3 id="stage-ordering-impact"><a class="header" href="#stage-ordering-impact">Stage Ordering Impact</a></h3>
<p><strong>Optimal:</strong></p>
<pre><code class="language-text">Checksum → Compress (6:1 ratio) → Encrypt → Checksum
1 GB → 1 GB → 167 MB → 167 MB → 167 MB
</code></pre>
<p><strong>Suboptimal:</strong></p>
<pre><code class="language-text">Checksum → Encrypt → Compress (1.1:1 ratio) → Checksum
1 GB → 1 GB → 1 GB → 909 MB → 909 MB
</code></pre>
<p>Encrypting before compression reduces compression ratio from 6:1 to 1.1:1.</p>
<h3 id="memory-usage"><a class="header" href="#memory-usage">Memory Usage</a></h3>
<p>Per-stage memory usage:</p>
<div class="table-wrapper"><table><thead><tr><th>Stage Type</th><th>Memory per Chunk</th><th>Notes</th></tr></thead><tbody>
<tr><td>Compression</td><td>2-3x chunk size</td><td>Compression buffers</td></tr>
<tr><td>Encryption</td><td>1-1.5x chunk size</td><td>Encryption overhead</td></tr>
<tr><td>Checksum</td><td>~256 bytes</td><td>Hash state only</td></tr>
<tr><td>PassThrough</td><td>1x chunk size</td><td>No additional memory</td></tr>
</tbody></table>
</div>
<h3 id="cpu-utilization"><a class="header" href="#cpu-utilization">CPU Utilization</a></h3>
<p>CPU-intensive stages:</p>
<ol>
<li><strong>Compression</strong>: High CPU usage (especially Brotli level 9+)</li>
<li><strong>Encryption</strong>: Moderate CPU usage (AES-NI acceleration helps)</li>
<li><strong>Checksum</strong>: Low CPU usage (Blake3 faster than SHA-256)</li>
</ol>
<hr />
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<h3 id="1-stage-naming"><a class="header" href="#1-stage-naming">1. Stage Naming</a></h3>
<p>Use descriptive, kebab-case names:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good
"input-checksum", "fast-compression", "secure-encryption"

// ❌ Bad
"stage1", "s", "MyStage"
<span class="boring">}</span></code></pre></pre>
<h3 id="2-configuration-validation"><a class="header" href="#2-configuration-validation">2. Configuration Validation</a></h3>
<p>Always validate configurations:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let stage = PipelineStage::new(/* ... */)?;
stage.validate()?;  // Validate before execution
<span class="boring">}</span></code></pre></pre>
<h3 id="3-optimal-ordering"><a class="header" href="#3-optimal-ordering">3. Optimal Ordering</a></h3>
<p>Follow the recommended order:</p>
<pre><code class="language-text">1. Input Checksum
2. Compression
3. Encryption
4. Output Checksum
</code></pre>
<h3 id="4-enabledisable-vs-remove"><a class="header" href="#4-enabledisable-vs-remove">4. Enable/Disable vs. Remove</a></h3>
<p>Prefer disabling over removing stages:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Preserve configuration
stage.set_enabled(false);

// ❌ Bad: Lose configuration
stages.retain(|s| s.name() != "compression");
<span class="boring">}</span></code></pre></pre>
<h3 id="5-resource-estimation"><a class="header" href="#5-resource-estimation">5. Resource Estimation</a></h3>
<p>Estimate resources before execution:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let requirements = executor.get_resource_requirements(&amp;stage, file_size).await?;

if requirements.memory_bytes &gt; available_memory {
    // Adjust chunk size or process sequentially
}
<span class="boring">}</span></code></pre></pre>
<h3 id="6-error-handling"><a class="header" href="#6-error-handling">6. Error Handling</a></h3>
<p>Handle stage-specific errors appropriately:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>match executor.execute(&amp;stage, chunk, &amp;mut context).await {
    Ok(result) =&gt; { /* success */ },
    Err(PipelineError::CompressionFailed(msg)) =&gt; {
        // Handle compression errors
    },
    Err(PipelineError::EncryptionFailed(msg)) =&gt; {
        // Handle encryption errors
    },
    Err(e) =&gt; {
        // Handle generic errors
    },
}
<span class="boring">}</span></code></pre></pre>
<h3 id="7-monitoring"><a class="header" href="#7-monitoring">7. Monitoring</a></h3>
<p>Track stage execution metrics:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let start = Instant::now();
let result = executor.execute(&amp;stage, chunk, &amp;mut context).await?;
let duration = start.elapsed();

println!("Stage '{}' processed {} bytes in {:?}",
    stage.name(),
    result.data().len(),
    duration
);
<span class="boring">}</span></code></pre></pre>
<h3 id="8-testing"><a class="header" href="#8-testing">8. Testing</a></h3>
<p>Test stages in isolation:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_compression_stage() {
    let stage = create_compression_stage();
    let executor = create_test_executor();
    let chunk = FileChunk::new(0, vec![0u8; 1024]);
    let mut context = ProcessingContext::new("test");

    let result = executor.execute(&amp;stage, chunk, &amp;mut context).await.unwrap();

    assert!(result.data().len() &lt; 1024);  // Compression worked
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h2>
<h3 id="issue-1-stage-validation-fails"><a class="header" href="#issue-1-stage-validation-fails">Issue 1: Stage Validation Fails</a></h3>
<p><strong>Symptom:</strong></p>
<pre><code class="language-text">Error: InvalidConfiguration("Stage name cannot be empty")
</code></pre>
<p><strong>Solution:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Ensure stage name is not empty
let stage = PipelineStage::new(
    "my-stage".to_string(),  // ✅ Non-empty name
    stage_type,
    config,
    order,
)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-2-incompatible-stage-order"><a class="header" href="#issue-2-incompatible-stage-order">Issue 2: Incompatible Stage Order</a></h3>
<p><strong>Symptom:</strong></p>
<pre><code class="language-text">Error: IncompatibleStages("Cannot encrypt before compressing")
</code></pre>
<p><strong>Solution:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Check compatibility before adding stages
if !previous_stage.is_compatible_with(&amp;new_stage) {
    // Reorder stages
}
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-3-chunk-size-validation-error"><a class="header" href="#issue-3-chunk-size-validation-error">Issue 3: Chunk Size Validation Error</a></h3>
<p><strong>Symptom:</strong></p>
<pre><code class="language-text">Error: InvalidConfiguration("Chunk size must be between 1KB and 100MB")
</code></pre>
<p><strong>Solution:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut config = StageConfiguration::default();
config.chunk_size = Some(4 * 1024 * 1024);  // ✅ 4 MB (valid range)
// config.chunk_size = Some(512);  // ❌ Too small (&lt; 1KB)
// config.chunk_size = Some(200_000_000);  // ❌ Too large (&gt; 100MB)
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-4-out-of-memory-during-execution"><a class="header" href="#issue-4-out-of-memory-during-execution">Issue 4: Out of Memory During Execution</a></h3>
<p><strong>Symptom:</strong></p>
<pre><code class="language-text">Error: ResourceExhaustion("Insufficient memory for stage execution")
</code></pre>
<p><strong>Solution:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Reduce chunk size or disable parallel processing
let mut config = stage.configuration().clone();
config.chunk_size = Some(1 * 1024 * 1024);  // Reduce to 1 MB
config.parallel_processing = false;  // Disable parallel
stage.update_configuration(config);
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-5-stage-executor-not-found"><a class="header" href="#issue-5-stage-executor-not-found">Issue 5: Stage Executor Not Found</a></h3>
<p><strong>Symptom:</strong></p>
<pre><code class="language-text">Error: ExecutorNotFound("No executor for stage type 'CustomStage'")
</code></pre>
<p><strong>Solution:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Check supported stage types
let supported = executor.supported_stage_types();
println!("Supported: {:?}", supported);

// Use a supported stage type
let stage = PipelineStage::new(
    "compression".to_string(),
    StageType::Compression,  // ✅ Supported type
    config,
    0,
)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-6-performance-degradation"><a class="header" href="#issue-6-performance-degradation">Issue 6: Performance Degradation</a></h3>
<p><strong>Symptom:</strong> Stage execution is slower than expected.</p>
<p><strong>Diagnosis:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let requirements = executor.get_resource_requirements(&amp;stage, file_size).await?;
let duration = executor.estimate_processing_time(&amp;stage, file_size).await?;

println!("Expected duration: {:?}", duration);
println!("Memory needed: {}", Byte::from_bytes(requirements.memory_bytes));
<span class="boring">}</span></code></pre></pre>
<p><strong>Solutions:</strong></p>
<ul>
<li>Enable parallel processing for compression stages</li>
<li>Increase chunk size for large files</li>
<li>Use faster algorithms (e.g., Lz4 instead of Brotli)</li>
<li>Check system resource availability</li>
</ul>
<hr />
<h2 id="testing-strategies"><a class="header" href="#testing-strategies">Testing Strategies</a></h2>
<h3 id="unit-tests"><a class="header" href="#unit-tests">Unit Tests</a></h3>
<p>Test individual stage operations:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[test]
fn test_stage_creation() {
    let stage = PipelineStage::new(
        "test-stage".to_string(),
        StageType::Compression,
        StageConfiguration::default(),
        0,
    );
    assert!(stage.is_ok());
}

#[test]
fn test_stage_validation() {
    let stage = PipelineStage::new(
        "".to_string(),  // Empty name
        StageType::Compression,
        StageConfiguration::default(),
        0,
    );
    assert!(stage.is_err());
}
<span class="boring">}</span></code></pre></pre>
<h3 id="integration-tests"><a class="header" href="#integration-tests">Integration Tests</a></h3>
<p>Test stage execution with real executors:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_compression_integration() {
    let compression_service = create_compression_service();
    let encryption_service = create_encryption_service();
    let executor = BasicStageExecutor::new(compression_service, encryption_service);

    let stage = create_compression_stage();
    let chunk = FileChunk::new(0, vec![0u8; 10000]);
    let mut context = ProcessingContext::new("test-pipeline");

    let result = executor.execute(&amp;stage, chunk, &amp;mut context).await.unwrap();

    assert!(result.data().len() &lt; 10000);  // Verify compression
}
<span class="boring">}</span></code></pre></pre>
<h3 id="property-based-tests"><a class="header" href="#property-based-tests">Property-Based Tests</a></h3>
<p>Test stage invariants:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[quickcheck]
fn stage_order_preserved(order: u32) -&gt; bool {
    let stage = PipelineStage::new(
        "test".to_string(),
        StageType::Checksum,
        StageConfiguration::default(),
        order,
    ).unwrap();

    stage.order() == order
}
<span class="boring">}</span></code></pre></pre>
<h3 id="compatibility-tests"><a class="header" href="#compatibility-tests">Compatibility Tests</a></h3>
<p>Test stage compatibility matrix:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[test]
fn test_compression_encryption_compatibility() {
    let compression = create_stage(StageType::Compression, 0);
    let encryption = create_stage(StageType::Encryption, 1);

    assert!(compression.is_compatible_with(&amp;encryption));
    assert!(encryption.is_compatible_with(&amp;create_stage(StageType::Checksum, 2)));
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="next-steps-10"><a class="header" href="#next-steps-10">Next Steps</a></h2>
<p>After understanding stage processing fundamentals, explore specific implementations:</p>
<h3 id="detailed-stage-implementations"><a class="header" href="#detailed-stage-implementations">Detailed Stage Implementations</a></h3>
<ol>
<li><strong><a href="implementation/compression.html">Compression</a></strong>: Deep dive into compression algorithms and performance tuning</li>
<li><strong><a href="implementation/encryption.html">Encryption</a></strong>: Encryption implementation, key management, and security considerations</li>
<li><strong><a href="implementation/integrity.html">Integrity Checking</a></strong>: Checksum algorithms and verification strategies</li>
</ol>
<h3 id="related-topics"><a class="header" href="#related-topics">Related Topics</a></h3>
<ul>
<li><strong><a href="implementation/persistence.html">Data Persistence</a></strong>: How stages are persisted and retrieved from the database</li>
<li><strong><a href="implementation/file-io.html">File I/O</a></strong>: File chunking and binary format for stage data</li>
<li><strong><a href="implementation/observability.html">Observability</a></strong>: Monitoring stage execution and performance</li>
</ul>
<h3 id="advanced-topics"><a class="header" href="#advanced-topics">Advanced Topics</a></h3>
<ul>
<li><strong><a href="implementation/../advanced/concurrency.html">Concurrency Model</a></strong>: Parallel stage execution and thread pooling</li>
<li><strong><a href="implementation/../advanced/performance.html">Performance Optimization</a></strong>: Benchmarking and profiling stages</li>
<li><strong><a href="implementation/../advanced/custom-stages.html">Custom Stages</a></strong>: Implementing custom stage types</li>
</ul>
<hr />
<h2 id="summary"><a class="header" href="#summary">Summary</a></h2>
<p><strong>Key Takeaways:</strong></p>
<ol>
<li><strong>Stages</strong> are the fundamental building blocks of pipelines, each performing a specific transformation</li>
<li><strong>Five stage types</strong> are supported: Compression, Encryption, Transform, Checksum, PassThrough</li>
<li><strong>PipelineStage</strong> is a domain entity with identity, configuration, and lifecycle management</li>
<li><strong>Stage compatibility</strong> rules ensure optimal ordering (compress before encrypt)</li>
<li><strong>StageExecutor</strong> trait provides async execution with resource estimation</li>
<li><strong>Resource management</strong> enables efficient execution planning and monitoring</li>
<li><strong>Best practices</strong> include proper naming, validation, and error handling</li>
</ol>
<p><strong>Configuration File Reference:</strong> <code>pipeline/src/domain/entities/pipeline_stage.rs</code>
<strong>Executor Interface:</strong> <code>pipeline-domain/src/repositories/stage_executor.rs:156</code>
<strong>Executor Implementation:</strong> <code>pipeline/src/infrastructure/repositories/stage_executor.rs:175</code></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="compression-implementation"><a class="header" href="#compression-implementation">Compression Implementation</a></h1>
<p><strong>Version:</strong> 1.0
<strong>Date:</strong> 2025-01-04
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner, Claude Code
<strong>Status:</strong> Active</p>
<h2 id="overview-5"><a class="header" href="#overview-5">Overview</a></h2>
<p>The compression service provides multiple compression algorithms optimized for different use cases. It's implemented as an infrastructure adapter that implements the domain's <code>CompressionService</code> trait.</p>
<p><strong>File:</strong> <code>pipeline/src/infrastructure/adapters/compression_service_adapter.rs</code></p>
<h2 id="supported-algorithms"><a class="header" href="#supported-algorithms">Supported Algorithms</a></h2>
<h3 id="brotli"><a class="header" href="#brotli">Brotli</a></h3>
<ul>
<li><strong>Best for:</strong> Web content, text files, logs</li>
<li><strong>Compression ratio:</strong> Excellent (typically 15-25% better than gzip)</li>
<li><strong>Speed:</strong> Slower compression, fast decompression</li>
<li><strong>Memory:</strong> Higher memory usage (~10-20 MB)</li>
<li><strong>Library:</strong> <code>brotli</code> crate</li>
</ul>
<p><strong>Use cases:</strong></p>
<ul>
<li>Archival storage where size is critical</li>
<li>Web assets (HTML, CSS, JavaScript)</li>
<li>Log files with repetitive patterns</li>
</ul>
<p><strong>Performance characteristics:</strong></p>
<pre><code class="language-text">File Type    | Compression Ratio | Speed      | Memory
-------------|-------------------|------------|--------
Text logs    | 85-90%           | Slow       | High
HTML/CSS     | 80-85%           | Slow       | High
Binary data  | 60-70%           | Moderate   | High
</code></pre>
<h3 id="gzip"><a class="header" href="#gzip">Gzip</a></h3>
<ul>
<li><strong>Best for:</strong> General-purpose compression</li>
<li><strong>Compression ratio:</strong> Good (industry standard)</li>
<li><strong>Speed:</strong> Moderate compression and decompression</li>
<li><strong>Memory:</strong> Moderate usage (~5-10 MB)</li>
<li><strong>Library:</strong> <code>flate2</code> crate</li>
</ul>
<p><strong>Use cases:</strong></p>
<ul>
<li>General file compression</li>
<li>Compatibility with other systems</li>
<li>Balanced performance needs</li>
</ul>
<p><strong>Performance characteristics:</strong></p>
<pre><code class="language-text">File Type    | Compression Ratio | Speed      | Memory
-------------|-------------------|------------|--------
Text logs    | 75-80%           | Moderate   | Moderate
HTML/CSS     | 70-75%           | Moderate   | Moderate
Binary data  | 50-60%           | Moderate   | Moderate
</code></pre>
<h3 id="zstandard-zstd"><a class="header" href="#zstandard-zstd">Zstandard (Zstd)</a></h3>
<ul>
<li><strong>Best for:</strong> Modern systems, real-time compression</li>
<li><strong>Compression ratio:</strong> Very good (better than gzip)</li>
<li><strong>Speed:</strong> Very fast compression and decompression</li>
<li><strong>Memory:</strong> Efficient (~5-15 MB depending on level)</li>
<li><strong>Library:</strong> <code>zstd</code> crate</li>
</ul>
<p><strong>Use cases:</strong></p>
<ul>
<li>Real-time data processing</li>
<li>Large file compression</li>
<li>Network transmission</li>
<li>Modern backup systems</li>
</ul>
<p><strong>Performance characteristics:</strong></p>
<pre><code class="language-text">File Type    | Compression Ratio | Speed      | Memory
-------------|-------------------|------------|--------
Text logs    | 80-85%           | Fast       | Low
HTML/CSS     | 75-80%           | Fast       | Low
Binary data  | 55-65%           | Fast       | Low
</code></pre>
<h3 id="lz4"><a class="header" href="#lz4">LZ4</a></h3>
<ul>
<li><strong>Best for:</strong> Real-time applications, live streams</li>
<li><strong>Compression ratio:</strong> Moderate</li>
<li><strong>Speed:</strong> Extremely fast (fastest available)</li>
<li><strong>Memory:</strong> Very low usage (~1-5 MB)</li>
<li><strong>Library:</strong> <code>lz4</code> crate</li>
</ul>
<p><strong>Use cases:</strong></p>
<ul>
<li>Real-time data streams</li>
<li>Low-latency requirements</li>
<li>Systems with limited memory</li>
<li>Network protocols</li>
</ul>
<p><strong>Performance characteristics:</strong></p>
<pre><code class="language-text">File Type    | Compression Ratio | Speed         | Memory
-------------|-------------------|---------------|--------
Text logs    | 60-70%           | Very Fast     | Very Low
HTML/CSS     | 55-65%           | Very Fast     | Very Low
Binary data  | 40-50%           | Very Fast     | Very Low
</code></pre>
<h2 id="architecture-1"><a class="header" href="#architecture-1">Architecture</a></h2>
<h3 id="service-interface-domain-layer"><a class="header" href="#service-interface-domain-layer">Service Interface (Domain Layer)</a></h3>
<p>The domain layer defines what compression operations are needed:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// pipeline-domain/src/services/compression_service.rs
use async_trait::async_trait;
use crate::value_objects::Algorithm;
use crate::error::PipelineError;

#[async_trait]
pub trait CompressionService: Send + Sync {
    /// Compress data using the specified algorithm
    async fn compress(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt;;

    /// Decompress data using the specified algorithm
    async fn decompress(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt;;
}
<span class="boring">}</span></code></pre></pre>
<h3 id="service-implementation-infrastructure-layer"><a class="header" href="#service-implementation-infrastructure-layer">Service Implementation (Infrastructure Layer)</a></h3>
<p>The infrastructure layer provides the concrete implementation:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// pipeline/src/infrastructure/adapters/compression_service_adapter.rs
pub struct CompressionServiceAdapter {
    // Configuration and state
}

#[async_trait]
impl CompressionService for CompressionServiceAdapter {
    async fn compress(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        // Route to appropriate algorithm
        match algorithm.name() {
            "brotli" =&gt; self.compress_brotli(data),
            "gzip" =&gt; self.compress_gzip(data),
            "zstd" =&gt; self.compress_zstd(data),
            "lz4" =&gt; self.compress_lz4(data),
            _ =&gt; Err(PipelineError::UnsupportedAlgorithm(
                algorithm.name().to_string()
            )),
        }
    }

    async fn decompress(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        // Route to appropriate algorithm
        match algorithm.name() {
            "brotli" =&gt; self.decompress_brotli(data),
            "gzip" =&gt; self.decompress_gzip(data),
            "zstd" =&gt; self.decompress_zstd(data),
            "lz4" =&gt; self.decompress_lz4(data),
            _ =&gt; Err(PipelineError::UnsupportedAlgorithm(
                algorithm.name().to_string()
            )),
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="algorithm-implementations"><a class="header" href="#algorithm-implementations">Algorithm Implementations</a></h2>
<h3 id="brotli-implementation"><a class="header" href="#brotli-implementation">Brotli Implementation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl CompressionServiceAdapter {
    fn compress_brotli(&amp;self, data: &amp;[u8]) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        use brotli::enc::BrotliEncoderParams;
        use std::io::Cursor;

        let mut compressed = Vec::new();
        let mut params = BrotliEncoderParams::default();

        // Quality level 11 = maximum compression
        params.quality = 11;

        brotli::BrotliCompress(
            &amp;mut Cursor::new(data),
            &amp;mut compressed,
            &amp;params,
        ).map_err(|e| PipelineError::CompressionError(e.to_string()))?;

        Ok(compressed)
    }

    fn decompress_brotli(&amp;self, data: &amp;[u8]) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        use brotli::Decompressor;
        use std::io::Read;

        let mut decompressed = Vec::new();
        let mut decompressor = Decompressor::new(data, 4096);

        decompressor.read_to_end(&amp;mut decompressed)
            .map_err(|e| PipelineError::DecompressionError(e.to_string()))?;

        Ok(decompressed)
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="gzip-implementation"><a class="header" href="#gzip-implementation">Gzip Implementation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl CompressionServiceAdapter {
    fn compress_gzip(&amp;self, data: &amp;[u8]) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        use flate2::write::GzEncoder;
        use flate2::Compression;
        use std::io::Write;

        let mut encoder = GzEncoder::new(Vec::new(), Compression::default());
        encoder.write_all(data)
            .map_err(|e| PipelineError::CompressionError(e.to_string()))?;

        encoder.finish()
            .map_err(|e| PipelineError::CompressionError(e.to_string()))
    }

    fn decompress_gzip(&amp;self, data: &amp;[u8]) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        use flate2::read::GzDecoder;
        use std::io::Read;

        let mut decoder = GzDecoder::new(data);
        let mut decompressed = Vec::new();

        decoder.read_to_end(&amp;mut decompressed)
            .map_err(|e| PipelineError::DecompressionError(e.to_string()))?;

        Ok(decompressed)
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="zstandard-implementation"><a class="header" href="#zstandard-implementation">Zstandard Implementation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl CompressionServiceAdapter {
    fn compress_zstd(&amp;self, data: &amp;[u8]) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        // Level 3 provides good balance of speed and compression
        zstd::encode_all(data, 3)
            .map_err(|e| PipelineError::CompressionError(e.to_string()))
    }

    fn decompress_zstd(&amp;self, data: &amp;[u8]) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        zstd::decode_all(data)
            .map_err(|e| PipelineError::DecompressionError(e.to_string()))
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="lz4-implementation"><a class="header" href="#lz4-implementation">LZ4 Implementation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl CompressionServiceAdapter {
    fn compress_lz4(&amp;self, data: &amp;[u8]) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        lz4::block::compress(data, None, false)
            .map_err(|e| PipelineError::CompressionError(e.to_string()))
    }

    fn decompress_lz4(&amp;self, data: &amp;[u8]) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        // Need to know original size for LZ4
        // This is stored in the file metadata
        lz4::block::decompress(data, None)
            .map_err(|e| PipelineError::DecompressionError(e.to_string()))
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="performance-optimizations"><a class="header" href="#performance-optimizations">Performance Optimizations</a></h2>
<h3 id="parallel-chunk-processing-1"><a class="header" href="#parallel-chunk-processing-1">Parallel Chunk Processing</a></h3>
<p>The compression service processes file chunks in parallel using Rayon:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use rayon::prelude::*;

pub async fn compress_chunks(
    chunks: Vec&lt;FileChunk&gt;,
    algorithm: &amp;Algorithm,
    compression_service: &amp;Arc&lt;dyn CompressionService&gt;,
) -&gt; Result&lt;Vec&lt;CompressedChunk&gt;, PipelineError&gt; {
    // Process chunks in parallel
    chunks.par_iter()
        .map(|chunk| {
            // Compress each chunk independently
            let compressed_data = compression_service
                .compress(&amp;chunk.data, algorithm)?;

            Ok(CompressedChunk {
                sequence: chunk.sequence,
                data: compressed_data,
                original_size: chunk.data.len(),
            })
        })
        .collect()
}
<span class="boring">}</span></code></pre></pre>
<h3 id="memory-management"><a class="header" href="#memory-management">Memory Management</a></h3>
<p>Efficient buffer management reduces allocations:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct CompressionBuffer {
    input_buffer: Vec&lt;u8&gt;,
    output_buffer: Vec&lt;u8&gt;,
}

impl CompressionBuffer {
    pub fn new(chunk_size: usize) -&gt; Self {
        Self {
            // Pre-allocate buffers
            input_buffer: Vec::with_capacity(chunk_size),
            output_buffer: Vec::with_capacity(chunk_size * 2), // Assume 2x for safety
        }
    }

    pub fn compress(&amp;mut self, data: &amp;[u8], algorithm: &amp;Algorithm) -&gt; Result&lt;&amp;[u8]&gt; {
        // Reuse buffers instead of allocating new ones
        self.input_buffer.clear();
        self.output_buffer.clear();

        self.input_buffer.extend_from_slice(data);
        // Compress from input_buffer to output_buffer
        // ...

        Ok(&amp;self.output_buffer)
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="adaptive-compression-levels"><a class="header" href="#adaptive-compression-levels">Adaptive Compression Levels</a></h3>
<p>Adjust compression levels based on data characteristics:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn select_compression_level(data: &amp;[u8]) -&gt; u32 {
    // Analyze data entropy
    let entropy = calculate_entropy(data);

    if entropy &lt; 0.5 {
        // Low entropy (highly repetitive) - use maximum compression
        11
    } else if entropy &lt; 0.7 {
        // Medium entropy - balanced compression
        6
    } else {
        // High entropy (random-like) - fast compression
        3
    }
}

fn calculate_entropy(data: &amp;[u8]) -&gt; f64 {
    // Calculate Shannon entropy
    let mut freq = [0u32; 256];
    for &amp;byte in data {
        freq[byte as usize] += 1;
    }

    let len = data.len() as f64;
    freq.iter()
        .filter(|&amp;&amp;f| f &gt; 0)
        .map(|&amp;f| {
            let p = f as f64 / len;
            -p * p.log2()
        })
        .sum()
}
<span class="boring">}</span></code></pre></pre>
<h2 id="configuration"><a class="header" href="#configuration">Configuration</a></h2>
<h3 id="compression-levels"><a class="header" href="#compression-levels">Compression Levels</a></h3>
<p>Different algorithms support different compression levels:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct CompressionConfig {
    pub algorithm: Algorithm,
    pub level: CompressionLevel,
    pub chunk_size: usize,
    pub parallel_chunks: usize,
}

pub enum CompressionLevel {
    Fastest,      // LZ4, Zstd level 1
    Fast,         // Zstd level 3, Gzip level 1
    Balanced,     // Zstd level 6, Gzip level 6
    Best,         // Brotli level 11, Gzip level 9
    BestSize,     // Brotli level 11 with maximum window
}

impl CompressionConfig {
    pub fn for_speed() -&gt; Self {
        Self {
            algorithm: Algorithm::lz4(),
            level: CompressionLevel::Fastest,
            chunk_size: 64 * 1024 * 1024, // 64 MB chunks
            parallel_chunks: num_cpus::get(),
        }
    }

    pub fn for_size() -&gt; Self {
        Self {
            algorithm: Algorithm::brotli(),
            level: CompressionLevel::BestSize,
            chunk_size: 4 * 1024 * 1024, // 4 MB chunks for better compression
            parallel_chunks: num_cpus::get(),
        }
    }

    pub fn balanced() -&gt; Self {
        Self {
            algorithm: Algorithm::zstd(),
            level: CompressionLevel::Balanced,
            chunk_size: 16 * 1024 * 1024, // 16 MB chunks
            parallel_chunks: num_cpus::get(),
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="error-handling"><a class="header" href="#error-handling">Error Handling</a></h2>
<p>Comprehensive error handling for compression failures:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Debug, thiserror::Error)]
pub enum CompressionError {
    #[error("Compression failed: {0}")]
    CompressionFailed(String),

    #[error("Decompression failed: {0}")]
    DecompressionFailed(String),

    #[error("Unsupported algorithm: {0}")]
    UnsupportedAlgorithm(String),

    #[error("Invalid compression level: {0}")]
    InvalidLevel(u32),

    #[error("Buffer overflow during compression")]
    BufferOverflow,

    #[error("Corrupted compressed data")]
    CorruptedData,
}

impl From&lt;CompressionError&gt; for PipelineError {
    fn from(err: CompressionError) -&gt; Self {
        match err {
            CompressionError::CompressionFailed(msg) =&gt;
                PipelineError::CompressionError(msg),
            CompressionError::DecompressionFailed(msg) =&gt;
                PipelineError::DecompressionError(msg),
            CompressionError::UnsupportedAlgorithm(algo) =&gt;
                PipelineError::UnsupportedAlgorithm(algo),
            _ =&gt; PipelineError::CompressionError(err.to_string()),
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="usage-examples-1"><a class="header" href="#usage-examples-1">Usage Examples</a></h2>
<h3 id="basic-compression"><a class="header" href="#basic-compression">Basic Compression</a></h3>
<pre><pre class="playground"><code class="language-rust">use pipeline::infrastructure::adapters::CompressionServiceAdapter;
use pipeline_domain::services::CompressionService;
use pipeline_domain::value_objects::Algorithm;

#[tokio::main]
async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    // Create compression service
    let compression = CompressionServiceAdapter::new();

    // Compress data
    let data = b"Hello, World!".to_vec();
    let compressed = compression.compress(&amp;data, &amp;Algorithm::zstd()).await?;

    println!("Original size: {} bytes", data.len());
    println!("Compressed size: {} bytes", compressed.len());
    println!("Compression ratio: {:.2}%",
        (1.0 - compressed.len() as f64 / data.len() as f64) * 100.0);

    // Decompress data
    let decompressed = compression.decompress(&amp;compressed, &amp;Algorithm::zstd()).await?;
    assert_eq!(data, decompressed);

    Ok(())
}</code></pre></pre>
<h3 id="comparing-algorithms"><a class="header" href="#comparing-algorithms">Comparing Algorithms</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn compare_algorithms(data: &amp;[u8]) -&gt; Result&lt;(), PipelineError&gt; {
    let compression = CompressionServiceAdapter::new();
    let algorithms = vec![
        Algorithm::brotli(),
        Algorithm::gzip(),
        Algorithm::zstd(),
        Algorithm::lz4(),
    ];

    println!("Original size: {} bytes\n", data.len());

    for algo in algorithms {
        let start = Instant::now();
        let compressed = compression.compress(data, &amp;algo).await?;
        let compress_time = start.elapsed();

        let start = Instant::now();
        let _decompressed = compression.decompress(&amp;compressed, &amp;algo).await?;
        let decompress_time = start.elapsed();

        println!("Algorithm: {}", algo.name());
        println!("  Compressed size: {} bytes ({:.2}% reduction)",
            compressed.len(),
            (1.0 - compressed.len() as f64 / data.len() as f64) * 100.0
        );
        println!("  Compression time: {:?}", compress_time);
        println!("  Decompression time: {:?}\n", decompress_time);
    }

    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h2 id="benchmarks"><a class="header" href="#benchmarks">Benchmarks</a></h2>
<p>Typical performance on a modern system (Intel i7, 16GB RAM):</p>
<pre><code class="language-text">Algorithm | File Size | Comp. Time | Decomp. Time | Ratio | Throughput
----------|-----------|------------|--------------|-------|------------
Brotli    | 100 MB    | 8.2s       | 0.4s         | 82%   | 12 MB/s
Gzip      | 100 MB    | 1.5s       | 0.6s         | 75%   | 67 MB/s
Zstd      | 100 MB    | 0.8s       | 0.3s         | 78%   | 125 MB/s
LZ4       | 100 MB    | 0.2s       | 0.1s         | 60%   | 500 MB/s
</code></pre>
<h2 id="best-practices-1"><a class="header" href="#best-practices-1">Best Practices</a></h2>
<h3 id="choosing-the-right-algorithm"><a class="header" href="#choosing-the-right-algorithm">Choosing the Right Algorithm</a></h3>
<p><strong>Use Brotli when:</strong></p>
<ul>
<li>Storage space is critical</li>
<li>Compression time is not a concern</li>
<li>Data will be compressed once, decompressed many times (web assets)</li>
</ul>
<p><strong>Use Gzip when:</strong></p>
<ul>
<li>Compatibility with other systems is required</li>
<li>Balanced performance is needed</li>
<li>Working with legacy systems</li>
</ul>
<p><strong>Use Zstandard when:</strong></p>
<ul>
<li>Modern systems are available</li>
<li>Both speed and compression ratio matter</li>
<li>Real-time processing is needed</li>
</ul>
<p><strong>Use LZ4 when:</strong></p>
<ul>
<li>Speed is the top priority</li>
<li>Working with live data streams</li>
<li>Low latency is critical</li>
<li>Memory is limited</li>
</ul>
<h3 id="chunk-size-selection-1"><a class="header" href="#chunk-size-selection-1">Chunk Size Selection</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// For maximum compression
let chunk_size = 4 * 1024 * 1024;  // 4 MB

// For balanced performance
let chunk_size = 16 * 1024 * 1024; // 16 MB

// For maximum speed
let chunk_size = 64 * 1024 * 1024; // 64 MB
<span class="boring">}</span></code></pre></pre>
<h3 id="memory-considerations"><a class="header" href="#memory-considerations">Memory Considerations</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Estimate memory usage
fn estimate_memory_usage(
    chunk_size: usize,
    parallel_chunks: usize,
    algorithm: &amp;Algorithm,
) -&gt; usize {
    let per_chunk_overhead = match algorithm.name() {
        "brotli" =&gt; chunk_size * 2,  // Brotli uses ~2x for internal buffers
        "gzip" =&gt; chunk_size,         // Gzip uses ~1x
        "zstd" =&gt; chunk_size / 2,     // Zstd is efficient
        "lz4" =&gt; chunk_size / 4,      // LZ4 is very efficient
        _ =&gt; chunk_size,
    };

    per_chunk_overhead * parallel_chunks
}
<span class="boring">}</span></code></pre></pre>
<h2 id="next-steps-11"><a class="header" href="#next-steps-11">Next Steps</a></h2>
<p>Now that you understand compression implementation:</p>
<ul>
<li><a href="implementation/encryption.html">Encryption Implementation</a> - Data encryption details</li>
<li><a href="implementation/integrity.html">Integrity Verification</a> - Checksum implementation</li>
<li><a href="implementation/file-io.html">File I/O</a> - Efficient file operations</li>
<li><a href="implementation/../advanced/performance.html">Performance Tuning</a> - Optimization strategies</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="encryption-implementation"><a class="header" href="#encryption-implementation">Encryption Implementation</a></h1>
<p><strong>Version:</strong> 1.0
<strong>Date:</strong> 2025-01-04
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner, Claude Code
<strong>Status:</strong> Active</p>
<h2 id="overview-6"><a class="header" href="#overview-6">Overview</a></h2>
<p>The encryption service provides authenticated encryption with multiple algorithms, secure key management, and automatic integrity verification. It's implemented as an infrastructure adapter that implements the domain's <code>EncryptionService</code> trait.</p>
<p><strong>File:</strong> <code>pipeline/src/infrastructure/adapters/encryption_service_adapter.rs</code></p>
<h2 id="supported-algorithms-1"><a class="header" href="#supported-algorithms-1">Supported Algorithms</a></h2>
<h3 id="aes-256-gcm-advanced-encryption-standard"><a class="header" href="#aes-256-gcm-advanced-encryption-standard">AES-256-GCM (Advanced Encryption Standard)</a></h3>
<ul>
<li><strong>Key size:</strong> 256 bits (32 bytes)</li>
<li><strong>Nonce size:</strong> 96 bits (12 bytes)</li>
<li><strong>Security:</strong> Industry standard, FIPS 140-2 approved</li>
<li><strong>Performance:</strong> Excellent with AES-NI hardware acceleration</li>
<li><strong>Library:</strong> <code>aes-gcm</code> crate</li>
</ul>
<p><strong>Best for:</strong></p>
<ul>
<li>Compliance requirements (FIPS, government)</li>
<li>Systems with AES-NI support</li>
<li>Maximum security requirements</li>
<li>Long-term data protection</li>
</ul>
<p><strong>Performance characteristics:</strong></p>
<pre><code class="language-text">Operation   | With AES-NI | Without AES-NI
------------|-------------|----------------
Encryption  | 2-3 GB/s    | 100-200 MB/s
Decryption  | 2-3 GB/s    | 100-200 MB/s
Key setup   | &lt; 1 μs      | &lt; 1 μs
Memory      | Low         | Low
</code></pre>
<h3 id="chacha20-poly1305-stream-cipher"><a class="header" href="#chacha20-poly1305-stream-cipher">ChaCha20-Poly1305 (Stream Cipher)</a></h3>
<ul>
<li><strong>Key size:</strong> 256 bits (32 bytes)</li>
<li><strong>Nonce size:</strong> 96 bits (12 bytes)</li>
<li><strong>Security:</strong> Modern, constant-time implementation</li>
<li><strong>Performance:</strong> Consistent across all platforms</li>
<li><strong>Library:</strong> <code>chacha20poly1305</code> crate</li>
</ul>
<p><strong>Best for:</strong></p>
<ul>
<li>Systems without AES-NI</li>
<li>Mobile/embedded devices</li>
<li>Constant-time requirements</li>
<li>Side-channel attack resistance</li>
</ul>
<p><strong>Performance characteristics:</strong></p>
<pre><code class="language-text">Operation   | Any Platform
------------|-------------
Encryption  | 500-800 MB/s
Decryption  | 500-800 MB/s
Key setup   | &lt; 1 μs
Memory      | Low
</code></pre>
<h3 id="aes-128-gcm-faster-aes-variant"><a class="header" href="#aes-128-gcm-faster-aes-variant">AES-128-GCM (Faster AES Variant)</a></h3>
<ul>
<li><strong>Key size:</strong> 128 bits (16 bytes)</li>
<li><strong>Nonce size:</strong> 96 bits (12 bytes)</li>
<li><strong>Security:</strong> Very secure, faster than AES-256</li>
<li><strong>Performance:</strong> ~30% faster than AES-256</li>
<li><strong>Library:</strong> <code>aes-gcm</code> crate</li>
</ul>
<p><strong>Best for:</strong></p>
<ul>
<li>High-performance requirements</li>
<li>Short-term data protection</li>
<li>Real-time encryption</li>
<li>Bandwidth-constrained systems</li>
</ul>
<p><strong>Performance characteristics:</strong></p>
<pre><code class="language-text">Operation   | With AES-NI | Without AES-NI
------------|-------------|----------------
Encryption  | 3-4 GB/s    | 150-250 MB/s
Decryption  | 3-4 GB/s    | 150-250 MB/s
Key setup   | &lt; 1 μs      | &lt; 1 μs
Memory      | Low         | Low
</code></pre>
<h2 id="security-features"><a class="header" href="#security-features">Security Features</a></h2>
<h3 id="authenticated-encryption-aead"><a class="header" href="#authenticated-encryption-aead">Authenticated Encryption (AEAD)</a></h3>
<p>All algorithms provide Authenticated Encryption with Associated Data (AEAD):</p>
<pre><code class="language-text">Plaintext → Encrypt → Ciphertext + Authentication Tag
                ↓
            Detects tampering
</code></pre>
<p><strong>Properties:</strong></p>
<ul>
<li><strong>Confidentiality:</strong> Data is encrypted and unreadable</li>
<li><strong>Integrity:</strong> Any modification is detected</li>
<li><strong>Authentication:</strong> Verifies data origin</li>
</ul>
<h3 id="nonce-management"><a class="header" href="#nonce-management">Nonce Management</a></h3>
<p>Each encryption operation requires a unique nonce (number used once):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Nonces are automatically generated for each chunk
pub struct EncryptionContext {
    key: SecretKey,
    nonce_counter: AtomicU64,  // Incrementing counter
}

impl EncryptionContext {
    fn next_nonce(&amp;self) -&gt; Nonce {
        let counter = self.nonce_counter.fetch_add(1, Ordering::SeqCst);

        // Generate nonce from counter
        let mut nonce = [0u8; 12];
        nonce[0..8].copy_from_slice(&amp;counter.to_le_bytes());

        Nonce::from_slice(&amp;nonce)
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Important:</strong> Never reuse a nonce with the same key!</p>
<h3 id="key-derivation"><a class="header" href="#key-derivation">Key Derivation</a></h3>
<p>Derive encryption keys from passwords using secure KDFs:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub enum KeyDerivationFunction {
    Argon2,   // Memory-hard, GPU-resistant
    Scrypt,   // Memory-hard, tunable
    PBKDF2,   // Standard, widely supported
}

// Derive key from password
pub fn derive_key(
    password: &amp;[u8],
    salt: &amp;[u8],
    kdf: KeyDerivationFunction,
) -&gt; Result&lt;SecretKey, PipelineError&gt; {
    match kdf {
        KeyDerivationFunction::Argon2 =&gt; {
            // Memory: 64 MB, Iterations: 3, Parallelism: 4
            argon2::hash_raw(password, salt, &amp;argon2::Config::default())
        }
        KeyDerivationFunction::Scrypt =&gt; {
            // N=16384, r=8, p=1
            scrypt::scrypt(password, salt, &amp;scrypt::Params::new(14, 8, 1)?)
        }
        KeyDerivationFunction::PBKDF2 =&gt; {
            // 100,000 iterations
            pbkdf2::pbkdf2_hmac::&lt;sha2::Sha256&gt;(password, salt, 100_000)
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="architecture-2"><a class="header" href="#architecture-2">Architecture</a></h2>
<h3 id="service-interface-domain-layer-1"><a class="header" href="#service-interface-domain-layer-1">Service Interface (Domain Layer)</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// pipeline-domain/src/services/encryption_service.rs
use async_trait::async_trait;
use crate::value_objects::Algorithm;
use crate::error::PipelineError;

#[async_trait]
pub trait EncryptionService: Send + Sync {
    /// Encrypt data using the specified algorithm
    async fn encrypt(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
        key: &amp;EncryptionKey,
    ) -&gt; Result&lt;EncryptedData, PipelineError&gt;;

    /// Decrypt data using the specified algorithm
    async fn decrypt(
        &amp;self,
        encrypted: &amp;EncryptedData,
        algorithm: &amp;Algorithm,
        key: &amp;EncryptionKey,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt;;
}

/// Encrypted data with nonce and authentication tag
pub struct EncryptedData {
    pub ciphertext: Vec&lt;u8&gt;,
    pub nonce: Vec&lt;u8&gt;,        // 12 bytes
    pub tag: Vec&lt;u8&gt;,          // 16 bytes (authentication tag)
}
<span class="boring">}</span></code></pre></pre>
<h3 id="service-implementation-infrastructure-layer-1"><a class="header" href="#service-implementation-infrastructure-layer-1">Service Implementation (Infrastructure Layer)</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// pipeline/src/infrastructure/adapters/encryption_service_adapter.rs
use aes_gcm::{Aes256Gcm, Key, Nonce};
use aes_gcm::aead::{Aead, NewAead};
use chacha20poly1305::ChaCha20Poly1305;

pub struct EncryptionServiceAdapter {
    // Secure key storage
    key_store: Arc&lt;RwLock&lt;KeyStore&gt;&gt;,
}

#[async_trait]
impl EncryptionService for EncryptionServiceAdapter {
    async fn encrypt(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
        key: &amp;EncryptionKey,
    ) -&gt; Result&lt;EncryptedData, PipelineError&gt; {
        match algorithm.name() {
            "aes-256-gcm" =&gt; self.encrypt_aes_256_gcm(data, key),
            "chacha20-poly1305" =&gt; self.encrypt_chacha20(data, key),
            "aes-128-gcm" =&gt; self.encrypt_aes_128_gcm(data, key),
            _ =&gt; Err(PipelineError::UnsupportedAlgorithm(
                algorithm.name().to_string()
            )),
        }
    }

    async fn decrypt(
        &amp;self,
        encrypted: &amp;EncryptedData,
        algorithm: &amp;Algorithm,
        key: &amp;EncryptionKey,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        match algorithm.name() {
            "aes-256-gcm" =&gt; self.decrypt_aes_256_gcm(encrypted, key),
            "chacha20-poly1305" =&gt; self.decrypt_chacha20(encrypted, key),
            "aes-128-gcm" =&gt; self.decrypt_aes_128_gcm(encrypted, key),
            _ =&gt; Err(PipelineError::UnsupportedAlgorithm(
                algorithm.name().to_string()
            )),
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="algorithm-implementations-1"><a class="header" href="#algorithm-implementations-1">Algorithm Implementations</a></h2>
<h3 id="aes-256-gcm-implementation"><a class="header" href="#aes-256-gcm-implementation">AES-256-GCM Implementation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl EncryptionServiceAdapter {
    fn encrypt_aes_256_gcm(
        &amp;self,
        data: &amp;[u8],
        key: &amp;EncryptionKey,
    ) -&gt; Result&lt;EncryptedData, PipelineError&gt; {
        use aes_gcm::{Aes256Gcm, Key, Nonce};
        use aes_gcm::aead::{Aead, NewAead};

        // Create cipher from key
        let key = Key::from_slice(key.as_bytes());
        let cipher = Aes256Gcm::new(key);

        // Generate unique nonce
        let nonce = self.generate_nonce();
        let nonce_obj = Nonce::from_slice(&amp;nonce);

        // Encrypt with authentication
        let ciphertext = cipher
            .encrypt(nonce_obj, data)
            .map_err(|e| PipelineError::EncryptionError(e.to_string()))?;

        // Split ciphertext and tag
        let (ciphertext_bytes, tag) = ciphertext.split_at(ciphertext.len() - 16);

        Ok(EncryptedData {
            ciphertext: ciphertext_bytes.to_vec(),
            nonce: nonce.to_vec(),
            tag: tag.to_vec(),
        })
    }

    fn decrypt_aes_256_gcm(
        &amp;self,
        encrypted: &amp;EncryptedData,
        key: &amp;EncryptionKey,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        use aes_gcm::{Aes256Gcm, Key, Nonce};
        use aes_gcm::aead::{Aead, NewAead};

        // Create cipher from key
        let key = Key::from_slice(key.as_bytes());
        let cipher = Aes256Gcm::new(key);

        // Reconstruct nonce
        let nonce = Nonce::from_slice(&amp;encrypted.nonce);

        // Combine ciphertext and tag
        let mut combined = encrypted.ciphertext.clone();
        combined.extend_from_slice(&amp;encrypted.tag);

        // Decrypt and verify authentication
        let plaintext = cipher
            .decrypt(nonce, combined.as_slice())
            .map_err(|e| PipelineError::DecryptionError(
                format!("Decryption failed (possibly tampered): {}", e)
            ))?;

        Ok(plaintext)
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="chacha20-poly1305-implementation"><a class="header" href="#chacha20-poly1305-implementation">ChaCha20-Poly1305 Implementation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl EncryptionServiceAdapter {
    fn encrypt_chacha20(
        &amp;self,
        data: &amp;[u8],
        key: &amp;EncryptionKey,
    ) -&gt; Result&lt;EncryptedData, PipelineError&gt; {
        use chacha20poly1305::{ChaCha20Poly1305, Key, Nonce};
        use chacha20poly1305::aead::{Aead, NewAead};

        // Create cipher from key
        let key = Key::from_slice(key.as_bytes());
        let cipher = ChaCha20Poly1305::new(key);

        // Generate unique nonce
        let nonce = self.generate_nonce();
        let nonce_obj = Nonce::from_slice(&amp;nonce);

        // Encrypt with authentication
        let ciphertext = cipher
            .encrypt(nonce_obj, data)
            .map_err(|e| PipelineError::EncryptionError(e.to_string()))?;

        // Split ciphertext and tag
        let (ciphertext_bytes, tag) = ciphertext.split_at(ciphertext.len() - 16);

        Ok(EncryptedData {
            ciphertext: ciphertext_bytes.to_vec(),
            nonce: nonce.to_vec(),
            tag: tag.to_vec(),
        })
    }

    fn decrypt_chacha20(
        &amp;self,
        encrypted: &amp;EncryptedData,
        key: &amp;EncryptionKey,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        use chacha20poly1305::{ChaCha20Poly1305, Key, Nonce};
        use chacha20poly1305::aead::{Aead, NewAead};

        // Create cipher from key
        let key = Key::from_slice(key.as_bytes());
        let cipher = ChaCha20Poly1305::new(key);

        // Reconstruct nonce
        let nonce = Nonce::from_slice(&amp;encrypted.nonce);

        // Combine ciphertext and tag
        let mut combined = encrypted.ciphertext.clone();
        combined.extend_from_slice(&amp;encrypted.tag);

        // Decrypt and verify authentication
        let plaintext = cipher
            .decrypt(nonce, combined.as_slice())
            .map_err(|e| PipelineError::DecryptionError(
                format!("Decryption failed (possibly tampered): {}", e)
            ))?;

        Ok(plaintext)
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="key-management"><a class="header" href="#key-management">Key Management</a></h2>
<h3 id="secure-key-storage"><a class="header" href="#secure-key-storage">Secure Key Storage</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use zeroize::Zeroize;

/// Secure key that zeroizes on drop
pub struct SecretKey {
    bytes: Vec&lt;u8&gt;,
}

impl SecretKey {
    pub fn new(bytes: Vec&lt;u8&gt;) -&gt; Self {
        Self { bytes }
    }

    pub fn as_bytes(&amp;self) -&gt; &amp;[u8] {
        &amp;self.bytes
    }

    /// Generate random key
    pub fn generate(size: usize) -&gt; Self {
        use rand::RngCore;
        let mut bytes = vec![0u8; size];
        rand::thread_rng().fill_bytes(&amp;mut bytes);
        Self::new(bytes)
    }
}

impl Drop for SecretKey {
    fn drop(&amp;mut self) {
        // Securely wipe key from memory
        self.bytes.zeroize();
    }
}

impl Zeroize for SecretKey {
    fn zeroize(&amp;mut self) {
        self.bytes.zeroize();
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="key-rotation"><a class="header" href="#key-rotation">Key Rotation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct KeyRotation {
    current_key: SecretKey,
    previous_key: Option&lt;SecretKey&gt;,
    rotation_interval: Duration,
    last_rotation: Instant,
}

impl KeyRotation {
    pub fn rotate(&amp;mut self) -&gt; Result&lt;(), PipelineError&gt; {
        // Save current key as previous
        let old_key = std::mem::replace(
            &amp;mut self.current_key,
            SecretKey::generate(32),
        );
        self.previous_key = Some(old_key);
        self.last_rotation = Instant::now();

        Ok(())
    }

    pub fn should_rotate(&amp;self) -&gt; bool {
        self.last_rotation.elapsed() &gt;= self.rotation_interval
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="performance-optimizations-1"><a class="header" href="#performance-optimizations-1">Performance Optimizations</a></h2>
<h3 id="parallel-chunk-encryption"><a class="header" href="#parallel-chunk-encryption">Parallel Chunk Encryption</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use rayon::prelude::*;

pub async fn encrypt_chunks(
    chunks: Vec&lt;FileChunk&gt;,
    algorithm: &amp;Algorithm,
    key: &amp;SecretKey,
    encryption_service: &amp;Arc&lt;dyn EncryptionService&gt;,
) -&gt; Result&lt;Vec&lt;EncryptedChunk&gt;, PipelineError&gt; {
    // Encrypt chunks in parallel
    chunks.par_iter()
        .map(|chunk| {
            let encrypted = encryption_service
                .encrypt(&amp;chunk.data, algorithm, key)?;

            Ok(EncryptedChunk {
                sequence: chunk.sequence,
                data: encrypted,
                original_size: chunk.data.len(),
            })
        })
        .collect()
}
<span class="boring">}</span></code></pre></pre>
<h3 id="hardware-acceleration"><a class="header" href="#hardware-acceleration">Hardware Acceleration</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Detect AES-NI support
pub fn has_aes_ni() -&gt; bool {
    #[cfg(target_arch = "x86_64")]
    {
        use std::arch::x86_64::*;
        is_x86_feature_detected!("aes")
    }
    #[cfg(not(target_arch = "x86_64"))]
    {
        false
    }
}

// Select algorithm based on hardware
pub fn select_algorithm() -&gt; Algorithm {
    if has_aes_ni() {
        Algorithm::aes_256_gcm()  // Fast with hardware support
    } else {
        Algorithm::chacha20_poly1305()  // Consistent without hardware
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="configuration-1"><a class="header" href="#configuration-1">Configuration</a></h2>
<h3 id="encryption-configuration-1"><a class="header" href="#encryption-configuration-1">Encryption Configuration</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct EncryptionConfig {
    pub algorithm: Algorithm,
    pub key_derivation: KeyDerivationFunction,
    pub key_rotation_interval: Duration,
    pub nonce_reuse_prevention: bool,
}

impl EncryptionConfig {
    pub fn maximum_security() -&gt; Self {
        Self {
            algorithm: Algorithm::aes_256_gcm(),
            key_derivation: KeyDerivationFunction::Argon2,
            key_rotation_interval: Duration::from_secs(86400), // 24 hours
            nonce_reuse_prevention: true,
        }
    }

    pub fn balanced() -&gt; Self {
        Self {
            algorithm: if has_aes_ni() {
                Algorithm::aes_256_gcm()
            } else {
                Algorithm::chacha20_poly1305()
            },
            key_derivation: KeyDerivationFunction::Scrypt,
            key_rotation_interval: Duration::from_secs(604800), // 7 days
            nonce_reuse_prevention: true,
        }
    }

    pub fn high_performance() -&gt; Self {
        Self {
            algorithm: if has_aes_ni() {
                Algorithm::aes_128_gcm()
            } else {
                Algorithm::chacha20_poly1305()
            },
            key_derivation: KeyDerivationFunction::PBKDF2,
            key_rotation_interval: Duration::from_secs(2592000), // 30 days
            nonce_reuse_prevention: true,
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="error-handling-1"><a class="header" href="#error-handling-1">Error Handling</a></h2>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Debug, thiserror::Error)]
pub enum EncryptionError {
    #[error("Encryption failed: {0}")]
    EncryptionFailed(String),

    #[error("Decryption failed: {0}")]
    DecryptionFailed(String),

    #[error("Authentication failed - data may be tampered")]
    AuthenticationFailed,

    #[error("Invalid key length: expected {expected}, got {actual}")]
    InvalidKeyLength { expected: usize, actual: usize },

    #[error("Nonce reuse detected")]
    NonceReuse,

    #[error("Key derivation failed: {0}")]
    KeyDerivationFailed(String),
}

impl From&lt;EncryptionError&gt; for PipelineError {
    fn from(err: EncryptionError) -&gt; Self {
        match err {
            EncryptionError::EncryptionFailed(msg) =&gt;
                PipelineError::EncryptionError(msg),
            EncryptionError::DecryptionFailed(msg) =&gt;
                PipelineError::DecryptionError(msg),
            EncryptionError::AuthenticationFailed =&gt;
                PipelineError::IntegrityError("Authentication failed".to_string()),
            _ =&gt; PipelineError::EncryptionError(err.to_string()),
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="usage-examples-2"><a class="header" href="#usage-examples-2">Usage Examples</a></h2>
<h3 id="basic-encryption"><a class="header" href="#basic-encryption">Basic Encryption</a></h3>
<pre><pre class="playground"><code class="language-rust">use pipeline::infrastructure::adapters::EncryptionServiceAdapter;
use pipeline_domain::services::EncryptionService;
use pipeline_domain::value_objects::Algorithm;

#[tokio::main]
async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    // Create encryption service
    let encryption = EncryptionServiceAdapter::new();

    // Generate encryption key
    let key = SecretKey::generate(32); // 256 bits

    // Encrypt data
    let data = b"Sensitive information";
    let encrypted = encryption.encrypt(
        data,
        &amp;Algorithm::aes_256_gcm(),
        &amp;key
    ).await?;

    println!("Original size: {} bytes", data.len());
    println!("Encrypted size: {} bytes", encrypted.ciphertext.len());
    println!("Nonce: {} bytes", encrypted.nonce.len());
    println!("Tag: {} bytes", encrypted.tag.len());

    // Decrypt data
    let decrypted = encryption.decrypt(
        &amp;encrypted,
        &amp;Algorithm::aes_256_gcm(),
        &amp;key
    ).await?;

    assert_eq!(data, decrypted.as_slice());
    println!("✓ Decryption successful");

    Ok(())
}</code></pre></pre>
<h3 id="password-based-encryption"><a class="header" href="#password-based-encryption">Password-Based Encryption</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn encrypt_with_password(
    data: &amp;[u8],
    password: &amp;str,
) -&gt; Result&lt;EncryptedData, PipelineError&gt; {
    // Generate random salt
    let salt = SecretKey::generate(16);

    // Derive key from password
    let key = derive_key(
        password.as_bytes(),
        salt.as_bytes(),
        KeyDerivationFunction::Argon2,
    )?;

    // Encrypt data
    let encryption = EncryptionServiceAdapter::new();
    let encrypted = encryption.encrypt(
        data,
        &amp;Algorithm::aes_256_gcm(),
        &amp;key,
    ).await?;

    // Store salt with encrypted data
    encrypted.salt = salt.as_bytes().to_vec();

    Ok(encrypted)
}
<span class="boring">}</span></code></pre></pre>
<h3 id="tamper-detection"><a class="header" href="#tamper-detection">Tamper Detection</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn decrypt_with_verification(
    encrypted: &amp;EncryptedData,
    key: &amp;SecretKey,
) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
    let encryption = EncryptionServiceAdapter::new();

    // Attempt decryption (will fail if tampered)
    match encryption.decrypt(encrypted, &amp;Algorithm::aes_256_gcm(), key).await {
        Ok(plaintext) =&gt; {
            println!("✓ Data is authentic and unmodified");
            Ok(plaintext)
        }
        Err(PipelineError::DecryptionError(_)) =&gt; {
            eprintln!("✗ Data has been tampered with!");
            Err(PipelineError::IntegrityError(
                "Authentication failed - data may be tampered".to_string()
            ))
        }
        Err(e) =&gt; Err(e),
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="benchmarks-1"><a class="header" href="#benchmarks-1">Benchmarks</a></h2>
<p>Typical performance on modern systems:</p>
<pre><code class="language-text">Algorithm          | File Size | Encrypt Time | Decrypt Time | Throughput
-------------------|-----------|--------------|--------------|------------
AES-256-GCM (NI)   | 100 MB    | 0.04s        | 0.04s        | 2.5 GB/s
AES-256-GCM (SW)   | 100 MB    | 0.8s         | 0.8s         | 125 MB/s
ChaCha20-Poly1305  | 100 MB    | 0.15s        | 0.15s        | 670 MB/s
AES-128-GCM (NI)   | 100 MB    | 0.03s        | 0.03s        | 3.3 GB/s
</code></pre>
<h2 id="best-practices-2"><a class="header" href="#best-practices-2">Best Practices</a></h2>
<h3 id="algorithm-selection"><a class="header" href="#algorithm-selection">Algorithm Selection</a></h3>
<p><strong>Use AES-256-GCM when:</strong></p>
<ul>
<li>Compliance requires FIPS-approved encryption</li>
<li>Long-term data protection is needed</li>
<li>Hardware has AES-NI support</li>
<li>Maximum security is required</li>
</ul>
<p><strong>Use ChaCha20-Poly1305 when:</strong></p>
<ul>
<li>Running on platforms without AES-NI</li>
<li>Constant-time execution is critical</li>
<li>Side-channel resistance is needed</li>
<li>Mobile/embedded deployment</li>
</ul>
<p><strong>Use AES-128-GCM when:</strong></p>
<ul>
<li>Maximum performance is required</li>
<li>Short-term data protection is sufficient</li>
<li>Hardware has AES-NI support</li>
</ul>
<h3 id="key-management-1"><a class="header" href="#key-management-1">Key Management</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Secure key handling
let key = SecretKey::generate(32);
let encrypted = encrypt(data, &amp;key)?;
// Key is automatically zeroized on drop

// ❌ BAD: Exposing key in logs
println!("Key: {:?}", key);  // Never log keys!

// ✅ GOOD: Key derivation from password
let key = derive_key(password, salt, KeyDerivationFunction::Argon2)?;

// ❌ BAD: Weak key derivation
let key = sha256(password);  // Not secure!
<span class="boring">}</span></code></pre></pre>
<h3 id="nonce-management-1"><a class="header" href="#nonce-management-1">Nonce Management</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Unique nonce per encryption
let nonce = generate_unique_nonce();

// ❌ BAD: Reusing nonces
let nonce = [0u8; 12];  // NEVER reuse nonces!

// ✅ GOOD: Counter-based nonces
let nonce_counter = AtomicU64::new(0);
let nonce = generate_nonce_from_counter(nonce_counter.fetch_add(1));
<span class="boring">}</span></code></pre></pre>
<h3 id="authentication-verification"><a class="header" href="#authentication-verification">Authentication Verification</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Always verify authentication
match decrypt(encrypted, key) {
    Ok(data) =&gt; process(data),
    Err(e) =&gt; {
        log::error!("Decryption failed - possible tampering");
        return Err(e);
    }
}

// ❌ BAD: Ignoring authentication failures
let data = decrypt(encrypted, key).unwrap_or_default();  // Dangerous!
<span class="boring">}</span></code></pre></pre>
<h2 id="security-considerations"><a class="header" href="#security-considerations">Security Considerations</a></h2>
<h3 id="nonce-uniqueness"><a class="header" href="#nonce-uniqueness">Nonce Uniqueness</a></h3>
<ul>
<li><strong>Critical:</strong> Never reuse a nonce with the same key</li>
<li>Use counter-based or random nonces</li>
<li>Rotate keys after 2^32 encryptions (GCM limit)</li>
</ul>
<h3 id="key-strength"><a class="header" href="#key-strength">Key Strength</a></h3>
<ul>
<li>Minimum 256 bits for long-term security</li>
<li>Use cryptographically secure random number generators</li>
<li>Derive keys properly from passwords (use Argon2)</li>
</ul>
<h3 id="memory-security"><a class="header" href="#memory-security">Memory Security</a></h3>
<ul>
<li>Keys are automatically zeroized on drop</li>
<li>Avoid cloning keys unnecessarily</li>
<li>Don't log or print keys</li>
</ul>
<h3 id="side-channel-attacks"><a class="header" href="#side-channel-attacks">Side-Channel Attacks</a></h3>
<ul>
<li>ChaCha20 provides constant-time execution</li>
<li>AES requires AES-NI for timing attack resistance</li>
<li>Validate all inputs before decryption</li>
</ul>
<h2 id="next-steps-12"><a class="header" href="#next-steps-12">Next Steps</a></h2>
<p>Now that you understand encryption implementation:</p>
<ul>
<li><a href="implementation/integrity.html">Integrity Verification</a> - Checksum and hashing</li>
<li><a href="implementation/../advanced/key-management.html">Key Management</a> - Advanced key handling</li>
<li><a href="implementation/../advanced/security.html">Security Best Practices</a> - Comprehensive security guide</li>
<li><a href="implementation/compression.html">Compression</a> - Data compression before encryption</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="integrity-verification"><a class="header" href="#integrity-verification">Integrity Verification</a></h1>
<p><strong>Version:</strong> 1.0
<strong>Date:</strong> 2025-01-04
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner, Claude Code
<strong>Status:</strong> Active</p>
<h2 id="overview-7"><a class="header" href="#overview-7">Overview</a></h2>
<p>Integrity verification ensures data hasn't been corrupted or tampered with during processing. The pipeline system uses cryptographic hash functions to calculate checksums at various stages, providing strong guarantees about data integrity.</p>
<p>The checksum service operates in two modes:</p>
<ul>
<li><strong>Calculate Mode</strong>: Generates checksums for data chunks</li>
<li><strong>Verify Mode</strong>: Validates existing checksums to detect tampering</li>
</ul>
<h2 id="supported-algorithms-2"><a class="header" href="#supported-algorithms-2">Supported Algorithms</a></h2>
<h3 id="sha-256-recommended"><a class="header" href="#sha-256-recommended">SHA-256 (Recommended)</a></h3>
<p><strong>Industry-standard cryptographic hash function</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::value_objects::Algorithm;

let algorithm = Algorithm::sha256();
<span class="boring">}</span></code></pre></pre>
<p><strong>Characteristics:</strong></p>
<ul>
<li><strong>Hash Size</strong>: 256 bits (32 bytes)</li>
<li><strong>Security</strong>: Cryptographically secure, collision-resistant</li>
<li><strong>Performance</strong>: ~500 MB/s (software), ~2 GB/s (hardware accelerated)</li>
<li><strong>Use Cases</strong>: General-purpose integrity verification</li>
</ul>
<p><strong>When to Use:</strong></p>
<ul>
<li>✅ General-purpose integrity verification</li>
<li>✅ Compliance requirements (FIPS 180-4)</li>
<li>✅ Cross-platform compatibility</li>
<li>✅ Hardware acceleration available (SHA extensions)</li>
</ul>
<h3 id="sha-512"><a class="header" href="#sha-512">SHA-512</a></h3>
<p><strong>Stronger variant of SHA-2 family</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let algorithm = Algorithm::sha512();
<span class="boring">}</span></code></pre></pre>
<p><strong>Characteristics:</strong></p>
<ul>
<li><strong>Hash Size</strong>: 512 bits (64 bytes)</li>
<li><strong>Security</strong>: Higher security margin than SHA-256</li>
<li><strong>Performance</strong>: ~400 MB/s (software), faster on 64-bit systems</li>
<li><strong>Use Cases</strong>: High-security requirements, 64-bit optimized systems</li>
</ul>
<p><strong>When to Use:</strong></p>
<ul>
<li>✅ Maximum security requirements</li>
<li>✅ 64-bit systems (better performance)</li>
<li>✅ Long-term archival (future-proof security)</li>
<li>❌ Resource-constrained systems (larger output)</li>
</ul>
<h3 id="blake3"><a class="header" href="#blake3">BLAKE3</a></h3>
<p><strong>Modern, high-performance cryptographic hash</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let algorithm = Algorithm::blake3();
<span class="boring">}</span></code></pre></pre>
<p><strong>Characteristics:</strong></p>
<ul>
<li><strong>Hash Size</strong>: 256 bits (32 bytes, configurable)</li>
<li><strong>Security</strong>: Based on BLAKE2, ChaCha stream cipher</li>
<li><strong>Performance</strong>: ~3 GB/s (parallelizable, SIMD-optimized)</li>
<li><strong>Use Cases</strong>: High-throughput processing, modern systems</li>
</ul>
<p><strong>When to Use:</strong></p>
<ul>
<li>✅ Maximum performance requirements</li>
<li>✅ Large file processing (highly parallelizable)</li>
<li>✅ Modern CPUs with SIMD support</li>
<li>✅ No regulatory compliance requirements</li>
<li>❌ FIPS compliance needed (not FIPS certified)</li>
</ul>
<h3 id="algorithm-comparison"><a class="header" href="#algorithm-comparison">Algorithm Comparison</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Algorithm</th><th>Hash Size</th><th>Throughput</th><th>Security</th><th>Hardware Accel</th><th>FIPS</th></tr></thead><tbody>
<tr><td>SHA-256</td><td>256 bits</td><td>500 MB/s</td><td>Strong</td><td>✅ (SHA-NI)</td><td>✅</td></tr>
<tr><td>SHA-512</td><td>512 bits</td><td>400 MB/s</td><td>Stronger</td><td>✅ (SHA-NI)</td><td>✅</td></tr>
<tr><td>BLAKE3</td><td>256 bits</td><td>3 GB/s</td><td>Strong</td><td>✅ (SIMD)</td><td>❌</td></tr>
</tbody></table>
</div>
<p><strong>Performance measured on Intel i7-10700K @ 3.8 GHz</strong></p>
<h2 id="architecture-3"><a class="header" href="#architecture-3">Architecture</a></h2>
<h3 id="service-interface"><a class="header" href="#service-interface">Service Interface</a></h3>
<p>The domain layer defines the checksum service interface:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::services::ChecksumService;
use pipeline_domain::entities::ProcessingContext;
use pipeline_domain::value_objects::FileChunk;
use pipeline_domain::PipelineError;

/// Domain service for integrity verification
pub trait ChecksumService: Send + Sync {
    /// Process a chunk and update the running checksum
    fn process_chunk(
        &amp;self,
        chunk: FileChunk,
        context: &amp;mut ProcessingContext,
        stage_name: &amp;str,
    ) -&gt; Result&lt;FileChunk, PipelineError&gt;;

    /// Get the final checksum value
    fn get_checksum(
        &amp;self,
        context: &amp;ProcessingContext,
        stage_name: &amp;str
    ) -&gt; Option&lt;String&gt;;
}
<span class="boring">}</span></code></pre></pre>
<h3 id="implementation-2"><a class="header" href="#implementation-2">Implementation</a></h3>
<p>The infrastructure layer provides concrete implementations:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::services::{ChecksumService, ChecksumProcessor};

/// Concrete checksum processor using SHA-256
pub struct ChecksumProcessor {
    pub algorithm: String,
    pub verify_existing: bool,
}

impl ChecksumProcessor {
    pub fn new(algorithm: String, verify_existing: bool) -&gt; Self {
        Self {
            algorithm,
            verify_existing,
        }
    }

    /// Creates a SHA-256 processor
    pub fn sha256_processor(verify_existing: bool) -&gt; Self {
        Self::new("SHA256".to_string(), verify_existing)
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="algorithm-implementations-2"><a class="header" href="#algorithm-implementations-2">Algorithm Implementations</a></h2>
<h3 id="sha-256-implementation"><a class="header" href="#sha-256-implementation">SHA-256 Implementation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use sha2::{Digest, Sha256};

impl ChecksumProcessor {
    /// Calculate SHA-256 checksum
    pub fn calculate_sha256(&amp;self, data: &amp;[u8]) -&gt; String {
        let mut hasher = Sha256::new();
        hasher.update(data);
        format!("{:x}", hasher.finalize())
    }

    /// Incremental SHA-256 hashing
    pub fn update_hash(&amp;self, hasher: &amp;mut Sha256, chunk: &amp;FileChunk) {
        hasher.update(chunk.data());
    }

    /// Finalize hash and return hex string
    pub fn finalize_hash(&amp;self, hasher: Sha256) -&gt; String {
        format!("{:x}", hasher.finalize())
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Key Features:</strong></p>
<ul>
<li>Incremental hashing for streaming large files</li>
<li>Memory-efficient (constant 32-byte state)</li>
<li>Hardware acceleration with SHA-NI instructions</li>
</ul>
<h3 id="sha-512-implementation"><a class="header" href="#sha-512-implementation">SHA-512 Implementation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use sha2::{Sha512};

impl ChecksumProcessor {
    /// Calculate SHA-512 checksum
    pub fn calculate_sha512(&amp;self, data: &amp;[u8]) -&gt; String {
        let mut hasher = Sha512::new();
        hasher.update(data);
        format!("{:x}", hasher.finalize())
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Key Features:</strong></p>
<ul>
<li>512-bit output for higher security margin</li>
<li>Optimized for 64-bit architectures</li>
<li>Suitable for long-term archival</li>
</ul>
<h3 id="blake3-implementation"><a class="header" href="#blake3-implementation">BLAKE3 Implementation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use blake3::Hasher;

impl ChecksumProcessor {
    /// Calculate BLAKE3 checksum
    pub fn calculate_blake3(&amp;self, data: &amp;[u8]) -&gt; String {
        let mut hasher = Hasher::new();
        hasher.update(data);
        hasher.finalize().to_hex().to_string()
    }

    /// Parallel BLAKE3 hashing
    pub fn calculate_blake3_parallel(&amp;self, chunks: &amp;[&amp;[u8]]) -&gt; String {
        let mut hasher = Hasher::new();
        for chunk in chunks {
            hasher.update(chunk);
        }
        hasher.finalize().to_hex().to_string()
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Key Features:</strong></p>
<ul>
<li>Highly parallelizable (uses Rayon internally)</li>
<li>SIMD-optimized for modern CPUs</li>
<li>Incremental and streaming support</li>
<li>Up to 6x faster than SHA-256</li>
</ul>
<h2 id="chunk-processing"><a class="header" href="#chunk-processing">Chunk Processing</a></h2>
<h3 id="chunkprocessor-trait"><a class="header" href="#chunkprocessor-trait">ChunkProcessor Trait</a></h3>
<p>The checksum service implements the <code>ChunkProcessor</code> trait for integration with the pipeline:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::services::file_processor_service::ChunkProcessor;

impl ChunkProcessor for ChecksumProcessor {
    /// Process chunk with checksum calculation/verification
    fn process_chunk(&amp;self, chunk: &amp;FileChunk) -&gt; Result&lt;FileChunk, PipelineError&gt; {
        // Step 1: Verify existing checksum if requested
        if self.verify_existing &amp;&amp; chunk.checksum().is_some() {
            let is_valid = chunk.verify_integrity()?;
            if !is_valid {
                return Err(PipelineError::IntegrityError(format!(
                    "Checksum verification failed for chunk {}",
                    chunk.sequence_number()
                )));
            }
        }

        // Step 2: Ensure chunk has checksum (calculate if missing)
        if chunk.checksum().is_none() {
            chunk.with_calculated_checksum()
        } else {
            Ok(chunk.clone())
        }
    }

    fn name(&amp;self) -&gt; &amp;str {
        "ChecksumProcessor"
    }

    fn modifies_data(&amp;self) -&gt; bool {
        false // Only modifies metadata
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="integrity-verification-1"><a class="header" href="#integrity-verification-1">Integrity Verification</a></h3>
<p>The <code>FileChunk</code> value object provides built-in integrity verification:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl FileChunk {
    /// Verify chunk integrity against stored checksum
    pub fn verify_integrity(&amp;self) -&gt; Result&lt;bool, PipelineError&gt; {
        match &amp;self.checksum {
            Some(stored_checksum) =&gt; {
                let calculated = Self::calculate_checksum(self.data());
                Ok(*stored_checksum == calculated)
            }
            None =&gt; Err(PipelineError::InvalidConfiguration(
                "No checksum to verify".to_string()
            )),
        }
    }

    /// Calculate checksum for chunk data
    fn calculate_checksum(data: &amp;[u8]) -&gt; String {
        use sha2::{Digest, Sha256};
        let mut hasher = Sha256::new();
        hasher.update(data);
        format!("{:x}", hasher.finalize())
    }

    /// Create new chunk with calculated checksum
    pub fn with_calculated_checksum(&amp;self) -&gt; Result&lt;FileChunk, PipelineError&gt; {
        let checksum = Self::calculate_checksum(self.data());
        Ok(FileChunk {
            sequence_number: self.sequence_number,
            data: self.data.clone(),
            checksum: Some(checksum),
            metadata: self.metadata.clone(),
        })
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="performance-optimizations-2"><a class="header" href="#performance-optimizations-2">Performance Optimizations</a></h2>
<h3 id="parallel-chunk-processing-2"><a class="header" href="#parallel-chunk-processing-2">Parallel Chunk Processing</a></h3>
<p>Process multiple chunks in parallel using Rayon:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use rayon::prelude::*;

impl ChecksumProcessor {
    /// Process chunks in parallel for maximum throughput
    pub fn process_chunks_parallel(
        &amp;self,
        chunks: &amp;[FileChunk]
    ) -&gt; Result&lt;Vec&lt;FileChunk&gt;, PipelineError&gt; {
        chunks
            .par_iter()
            .map(|chunk| self.process_chunk(chunk))
            .collect()
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Performance Benefits:</strong></p>
<ul>
<li><strong>Linear Scaling</strong>: Performance scales with CPU cores</li>
<li><strong>No Contention</strong>: Each chunk processed independently</li>
<li><strong>2-4x Speedup</strong>: On typical multi-core systems</li>
</ul>
<h3 id="hardware-acceleration-1"><a class="header" href="#hardware-acceleration-1">Hardware Acceleration</a></h3>
<p>Leverage CPU crypto extensions when available:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Check for SHA hardware acceleration
pub fn has_sha_extensions() -&gt; bool {
    #[cfg(target_arch = "x86_64")]
    {
        is_x86_feature_detected!("sha")
    }
    #[cfg(not(target_arch = "x86_64"))]
    {
        false
    }
}

/// Select optimal algorithm based on hardware
pub fn optimal_hash_algorithm() -&gt; Algorithm {
    if has_sha_extensions() {
        Algorithm::sha256() // Hardware accelerated
    } else {
        Algorithm::blake3() // Software optimized
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="memory-management-1"><a class="header" href="#memory-management-1">Memory Management</a></h3>
<p>Minimize allocations during hash calculation:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl ChecksumProcessor {
    /// Reuse buffer for hash calculations
    pub fn calculate_with_buffer(
        &amp;self,
        data: &amp;[u8],
        buffer: &amp;mut Vec&lt;u8&gt;
    ) -&gt; String {
        buffer.clear();
        buffer.extend_from_slice(data);
        self.calculate_sha256(buffer)
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="configuration-2"><a class="header" href="#configuration-2">Configuration</a></h2>
<h3 id="stage-configuration-2"><a class="header" href="#stage-configuration-2">Stage Configuration</a></h3>
<p>Configure integrity stages in your pipeline:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::entities::PipelineStage;
use pipeline_domain::value_objects::{Algorithm, StageType};

// Input integrity verification
let input_stage = PipelineStage::new(
    "input_checksum",
    StageType::Integrity,
    Algorithm::sha256(),
)?;

// Output integrity verification
let output_stage = PipelineStage::new(
    "output_checksum",
    StageType::Integrity,
    Algorithm::blake3(), // Faster for final verification
)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="verification-mode"><a class="header" href="#verification-mode">Verification Mode</a></h3>
<p>Enable checksum verification for existing data:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Calculate checksums only (default)
let processor = ChecksumProcessor::new("SHA256".to_string(), false);

// Verify existing checksums before processing
let verifying_processor = ChecksumProcessor::new("SHA256".to_string(), true);
<span class="boring">}</span></code></pre></pre>
<h3 id="algorithm-selection-1"><a class="header" href="#algorithm-selection-1">Algorithm Selection</a></h3>
<p>Choose algorithm based on requirements:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn select_hash_algorithm(
    security_level: SecurityLevel,
    performance_priority: bool,
) -&gt; Algorithm {
    match (security_level, performance_priority) {
        (SecurityLevel::Maximum, _) =&gt; Algorithm::sha512(),
        (SecurityLevel::High, false) =&gt; Algorithm::sha256(),
        (SecurityLevel::High, true) =&gt; Algorithm::blake3(),
        (SecurityLevel::Standard, _) =&gt; Algorithm::blake3(),
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="error-handling-2"><a class="header" href="#error-handling-2">Error Handling</a></h2>
<h3 id="error-types"><a class="header" href="#error-types">Error Types</a></h3>
<p>The service handles various error conditions:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub enum IntegrityError {
    /// Checksum verification failed
    ChecksumMismatch {
        expected: String,
        actual: String,
        chunk: u64,
    },

    /// Invalid algorithm specified
    UnsupportedAlgorithm(String),

    /// Hash calculation failed
    HashCalculationError(String),

    /// Chunk data corrupted
    CorruptedData {
        chunk: u64,
        reason: String,
    },
}
<span class="boring">}</span></code></pre></pre>
<h3 id="error-recovery"><a class="header" href="#error-recovery">Error Recovery</a></h3>
<p>Handle integrity errors gracefully:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl ChecksumProcessor {
    pub fn process_with_retry(
        &amp;self,
        chunk: &amp;FileChunk,
        max_retries: u32
    ) -&gt; Result&lt;FileChunk, PipelineError&gt; {
        let mut attempts = 0;

        loop {
            match self.process_chunk(chunk) {
                Ok(result) =&gt; return Ok(result),
                Err(PipelineError::IntegrityError(msg)) if attempts &lt; max_retries =&gt; {
                    attempts += 1;
                    eprintln!("Integrity check failed (attempt {}/{}): {}",
                        attempts, max_retries, msg);
                    continue;
                }
                Err(e) =&gt; return Err(e),
            }
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="usage-examples-3"><a class="header" href="#usage-examples-3">Usage Examples</a></h2>
<h3 id="basic-checksum-calculation"><a class="header" href="#basic-checksum-calculation">Basic Checksum Calculation</a></h3>
<p>Calculate SHA-256 checksums for data:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::services::ChecksumProcessor;

fn calculate_file_checksum(data: &amp;[u8]) -&gt; Result&lt;String, PipelineError&gt; {
    let processor = ChecksumProcessor::sha256_processor(false);
    let checksum = processor.calculate_sha256(data);
    Ok(checksum)
}

// Example usage
let data = b"Hello, world!";
let checksum = calculate_file_checksum(data)?;
println!("SHA-256: {}", checksum);
// Output: SHA-256: 315f5bdb76d078c43b8ac0064e4a0164612b1fce77c869345bfc94c75894edd3
<span class="boring">}</span></code></pre></pre>
<h3 id="integrity-verification-2"><a class="header" href="#integrity-verification-2">Integrity Verification</a></h3>
<p>Verify data hasn't been tampered with:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::value_objects::FileChunk;

fn verify_chunk_integrity(chunk: &amp;FileChunk) -&gt; Result&lt;bool, PipelineError&gt; {
    let processor = ChecksumProcessor::sha256_processor(true);

    // Process with verification enabled
    match processor.process_chunk(chunk) {
        Ok(_) =&gt; Ok(true),
        Err(PipelineError::IntegrityError(_)) =&gt; Ok(false),
        Err(e) =&gt; Err(e),
    }
}

// Example usage
let chunk = FileChunk::new(0, data.to_vec())?
    .with_calculated_checksum()?;

if verify_chunk_integrity(&amp;chunk)? {
    println!("✓ Chunk integrity verified");
} else {
    println!("✗ Chunk has been tampered with!");
}
<span class="boring">}</span></code></pre></pre>
<h3 id="pipeline-integration"><a class="header" href="#pipeline-integration">Pipeline Integration</a></h3>
<p>Integrate checksums into processing pipeline:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::entities::{Pipeline, PipelineStage};

fn create_verified_pipeline() -&gt; Result&lt;Pipeline, PipelineError&gt; {
    let stages = vec![
        // Input verification
        PipelineStage::new(
            "input_checksum",
            StageType::Integrity,
            Algorithm::sha256(),
        )?,

        // Processing stages...
        PipelineStage::new(
            "compression",
            StageType::Compression,
            Algorithm::zstd(),
        )?,

        // Output verification
        PipelineStage::new(
            "output_checksum",
            StageType::Integrity,
            Algorithm::sha256(),
        )?,
    ];

    Pipeline::new("verified-pipeline".to_string(), stages)
}
<span class="boring">}</span></code></pre></pre>
<h3 id="parallel-processing-3"><a class="header" href="#parallel-processing-3">Parallel Processing</a></h3>
<p>Process multiple chunks with maximum performance:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use rayon::prelude::*;

fn hash_large_file(chunks: Vec&lt;FileChunk&gt;) -&gt; Result&lt;Vec&lt;String&gt;, PipelineError&gt; {
    let processor = ChecksumProcessor::sha256_processor(false);

    chunks.par_iter()
        .map(|chunk| processor.calculate_sha256(chunk.data()))
        .collect()
}

// Example: Hash 1000 chunks in parallel
let checksums = hash_large_file(chunks)?;
println!("Processed {} chunks", checksums.len());
<span class="boring">}</span></code></pre></pre>
<h2 id="benchmarks-2"><a class="header" href="#benchmarks-2">Benchmarks</a></h2>
<h3 id="sha-256-performance"><a class="header" href="#sha-256-performance">SHA-256 Performance</a></h3>
<p><strong>File Size: 100 MB, Chunk Size: 1 MB</strong></p>
<div class="table-wrapper"><table><thead><tr><th>Configuration</th><th>Throughput</th><th>Total Time</th><th>CPU Usage</th></tr></thead><tbody>
<tr><td>Single-threaded</td><td>500 MB/s</td><td>200ms</td><td>100% (1 core)</td></tr>
<tr><td>Parallel (4 cores)</td><td>1.8 GB/s</td><td>56ms</td><td>400% (4 cores)</td></tr>
<tr><td>Hardware accel</td><td>2.0 GB/s</td><td>50ms</td><td>100% (1 core)</td></tr>
</tbody></table>
</div>
<h3 id="sha-512-performance"><a class="header" href="#sha-512-performance">SHA-512 Performance</a></h3>
<p><strong>File Size: 100 MB, Chunk Size: 1 MB</strong></p>
<div class="table-wrapper"><table><thead><tr><th>Configuration</th><th>Throughput</th><th>Total Time</th><th>CPU Usage</th></tr></thead><tbody>
<tr><td>Single-threaded</td><td>400 MB/s</td><td>250ms</td><td>100% (1 core)</td></tr>
<tr><td>Parallel (4 cores)</td><td>1.5 GB/s</td><td>67ms</td><td>400% (4 cores)</td></tr>
</tbody></table>
</div>
<h3 id="blake3-performance"><a class="header" href="#blake3-performance">BLAKE3 Performance</a></h3>
<p><strong>File Size: 100 MB, Chunk Size: 1 MB</strong></p>
<div class="table-wrapper"><table><thead><tr><th>Configuration</th><th>Throughput</th><th>Total Time</th><th>CPU Usage</th></tr></thead><tbody>
<tr><td>Single-threaded</td><td>1.2 GB/s</td><td>83ms</td><td>100% (1 core)</td></tr>
<tr><td>Parallel (4 cores)</td><td>3.2 GB/s</td><td>31ms</td><td>400% (4 cores)</td></tr>
<tr><td>SIMD optimized</td><td>3.5 GB/s</td><td>29ms</td><td>100% (1 core)</td></tr>
</tbody></table>
</div>
<p><strong>Test Environment:</strong> Intel i7-10700K @ 3.8 GHz, 32GB RAM, Ubuntu 22.04</p>
<h3 id="algorithm-recommendations-by-use-case"><a class="header" href="#algorithm-recommendations-by-use-case">Algorithm Recommendations by Use Case</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Use Case</th><th>Recommended Algorithm</th><th>Reason</th></tr></thead><tbody>
<tr><td>General integrity</td><td>SHA-256</td><td>Industry standard, FIPS certified</td></tr>
<tr><td>High security</td><td>SHA-512</td><td>Larger output, stronger security margin</td></tr>
<tr><td>High throughput</td><td>BLAKE3</td><td>3-6x faster, highly parallelizable</td></tr>
<tr><td>Compliance</td><td>SHA-256</td><td>FIPS 180-4 certified</td></tr>
<tr><td>Archival</td><td>SHA-512</td><td>Future-proof security</td></tr>
<tr><td>Real-time</td><td>BLAKE3</td><td>Lowest latency</td></tr>
</tbody></table>
</div>
<h2 id="best-practices-3"><a class="header" href="#best-practices-3">Best Practices</a></h2>
<h3 id="algorithm-selection-2"><a class="header" href="#algorithm-selection-2">Algorithm Selection</a></h3>
<p><strong>Choose the right algorithm for your requirements:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Compliance requirements
if needs_fips_compliance {
    Algorithm::sha256() // FIPS 180-4 certified
}
// Maximum security
else if security_level == SecurityLevel::Maximum {
    Algorithm::sha512() // Stronger security margin
}
// Performance critical
else if throughput_priority {
    Algorithm::blake3() // 3-6x faster
}
// Default
else {
    Algorithm::sha256() // Industry standard
}
<span class="boring">}</span></code></pre></pre>
<h3 id="verification-strategy"><a class="header" href="#verification-strategy">Verification Strategy</a></h3>
<p><strong>Implement defense-in-depth verification:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 1. Input verification (detect source corruption)
let input_checksum_stage = PipelineStage::new(
    "input_verify",
    StageType::Integrity,
    Algorithm::sha256(),
)?;

// 2. Processing stages...

// 3. Output verification (detect processing corruption)
let output_checksum_stage = PipelineStage::new(
    "output_verify",
    StageType::Integrity,
    Algorithm::sha256(),
)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="performance-optimization"><a class="header" href="#performance-optimization">Performance Optimization</a></h3>
<p><strong>Optimize for your workload:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Small files (&lt;10 MB): Use single-threaded
if file_size &lt; 10 * 1024 * 1024 {
    processor.calculate_sha256(data)
}
// Large files: Use parallel processing
else {
    processor.process_chunks_parallel(&amp;chunks)
}

// Hardware acceleration available: Use SHA-256
if has_sha_extensions() {
    Algorithm::sha256()
}
// No hardware acceleration: Use BLAKE3
else {
    Algorithm::blake3()
}
<span class="boring">}</span></code></pre></pre>
<h3 id="error-handling-3"><a class="header" href="#error-handling-3">Error Handling</a></h3>
<p><strong>Handle integrity failures appropriately:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>match processor.process_chunk(&amp;chunk) {
    Ok(verified_chunk) =&gt; {
        // Integrity verified, continue processing
        process_chunk(verified_chunk)
    }
    Err(PipelineError::IntegrityError(msg)) =&gt; {
        // Log error and attempt recovery
        eprintln!("Integrity failure: {}", msg);

        // Option 1: Retry from source
        let fresh_chunk = reload_chunk_from_source()?;
        processor.process_chunk(&amp;fresh_chunk)
    }
    Err(e) =&gt; return Err(e),
}
<span class="boring">}</span></code></pre></pre>
<h2 id="security-considerations-1"><a class="header" href="#security-considerations-1">Security Considerations</a></h2>
<h3 id="cryptographic-strength"><a class="header" href="#cryptographic-strength">Cryptographic Strength</a></h3>
<p><strong>All supported algorithms are cryptographically secure:</strong></p>
<ul>
<li><strong>SHA-256</strong>: 128-bit security level (2^128 operations for collision)</li>
<li><strong>SHA-512</strong>: 256-bit security level (2^256 operations for collision)</li>
<li><strong>BLAKE3</strong>: 128-bit security level (based on ChaCha20)</li>
</ul>
<h3 id="collision-resistance"><a class="header" href="#collision-resistance">Collision Resistance</a></h3>
<p><strong>Practical collision resistance:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// SHA-256 collision resistance: ~2^128 operations
// Effectively impossible with current technology
let sha256_security_bits = 128;

// SHA-512 collision resistance: ~2^256 operations
// Provides future-proof security margin
let sha512_security_bits = 256;
<span class="boring">}</span></code></pre></pre>
<h3 id="tampering-detection"><a class="header" href="#tampering-detection">Tampering Detection</a></h3>
<p><strong>Checksums detect any modification:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Even single-bit changes produce completely different hashes
let original = "Hello, World!";
let tampered = "Hello, world!"; // Changed 'W' to 'w'

let hash1 = processor.calculate_sha256(original.as_bytes());
let hash2 = processor.calculate_sha256(tampered.as_bytes());

assert_ne!(hash1, hash2); // Completely different hashes
<span class="boring">}</span></code></pre></pre>
<h3 id="not-for-authentication"><a class="header" href="#not-for-authentication">Not for Authentication</a></h3>
<p><strong>Important:</strong> Checksums alone don't provide authentication:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ❌ WRONG: Checksum alone doesn't prove authenticity
let checksum = calculate_sha256(data);
// Attacker can modify data AND update checksum

// ✅ CORRECT: Use HMAC for authentication
let hmac = calculate_hmac_sha256(data, secret_key);
// Attacker cannot forge HMAC without secret key
<span class="boring">}</span></code></pre></pre>
<p><strong>Use HMAC or digital signatures for authentication.</strong></p>
<h2 id="next-steps-13"><a class="header" href="#next-steps-13">Next Steps</a></h2>
<p>Now that you understand integrity verification:</p>
<ul>
<li><a href="implementation/repositories.html">Repositories</a> - Data persistence patterns</li>
<li><a href="implementation/binary-format.html">Binary Format</a> - File format with embedded checksums</li>
<li><a href="implementation/../advanced/error-handling.html">Error Handling</a> - Comprehensive error strategies</li>
<li><a href="implementation/../advanced/performance.html">Performance</a> - Advanced optimization techniques</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-persistence"><a class="header" href="#data-persistence">Data Persistence</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>This chapter provides a comprehensive overview of the data persistence architecture in the adaptive pipeline system. Learn how the repository pattern, SQLite database, and schema management work together to provide reliable, efficient data storage.</p>
<hr />
<h2 id="table-of-contents-1"><a class="header" href="#table-of-contents-1">Table of Contents</a></h2>
<ul>
<li><a href="implementation/persistence.html#overview">Overview</a></li>
<li><a href="implementation/persistence.html#persistence-architecture">Persistence Architecture</a></li>
<li><a href="implementation/persistence.html#repository-pattern">Repository Pattern</a></li>
<li><a href="implementation/persistence.html#database-choice-sqlite">Database Choice: SQLite</a></li>
<li><a href="implementation/persistence.html#storage-architecture">Storage Architecture</a></li>
<li><a href="implementation/persistence.html#transaction-management">Transaction Management</a></li>
<li><a href="implementation/persistence.html#connection-management">Connection Management</a></li>
<li><a href="implementation/persistence.html#data-mapping">Data Mapping</a></li>
<li><a href="implementation/persistence.html#performance-optimization">Performance Optimization</a></li>
<li><a href="implementation/persistence.html#usage-examples">Usage Examples</a></li>
<li><a href="implementation/persistence.html#best-practices">Best Practices</a></li>
<li><a href="implementation/persistence.html#troubleshooting">Troubleshooting</a></li>
<li><a href="implementation/persistence.html#testing-strategies">Testing Strategies</a></li>
<li><a href="implementation/persistence.html#next-steps">Next Steps</a></li>
</ul>
<hr />
<h2 id="overview-8"><a class="header" href="#overview-8">Overview</a></h2>
<p><strong>Data persistence</strong> in the adaptive pipeline system follows Domain-Driven Design principles, separating domain logic from infrastructure concerns through the repository pattern. The system uses SQLite for reliable, zero-configuration data storage with full ACID transaction support.</p>
<h3 id="key-features"><a class="header" href="#key-features">Key Features</a></h3>
<ul>
<li><strong>Repository Pattern</strong>: Abstraction layer between domain and infrastructure</li>
<li><strong>SQLite Database</strong>: Embedded database with zero configuration</li>
<li><strong>Schema Management</strong>: Automated migrations with sqlx</li>
<li><strong>ACID Transactions</strong>: Full transactional support for data consistency</li>
<li><strong>Connection Pooling</strong>: Efficient connection management</li>
<li><strong>Type Safety</strong>: Compile-time query validation</li>
</ul>
<h3 id="persistence-stack"><a class="header" href="#persistence-stack">Persistence Stack</a></h3>
<pre><code class="language-text">┌──────────────────────────────────────────────────────────┐
│                    Domain Layer                          │
│  ┌────────────────────────────────────────────────┐     │
│  │   PipelineRepository (Trait)                   │     │
│  │   - save(), find_by_id(), list_all()          │     │
│  └────────────────────────────────────────────────┘     │
└──────────────────────────────────────────────────────────┘
                         ↓ implements
┌──────────────────────────────────────────────────────────┐
│                Infrastructure Layer                       │
│  ┌────────────────────────────────────────────────┐     │
│  │   SqlitePipelineRepository                     │     │
│  │   - Concrete SQLite implementation             │     │
│  └────────────────────────────────────────────────┘     │
│                         ↓ uses                           │
│  ┌────────────────────────────────────────────────┐     │
│  │   Schema Management                            │     │
│  │   - Migrations, initialization                 │     │
│  └────────────────────────────────────────────────┘     │
└──────────────────────────────────────────────────────────┘
                         ↓ persists to
┌──────────────────────────────────────────────────────────┐
│                  SQLite Database                         │
│  ┌────────────┬──────────────┬──────────────────┐      │
│  │ pipelines  │pipeline_stage│pipeline_config   │      │
│  └────────────┴──────────────┴──────────────────┘      │
└──────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="design-principles-1"><a class="header" href="#design-principles-1">Design Principles</a></h3>
<ol>
<li><strong>Separation of Concerns</strong>: Domain logic independent of storage technology</li>
<li><strong>Testability</strong>: Easy mocking with in-memory implementations</li>
<li><strong>Flexibility</strong>: Support for different storage backends</li>
<li><strong>Consistency</strong>: ACID transactions ensure data integrity</li>
<li><strong>Performance</strong>: Connection pooling and query optimization</li>
</ol>
<hr />
<h2 id="persistence-architecture"><a class="header" href="#persistence-architecture">Persistence Architecture</a></h2>
<p>The persistence layer follows a three-tier architecture aligned with Domain-Driven Design.</p>
<h3 id="architectural-layers"><a class="header" href="#architectural-layers">Architectural Layers</a></h3>
<pre><code class="language-text">┌─────────────────────────────────────────────────────────────┐
│ Application Layer                                           │
│  - PipelineService uses repository trait                    │
│  - Business logic remains persistence-agnostic              │
└─────────────────────────────────────────────────────────────┘
                         ↓
┌─────────────────────────────────────────────────────────────┐
│ Domain Layer                                                │
│  - PipelineRepository trait (abstract interface)            │
│  - Pipeline, PipelineStage entities                         │
│  - No infrastructure dependencies                           │
└─────────────────────────────────────────────────────────────┘
                         ↓
┌─────────────────────────────────────────────────────────────┐
│ Infrastructure Layer                                        │
│  - SqlitePipelineRepository (concrete implementation)       │
│  - Schema management and migrations                         │
│  - Connection pooling and transaction management            │
└─────────────────────────────────────────────────────────────┘
                         ↓
┌─────────────────────────────────────────────────────────────┐
│ Storage Layer                                               │
│  - SQLite database file                                     │
│  - Indexes and constraints                                  │
│  - Migration history tracking                               │
└─────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="component-relationships"><a class="header" href="#component-relationships">Component Relationships</a></h3>
<p><strong>Domain-to-Infrastructure Flow:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Application code depends on domain trait
use pipeline_domain::repositories::PipelineRepository;

async fn create_pipeline(
    repo: &amp;dyn PipelineRepository,
    name: String,
) -&gt; Result&lt;Pipeline, PipelineError&gt; {
    let pipeline = Pipeline::new(name)?;
    repo.save(&amp;pipeline).await?;
    Ok(pipeline)
}

// Infrastructure provides concrete implementation
use pipeline::infrastructure::repositories::SqlitePipelineRepository;

let repository = SqlitePipelineRepository::new(pool);
let pipeline = create_pipeline(&amp;repository, "my-pipeline".to_string()).await?;
<span class="boring">}</span></code></pre></pre>
<h3 id="benefits-of-this-architecture"><a class="header" href="#benefits-of-this-architecture">Benefits of This Architecture</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Benefit</th><th>Description</th></tr></thead><tbody>
<tr><td><strong>Domain Independence</strong></td><td>Business logic doesn't depend on SQLite specifics</td></tr>
<tr><td><strong>Testability</strong></td><td>Easy to mock repositories for unit testing</td></tr>
<tr><td><strong>Flexibility</strong></td><td>Can swap SQLite for PostgreSQL without changing domain</td></tr>
<tr><td><strong>Maintainability</strong></td><td>Clear separation makes code easier to understand</td></tr>
<tr><td><strong>Type Safety</strong></td><td>Compile-time verification of database operations</td></tr>
</tbody></table>
</div>
<hr />
<h2 id="repository-pattern-1"><a class="header" href="#repository-pattern-1">Repository Pattern</a></h2>
<p>The repository pattern provides an abstraction layer between domain entities and data storage.</p>
<h3 id="repository-pattern-benefits"><a class="header" href="#repository-pattern-benefits">Repository Pattern Benefits</a></h3>
<p><strong>1. Separation of Concerns</strong></p>
<p>Domain logic remains free from persistence details:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Domain layer - storage-agnostic
#[async_trait]
pub trait PipelineRepository: Send + Sync {
    async fn save(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt;;
    async fn find_by_id(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;Option&lt;Pipeline&gt;, PipelineError&gt;;
    async fn list_all(&amp;self) -&gt; Result&lt;Vec&lt;Pipeline&gt;, PipelineError&gt;;
    async fn delete(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;bool, PipelineError&gt;;
}
<span class="boring">}</span></code></pre></pre>
<p><strong>2. Implementation Flexibility</strong></p>
<p>Multiple storage backends can implement the same interface:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// SQLite implementation
pub struct SqlitePipelineRepository { /* ... */ }

// PostgreSQL implementation
pub struct PostgresPipelineRepository { /* ... */ }

// In-memory testing implementation
pub struct InMemoryPipelineRepository { /* ... */ }

// All implement the same PipelineRepository trait
<span class="boring">}</span></code></pre></pre>
<p><strong>3. Enhanced Testability</strong></p>
<p>Mock implementations simplify testing:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(test)]
struct MockPipelineRepository {
    pipelines: Arc&lt;Mutex&lt;HashMap&lt;PipelineId, Pipeline&gt;&gt;&gt;,
}

#[async_trait]
impl PipelineRepository for MockPipelineRepository {
    async fn save(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt; {
        let mut pipelines = self.pipelines.lock().await;
        pipelines.insert(pipeline.id().clone(), pipeline.clone());
        Ok(())
    }
    // ... implement other methods
}
<span class="boring">}</span></code></pre></pre>
<h3 id="repository-interface-design"><a class="header" href="#repository-interface-design">Repository Interface Design</a></h3>
<p><strong>Method Categories:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>Category</th><th>Methods</th><th>Purpose</th></tr></thead><tbody>
<tr><td><strong>CRUD</strong></td><td><code>save()</code>, <code>find_by_id()</code>, <code>update()</code>, <code>delete()</code></td><td>Basic operations</td></tr>
<tr><td><strong>Queries</strong></td><td><code>find_by_name()</code>, <code>find_all()</code>, <code>list_paginated()</code></td><td>Data retrieval</td></tr>
<tr><td><strong>Validation</strong></td><td><code>exists()</code>, <code>count()</code></td><td>Existence checks</td></tr>
<tr><td><strong>Lifecycle</strong></td><td><code>archive()</code>, <code>restore()</code>, <code>list_archived()</code></td><td>Soft deletion</td></tr>
<tr><td><strong>Search</strong></td><td><code>find_by_config()</code></td><td>Advanced queries</td></tr>
</tbody></table>
</div>
<p><strong>Complete Interface:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[async_trait]
pub trait PipelineRepository: Send + Sync {
    // CRUD Operations
    async fn save(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt;;
    async fn find_by_id(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;Option&lt;Pipeline&gt;, PipelineError&gt;;
    async fn update(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt;;
    async fn delete(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;bool, PipelineError&gt;;

    // Query Operations
    async fn find_by_name(&amp;self, name: &amp;str) -&gt; Result&lt;Option&lt;Pipeline&gt;, PipelineError&gt;;
    async fn find_all(&amp;self) -&gt; Result&lt;Vec&lt;Pipeline&gt;, PipelineError&gt;;
    async fn list_all(&amp;self) -&gt; Result&lt;Vec&lt;Pipeline&gt;, PipelineError&gt;;
    async fn list_paginated(&amp;self, offset: usize, limit: usize)
        -&gt; Result&lt;Vec&lt;Pipeline&gt;, PipelineError&gt;;

    // Validation Operations
    async fn exists(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;bool, PipelineError&gt;;
    async fn count(&amp;self) -&gt; Result&lt;usize, PipelineError&gt;;

    // Lifecycle Operations
    async fn archive(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;bool, PipelineError&gt;;
    async fn restore(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;bool, PipelineError&gt;;
    async fn list_archived(&amp;self) -&gt; Result&lt;Vec&lt;Pipeline&gt;, PipelineError&gt;;

    // Search Operations
    async fn find_by_config(&amp;self, key: &amp;str, value: &amp;str)
        -&gt; Result&lt;Vec&lt;Pipeline&gt;, PipelineError&gt;;
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="database-choice-sqlite"><a class="header" href="#database-choice-sqlite">Database Choice: SQLite</a></h2>
<p>The system uses <strong>SQLite</strong> as the default database for its simplicity, reliability, and zero-configuration deployment.</p>
<h3 id="why-sqlite"><a class="header" href="#why-sqlite">Why SQLite?</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Advantage</th><th>Description</th></tr></thead><tbody>
<tr><td><strong>Zero Configuration</strong></td><td>No database server to install or configure</td></tr>
<tr><td><strong>Single File</strong></td><td>Entire database stored in one file</td></tr>
<tr><td><strong>ACID Compliant</strong></td><td>Full transactional support</td></tr>
<tr><td><strong>Cross-Platform</strong></td><td>Works on Linux, macOS, Windows</td></tr>
<tr><td><strong>Embedded</strong></td><td>Runs in-process, no network overhead</td></tr>
<tr><td><strong>Reliable</strong></td><td>Battle-tested, used in production worldwide</td></tr>
<tr><td><strong>Fast</strong></td><td>Optimized for local file access</td></tr>
</tbody></table>
</div>
<h3 id="sqlite-characteristics"><a class="header" href="#sqlite-characteristics">SQLite Characteristics</a></h3>
<p><strong>Performance Profile:</strong></p>
<pre><code class="language-text">Operation          | Speed      | Notes
-------------------|------------|--------------------------------
Single INSERT      | ~0.1ms     | Very fast for local file
Batch INSERT       | ~10ms/1000 | Use transactions for batching
Single SELECT      | ~0.05ms    | Fast with proper indexes
Complex JOIN       | ~1-5ms     | Depends on dataset size
Full table scan    | ~10ms/10K  | Avoid without indexes
</code></pre>
<p><strong>Limitations:</strong></p>
<ul>
<li><strong>Concurrent Writes</strong>: Only one writer at a time (readers can be concurrent)</li>
<li><strong>Network Access</strong>: Not designed for network file systems</li>
<li><strong>Database Size</strong>: Practical limit ~281 TB (theoretical limit)</li>
<li><strong>Scalability</strong>: Best for single-server deployments</li>
</ul>
<p><strong>When SQLite is Ideal:</strong></p>
<p>✅ Single-server applications
✅ Embedded systems
✅ Desktop applications
✅ Development and testing
✅ Low-to-medium write concurrency</p>
<p><strong>When to Consider Alternatives:</strong></p>
<p>❌ High concurrent write workload
❌ Multi-server deployments
❌ Network file systems
❌ Very large datasets (&gt; 100GB)</p>
<h3 id="sqlite-configuration"><a class="header" href="#sqlite-configuration">SQLite Configuration</a></h3>
<p><strong>Connection String:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Local file database
let url = "sqlite://./pipeline.db";

// In-memory database (testing)
let url = "sqlite::memory:";

// Custom connection options
use sqlx::sqlite::SqliteConnectOptions;
let options = SqliteConnectOptions::new()
    .filename("./pipeline.db")
    .create_if_missing(true)
    .foreign_keys(true)
    .journal_mode(sqlx::sqlite::SqliteJournalMode::Wal);
<span class="boring">}</span></code></pre></pre>
<p><strong>Connection Pool Configuration:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use sqlx::sqlite::SqlitePoolOptions;

let pool = SqlitePoolOptions::new()
    .max_connections(5)          // Connection pool size
    .min_connections(1)          // Minimum connections
    .acquire_timeout(Duration::from_secs(30))
    .idle_timeout(Duration::from_secs(600))
    .connect(&amp;database_url)
    .await?;
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="storage-architecture"><a class="header" href="#storage-architecture">Storage Architecture</a></h2>
<p>The storage layer uses a normalized relational schema with five core tables.</p>
<h3 id="database-schema-overview"><a class="header" href="#database-schema-overview">Database Schema Overview</a></h3>
<pre><code class="language-text">┌─────────────┐
│  pipelines  │ (id, name, archived, created_at, updated_at)
└──────┬──────┘
       │ 1:N
       ├──────────────────────────┐
       │                          │
       ↓                          ↓
┌──────────────────┐    ┌──────────────────┐
│ pipeline_stages  │    │pipeline_config   │
│ (id, pipeline_id,│    │(pipeline_id, key,│
│  name, type,     │    │ value)           │
│  algorithm, ...)  │    └──────────────────┘
└──────┬───────────┘
       │ 1:N
       ↓
┌──────────────────┐
│stage_parameters  │
│(stage_id, key,   │
│ value)           │
└──────────────────┘
</code></pre>
<h3 id="table-purposes"><a class="header" href="#table-purposes">Table Purposes</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Table</th><th>Purpose</th><th>Key Fields</th></tr></thead><tbody>
<tr><td><strong>pipelines</strong></td><td>Core pipeline entity</td><td>id (PK), name (UNIQUE), archived</td></tr>
<tr><td><strong>pipeline_stages</strong></td><td>Stage definitions</td><td>id (PK), pipeline_id (FK), stage_order</td></tr>
<tr><td><strong>pipeline_configuration</strong></td><td>Pipeline-level config</td><td>(pipeline_id, key) composite PK</td></tr>
<tr><td><strong>stage_parameters</strong></td><td>Stage-level parameters</td><td>(stage_id, key) composite PK</td></tr>
<tr><td><strong>processing_metrics</strong></td><td>Execution metrics</td><td>pipeline_id (PK/FK), progress, performance</td></tr>
</tbody></table>
</div>
<h3 id="schema-design-principles"><a class="header" href="#schema-design-principles">Schema Design Principles</a></h3>
<p><strong>1. Normalization</strong></p>
<p>Data is normalized to reduce redundancy:</p>
<pre><code class="language-sql">-- ✅ Normalized: Stages reference pipeline
CREATE TABLE pipeline_stages (
    id TEXT PRIMARY KEY,
    pipeline_id TEXT NOT NULL,  -- Foreign key
    FOREIGN KEY (pipeline_id) REFERENCES pipelines(id) ON DELETE CASCADE
);

-- ❌ Denormalized: Duplicating pipeline data
-- Each stage would store pipeline name, created_at, etc.
</code></pre>
<p><strong>2. Referential Integrity</strong></p>
<p>Foreign keys enforce data consistency:</p>
<pre><code class="language-sql">-- CASCADE DELETE: Deleting pipeline removes all stages
FOREIGN KEY (pipeline_id) REFERENCES pipelines(id) ON DELETE CASCADE

-- Orphaned stages cannot exist
</code></pre>
<p><strong>3. Indexing Strategy</strong></p>
<p>Indexes optimize common queries:</p>
<pre><code class="language-sql">-- Index on foreign keys for JOIN performance
CREATE INDEX idx_pipeline_stages_pipeline_id ON pipeline_stages(pipeline_id);

-- Index on frequently queried fields
CREATE INDEX idx_pipelines_name ON pipelines(name);
CREATE INDEX idx_pipelines_archived ON pipelines(archived);
</code></pre>
<p><strong>4. Timestamps</strong></p>
<p>All entities track creation and modification:</p>
<pre><code class="language-sql">created_at TEXT NOT NULL,  -- RFC 3339 format
updated_at TEXT NOT NULL   -- RFC 3339 format
</code></pre>
<h3 id="schema-initialization"><a class="header" href="#schema-initialization">Schema Initialization</a></h3>
<p><strong>Automated Migration System:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline::infrastructure::repositories::schema;

// High-level initialization (recommended)
let pool = schema::initialize_database("sqlite://./pipeline.db").await?;
// Database created, migrations applied, ready to use!

// Manual initialization
schema::create_database_if_missing("sqlite://./pipeline.db").await?;
let pool = SqlitePool::connect("sqlite://./pipeline.db").await?;
schema::ensure_schema(&amp;pool).await?;
<span class="boring">}</span></code></pre></pre>
<p><strong>Migration Tracking:</strong></p>
<pre><code class="language-sql">-- sqlx automatically creates this table
CREATE TABLE _sqlx_migrations (
    version BIGINT PRIMARY KEY,
    description TEXT NOT NULL,
    installed_on TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    success BOOLEAN NOT NULL,
    checksum BLOB NOT NULL,
    execution_time BIGINT NOT NULL
);
</code></pre>
<p>For complete schema details, see <a href="implementation/schema.html">Schema Management</a>.</p>
<hr />
<h2 id="transaction-management"><a class="header" href="#transaction-management">Transaction Management</a></h2>
<p>SQLite provides full ACID transaction support for data consistency.</p>
<h3 id="acid-properties"><a class="header" href="#acid-properties">ACID Properties</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Property</th><th>SQLite Implementation</th></tr></thead><tbody>
<tr><td><strong>Atomicity</strong></td><td>All-or-nothing commits via rollback journal</td></tr>
<tr><td><strong>Consistency</strong></td><td>Foreign keys, constraints enforce invariants</td></tr>
<tr><td><strong>Isolation</strong></td><td>Serializable isolation (single writer)</td></tr>
<tr><td><strong>Durability</strong></td><td>WAL mode ensures data persists after commit</td></tr>
</tbody></table>
</div>
<h3 id="transaction-usage"><a class="header" href="#transaction-usage">Transaction Usage</a></h3>
<p><strong>Explicit Transactions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Begin transaction
let mut tx = pool.begin().await?;

// Perform multiple operations
sqlx::query("INSERT INTO pipelines (id, name, created_at, updated_at) VALUES (?, ?, ?, ?)")
    .bind(&amp;id)
    .bind(&amp;name)
    .bind(&amp;now)
    .bind(&amp;now)
    .execute(&amp;mut *tx)
    .await?;

sqlx::query("INSERT INTO pipeline_stages (id, pipeline_id, name, stage_type, stage_order, algorithm, created_at, updated_at) VALUES (?, ?, ?, ?, ?, ?, ?, ?)")
    .bind(&amp;stage_id)
    .bind(&amp;id)
    .bind("compression")
    .bind("compression")
    .bind(0)
    .bind("brotli")
    .bind(&amp;now)
    .bind(&amp;now)
    .execute(&amp;mut *tx)
    .await?;

// Commit transaction (or rollback on error)
tx.commit().await?;
<span class="boring">}</span></code></pre></pre>
<p><strong>Automatic Rollback:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn save_pipeline_with_stages(
    pool: &amp;SqlitePool,
    pipeline: &amp;Pipeline,
) -&gt; Result&lt;(), PipelineError&gt; {
    let mut tx = pool.begin().await?;

    // Insert pipeline
    insert_pipeline(&amp;mut tx, pipeline).await?;

    // Insert all stages
    for stage in pipeline.stages() {
        insert_stage(&amp;mut tx, stage).await?;
    }

    // Commit (or automatic rollback if any operation fails)
    tx.commit().await?;
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h3 id="transaction-best-practices"><a class="header" href="#transaction-best-practices">Transaction Best Practices</a></h3>
<p><strong>1. Keep Transactions Short</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Short transaction
let mut tx = pool.begin().await?;
sqlx::query("INSERT INTO ...").execute(&amp;mut *tx).await?;
tx.commit().await?;

// ❌ Bad: Long-running transaction
let mut tx = pool.begin().await?;
expensive_computation().await;  // Don't do this inside transaction!
sqlx::query("INSERT INTO ...").execute(&amp;mut *tx).await?;
tx.commit().await?;
<span class="boring">}</span></code></pre></pre>
<p><strong>2. Handle Errors Gracefully</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>match save_pipeline(&amp;pool, &amp;pipeline).await {
    Ok(()) =&gt; info!("Pipeline saved successfully"),
    Err(e) =&gt; {
        error!("Failed to save pipeline: {}", e);
        // Transaction automatically rolled back
        return Err(e);
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>3. Use Connection Pool</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Use pool for automatic connection management
async fn save(pool: &amp;SqlitePool, data: &amp;Data) -&gt; Result&lt;(), Error&gt; {
    sqlx::query("INSERT ...").execute(pool).await?;
    Ok(())
}

// ❌ Bad: Creating new connections
async fn save(url: &amp;str, data: &amp;Data) -&gt; Result&lt;(), Error&gt; {
    let pool = SqlitePool::connect(url).await?;  // Expensive!
    sqlx::query("INSERT ...").execute(&amp;pool).await?;
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="connection-management"><a class="header" href="#connection-management">Connection Management</a></h2>
<p>Efficient connection management is crucial for performance and resource utilization.</p>
<h3 id="connection-pooling"><a class="header" href="#connection-pooling">Connection Pooling</a></h3>
<p><strong>SqlitePool Benefits:</strong></p>
<ul>
<li><strong>Connection Reuse</strong>: Avoid overhead of creating new connections</li>
<li><strong>Concurrency Control</strong>: Limit concurrent database access</li>
<li><strong>Automatic Cleanup</strong>: Close idle connections automatically</li>
<li><strong>Health Monitoring</strong>: Detect and recover from connection failures</li>
</ul>
<p><strong>Pool Configuration:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use sqlx::sqlite::SqlitePoolOptions;
use std::time::Duration;

let pool = SqlitePoolOptions::new()
    // Maximum number of connections in pool
    .max_connections(5)

    // Minimum number of idle connections
    .min_connections(1)

    // Timeout for acquiring connection from pool
    .acquire_timeout(Duration::from_secs(30))

    // Close connections idle for this duration
    .idle_timeout(Duration::from_secs(600))

    // Maximum lifetime of a connection
    .max_lifetime(Duration::from_secs(3600))

    // Test connection before returning from pool
    .test_before_acquire(true)

    .connect(&amp;database_url)
    .await?;
<span class="boring">}</span></code></pre></pre>
<h3 id="connection-lifecycle"><a class="header" href="#connection-lifecycle">Connection Lifecycle</a></h3>
<pre><code class="language-text">1. Application requests connection
   ↓
2. Pool checks for available connection
   ├─ Available → Reuse existing connection
   └─ Not available → Create new connection (if under max)
   ↓
3. Application uses connection
   ↓
4. Application returns connection to pool
   ↓
5. Pool keeps connection alive (if under idle_timeout)
   ↓
6. Connection eventually closed (after max_lifetime)
</code></pre>
<h3 id="performance-tuning"><a class="header" href="#performance-tuning">Performance Tuning</a></h3>
<p><strong>Optimal Pool Size:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// For CPU-bound workloads
let pool_size = num_cpus::get();

// For I/O-bound workloads
let pool_size = num_cpus::get() * 2;

// For SQLite (single writer)
let pool_size = 5;  // Conservative for write-heavy workloads
<span class="boring">}</span></code></pre></pre>
<p><strong>Connection Timeout Strategies:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>Scenario</th><th>Timeout</th><th>Rationale</th></tr></thead><tbody>
<tr><td><strong>Web API</strong></td><td>5-10 seconds</td><td>Fail fast for user requests</td></tr>
<tr><td><strong>Background Job</strong></td><td>30-60 seconds</td><td>More tolerance for delays</td></tr>
<tr><td><strong>Batch Processing</strong></td><td>2-5 minutes</td><td>Long-running operations acceptable</td></tr>
</tbody></table>
</div>
<hr />
<h2 id="data-mapping-1"><a class="header" href="#data-mapping-1">Data Mapping</a></h2>
<p>Data mapping converts between domain entities and database records.</p>
<h3 id="entity-to-row-mapping"><a class="header" href="#entity-to-row-mapping">Entity-to-Row Mapping</a></h3>
<p><strong>Domain Entity:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct Pipeline {
    id: PipelineId,
    name: String,
    stages: Vec&lt;PipelineStage&gt;,
    archived: bool,
    created_at: DateTime&lt;Utc&gt;,
    updated_at: DateTime&lt;Utc&gt;,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Database Row:</strong></p>
<pre><code class="language-sql">CREATE TABLE pipelines (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL UNIQUE,
    archived BOOLEAN NOT NULL DEFAULT false,
    created_at TEXT NOT NULL,
    updated_at TEXT NOT NULL
);
</code></pre>
<p><strong>Mapping Logic:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Domain → Database (serialize)
let id_str = pipeline.id().to_string();
let name_str = pipeline.name();
let archived_bool = pipeline.is_archived();
let created_at_str = pipeline.created_at().to_rfc3339();
let updated_at_str = pipeline.updated_at().to_rfc3339();

sqlx::query("INSERT INTO pipelines (id, name, archived, created_at, updated_at) VALUES (?, ?, ?, ?, ?)")
    .bind(id_str)
    .bind(name_str)
    .bind(archived_bool)
    .bind(created_at_str)
    .bind(updated_at_str)
    .execute(pool)
    .await?;

// Database → Domain (deserialize)
let row = sqlx::query("SELECT * FROM pipelines WHERE id = ?")
    .bind(id_str)
    .fetch_one(pool)
    .await?;

let id = PipelineId::from(row.get::&lt;String, _&gt;("id"));
let name = row.get::&lt;String, _&gt;("name");
let archived = row.get::&lt;bool, _&gt;("archived");
let created_at = DateTime::parse_from_rfc3339(row.get::&lt;String, _&gt;("created_at"))?;
let updated_at = DateTime::parse_from_rfc3339(row.get::&lt;String, _&gt;("updated_at"))?;
<span class="boring">}</span></code></pre></pre>
<h3 id="type-conversions"><a class="header" href="#type-conversions">Type Conversions</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Rust Type</th><th>SQLite Type</th><th>Conversion</th></tr></thead><tbody>
<tr><td><code>String</code></td><td><code>TEXT</code></td><td>Direct mapping</td></tr>
<tr><td><code>i64</code></td><td><code>INTEGER</code></td><td>Direct mapping</td></tr>
<tr><td><code>f64</code></td><td><code>REAL</code></td><td>Direct mapping</td></tr>
<tr><td><code>bool</code></td><td><code>INTEGER</code> (0/1)</td><td><code>sqlx</code> handles conversion</td></tr>
<tr><td><code>DateTime&lt;Utc&gt;</code></td><td><code>TEXT</code></td><td>RFC 3339 string format</td></tr>
<tr><td><code>PipelineId</code></td><td><code>TEXT</code></td><td>UUID string representation</td></tr>
<tr><td><code>Vec&lt;u8&gt;</code></td><td><code>BLOB</code></td><td>Direct binary mapping</td></tr>
<tr><td><code>Option&lt;T&gt;</code></td><td><code>NULL</code> / value</td><td><code>NULL</code> for <code>None</code></td></tr>
</tbody></table>
</div>
<h3 id="handling-relationships"><a class="header" href="#handling-relationships">Handling Relationships</a></h3>
<p><strong>One-to-Many (Pipeline → Stages):</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn load_pipeline_with_stages(
    pool: &amp;SqlitePool,
    id: &amp;PipelineId,
) -&gt; Result&lt;Pipeline, PipelineError&gt; {
    // Load pipeline
    let pipeline_row = sqlx::query("SELECT * FROM pipelines WHERE id = ?")
        .bind(id.to_string())
        .fetch_one(pool)
        .await?;

    // Load related stages
    let stage_rows = sqlx::query("SELECT * FROM pipeline_stages WHERE pipeline_id = ? ORDER BY stage_order")
        .bind(id.to_string())
        .fetch_all(pool)
        .await?;

    // Map to domain entities
    let pipeline = map_pipeline_row(pipeline_row)?;
    let stages = stage_rows.into_iter()
        .map(map_stage_row)
        .collect::&lt;Result&lt;Vec&lt;_&gt;, _&gt;&gt;()?;

    // Combine into aggregate
    pipeline.with_stages(stages)
}
<span class="boring">}</span></code></pre></pre>
<p>For detailed repository implementation, see <a href="implementation/repositories.html">Repository Implementation</a>.</p>
<hr />
<h2 id="performance-optimization-1"><a class="header" href="#performance-optimization-1">Performance Optimization</a></h2>
<p>Several strategies optimize persistence performance.</p>
<h3 id="query-optimization"><a class="header" href="#query-optimization">Query Optimization</a></h3>
<p><strong>1. Use Indexes Effectively</strong></p>
<pre><code class="language-sql">-- Index on frequently queried columns
CREATE INDEX idx_pipelines_name ON pipelines(name);
CREATE INDEX idx_pipelines_archived ON pipelines(archived);

-- Index on foreign keys for JOINs
CREATE INDEX idx_pipeline_stages_pipeline_id ON pipeline_stages(pipeline_id);
</code></pre>
<p><strong>2. Avoid N+1 Queries</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ❌ Bad: N+1 query problem
for pipeline_id in pipeline_ids {
    let pipeline = repo.find_by_id(&amp;pipeline_id).await?;
    // Process pipeline...
}

// ✅ Good: Single batch query
let pipelines = repo.find_all().await?;
for pipeline in pipelines {
    // Process pipeline...
}
<span class="boring">}</span></code></pre></pre>
<p><strong>3. Use Prepared Statements</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// sqlx automatically uses prepared statements
let pipeline = sqlx::query_as::&lt;_, Pipeline&gt;("SELECT * FROM pipelines WHERE id = ?")
    .bind(id)
    .fetch_one(pool)
    .await?;
<span class="boring">}</span></code></pre></pre>
<h3 id="connection-pool-tuning"><a class="header" href="#connection-pool-tuning">Connection Pool Tuning</a></h3>
<p><strong>Optimal Settings:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// For low-concurrency (CLI tools)
.max_connections(2)
.min_connections(1)

// For medium-concurrency (web services)
.max_connections(5)
.min_connections(2)

// For high-concurrency (not recommended for SQLite writes)
.max_connections(10)  // Reading only
.min_connections(5)
<span class="boring">}</span></code></pre></pre>
<h3 id="batch-operations"><a class="header" href="#batch-operations">Batch Operations</a></h3>
<p><strong>Batch Inserts:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn save_multiple_pipelines(
    pool: &amp;SqlitePool,
    pipelines: &amp;[Pipeline],
) -&gt; Result&lt;(), PipelineError&gt; {
    let mut tx = pool.begin().await?;

    for pipeline in pipelines {
        sqlx::query("INSERT INTO pipelines (id, name, created_at, updated_at) VALUES (?, ?, ?, ?)")
            .bind(pipeline.id().to_string())
            .bind(pipeline.name())
            .bind(pipeline.created_at().to_rfc3339())
            .bind(pipeline.updated_at().to_rfc3339())
            .execute(&amp;mut *tx)
            .await?;
    }

    tx.commit().await?;
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h3 id="performance-benchmarks"><a class="header" href="#performance-benchmarks">Performance Benchmarks</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Latency</th><th>Throughput</th><th>Notes</th></tr></thead><tbody>
<tr><td>Single INSERT</td><td>~0.1ms</td><td>~10K/sec</td><td>Without transaction</td></tr>
<tr><td>Batch INSERT (1000)</td><td>~10ms</td><td>~100K/sec</td><td>Within transaction</td></tr>
<tr><td>Single SELECT by ID</td><td>~0.05ms</td><td>~20K/sec</td><td>With index</td></tr>
<tr><td>SELECT with JOIN</td><td>~0.5ms</td><td>~2K/sec</td><td>Two-table join</td></tr>
<tr><td>Full table scan (10K rows)</td><td>~10ms</td><td>~1K/sec</td><td>Without index</td></tr>
</tbody></table>
</div>
<hr />
<h2 id="usage-examples-4"><a class="header" href="#usage-examples-4">Usage Examples</a></h2>
<h3 id="example-1-basic-crud-operations"><a class="header" href="#example-1-basic-crud-operations">Example 1: Basic CRUD Operations</a></h3>
<pre><pre class="playground"><code class="language-rust">use pipeline::infrastructure::repositories::{schema, SqlitePipelineRepository};
use pipeline_domain::Pipeline;

#[tokio::main]
async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    // Initialize database
    let pool = schema::initialize_database("sqlite://./pipeline.db").await?;
    let repo = SqlitePipelineRepository::new(pool);

    // Create pipeline
    let pipeline = Pipeline::new("my-pipeline".to_string())?;
    repo.save(&amp;pipeline).await?;
    println!("Saved pipeline: {}", pipeline.id());

    // Read pipeline
    let loaded = repo.find_by_id(pipeline.id()).await?
        .ok_or("Pipeline not found")?;
    println!("Loaded pipeline: {}", loaded.name());

    // Update pipeline
    let mut updated = loaded;
    updated.update_name("renamed-pipeline".to_string())?;
    repo.update(&amp;updated).await?;

    // Delete pipeline
    repo.delete(updated.id()).await?;
    println!("Deleted pipeline");

    Ok(())
}</code></pre></pre>
<h3 id="example-2-transaction-management"><a class="header" href="#example-2-transaction-management">Example 2: Transaction Management</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use sqlx::SqlitePool;

async fn save_pipeline_atomically(
    pool: &amp;SqlitePool,
    pipeline: &amp;Pipeline,
) -&gt; Result&lt;(), PipelineError&gt; {
    // Begin transaction
    let mut tx = pool.begin().await?;

    // Insert pipeline
    sqlx::query("INSERT INTO pipelines (id, name, created_at, updated_at) VALUES (?, ?, ?, ?)")
        .bind(pipeline.id().to_string())
        .bind(pipeline.name())
        .bind(pipeline.created_at().to_rfc3339())
        .bind(pipeline.updated_at().to_rfc3339())
        .execute(&amp;mut *tx)
        .await?;

    // Insert all stages
    for (i, stage) in pipeline.stages().iter().enumerate() {
        sqlx::query("INSERT INTO pipeline_stages (id, pipeline_id, name, stage_type, stage_order, algorithm, created_at, updated_at) VALUES (?, ?, ?, ?, ?, ?, ?, ?)")
            .bind(stage.id().to_string())
            .bind(pipeline.id().to_string())
            .bind(stage.name())
            .bind(stage.stage_type().to_string())
            .bind(i as i64)
            .bind(stage.algorithm())
            .bind(stage.created_at().to_rfc3339())
            .bind(stage.updated_at().to_rfc3339())
            .execute(&amp;mut *tx)
            .await?;
    }

    // Commit transaction (or rollback on error)
    tx.commit().await?;

    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h3 id="example-3-query-with-pagination"><a class="header" href="#example-3-query-with-pagination">Example 3: Query with Pagination</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn list_pipelines_paginated(
    repo: &amp;dyn PipelineRepository,
    page: usize,
    page_size: usize,
) -&gt; Result&lt;Vec&lt;Pipeline&gt;, PipelineError&gt; {
    let offset = page * page_size;
    repo.list_paginated(offset, page_size).await
}

// Usage
let page_1 = list_pipelines_paginated(&amp;repo, 0, 10).await?;  // First 10
let page_2 = list_pipelines_paginated(&amp;repo, 1, 10).await?;  // Next 10
<span class="boring">}</span></code></pre></pre>
<h3 id="example-4-archive-management"><a class="header" href="#example-4-archive-management">Example 4: Archive Management</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn archive_old_pipelines(
    repo: &amp;dyn PipelineRepository,
    cutoff_date: DateTime&lt;Utc&gt;,
) -&gt; Result&lt;usize, PipelineError&gt; {
    let pipelines = repo.find_all().await?;
    let mut archived_count = 0;

    for pipeline in pipelines {
        if pipeline.created_at() &lt; &amp;cutoff_date {
            repo.archive(pipeline.id()).await?;
            archived_count += 1;
        }
    }

    Ok(archived_count)
}
<span class="boring">}</span></code></pre></pre>
<h3 id="example-5-connection-pool-management"><a class="header" href="#example-5-connection-pool-management">Example 5: Connection Pool Management</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use sqlx::sqlite::SqlitePoolOptions;
use std::time::Duration;

async fn create_optimized_pool(database_url: &amp;str) -&gt; Result&lt;SqlitePool, sqlx::Error&gt; {
    let pool = SqlitePoolOptions::new()
        .max_connections(5)
        .min_connections(1)
        .acquire_timeout(Duration::from_secs(30))
        .idle_timeout(Duration::from_secs(600))
        .max_lifetime(Duration::from_secs(3600))
        .connect(database_url)
        .await?;

    Ok(pool)
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="best-practices-4"><a class="header" href="#best-practices-4">Best Practices</a></h2>
<h3 id="1-use-transactions-for-multi-step-operations"><a class="header" href="#1-use-transactions-for-multi-step-operations">1. Use Transactions for Multi-Step Operations</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Atomic multi-step operation
async fn create_pipeline_with_stages(pool: &amp;SqlitePool, pipeline: &amp;Pipeline) -&gt; Result&lt;(), Error&gt; {
    let mut tx = pool.begin().await?;
    insert_pipeline(&amp;mut tx, pipeline).await?;
    insert_stages(&amp;mut tx, pipeline.stages()).await?;
    tx.commit().await?;
    Ok(())
}

// ❌ Bad: Non-atomic operations
async fn create_pipeline_with_stages(pool: &amp;SqlitePool, pipeline: &amp;Pipeline) -&gt; Result&lt;(), Error&gt; {
    insert_pipeline(pool, pipeline).await?;
    insert_stages(pool, pipeline.stages()).await?;  // May fail, leaving orphaned pipeline
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h3 id="2-always-use-connection-pooling"><a class="header" href="#2-always-use-connection-pooling">2. Always Use Connection Pooling</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Reuse pool
let pool = SqlitePool::connect(&amp;url).await?;
let repo = SqlitePipelineRepository::new(pool.clone());
let service = PipelineService::new(Arc::new(repo));

// ❌ Bad: Create new connections
for _ in 0..100 {
    let pool = SqlitePool::connect(&amp;url).await?;  // Expensive!
    // ...
}
<span class="boring">}</span></code></pre></pre>
<h3 id="3-handle-database-errors-gracefully"><a class="header" href="#3-handle-database-errors-gracefully">3. Handle Database Errors Gracefully</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>match repo.save(&amp;pipeline).await {
    Ok(()) =&gt; info!("Pipeline saved successfully"),
    Err(PipelineError::DatabaseError(msg)) if msg.contains("UNIQUE constraint") =&gt; {
        warn!("Pipeline already exists: {}", pipeline.name());
    }
    Err(e) =&gt; {
        error!("Failed to save pipeline: {}", e);
        return Err(e);
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="4-use-indexes-for-frequently-queried-fields"><a class="header" href="#4-use-indexes-for-frequently-queried-fields">4. Use Indexes for Frequently Queried Fields</a></h3>
<pre><code class="language-sql">-- ✅ Good: Index on query columns
CREATE INDEX idx_pipelines_name ON pipelines(name);
SELECT * FROM pipelines WHERE name = ?;  -- Fast!

-- ❌ Bad: No index on query column
-- No index on 'name'
SELECT * FROM pipelines WHERE name = ?;  -- Slow (full table scan)
</code></pre>
<h3 id="5-keep-transactions-short"><a class="header" href="#5-keep-transactions-short">5. Keep Transactions Short</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Short transaction
let mut tx = pool.begin().await?;
sqlx::query("INSERT ...").execute(&amp;mut *tx).await?;
tx.commit().await?;

// ❌ Bad: Long transaction holding locks
let mut tx = pool.begin().await?;
expensive_computation().await;  // Don't do this!
sqlx::query("INSERT ...").execute(&amp;mut *tx).await?;
tx.commit().await?;
<span class="boring">}</span></code></pre></pre>
<h3 id="6-validate-data-before-persisting"><a class="header" href="#6-validate-data-before-persisting">6. Validate Data Before Persisting</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Validate before save
async fn save_pipeline(repo: &amp;dyn PipelineRepository, pipeline: &amp;Pipeline) -&gt; Result&lt;(), Error&gt; {
    pipeline.validate()?;  // Validate first
    repo.save(pipeline).await?;
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h3 id="7-use-migrations-for-schema-changes"><a class="header" href="#7-use-migrations-for-schema-changes">7. Use Migrations for Schema Changes</a></h3>
<pre><code class="language-bash"># ✅ Good: Create migration
sqlx migrate add add_archived_column

# Edit migration file
# migrations/20250101000001_add_archived_column.sql
ALTER TABLE pipelines ADD COLUMN archived BOOLEAN NOT NULL DEFAULT false;

# Apply migration
sqlx migrate run
</code></pre>
<hr />
<h2 id="troubleshooting-1"><a class="header" href="#troubleshooting-1">Troubleshooting</a></h2>
<h3 id="issue-1-database-locked-error"><a class="header" href="#issue-1-database-locked-error">Issue 1: Database Locked Error</a></h3>
<p><strong>Symptom:</strong></p>
<pre><code class="language-text">Error: database is locked
</code></pre>
<p><strong>Causes:</strong></p>
<ul>
<li>Long-running transaction blocking other operations</li>
<li>Multiple writers attempting simultaneous writes</li>
<li>Connection not released back to pool</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 1. Reduce transaction duration
let mut tx = pool.begin().await?;
// Do minimal work inside transaction
sqlx::query("INSERT ...").execute(&amp;mut *tx).await?;
tx.commit().await?;

// 2. Enable WAL mode for better concurrency
use sqlx::sqlite::SqliteConnectOptions;
let options = SqliteConnectOptions::new()
    .filename("./pipeline.db")
    .journal_mode(sqlx::sqlite::SqliteJournalMode::Wal);

// 3. Increase busy timeout
.busy_timeout(Duration::from_secs(5))
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-2-connection-pool-exhausted"><a class="header" href="#issue-2-connection-pool-exhausted">Issue 2: Connection Pool Exhausted</a></h3>
<p><strong>Symptom:</strong></p>
<pre><code class="language-text">Error: timed out while waiting for an open connection
</code></pre>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 1. Increase pool size
.max_connections(10)

// 2. Increase acquire timeout
.acquire_timeout(Duration::from_secs(60))

// 3. Ensure connections are returned
async fn query_data(pool: &amp;SqlitePool) -&gt; Result&lt;(), Error&gt; {
    let result = sqlx::query("SELECT ...").fetch_all(pool).await?;
    // Connection automatically returned to pool
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-3-foreign-key-constraint-failed"><a class="header" href="#issue-3-foreign-key-constraint-failed">Issue 3: Foreign Key Constraint Failed</a></h3>
<p><strong>Symptom:</strong></p>
<pre><code class="language-text">Error: FOREIGN KEY constraint failed
</code></pre>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 1. Ensure foreign keys are enabled
.foreign_keys(true)

// 2. Verify referenced record exists
let exists = sqlx::query_scalar("SELECT EXISTS(SELECT 1 FROM pipelines WHERE id = ?)")
    .bind(&amp;pipeline_id)
    .fetch_one(pool)
    .await?;

if !exists {
    return Err("Pipeline not found".into());
}

// 3. Use CASCADE DELETE for automatic cleanup
CREATE TABLE pipeline_stages (
    ...
    FOREIGN KEY (pipeline_id) REFERENCES pipelines(id) ON DELETE CASCADE
);
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-4-migration-checksum-mismatch"><a class="header" href="#issue-4-migration-checksum-mismatch">Issue 4: Migration Checksum Mismatch</a></h3>
<p><strong>Symptom:</strong></p>
<pre><code class="language-text">Error: migration checksum mismatch
</code></pre>
<p><strong>Solutions:</strong></p>
<pre><code class="language-bash"># Option 1: Revert migration
sqlx migrate revert

# Option 2: Reset database (development only!)
rm pipeline.db
sqlx migrate run

# Option 3: Create new migration to fix
sqlx migrate add fix_schema_issue
</code></pre>
<h3 id="issue-5-query-performance-degradation"><a class="header" href="#issue-5-query-performance-degradation">Issue 5: Query Performance Degradation</a></h3>
<p><strong>Diagnosis:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Enable query logging
RUST_LOG=sqlx=debug cargo run

// Analyze slow queries
let start = Instant::now();
let result = query.fetch_all(pool).await?;
let duration = start.elapsed();
if duration &gt; Duration::from_millis(100) {
    warn!("Slow query: {:?}", duration);
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Solutions:</strong></p>
<pre><code class="language-sql">-- 1. Add missing indexes
CREATE INDEX idx_pipelines_created_at ON pipelines(created_at);

-- 2. Use EXPLAIN QUERY PLAN
EXPLAIN QUERY PLAN SELECT * FROM pipelines WHERE name = ?;

-- 3. Optimize query
-- Before: Full table scan
SELECT * FROM pipelines WHERE lower(name) = ?;

-- After: Use index
SELECT * FROM pipelines WHERE name = ?;
</code></pre>
<hr />
<h2 id="testing-strategies-1"><a class="header" href="#testing-strategies-1">Testing Strategies</a></h2>
<h3 id="unit-testing-with-mock-repository"><a class="header" href="#unit-testing-with-mock-repository">Unit Testing with Mock Repository</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(test)]
mod tests {
    use super::*;
    use async_trait::async_trait;
    use std::sync::Arc;
    use tokio::sync::Mutex;

    struct MockPipelineRepository {
        pipelines: Arc&lt;Mutex&lt;HashMap&lt;PipelineId, Pipeline&gt;&gt;&gt;,
    }

    #[async_trait]
    impl PipelineRepository for MockPipelineRepository {
        async fn save(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt; {
            let mut pipelines = self.pipelines.lock().await;
            pipelines.insert(pipeline.id().clone(), pipeline.clone());
            Ok(())
        }

        async fn find_by_id(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;Option&lt;Pipeline&gt;, PipelineError&gt; {
            let pipelines = self.pipelines.lock().await;
            Ok(pipelines.get(id).cloned())
        }

        // ... implement other methods
    }

    #[tokio::test]
    async fn test_save_and_load() {
        let repo = MockPipelineRepository {
            pipelines: Arc::new(Mutex::new(HashMap::new())),
        };

        let pipeline = Pipeline::new("test".to_string()).unwrap();
        repo.save(&amp;pipeline).await.unwrap();

        let loaded = repo.find_by_id(pipeline.id()).await.unwrap().unwrap();
        assert_eq!(loaded.name(), "test");
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="integration-testing-with-sqlite"><a class="header" href="#integration-testing-with-sqlite">Integration Testing with SQLite</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_sqlite_repository_integration() {
    // Use in-memory database for tests
    let pool = schema::initialize_database("sqlite::memory:").await.unwrap();
    let repo = SqlitePipelineRepository::new(pool);

    // Create pipeline
    let pipeline = Pipeline::new("integration-test".to_string()).unwrap();
    repo.save(&amp;pipeline).await.unwrap();

    // Verify persistence
    let loaded = repo.find_by_id(pipeline.id()).await.unwrap().unwrap();
    assert_eq!(loaded.name(), "integration-test");

    // Verify stages are loaded
    assert_eq!(loaded.stages().len(), pipeline.stages().len());
}
<span class="boring">}</span></code></pre></pre>
<h3 id="transaction-testing"><a class="header" href="#transaction-testing">Transaction Testing</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_transaction_rollback() {
    let pool = schema::initialize_database("sqlite::memory:").await.unwrap();

    let result = async {
        let mut tx = pool.begin().await?;

        sqlx::query("INSERT INTO pipelines (id, name, created_at, updated_at) VALUES (?, ?, ?, ?)")
            .bind("test-id")
            .bind("test")
            .bind("2025-01-01T00:00:00Z")
            .bind("2025-01-01T00:00:00Z")
            .execute(&amp;mut *tx)
            .await?;

        // Simulate error
        return Err::&lt;(), sqlx::Error&gt;(sqlx::Error::RowNotFound);

        // This would commit, but error prevents it
        // tx.commit().await?;
    }.await;

    assert!(result.is_err());

    // Verify rollback - no pipeline should exist
    let count: i64 = sqlx::query_scalar("SELECT COUNT(*) FROM pipelines")
        .fetch_one(&amp;pool)
        .await
        .unwrap();
    assert_eq!(count, 0);
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="next-steps-14"><a class="header" href="#next-steps-14">Next Steps</a></h2>
<p>After understanding data persistence fundamentals, explore specific implementations:</p>
<h3 id="detailed-persistence-topics"><a class="header" href="#detailed-persistence-topics">Detailed Persistence Topics</a></h3>
<ol>
<li><strong><a href="implementation/repositories.html">Repository Implementation</a></strong>: Deep dive into repository pattern implementation</li>
<li><strong><a href="implementation/schema.html">Schema Management</a></strong>: Database schema design and migration strategies</li>
</ol>
<h3 id="related-topics-1"><a class="header" href="#related-topics-1">Related Topics</a></h3>
<ul>
<li><strong><a href="implementation/observability.html">Observability</a></strong>: Monitoring database operations and performance</li>
<li><strong><a href="implementation/stages.html">Stage Processing</a></strong>: How stages interact with persistence layer</li>
</ul>
<h3 id="advanced-topics-1"><a class="header" href="#advanced-topics-1">Advanced Topics</a></h3>
<ul>
<li><strong><a href="implementation/../advanced/performance.html">Performance Optimization</a></strong>: Database query optimization and profiling</li>
<li><strong><a href="implementation/../advanced/extending.html">Extending the Pipeline</a></strong>: Adding custom persistence backends</li>
</ul>
<hr />
<h2 id="summary-1"><a class="header" href="#summary-1">Summary</a></h2>
<p><strong>Key Takeaways:</strong></p>
<ol>
<li><strong>Repository Pattern</strong> provides abstraction between domain and infrastructure layers</li>
<li><strong>SQLite</strong> offers zero-configuration, ACID-compliant persistence</li>
<li><strong>Schema Management</strong> uses sqlx migrations for automated database evolution</li>
<li><strong>Connection Pooling</strong> optimizes resource utilization and performance</li>
<li><strong>Transactions</strong> ensure data consistency with ACID guarantees</li>
<li><strong>Data Mapping</strong> converts between domain entities and database records</li>
<li><strong>Performance</strong> optimized through indexing, batching, and query optimization</li>
</ol>
<p><strong>Architecture File References:</strong></p>
<ul>
<li><strong>Repository Interface:</strong> <code>pipeline-domain/src/repositories/pipeline_repository.rs:138</code></li>
<li><strong>SQLite Implementation:</strong> <code>pipeline/src/infrastructure/repositories/sqlite_pipeline_repository.rs:193</code></li>
<li><strong>Schema Management:</strong> <code>pipeline/src/infrastructure/repositories/schema.rs:18</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="repository-implementation"><a class="header" href="#repository-implementation">Repository Implementation</a></h1>
<p><strong>Version:</strong> 1.0
<strong>Date:</strong> 2025-01-04
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner, Claude Code
<strong>Status:</strong> Active</p>
<h2 id="overview-9"><a class="header" href="#overview-9">Overview</a></h2>
<p>The repository pattern provides an abstraction layer between the domain and data persistence, enabling the application to work with domain entities without knowing about database details. This separation allows for flexible storage implementations and easier testing.</p>
<p><strong>Key Benefits:</strong></p>
<ul>
<li><strong>Domain Independence</strong>: Business logic stays free from persistence concerns</li>
<li><strong>Testability</strong>: Easy mocking with in-memory implementations</li>
<li><strong>Flexibility</strong>: Support for different storage backends (SQLite, PostgreSQL, etc.)</li>
<li><strong>Consistency</strong>: Standardized data access patterns</li>
</ul>
<h2 id="repository-interface"><a class="header" href="#repository-interface">Repository Interface</a></h2>
<h3 id="domain-defined-contract"><a class="header" href="#domain-defined-contract">Domain-Defined Contract</a></h3>
<p>The domain layer defines the repository interface:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::repositories::PipelineRepository;
use pipeline_domain::entities::Pipeline;
use pipeline_domain::value_objects::PipelineId;
use pipeline_domain::PipelineError;
use async_trait::async_trait;

#[async_trait]
pub trait PipelineRepository: Send + Sync {
    /// Saves a pipeline
    async fn save(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt;;

    /// Finds a pipeline by ID
    async fn find_by_id(&amp;self, id: PipelineId)
        -&gt; Result&lt;Option&lt;Pipeline&gt;, PipelineError&gt;;

    /// Finds a pipeline by name
    async fn find_by_name(&amp;self, name: &amp;str)
        -&gt; Result&lt;Option&lt;Pipeline&gt;, PipelineError&gt;;

    /// Lists all pipelines
    async fn list_all(&amp;self) -&gt; Result&lt;Vec&lt;Pipeline&gt;, PipelineError&gt;;

    /// Lists pipelines with pagination
    async fn list_paginated(&amp;self, offset: usize, limit: usize)
        -&gt; Result&lt;Vec&lt;Pipeline&gt;, PipelineError&gt;;

    /// Updates a pipeline
    async fn update(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt;;

    /// Deletes a pipeline by ID
    async fn delete(&amp;self, id: PipelineId) -&gt; Result&lt;bool, PipelineError&gt;;

    /// Checks if a pipeline exists
    async fn exists(&amp;self, id: PipelineId) -&gt; Result&lt;bool, PipelineError&gt;;

    /// Counts total pipelines
    async fn count(&amp;self) -&gt; Result&lt;usize, PipelineError&gt;;

    /// Finds pipelines by configuration parameter
    async fn find_by_config(&amp;self, key: &amp;str, value: &amp;str)
        -&gt; Result&lt;Vec&lt;Pipeline&gt;, PipelineError&gt;;

    /// Archives a pipeline (soft delete)
    async fn archive(&amp;self, id: PipelineId) -&gt; Result&lt;bool, PipelineError&gt;;

    /// Restores an archived pipeline
    async fn restore(&amp;self, id: PipelineId) -&gt; Result&lt;bool, PipelineError&gt;;

    /// Lists archived pipelines
    async fn list_archived(&amp;self) -&gt; Result&lt;Vec&lt;Pipeline&gt;, PipelineError&gt;;
}
<span class="boring">}</span></code></pre></pre>
<h3 id="thread-safety"><a class="header" href="#thread-safety">Thread Safety</a></h3>
<p>All repository implementations must be <code>Send + Sync</code> for concurrent access:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ CORRECT: Thread-safe repository
pub struct SqlitePipelineRepository {
    pool: SqlitePool, // SqlitePool is Send + Sync
}

// ❌ WRONG: Not thread-safe
pub struct UnsafeRepository {
    conn: Rc&lt;Connection&gt;, // Rc is not Send or Sync
}
<span class="boring">}</span></code></pre></pre>
<h2 id="sqlite-implementation"><a class="header" href="#sqlite-implementation">SQLite Implementation</a></h2>
<h3 id="architecture-4"><a class="header" href="#architecture-4">Architecture</a></h3>
<p>The SQLite repository implements the domain interface using sqlx for type-safe queries:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::repositories::PipelineRepository;
use sqlx::SqlitePool;

pub struct SqlitePipelineRepository {
    pool: SqlitePool,
}

impl SqlitePipelineRepository {
    pub async fn new(database_path: &amp;str) -&gt; Result&lt;Self, PipelineError&gt; {
        let database_url = format!("sqlite:{}", database_path);
        let pool = SqlitePool::connect(&amp;database_url)
            .await
            .map_err(|e| PipelineError::database_error(
                format!("Failed to connect: {}", e)
            ))?;

        Ok(Self { pool })
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="database-schema"><a class="header" href="#database-schema">Database Schema</a></h3>
<p>The repository uses a normalized relational schema:</p>
<h4 id="pipelines-table"><a class="header" href="#pipelines-table">Pipelines Table</a></h4>
<pre><code class="language-sql">CREATE TABLE pipelines (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL UNIQUE,
    description TEXT,
    archived BOOLEAN NOT NULL DEFAULT 0,
    created_at TEXT NOT NULL,
    updated_at TEXT NOT NULL
);

CREATE INDEX idx_pipelines_name ON pipelines(name);
CREATE INDEX idx_pipelines_archived ON pipelines(archived);
</code></pre>
<h4 id="pipeline-stages-table"><a class="header" href="#pipeline-stages-table">Pipeline Stages Table</a></h4>
<pre><code class="language-sql">CREATE TABLE pipeline_stages (
    id TEXT PRIMARY KEY,
    pipeline_id TEXT NOT NULL,
    name TEXT NOT NULL,
    stage_type TEXT NOT NULL,
    algorithm TEXT NOT NULL,
    enabled BOOLEAN NOT NULL DEFAULT 1,
    order_index INTEGER NOT NULL,
    parallel_processing BOOLEAN NOT NULL DEFAULT 0,
    chunk_size INTEGER,
    created_at TEXT NOT NULL,
    updated_at TEXT NOT NULL,
    FOREIGN KEY (pipeline_id) REFERENCES pipelines(id) ON DELETE CASCADE
);

CREATE INDEX idx_stages_pipeline ON pipeline_stages(pipeline_id);
CREATE INDEX idx_stages_order ON pipeline_stages(pipeline_id, order_index);
</code></pre>
<h4 id="pipeline-configuration-table"><a class="header" href="#pipeline-configuration-table">Pipeline Configuration Table</a></h4>
<pre><code class="language-sql">CREATE TABLE pipeline_configuration (
    pipeline_id TEXT NOT NULL,
    key TEXT NOT NULL,
    value TEXT NOT NULL,
    PRIMARY KEY (pipeline_id, key),
    FOREIGN KEY (pipeline_id) REFERENCES pipelines(id) ON DELETE CASCADE
);
</code></pre>
<h4 id="pipeline-metrics-table"><a class="header" href="#pipeline-metrics-table">Pipeline Metrics Table</a></h4>
<pre><code class="language-sql">CREATE TABLE pipeline_metrics (
    id TEXT PRIMARY KEY,
    pipeline_id TEXT NOT NULL,
    bytes_processed INTEGER NOT NULL DEFAULT 0,
    bytes_total INTEGER NOT NULL DEFAULT 0,
    chunks_processed INTEGER NOT NULL DEFAULT 0,
    chunks_total INTEGER NOT NULL DEFAULT 0,
    start_time TEXT,
    end_time TEXT,
    throughput_mbps REAL NOT NULL DEFAULT 0.0,
    compression_ratio REAL,
    error_count INTEGER NOT NULL DEFAULT 0,
    warning_count INTEGER NOT NULL DEFAULT 0,
    recorded_at TEXT NOT NULL,
    FOREIGN KEY (pipeline_id) REFERENCES pipelines(id) ON DELETE CASCADE
);

CREATE INDEX idx_metrics_pipeline ON pipeline_metrics(pipeline_id);
</code></pre>
<h2 id="crud-operations"><a class="header" href="#crud-operations">CRUD Operations</a></h2>
<h3 id="create-save"><a class="header" href="#create-save">Create (Save)</a></h3>
<p>Save a complete pipeline with all related data:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[async_trait]
impl PipelineRepository for SqlitePipelineRepository {
    async fn save(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt; {
        // Start transaction for atomicity
        let mut tx = self.pool.begin().await
            .map_err(|e| PipelineError::database_error(
                format!("Failed to start transaction: {}", e)
            ))?;

        // Insert pipeline
        sqlx::query(
            "INSERT INTO pipelines
             (id, name, description, archived, created_at, updated_at)
             VALUES (?, ?, ?, ?, ?, ?)"
        )
        .bind(pipeline.id().to_string())
        .bind(pipeline.name())
        .bind(pipeline.description())
        .bind(pipeline.archived())
        .bind(pipeline.created_at().to_rfc3339())
        .bind(pipeline.updated_at().to_rfc3339())
        .execute(&amp;mut *tx)
        .await
        .map_err(|e| PipelineError::database_error(
            format!("Failed to insert pipeline: {}", e)
        ))?;

        // Insert stages
        for (index, stage) in pipeline.stages().iter().enumerate() {
            sqlx::query(
                "INSERT INTO pipeline_stages
                 (id, pipeline_id, name, stage_type, algorithm, enabled,
                  order_index, parallel_processing, chunk_size,
                  created_at, updated_at)
                 VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)"
            )
            .bind(stage.id().to_string())
            .bind(pipeline.id().to_string())
            .bind(stage.name())
            .bind(stage.stage_type().to_string())
            .bind(stage.algorithm().name())
            .bind(stage.enabled())
            .bind(index as i64)
            .bind(stage.parallel_processing())
            .bind(stage.chunk_size().map(|cs| cs.as_u64() as i64))
            .bind(stage.created_at().to_rfc3339())
            .bind(stage.updated_at().to_rfc3339())
            .execute(&amp;mut *tx)
            .await
            .map_err(|e| PipelineError::database_error(
                format!("Failed to insert stage: {}", e)
            ))?;
        }

        // Insert configuration
        for (key, value) in pipeline.configuration() {
            sqlx::query(
                "INSERT INTO pipeline_configuration (pipeline_id, key, value)
                 VALUES (?, ?, ?)"
            )
            .bind(pipeline.id().to_string())
            .bind(key)
            .bind(value)
            .execute(&amp;mut *tx)
            .await
            .map_err(|e| PipelineError::database_error(
                format!("Failed to insert config: {}", e)
            ))?;
        }

        // Commit transaction
        tx.commit().await
            .map_err(|e| PipelineError::database_error(
                format!("Failed to commit: {}", e)
            ))?;

        Ok(())
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="read-find"><a class="header" href="#read-find">Read (Find)</a></h3>
<p>Retrieve pipelines with all related data:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl SqlitePipelineRepository {
    async fn find_by_id(&amp;self, id: PipelineId)
        -&gt; Result&lt;Option&lt;Pipeline&gt;, PipelineError&gt; {
        // Fetch pipeline
        let pipeline_row = sqlx::query(
            "SELECT id, name, description, archived, created_at, updated_at
             FROM pipelines WHERE id = ?"
        )
        .bind(id.to_string())
        .fetch_optional(&amp;self.pool)
        .await
        .map_err(|e| PipelineError::database_error(
            format!("Failed to fetch pipeline: {}", e)
        ))?;

        let Some(row) = pipeline_row else {
            return Ok(None);
        };

        // Fetch stages
        let stage_rows = sqlx::query(
            "SELECT id, name, stage_type, algorithm, enabled,
                    order_index, parallel_processing, chunk_size,
                    created_at, updated_at
             FROM pipeline_stages
             WHERE pipeline_id = ?
             ORDER BY order_index"
        )
        .bind(id.to_string())
        .fetch_all(&amp;self.pool)
        .await
        .map_err(|e| PipelineError::database_error(
            format!("Failed to fetch stages: {}", e)
        ))?;

        // Fetch configuration
        let config_rows = sqlx::query(
            "SELECT key, value FROM pipeline_configuration
             WHERE pipeline_id = ?"
        )
        .bind(id.to_string())
        .fetch_all(&amp;self.pool)
        .await
        .map_err(|e| PipelineError::database_error(
            format!("Failed to fetch config: {}", e)
        ))?;

        // Map rows to domain entities
        let pipeline = self.map_to_pipeline(row, stage_rows, config_rows)?;

        Ok(Some(pipeline))
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="update"><a class="header" href="#update">Update</a></h3>
<p>Update existing pipeline:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl SqlitePipelineRepository {
    async fn update(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt; {
        let mut tx = self.pool.begin().await
            .map_err(|e| PipelineError::database_error(
                format!("Failed to start transaction: {}", e)
            ))?;

        // Update pipeline
        sqlx::query(
            "UPDATE pipelines
             SET name = ?, description = ?, archived = ?, updated_at = ?
             WHERE id = ?"
        )
        .bind(pipeline.name())
        .bind(pipeline.description())
        .bind(pipeline.archived())
        .bind(pipeline.updated_at().to_rfc3339())
        .bind(pipeline.id().to_string())
        .execute(&amp;mut *tx)
        .await
        .map_err(|e| PipelineError::database_error(
            format!("Failed to update pipeline: {}", e)
        ))?;

        // Delete and re-insert stages (simpler than updating)
        sqlx::query("DELETE FROM pipeline_stages WHERE pipeline_id = ?")
            .bind(pipeline.id().to_string())
            .execute(&amp;mut *tx)
            .await?;

        // Insert updated stages
        for (index, stage) in pipeline.stages().iter().enumerate() {
            // ... (same as save operation)
        }

        tx.commit().await
            .map_err(|e| PipelineError::database_error(
                format!("Failed to commit: {}", e)
            ))?;

        Ok(())
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="delete"><a class="header" href="#delete">Delete</a></h3>
<p>Remove pipeline and all related data:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl SqlitePipelineRepository {
    async fn delete(&amp;self, id: PipelineId) -&gt; Result&lt;bool, PipelineError&gt; {
        let result = sqlx::query("DELETE FROM pipelines WHERE id = ?")
            .bind(id.to_string())
            .execute(&amp;self.pool)
            .await
            .map_err(|e| PipelineError::database_error(
                format!("Failed to delete: {}", e)
            ))?;

        // CASCADE will automatically delete related records
        Ok(result.rows_affected() &gt; 0)
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="advanced-queries"><a class="header" href="#advanced-queries">Advanced Queries</a></h2>
<h3 id="pagination"><a class="header" href="#pagination">Pagination</a></h3>
<p>Efficiently paginate large result sets:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl SqlitePipelineRepository {
    async fn list_paginated(&amp;self, offset: usize, limit: usize)
        -&gt; Result&lt;Vec&lt;Pipeline&gt;, PipelineError&gt; {
        let rows = sqlx::query(
            "SELECT id, name, description, archived, created_at, updated_at
             FROM pipelines
             ORDER BY created_at DESC
             LIMIT ? OFFSET ?"
        )
        .bind(limit as i64)
        .bind(offset as i64)
        .fetch_all(&amp;self.pool)
        .await?;

        // Load stages and config for each pipeline
        let mut pipelines = Vec::new();
        for row in rows {
            let id = PipelineId::parse(&amp;row.get::&lt;String, _&gt;("id"))?;
            if let Some(pipeline) = self.find_by_id(id).await? {
                pipelines.push(pipeline);
            }
        }

        Ok(pipelines)
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="configuration-search"><a class="header" href="#configuration-search">Configuration Search</a></h3>
<p>Find pipelines by configuration:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl SqlitePipelineRepository {
    async fn find_by_config(&amp;self, key: &amp;str, value: &amp;str)
        -&gt; Result&lt;Vec&lt;Pipeline&gt;, PipelineError&gt; {
        let rows = sqlx::query(
            "SELECT DISTINCT p.id
             FROM pipelines p
             JOIN pipeline_configuration pc ON p.id = pc.pipeline_id
             WHERE pc.key = ? AND pc.value = ?"
        )
        .bind(key)
        .bind(value)
        .fetch_all(&amp;self.pool)
        .await?;

        let mut pipelines = Vec::new();
        for row in rows {
            let id = PipelineId::parse(&amp;row.get::&lt;String, _&gt;("id"))?;
            if let Some(pipeline) = self.find_by_id(id).await? {
                pipelines.push(pipeline);
            }
        }

        Ok(pipelines)
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="archive-operations"><a class="header" href="#archive-operations">Archive Operations</a></h3>
<p>Soft delete with archive/restore:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl SqlitePipelineRepository {
    async fn archive(&amp;self, id: PipelineId) -&gt; Result&lt;bool, PipelineError&gt; {
        let result = sqlx::query(
            "UPDATE pipelines SET archived = 1, updated_at = ?
             WHERE id = ?"
        )
        .bind(chrono::Utc::now().to_rfc3339())
        .bind(id.to_string())
        .execute(&amp;self.pool)
        .await?;

        Ok(result.rows_affected() &gt; 0)
    }

    async fn restore(&amp;self, id: PipelineId) -&gt; Result&lt;bool, PipelineError&gt; {
        let result = sqlx::query(
            "UPDATE pipelines SET archived = 0, updated_at = ?
             WHERE id = ?"
        )
        .bind(chrono::Utc::now().to_rfc3339())
        .bind(id.to_string())
        .execute(&amp;self.pool)
        .await?;

        Ok(result.rows_affected() &gt; 0)
    }

    async fn list_archived(&amp;self) -&gt; Result&lt;Vec&lt;Pipeline&gt;, PipelineError&gt; {
        let rows = sqlx::query(
            "SELECT id, name, description, archived, created_at, updated_at
             FROM pipelines WHERE archived = 1"
        )
        .fetch_all(&amp;self.pool)
        .await?;

        // Load full pipelines
        let mut pipelines = Vec::new();
        for row in rows {
            let id = PipelineId::parse(&amp;row.get::&lt;String, _&gt;("id"))?;
            if let Some(pipeline) = self.find_by_id(id).await? {
                pipelines.push(pipeline);
            }
        }

        Ok(pipelines)
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="transaction-management-1"><a class="header" href="#transaction-management-1">Transaction Management</a></h2>
<h3 id="acid-guarantees"><a class="header" href="#acid-guarantees">ACID Guarantees</a></h3>
<p>Ensure data consistency with transactions:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl SqlitePipelineRepository {
    /// Execute multiple operations atomically
    async fn save_multiple(&amp;self, pipelines: &amp;[Pipeline])
        -&gt; Result&lt;(), PipelineError&gt; {
        let mut tx = self.pool.begin().await?;

        for pipeline in pipelines {
            // All operations use the same transaction
            self.save_in_transaction(&amp;mut tx, pipeline).await?;
        }

        // Commit all or rollback all
        tx.commit().await
            .map_err(|e| PipelineError::database_error(
                format!("Transaction commit failed: {}", e)
            ))?;

        Ok(())
    }

    async fn save_in_transaction(
        &amp;self,
        tx: &amp;mut Transaction&lt;'_, Sqlite&gt;,
        pipeline: &amp;Pipeline
    ) -&gt; Result&lt;(), PipelineError&gt; {
        // Insert using transaction
        sqlx::query("INSERT INTO pipelines ...")
            .execute(&amp;mut **tx)
            .await?;

        Ok(())
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="rollback-on-error"><a class="header" href="#rollback-on-error">Rollback on Error</a></h3>
<p>Automatic rollback ensures consistency:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn complex_operation(&amp;self, pipeline: &amp;Pipeline)
    -&gt; Result&lt;(), PipelineError&gt; {
    let mut tx = self.pool.begin().await?;

    // Step 1: Insert pipeline
    sqlx::query("INSERT INTO pipelines ...")
        .execute(&amp;mut *tx)
        .await?;

    // Step 2: Insert stages
    for stage in pipeline.stages() {
        sqlx::query("INSERT INTO pipeline_stages ...")
            .execute(&amp;mut *tx)
            .await?;
        // If this fails, Step 1 is automatically rolled back
    }

    // Commit only if all steps succeed
    tx.commit().await?;
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h2 id="error-handling-4"><a class="header" href="#error-handling-4">Error Handling</a></h2>
<h3 id="database-errors"><a class="header" href="#database-errors">Database Errors</a></h3>
<p>Handle various database error types:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl SqlitePipelineRepository {
    async fn save(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt; {
        match sqlx::query("INSERT INTO pipelines ...").execute(&amp;self.pool).await {
            Ok(_) =&gt; Ok(()),
            Err(sqlx::Error::Database(db_err)) =&gt; {
                if db_err.is_unique_violation() {
                    Err(PipelineError::AlreadyExists(pipeline.id().to_string()))
                } else if db_err.is_foreign_key_violation() {
                    Err(PipelineError::InvalidReference(
                        "Invalid foreign key".to_string()
                    ))
                } else {
                    Err(PipelineError::database_error(db_err.to_string()))
                }
            }
            Err(e) =&gt; Err(PipelineError::database_error(e.to_string())),
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="connection-failures"><a class="header" href="#connection-failures">Connection Failures</a></h3>
<p>Handle connection issues gracefully:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl SqlitePipelineRepository {
    async fn with_retry&lt;F, T&gt;(&amp;self, mut operation: F) -&gt; Result&lt;T, PipelineError&gt;
    where
        F: FnMut() -&gt; BoxFuture&lt;'_, Result&lt;T, PipelineError&gt;&gt;,
    {
        let max_retries = 3;
        let mut attempts = 0;

        loop {
            match operation().await {
                Ok(result) =&gt; return Ok(result),
                Err(PipelineError::DatabaseError(_)) if attempts &lt; max_retries =&gt; {
                    attempts += 1;
                    tokio::time::sleep(
                        Duration::from_millis(100 * 2_u64.pow(attempts))
                    ).await;
                    continue;
                }
                Err(e) =&gt; return Err(e),
            }
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="performance-optimizations-3"><a class="header" href="#performance-optimizations-3">Performance Optimizations</a></h2>
<h3 id="connection-pooling-1"><a class="header" href="#connection-pooling-1">Connection Pooling</a></h3>
<p>Configure optimal pool settings:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use sqlx::sqlite::SqlitePoolOptions;

impl SqlitePipelineRepository {
    pub async fn new_with_pool_config(
        database_path: &amp;str,
        max_connections: u32,
    ) -&gt; Result&lt;Self, PipelineError&gt; {
        let database_url = format!("sqlite:{}", database_path);

        let pool = SqlitePoolOptions::new()
            .max_connections(max_connections)
            .min_connections(5)
            .acquire_timeout(Duration::from_secs(10))
            .idle_timeout(Duration::from_secs(600))
            .max_lifetime(Duration::from_secs(1800))
            .connect(&amp;database_url)
            .await?;

        Ok(Self { pool })
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="batch-operations-1"><a class="header" href="#batch-operations-1">Batch Operations</a></h3>
<p>Optimize bulk inserts:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl SqlitePipelineRepository {
    async fn save_batch(&amp;self, pipelines: &amp;[Pipeline])
        -&gt; Result&lt;(), PipelineError&gt; {
        let mut tx = self.pool.begin().await?;

        // Build batch insert query
        let mut query_builder = sqlx::QueryBuilder::new(
            "INSERT INTO pipelines
             (id, name, description, archived, created_at, updated_at)"
        );

        query_builder.push_values(pipelines, |mut b, pipeline| {
            b.push_bind(pipeline.id().to_string())
             .push_bind(pipeline.name())
             .push_bind(pipeline.description())
             .push_bind(pipeline.archived())
             .push_bind(pipeline.created_at().to_rfc3339())
             .push_bind(pipeline.updated_at().to_rfc3339());
        });

        query_builder.build()
            .execute(&amp;mut *tx)
            .await?;

        tx.commit().await?;
        Ok(())
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="query-optimization-1"><a class="header" href="#query-optimization-1">Query Optimization</a></h3>
<p>Use indexes and optimized queries:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Uses index on pipeline_id
sqlx::query(
    "SELECT * FROM pipeline_stages
     WHERE pipeline_id = ?
     ORDER BY order_index"
)
.bind(id)
.fetch_all(&amp;pool)
.await?;

// ❌ BAD: Full table scan
sqlx::query(
    "SELECT * FROM pipeline_stages
     WHERE name LIKE '%test%'"
)
.fetch_all(&amp;pool)
.await?;

// ✅ BETTER: Use full-text search or specific index
sqlx::query(
    "SELECT * FROM pipeline_stages
     WHERE name = ?"
)
.bind("test")
.fetch_all(&amp;pool)
.await?;
<span class="boring">}</span></code></pre></pre>
<h2 id="testing-strategies-2"><a class="header" href="#testing-strategies-2">Testing Strategies</a></h2>
<h3 id="in-memory-repository"><a class="header" href="#in-memory-repository">In-Memory Repository</a></h3>
<p>Create test implementation:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::collections::HashMap;
use std::sync::{Arc, Mutex};

pub struct InMemoryPipelineRepository {
    pipelines: Arc&lt;Mutex&lt;HashMap&lt;PipelineId, Pipeline&gt;&gt;&gt;,
}

impl InMemoryPipelineRepository {
    pub fn new() -&gt; Self {
        Self {
            pipelines: Arc::new(Mutex::new(HashMap::new())),
        }
    }
}

#[async_trait]
impl PipelineRepository for InMemoryPipelineRepository {
    async fn save(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt; {
        let mut pipelines = self.pipelines.lock().unwrap();

        if pipelines.contains_key(pipeline.id()) {
            return Err(PipelineError::AlreadyExists(
                pipeline.id().to_string()
            ));
        }

        pipelines.insert(pipeline.id().clone(), pipeline.clone());
        Ok(())
    }

    async fn find_by_id(&amp;self, id: PipelineId)
        -&gt; Result&lt;Option&lt;Pipeline&gt;, PipelineError&gt; {
        let pipelines = self.pipelines.lock().unwrap();
        Ok(pipelines.get(&amp;id).cloned())
    }

    // ... implement other methods
}
<span class="boring">}</span></code></pre></pre>
<h3 id="unit-tests-1"><a class="header" href="#unit-tests-1">Unit Tests</a></h3>
<p>Test repository operations:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_save_and_find() {
        let repo = InMemoryPipelineRepository::new();
        let pipeline = Pipeline::new("test".to_string(), vec![])?;

        // Save
        repo.save(&amp;pipeline).await.unwrap();

        // Find
        let found = repo.find_by_id(pipeline.id().clone())
            .await
            .unwrap()
            .unwrap();

        assert_eq!(found.id(), pipeline.id());
        assert_eq!(found.name(), pipeline.name());
    }

    #[tokio::test]
    async fn test_duplicate_save_fails() {
        let repo = InMemoryPipelineRepository::new();
        let pipeline = Pipeline::new("test".to_string(), vec![])?;

        repo.save(&amp;pipeline).await.unwrap();

        let result = repo.save(&amp;pipeline).await;
        assert!(matches!(result, Err(PipelineError::AlreadyExists(_))));
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="integration-tests-1"><a class="header" href="#integration-tests-1">Integration Tests</a></h3>
<p>Test with real database:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(test)]
mod integration_tests {
    use super::*;

    async fn create_test_db() -&gt; SqlitePipelineRepository {
        SqlitePipelineRepository::new(":memory:").await.unwrap()
    }

    #[tokio::test]
    async fn test_transaction_rollback() {
        let repo = create_test_db().await;
        let pipeline = Pipeline::new("test".to_string(), vec![])?;

        // Start transaction
        let mut tx = repo.pool.begin().await.unwrap();

        // Insert pipeline
        sqlx::query("INSERT INTO pipelines ...")
            .execute(&amp;mut *tx)
            .await
            .unwrap();

        // Rollback
        tx.rollback().await.unwrap();

        // Verify pipeline was not saved
        let found = repo.find_by_id(pipeline.id().clone()).await.unwrap();
        assert!(found.is_none());
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="best-practices-5"><a class="header" href="#best-practices-5">Best Practices</a></h2>
<h3 id="use-parameterized-queries"><a class="header" href="#use-parameterized-queries">Use Parameterized Queries</a></h3>
<p>Prevent SQL injection:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Parameterized query
sqlx::query("SELECT * FROM pipelines WHERE name = ?")
    .bind(name)
    .fetch_one(&amp;pool)
    .await?;

// ❌ BAD: String concatenation (SQL injection risk!)
let query = format!("SELECT * FROM pipelines WHERE name = '{}'", name);
sqlx::query(&amp;query).fetch_one(&amp;pool).await?;
<span class="boring">}</span></code></pre></pre>
<h3 id="handle-null-values"><a class="header" href="#handle-null-values">Handle NULL Values</a></h3>
<p>Properly handle nullable columns:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let description: Option&lt;String&gt; = row.try_get("description")?;
let chunk_size: Option&lt;i64&gt; = row.try_get("chunk_size")?;

let pipeline = Pipeline {
    description: description.unwrap_or_default(),
    chunk_size: chunk_size.map(|cs| ChunkSize::new(cs as u64)?),
    // ...
};
<span class="boring">}</span></code></pre></pre>
<h3 id="use-foreign-keys"><a class="header" href="#use-foreign-keys">Use Foreign Keys</a></h3>
<p>Maintain referential integrity:</p>
<pre><code class="language-sql">CREATE TABLE pipeline_stages (
    id TEXT PRIMARY KEY,
    pipeline_id TEXT NOT NULL,
    -- ... other columns
    FOREIGN KEY (pipeline_id)
        REFERENCES pipelines(id)
        ON DELETE CASCADE
);
</code></pre>
<h3 id="index-strategic-columns"><a class="header" href="#index-strategic-columns">Index Strategic Columns</a></h3>
<p>Optimize query performance:</p>
<pre><code class="language-sql">-- Primary lookups
CREATE INDEX idx_pipelines_id ON pipelines(id);
CREATE INDEX idx_pipelines_name ON pipelines(name);

-- Filtering
CREATE INDEX idx_pipelines_archived ON pipelines(archived);

-- Foreign keys
CREATE INDEX idx_stages_pipeline ON pipeline_stages(pipeline_id);

-- Sorting
CREATE INDEX idx_stages_order
    ON pipeline_stages(pipeline_id, order_index);
</code></pre>
<h2 id="next-steps-15"><a class="header" href="#next-steps-15">Next Steps</a></h2>
<p>Now that you understand repository implementation:</p>
<ul>
<li><a href="implementation/schema.html">Schema Management</a> - Database migrations and versioning</li>
<li><a href="implementation/binary-format.html">Binary Format</a> - File persistence patterns</li>
<li><a href="implementation/observability.html">Observability</a> - Monitoring and metrics</li>
<li><a href="implementation/../advanced/testing.html">Testing</a> - Comprehensive testing strategies</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="schema-management"><a class="header" href="#schema-management">Schema Management</a></h1>
<p><strong>Version</strong>: 1.0
<strong>Date</strong>: 2025-10-04
<strong>License</strong>: BSD-3-Clause
<strong>Copyright</strong>: (c) 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors</strong>: Michael Gardner
<strong>Status</strong>: Active</p>
<hr />
<h2 id="overview-10"><a class="header" href="#overview-10">Overview</a></h2>
<p>The Optimized Adaptive Pipeline uses <strong>SQLite</strong> for data persistence with an automated schema management system powered by <strong>sqlx migrations</strong>. This chapter explains the database schema design, migration strategy, and best practices for schema evolution.</p>
<h3 id="key-features-1"><a class="header" href="#key-features-1">Key Features</a></h3>
<ul>
<li><strong>Automatic Migrations</strong>: Schema automatically initialized and updated on startup</li>
<li><strong>Version Tracking</strong>: Migrations tracked in <code>_sqlx_migrations</code> table</li>
<li><strong>Idempotent</strong>: Safe to run migrations multiple times</li>
<li><strong>Normalized Design</strong>: Proper foreign keys and referential integrity</li>
<li><strong>Performance Indexed</strong>: Strategic indexes for common queries</li>
<li><strong>Test-Friendly</strong>: Support for in-memory databases</li>
</ul>
<hr />
<h2 id="database-schema-1"><a class="header" href="#database-schema-1">Database Schema</a></h2>
<h3 id="entity-relationship-diagram"><a class="header" href="#entity-relationship-diagram">Entity-Relationship Diagram</a></h3>
<pre><code class="language-text">┌─────────────────────────────────────────────────────────────┐
│                        pipelines                            │
├─────────────────────────────────────────────────────────────┤
│ id (PK)             TEXT                                    │
│ name                TEXT UNIQUE NOT NULL                    │
│ archived            BOOLEAN DEFAULT false                   │
│ created_at          TEXT NOT NULL                           │
│ updated_at          TEXT NOT NULL                           │
└────────────────┬────────────────────────────────────────────┘
                 │
                 │ 1:N
                 │
    ┌────────────┼──────────────────┐
    │            │                  │
    ▼            ▼                  ▼
┌─────────────────┐  ┌───────────────────────┐  ┌──────────────────┐
│ pipeline_stages │  │pipeline_configuration │  │processing_metrics│
├─────────────────┤  ├───────────────────────┤  ├──────────────────┤
│ id (PK)         │  │ pipeline_id (PK,FK)   │  │ pipeline_id (PK,FK)│
│ pipeline_id (FK)│  │ key (PK)              │  │ bytes_processed  │
│ name            │  │ value                 │  │ throughput_*     │
│ stage_type      │  │ archived              │  │ error_count      │
│ algorithm       │  │ created_at            │  │ ...              │
│ enabled         │  │ updated_at            │  └──────────────────┘
│ stage_order     │  └───────────────────────┘
│ ...             │
└────────┬────────┘
         │
         │ 1:N
         │
         ▼
┌──────────────────┐
│ stage_parameters │
├──────────────────┤
│ stage_id (PK,FK) │
│ key (PK)         │
│ value            │
│ archived         │
│ created_at       │
│ updated_at       │
└──────────────────┘
</code></pre>
<h3 id="tables-overview"><a class="header" href="#tables-overview">Tables Overview</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Table</th><th>Purpose</th><th>Relationships</th></tr></thead><tbody>
<tr><td><strong>pipelines</strong></td><td>Core pipeline configurations</td><td>Parent of stages, config, metrics</td></tr>
<tr><td><strong>pipeline_stages</strong></td><td>Processing stages within pipelines</td><td>Child of pipelines, parent of parameters</td></tr>
<tr><td><strong>pipeline_configuration</strong></td><td>Key-value configuration for pipelines</td><td>Child of pipelines</td></tr>
<tr><td><strong>stage_parameters</strong></td><td>Key-value parameters for stages</td><td>Child of pipeline_stages</td></tr>
<tr><td><strong>processing_metrics</strong></td><td>Execution metrics and statistics</td><td>Child of pipelines</td></tr>
</tbody></table>
</div>
<hr />
<h2 id="table-schemas"><a class="header" href="#table-schemas">Table Schemas</a></h2>
<h3 id="pipelines"><a class="header" href="#pipelines">pipelines</a></h3>
<p>The root table for pipeline management:</p>
<pre><code class="language-sql">CREATE TABLE IF NOT EXISTS pipelines (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL UNIQUE,
    archived BOOLEAN NOT NULL DEFAULT false,
    created_at TEXT NOT NULL,
    updated_at TEXT NOT NULL
);
</code></pre>
<p><strong>Columns</strong>:</p>
<ul>
<li><code>id</code>: UUID or unique identifier (e.g., "pipeline-123")</li>
<li><code>name</code>: Human-readable name (unique constraint)</li>
<li><code>archived</code>: Soft delete flag (false = active, true = archived)</li>
<li><code>created_at</code>: RFC3339 timestamp of creation</li>
<li><code>updated_at</code>: RFC3339 timestamp of last modification</li>
</ul>
<p><strong>Constraints</strong>:</p>
<ul>
<li>Primary key on <code>id</code></li>
<li>Unique constraint on <code>name</code></li>
<li>Indexed on <code>name WHERE archived = false</code> for active pipeline lookups</li>
</ul>
<h3 id="pipeline_stages"><a class="header" href="#pipeline_stages">pipeline_stages</a></h3>
<p>Defines the ordered stages within a pipeline:</p>
<pre><code class="language-sql">CREATE TABLE IF NOT EXISTS pipeline_stages (
    id TEXT PRIMARY KEY,
    pipeline_id TEXT NOT NULL,
    name TEXT NOT NULL,
    stage_type TEXT NOT NULL,
    enabled BOOLEAN NOT NULL DEFAULT TRUE,
    stage_order INTEGER NOT NULL,
    algorithm TEXT NOT NULL,
    parallel_processing BOOLEAN NOT NULL DEFAULT FALSE,
    chunk_size INTEGER,
    archived BOOLEAN NOT NULL DEFAULT FALSE,
    created_at TEXT NOT NULL,
    updated_at TEXT NOT NULL,
    FOREIGN KEY (pipeline_id) REFERENCES pipelines(id) ON DELETE CASCADE
);
</code></pre>
<p><strong>Columns</strong>:</p>
<ul>
<li><code>id</code>: Unique stage identifier</li>
<li><code>pipeline_id</code>: Foreign key to owning pipeline</li>
<li><code>name</code>: Stage name (e.g., "compression", "encryption")</li>
<li><code>stage_type</code>: Type of stage (enum: compression, encryption, checksum)</li>
<li><code>enabled</code>: Whether stage is active</li>
<li><code>stage_order</code>: Execution order (0-based)</li>
<li><code>algorithm</code>: Specific algorithm (e.g., "zstd", "aes-256-gcm")</li>
<li><code>parallel_processing</code>: Whether stage can process chunks in parallel</li>
<li><code>chunk_size</code>: Optional chunk size override for this stage</li>
<li><code>archived</code>: Soft delete flag</li>
<li><code>created_at</code>, <code>updated_at</code>: Timestamps</li>
</ul>
<p><strong>Constraints</strong>:</p>
<ul>
<li>Primary key on <code>id</code></li>
<li>Foreign key to <code>pipelines(id)</code> with CASCADE delete</li>
<li>Indexed on <code>(pipeline_id, stage_order)</code> for ordered retrieval</li>
<li>Indexed on <code>pipeline_id</code> for pipeline lookups</li>
</ul>
<h3 id="pipeline_configuration"><a class="header" href="#pipeline_configuration">pipeline_configuration</a></h3>
<p>Key-value configuration storage for pipelines:</p>
<pre><code class="language-sql">CREATE TABLE IF NOT EXISTS pipeline_configuration (
    pipeline_id TEXT NOT NULL,
    key TEXT NOT NULL,
    value TEXT NOT NULL,
    archived BOOLEAN NOT NULL DEFAULT FALSE,
    created_at TEXT NOT NULL,
    updated_at TEXT NOT NULL,
    PRIMARY KEY (pipeline_id, key),
    FOREIGN KEY (pipeline_id) REFERENCES pipelines(id) ON DELETE CASCADE
);
</code></pre>
<p><strong>Columns</strong>:</p>
<ul>
<li><code>pipeline_id</code>: Foreign key to pipeline</li>
<li><code>key</code>: Configuration key (e.g., "max_workers", "buffer_size")</li>
<li><code>value</code>: Configuration value (stored as TEXT, parsed by application)</li>
<li><code>archived</code>, <code>created_at</code>, <code>updated_at</code>: Standard metadata</li>
</ul>
<p><strong>Constraints</strong>:</p>
<ul>
<li>Composite primary key on <code>(pipeline_id, key)</code></li>
<li>Foreign key to <code>pipelines(id)</code> with CASCADE delete</li>
<li>Indexed on <code>pipeline_id</code></li>
</ul>
<p><strong>Usage Example</strong>:</p>
<pre><code>pipeline_id                          | key           | value
-------------------------------------|---------------|-------
pipeline-abc-123                     | max_workers   | 4
pipeline-abc-123                     | buffer_size   | 1048576
</code></pre>
<h3 id="stage_parameters"><a class="header" href="#stage_parameters">stage_parameters</a></h3>
<p>Key-value parameters for individual stages:</p>
<pre><code class="language-sql">CREATE TABLE IF NOT EXISTS stage_parameters (
    stage_id TEXT NOT NULL,
    key TEXT NOT NULL,
    value TEXT NOT NULL,
    archived BOOLEAN NOT NULL DEFAULT FALSE,
    created_at TEXT NOT NULL,
    updated_at TEXT NOT NULL,
    PRIMARY KEY (stage_id, key),
    FOREIGN KEY (stage_id) REFERENCES pipeline_stages(id) ON DELETE CASCADE
);
</code></pre>
<p><strong>Columns</strong>:</p>
<ul>
<li><code>stage_id</code>: Foreign key to stage</li>
<li><code>key</code>: Parameter key (e.g., "compression_level", "key_size")</li>
<li><code>value</code>: Parameter value (TEXT, parsed by stage)</li>
<li><code>archived</code>, <code>created_at</code>, <code>updated_at</code>: Standard metadata</li>
</ul>
<p><strong>Constraints</strong>:</p>
<ul>
<li>Composite primary key on <code>(stage_id, key)</code></li>
<li>Foreign key to <code>pipeline_stages(id)</code> with CASCADE delete</li>
<li>Indexed on <code>stage_id</code></li>
</ul>
<p><strong>Usage Example</strong>:</p>
<pre><code>stage_id                | key                | value
------------------------|--------------------|---------
stage-comp-456          | compression_level  | 9
stage-enc-789           | key_size           | 256
</code></pre>
<h3 id="processing_metrics"><a class="header" href="#processing_metrics">processing_metrics</a></h3>
<p>Tracks execution metrics for pipeline runs:</p>
<pre><code class="language-sql">CREATE TABLE IF NOT EXISTS processing_metrics (
    pipeline_id TEXT PRIMARY KEY,
    bytes_processed INTEGER NOT NULL DEFAULT 0,
    bytes_total INTEGER NOT NULL DEFAULT 0,
    chunks_processed INTEGER NOT NULL DEFAULT 0,
    chunks_total INTEGER NOT NULL DEFAULT 0,
    start_time_rfc3339 TEXT,
    end_time_rfc3339 TEXT,
    processing_duration_ms INTEGER,
    throughput_bytes_per_second REAL NOT NULL DEFAULT 0.0,
    compression_ratio REAL,
    error_count INTEGER NOT NULL DEFAULT 0,
    warning_count INTEGER NOT NULL DEFAULT 0,
    input_file_size_bytes INTEGER NOT NULL DEFAULT 0,
    output_file_size_bytes INTEGER NOT NULL DEFAULT 0,
    input_file_checksum TEXT,
    output_file_checksum TEXT,
    FOREIGN KEY (pipeline_id) REFERENCES pipelines(id) ON DELETE CASCADE
);
</code></pre>
<p><strong>Columns</strong>:</p>
<ul>
<li><code>pipeline_id</code>: Foreign key to pipeline (also primary key - one metric per pipeline)</li>
<li>Progress tracking: <code>bytes_processed</code>, <code>bytes_total</code>, <code>chunks_processed</code>, <code>chunks_total</code></li>
<li>Timing: <code>start_time_rfc3339</code>, <code>end_time_rfc3339</code>, <code>processing_duration_ms</code></li>
<li>Performance: <code>throughput_bytes_per_second</code>, <code>compression_ratio</code></li>
<li>Status: <code>error_count</code>, <code>warning_count</code></li>
<li>File info: <code>input_file_size_bytes</code>, <code>output_file_size_bytes</code></li>
<li>Integrity: <code>input_file_checksum</code>, <code>output_file_checksum</code></li>
</ul>
<p><strong>Constraints</strong>:</p>
<ul>
<li>Primary key on <code>pipeline_id</code></li>
<li>Foreign key to <code>pipelines(id)</code> with CASCADE delete</li>
</ul>
<hr />
<h2 id="migrations-with-sqlx"><a class="header" href="#migrations-with-sqlx">Migrations with sqlx</a></h2>
<h3 id="migration-files"><a class="header" href="#migration-files">Migration Files</a></h3>
<p>Migrations live in the <code>/migrations</code> directory at the project root:</p>
<pre><code class="language-text">migrations/
└── 20250101000000_initial_schema.sql
</code></pre>
<p><strong>Naming Convention</strong>: <code>{timestamp}_{description}.sql</code></p>
<ul>
<li>Timestamp: <code>YYYYMMDDHHMMSS</code> format</li>
<li>Description: Snake_case description of changes</li>
</ul>
<h3 id="migration-structure"><a class="header" href="#migration-structure">Migration Structure</a></h3>
<p>Each migration file contains:</p>
<pre><code class="language-sql">-- Migration: 20250101000000_initial_schema.sql
-- Description: Initial database schema for pipeline management

-- Table creation
CREATE TABLE IF NOT EXISTS pipelines (...);
CREATE TABLE IF NOT EXISTS pipeline_stages (...);
-- ... more tables ...

-- Index creation
CREATE INDEX IF NOT EXISTS idx_pipeline_stages_pipeline_id ON pipeline_stages(pipeline_id);
-- ... more indexes ...
</code></pre>
<h3 id="sqlx-migration-macro"><a class="header" href="#sqlx-migration-macro">sqlx Migration Macro</a></h3>
<p>The <code>sqlx::migrate!()</code> macro embeds migrations at compile time:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// In schema.rs
pub async fn ensure_schema(pool: &amp;SqlitePool) -&gt; Result&lt;(), sqlx::Error&gt; {
    debug!("Ensuring database schema is up to date");

    // Run migrations - sqlx will automatically track what's been applied
    sqlx::migrate!("../migrations").run(pool).await?;

    info!("Database schema is up to date");
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<p><strong>How it works</strong>:</p>
<ol>
<li><code>sqlx::migrate!("../migrations")</code> scans directory at compile time</li>
<li>Embeds migration SQL into binary</li>
<li><code>run(pool)</code> executes pending migrations at runtime</li>
<li>Tracks applied migrations in <code>_sqlx_migrations</code> table</li>
</ol>
<hr />
<h2 id="schema-initialization-1"><a class="header" href="#schema-initialization-1">Schema Initialization</a></h2>
<h3 id="automatic-initialization"><a class="header" href="#automatic-initialization">Automatic Initialization</a></h3>
<p>The schema module provides convenience functions for database setup:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// High-level initialization function
pub async fn initialize_database(database_url: &amp;str) -&gt; Result&lt;SqlitePool, sqlx::Error&gt; {
    // 1. Create database if it doesn't exist
    create_database_if_missing(database_url).await?;

    // 2. Connect to database
    let pool = SqlitePool::connect(database_url).await?;

    // 3. Run migrations
    ensure_schema(&amp;pool).await?;

    Ok(pool)
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Usage in application startup</strong>:</p>
<pre><pre class="playground"><code class="language-rust">use pipeline::infrastructure::repositories::schema;

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    // Initialize database with schema
    let pool = schema::initialize_database("sqlite://./pipeline.db").await?;

    // Database is ready to use!
    let repository = SqlitePipelineRepository::new(pool);

    Ok(())
}</code></pre></pre>
<h3 id="create-database-if-missing"><a class="header" href="#create-database-if-missing">Create Database if Missing</a></h3>
<p>For file-based SQLite databases:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub async fn create_database_if_missing(database_url: &amp;str) -&gt; Result&lt;(), sqlx::Error&gt; {
    if !sqlx::Sqlite::database_exists(database_url).await? {
        debug!("Database does not exist, creating: {}", database_url);
        sqlx::Sqlite::create_database(database_url).await?;
        info!("Created new SQLite database: {}", database_url);
    } else {
        debug!("Database already exists: {}", database_url);
    }
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Handles</strong>:</p>
<ul>
<li>New database creation</li>
<li>Existing database detection</li>
<li>File system permissions</li>
</ul>
<h3 id="in-memory-databases"><a class="header" href="#in-memory-databases">In-Memory Databases</a></h3>
<p>For testing, use in-memory databases:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_with_in_memory_db() {
    // No file system needed
    let pool = schema::initialize_database("sqlite::memory:")
        .await
        .unwrap();

    // Database is fully initialized in memory
    // ... run tests ...
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="migration-tracking"><a class="header" href="#migration-tracking">Migration Tracking</a></h2>
<h3 id="_sqlx_migrations-table"><a class="header" href="#_sqlx_migrations-table">_sqlx_migrations Table</a></h3>
<p>sqlx automatically creates a tracking table:</p>
<pre><code class="language-sql">CREATE TABLE _sqlx_migrations (
    version BIGINT PRIMARY KEY,
    description TEXT NOT NULL,
    installed_on TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    success BOOLEAN NOT NULL,
    checksum BLOB NOT NULL,
    execution_time BIGINT NOT NULL
);
</code></pre>
<p><strong>Columns</strong>:</p>
<ul>
<li><code>version</code>: Migration timestamp (e.g., 20250101000000)</li>
<li><code>description</code>: Migration description</li>
<li><code>installed_on</code>: When migration was applied</li>
<li><code>success</code>: Whether migration succeeded</li>
<li><code>checksum</code>: SHA256 of migration SQL</li>
<li><code>execution_time</code>: Duration in milliseconds</li>
</ul>
<h3 id="querying-applied-migrations"><a class="header" href="#querying-applied-migrations">Querying Applied Migrations</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let migrations: Vec&lt;(i64, String)&gt; = sqlx::query_as(
    "SELECT version, description FROM _sqlx_migrations ORDER BY version"
)
.fetch_all(&amp;pool)
.await?;

for (version, description) in migrations {
    println!("Applied migration: {} - {}", version, description);
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="adding-new-migrations"><a class="header" href="#adding-new-migrations">Adding New Migrations</a></h2>
<h3 id="step-1-create-migration-file"><a class="header" href="#step-1-create-migration-file">Step 1: Create Migration File</a></h3>
<p>Create a new file in <code>/migrations</code>:</p>
<pre><code class="language-bash"># Generate timestamp
TIMESTAMP=$(date +%Y%m%d%H%M%S)

# Create migration file
touch migrations/${TIMESTAMP}_add_pipeline_tags.sql
</code></pre>
<h3 id="step-2-write-migration-sql"><a class="header" href="#step-2-write-migration-sql">Step 2: Write Migration SQL</a></h3>
<pre><code class="language-sql">-- migrations/20250204120000_add_pipeline_tags.sql
-- Add tagging support for pipelines

CREATE TABLE IF NOT EXISTS pipeline_tags (
    pipeline_id TEXT NOT NULL,
    tag TEXT NOT NULL,
    created_at TEXT NOT NULL,
    PRIMARY KEY (pipeline_id, tag),
    FOREIGN KEY (pipeline_id) REFERENCES pipelines(id) ON DELETE CASCADE
);

CREATE INDEX IF NOT EXISTS idx_pipeline_tags_tag ON pipeline_tags(tag);
</code></pre>
<h3 id="step-3-test-migration"><a class="header" href="#step-3-test-migration">Step 3: Test Migration</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_new_migration() {
    let pool = schema::initialize_database("sqlite::memory:")
        .await
        .unwrap();

    // Verify new table exists
    let count: i64 = sqlx::query_scalar(
        "SELECT COUNT(*) FROM sqlite_master WHERE type='table' AND name='pipeline_tags'"
    )
    .fetch_one(&amp;pool)
    .await
    .unwrap();

    assert_eq!(count, 1);
}
<span class="boring">}</span></code></pre></pre>
<h3 id="step-4-rebuild"><a class="header" href="#step-4-rebuild">Step 4: Rebuild</a></h3>
<pre><code class="language-bash"># sqlx macro embeds migrations at compile time
cargo build
</code></pre>
<p>The next application start will automatically apply the new migration.</p>
<hr />
<h2 id="indexes-and-performance"><a class="header" href="#indexes-and-performance">Indexes and Performance</a></h2>
<h3 id="current-indexes"><a class="header" href="#current-indexes">Current Indexes</a></h3>
<pre><code class="language-sql">-- Ordered stage retrieval
CREATE INDEX idx_pipeline_stages_order
ON pipeline_stages(pipeline_id, stage_order);

-- Stage lookup by pipeline
CREATE INDEX idx_pipeline_stages_pipeline_id
ON pipeline_stages(pipeline_id);

-- Configuration lookup
CREATE INDEX idx_pipeline_configuration_pipeline_id
ON pipeline_configuration(pipeline_id);

-- Parameter lookup
CREATE INDEX idx_stage_parameters_stage_id
ON stage_parameters(stage_id);

-- Active pipelines only
CREATE INDEX idx_pipelines_name
ON pipelines(name) WHERE archived = false;
</code></pre>
<h3 id="index-strategy"><a class="header" href="#index-strategy">Index Strategy</a></h3>
<p><strong>When to add indexes</strong>:</p>
<ul>
<li>✅ Foreign key columns (for JOIN performance)</li>
<li>✅ Columns in WHERE clauses (for filtering)</li>
<li>✅ Columns in ORDER BY (for sorting)</li>
<li>✅ Partial indexes for common filters (e.g., <code>WHERE archived = false</code>)</li>
</ul>
<p><strong>When NOT to index</strong>:</p>
<ul>
<li>❌ Small tables (&lt; 1000 rows)</li>
<li>❌ Columns with low cardinality (few distinct values)</li>
<li>❌ Columns rarely used in queries</li>
<li>❌ Write-heavy columns (indexes slow INSERTs/UPDATEs)</li>
</ul>
<hr />
<h2 id="best-practices-6"><a class="header" href="#best-practices-6">Best Practices</a></h2>
<h3 id="-do"><a class="header" href="#-do">✅ DO</a></h3>
<p><strong>Use idempotent migrations</strong></p>
<pre><code class="language-sql">-- Safe to run multiple times
CREATE TABLE IF NOT EXISTS new_table (...);
CREATE INDEX IF NOT EXISTS idx_name ON table(column);
</code></pre>
<p><strong>Include rollback comments</strong></p>
<pre><code class="language-sql">-- Migration: Add user_id column
-- Rollback: DROP COLUMN is not supported in SQLite, recreate table

ALTER TABLE pipelines ADD COLUMN user_id TEXT;
</code></pre>
<p><strong>Use transactions for multi-statement migrations</strong></p>
<pre><code class="language-sql">BEGIN TRANSACTION;

CREATE TABLE new_table (...);
INSERT INTO new_table SELECT ...;
DROP TABLE old_table;

COMMIT;
</code></pre>
<p><strong>Test migrations with production-like data</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_migration_with_data() {
    let pool = schema::initialize_database("sqlite::memory:").await.unwrap();

    // Insert test data
    sqlx::query("INSERT INTO pipelines (...) VALUES (...)")
        .execute(&amp;pool)
        .await
        .unwrap();

    // Run migration
    schema::ensure_schema(&amp;pool).await.unwrap();

    // Verify data integrity
    // ...
}
<span class="boring">}</span></code></pre></pre>
<h3 id="-dont"><a class="header" href="#-dont">❌ DON'T</a></h3>
<p><strong>Don't modify existing migrations</strong></p>
<pre><code class="language-sql">-- BAD: Editing 20250101000000_initial_schema.sql after deployment
-- This will cause checksum mismatch!

-- GOOD: Create a new migration to alter the schema
-- migrations/20250204000000_modify_pipeline_name.sql
</code></pre>
<p><strong>Don't use database-specific features unnecessarily</strong></p>
<pre><code class="language-sql">-- BAD: SQLite-only (limits portability)
CREATE TABLE pipelines (
    id INTEGER PRIMARY KEY AUTOINCREMENT
);

-- GOOD: Portable approach
CREATE TABLE pipelines (
    id TEXT PRIMARY KEY  -- Application generates UUIDs
);
</code></pre>
<p><strong>Don't forget foreign key constraints</strong></p>
<pre><code class="language-sql">-- BAD: No referential integrity
CREATE TABLE pipeline_stages (
    pipeline_id TEXT NOT NULL
);

-- GOOD: Enforced relationships
CREATE TABLE pipeline_stages (
    pipeline_id TEXT NOT NULL,
    FOREIGN KEY (pipeline_id) REFERENCES pipelines(id) ON DELETE CASCADE
);
</code></pre>
<hr />
<h2 id="testing-schema-changes"><a class="header" href="#testing-schema-changes">Testing Schema Changes</a></h2>
<h3 id="unit-tests-2"><a class="header" href="#unit-tests-2">Unit Tests</a></h3>
<p>From <code>schema.rs</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_create_database_if_missing() {
    let temp = NamedTempFile::new().unwrap();
    let db_path = temp.path().to_str().unwrap();
    let db_url = format!("sqlite://{}", db_path);
    drop(temp); // Remove file

    // Should create the database
    create_database_if_missing(&amp;db_url).await.unwrap();

    // Should succeed if already exists
    create_database_if_missing(&amp;db_url).await.unwrap();
}
<span class="boring">}</span></code></pre></pre>
<h3 id="integration-tests-2"><a class="header" href="#integration-tests-2">Integration Tests</a></h3>
<p>From <code>schema_integration_test.rs</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_schema_migrations_run_automatically() {
    let pool = schema::initialize_database("sqlite::memory:")
        .await
        .unwrap();

    // Verify _sqlx_migrations table exists
    let result: i64 = sqlx::query_scalar(
        "SELECT COUNT(*) FROM _sqlx_migrations"
    )
    .fetch_one(&amp;pool)
    .await
    .unwrap();

    assert!(result &gt; 0, "At least one migration should be applied");
}
<span class="boring">}</span></code></pre></pre>
<h3 id="idempotency-tests"><a class="header" href="#idempotency-tests">Idempotency Tests</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_schema_idempotent_initialization() {
    let db_url = "sqlite::memory:";

    // Initialize twice - should not error
    let _pool1 = schema::initialize_database(db_url).await.unwrap();
    let _pool2 = schema::initialize_database(db_url).await.unwrap();
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="troubleshooting-2"><a class="header" href="#troubleshooting-2">Troubleshooting</a></h2>
<h3 id="issue-migration-checksum-mismatch"><a class="header" href="#issue-migration-checksum-mismatch">Issue: Migration checksum mismatch</a></h3>
<p><strong>Symptom</strong>: Error: "migration checksum mismatch"</p>
<p><strong>Cause</strong>: Existing migration file was modified after being applied</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># NEVER modify applied migrations!
# Instead, create a new migration to make changes

# If in development and migration hasn't been deployed:
# 1. Drop database
rm pipeline.db

# 2. Recreate with modified migration
cargo run
</code></pre>
<h3 id="issue-database-file-locked"><a class="header" href="#issue-database-file-locked">Issue: Database file locked</a></h3>
<p><strong>Symptom</strong>: Error: "database is locked"</p>
<p><strong>Cause</strong>: Another process has an exclusive lock</p>
<p><strong>Solution</strong>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Use connection pool with proper configuration
let pool = SqlitePool::connect_with(
    SqliteConnectOptions::from_str("sqlite://./pipeline.db")?
        .busy_timeout(Duration::from_secs(30))  // Wait for lock
        .journal_mode(SqliteJournalMode::Wal)    // Use WAL mode
)
.await?;
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-foreign-key-constraint-failed"><a class="header" href="#issue-foreign-key-constraint-failed">Issue: Foreign key constraint failed</a></h3>
<p><strong>Symptom</strong>: Error: "FOREIGN KEY constraint failed"</p>
<p><strong>Cause</strong>: Trying to insert/update with invalid foreign key</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-sql">-- Enable foreign key enforcement (SQLite default is OFF)
PRAGMA foreign_keys = ON;

-- Then verify referenced row exists before insert
SELECT id FROM pipelines WHERE id = ?;
</code></pre>
<hr />
<h2 id="next-steps-16"><a class="header" href="#next-steps-16">Next Steps</a></h2>
<ul>
<li><strong><a href="implementation/repositories.html">Repository Implementation</a></strong>: Using the schema in repositories</li>
<li><strong><a href="implementation/persistence.html">Data Persistence</a></strong>: Persistence patterns and strategies</li>
<li><strong><a href="implementation/../testing/integration-tests.html">Testing</a></strong>: Integration testing with databases</li>
</ul>
<hr />
<h2 id="references"><a class="header" href="#references">References</a></h2>
<ul>
<li><a href="https://github.com/launchbadge/sqlx/blob/main/sqlx-cli/README.md#create-and-run-migrations">sqlx Migrations Documentation</a></li>
<li><a href="https://www.sqlite.org/datatype3.html">SQLite Data Types</a></li>
<li><a href="https://www.sqlite.org/foreignkeys.html">SQLite Foreign Keys</a></li>
<li><a href="https://www.sqlite.org/queryplanner.html">SQLite Indexes</a></li>
<li>Source: <code>pipeline/src/infrastructure/repositories/schema.rs</code> (lines 1-157)</li>
<li>Source: <code>migrations/20250101000000_initial_schema.sql</code> (lines 1-81)</li>
<li>Source: <code>pipeline/tests/schema_integration_test.rs</code> (lines 1-110)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="file-io"><a class="header" href="#file-io">File I/O</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>This chapter provides a comprehensive overview of the file input/output architecture in the adaptive pipeline system. Learn how file chunks, type-safe paths, and streaming I/O work together to enable efficient, memory-safe file processing.</p>
<hr />
<h2 id="table-of-contents-2"><a class="header" href="#table-of-contents-2">Table of Contents</a></h2>
<ul>
<li><a href="implementation/file-io.html#overview">Overview</a></li>
<li><a href="implementation/file-io.html#file-io-architecture">File I/O Architecture</a></li>
<li><a href="implementation/file-io.html#filechunk-value-object">FileChunk Value Object</a></li>
<li><a href="implementation/file-io.html#filepath-value-object">FilePath Value Object</a></li>
<li><a href="implementation/file-io.html#chunksize-value-object">ChunkSize Value Object</a></li>
<li><a href="implementation/file-io.html#fileioservice-interface">FileIOService Interface</a></li>
<li><a href="implementation/file-io.html#file-reading">File Reading</a></li>
<li><a href="implementation/file-io.html#file-writing">File Writing</a></li>
<li><a href="implementation/file-io.html#memory-management">Memory Management</a></li>
<li><a href="implementation/file-io.html#error-handling">Error Handling</a></li>
<li><a href="implementation/file-io.html#performance-optimization">Performance Optimization</a></li>
<li><a href="implementation/file-io.html#usage-examples">Usage Examples</a></li>
<li><a href="implementation/file-io.html#best-practices">Best Practices</a></li>
<li><a href="implementation/file-io.html#troubleshooting">Troubleshooting</a></li>
<li><a href="implementation/file-io.html#testing-strategies">Testing Strategies</a></li>
<li><a href="implementation/file-io.html#next-steps">Next Steps</a></li>
</ul>
<hr />
<h2 id="overview-11"><a class="header" href="#overview-11">Overview</a></h2>
<p><strong>File I/O</strong> in the adaptive pipeline system is designed for efficient, memory-safe processing of files of any size through chunking, streaming, and intelligent memory management. The system uses immutable value objects and async I/O for optimal performance.</p>
<h3 id="key-features-2"><a class="header" href="#key-features-2">Key Features</a></h3>
<ul>
<li><strong>Chunked Processing</strong>: Files split into manageable chunks for parallel processing</li>
<li><strong>Streaming I/O</strong>: Process files without loading entirely into memory</li>
<li><strong>Type-Safe Paths</strong>: Compile-time path category enforcement</li>
<li><strong>Immutable Chunks</strong>: Thread-safe, corruption-proof file chunks</li>
<li><strong>Validated Sizes</strong>: Chunk sizes validated at creation</li>
<li><strong>Async Operations</strong>: Non-blocking I/O for better concurrency</li>
</ul>
<h3 id="file-io-stack"><a class="header" href="#file-io-stack">File I/O Stack</a></h3>
<pre><code class="language-text">┌──────────────────────────────────────────────────────────┐
│                  Application Layer                        │
│  ┌────────────────────────────────────────────────┐      │
│  │   File Processor Service                       │      │
│  │   - Orchestrates chunking and processing       │      │
│  └────────────────────────────────────────────────┘      │
└──────────────────────────────────────────────────────────┘
                         ↓ uses
┌──────────────────────────────────────────────────────────┐
│                    Domain Layer                           │
│  ┌────────────────────────────────────────────────┐      │
│  │   FileIOService (Trait)                        │      │
│  │   - read_file_chunks(), write_file_chunks()    │      │
│  └────────────────────────────────────────────────┘      │
│  ┌────────────┬───────────┬──────────────┐              │
│  │ FileChunk  │ FilePath  │  ChunkSize   │              │
│  │ (immutable)│(type-safe)│(validated)   │              │
│  └────────────┴───────────┴──────────────┘              │
└──────────────────────────────────────────────────────────┘
                         ↓ implements
┌──────────────────────────────────────────────────────────┐
│              Infrastructure Layer                         │
│  ┌────────────────────────────────────────────────┐      │
│  │   Async File I/O Implementation                │      │
│  │   - tokio::fs for async operations             │      │
│  │   - Streaming, chunking, buffering             │      │
│  └────────────────────────────────────────────────┘      │
└──────────────────────────────────────────────────────────┘
                         ↓ reads/writes
┌──────────────────────────────────────────────────────────┐
│                  File System                              │
│  - Input files, output files, temporary files            │
└──────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="design-principles-2"><a class="header" href="#design-principles-2">Design Principles</a></h3>
<ol>
<li><strong>Immutability</strong>: File chunks cannot be modified after creation</li>
<li><strong>Streaming</strong>: Process files without loading entirely into memory</li>
<li><strong>Type Safety</strong>: Compile-time path category enforcement</li>
<li><strong>Async-First</strong>: Non-blocking I/O for better concurrency</li>
<li><strong>Memory Efficiency</strong>: Bounded memory usage regardless of file size</li>
</ol>
<hr />
<h2 id="file-io-architecture"><a class="header" href="#file-io-architecture">File I/O Architecture</a></h2>
<p>The file I/O layer uses value objects and async services to provide efficient, safe file processing.</p>
<h3 id="architectural-components"><a class="header" href="#architectural-components">Architectural Components</a></h3>
<pre><code class="language-text">┌─────────────────────────────────────────────────────────────┐
│ Value Objects (Domain)                                      │
│  ┌────────────────┬────────────────┬─────────────────┐     │
│  │  FileChunk     │  FilePath&lt;T&gt;   │   ChunkSize     │     │
│  │  - Immutable   │  - Type-safe   │   - Validated   │     │
│  │  - UUID ID     │  - Category    │   - 1B-512MB    │     │
│  │  - Sequence #  │  - Validated   │   - Default 1MB │     │
│  └────────────────┴────────────────┴─────────────────┘     │
└─────────────────────────────────────────────────────────────┘
                         ↓
┌─────────────────────────────────────────────────────────────┐
│ Service Interface (Domain)                                  │
│  ┌──────────────────────────────────────────────────┐      │
│  │  FileIOService (async trait)                     │      │
│  │  - read_file_chunks()                            │      │
│  │  - write_file_chunks()                           │      │
│  │  - stream_chunks()                               │      │
│  └──────────────────────────────────────────────────┘      │
└─────────────────────────────────────────────────────────────┘
                         ↓
┌─────────────────────────────────────────────────────────────┐
│ Implementation (Infrastructure)                             │
│  ┌──────────────────────────────────────────────────┐      │
│  │  Async File I/O                                  │      │
│  │  - tokio::fs::File                               │      │
│  │  - Buffering, streaming                          │      │
│  │  - Memory mapping (large files)                  │      │
│  └──────────────────────────────────────────────────┘      │
└─────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="processing-flow"><a class="header" href="#processing-flow">Processing Flow</a></h3>
<p><strong>File → Chunks → Processing → Chunks → File:</strong></p>
<pre><code class="language-text">Input File (e.g., 100MB)
        ↓
Split into chunks (1MB each)
        ↓
┌─────────┬─────────┬─────────┬─────────┐
│ Chunk 0 │ Chunk 1 │ Chunk 2 │   ...   │
│ (1MB)   │ (1MB)   │ (1MB)   │ (1MB)   │
└─────────┴─────────┴─────────┴─────────┘
        ↓ parallel processing
┌─────────┬─────────┬─────────┬─────────┐
│Processed│Processed│Processed│   ...   │
│ Chunk 0 │ Chunk 1 │ Chunk 2 │ (1MB)   │
└─────────┴─────────┴─────────┴─────────┘
        ↓
Reassemble chunks
        ↓
Output File (100MB)
</code></pre>
<hr />
<h2 id="filechunk-value-object-1"><a class="header" href="#filechunk-value-object-1">FileChunk Value Object</a></h2>
<p><code>FileChunk</code> is an immutable value object representing a portion of a file for processing.</p>
<h3 id="filechunk-structure"><a class="header" href="#filechunk-structure">FileChunk Structure</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]
pub struct FileChunk {
    id: Uuid,                           // Unique identifier
    sequence_number: u64,               // Order in file (0-based)
    offset: u64,                        // Byte offset in original file
    size: ChunkSize,                    // Validated chunk size
    data: Vec&lt;u8&gt;,                      // Actual chunk data
    checksum: Option&lt;String&gt;,           // Optional SHA-256 checksum
    is_final: bool,                     // Last chunk flag
    created_at: DateTime&lt;Utc&gt;,          // Creation timestamp
}
<span class="boring">}</span></code></pre></pre>
<h3 id="key-characteristics-5"><a class="header" href="#key-characteristics-5">Key Characteristics</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Feature</th><th>Description</th></tr></thead><tbody>
<tr><td><strong>Immutability</strong></td><td>Once created, chunks cannot be modified</td></tr>
<tr><td><strong>Unique Identity</strong></td><td>Each chunk has a UUID for tracking</td></tr>
<tr><td><strong>Sequence Ordering</strong></td><td>Maintains position for reassembly</td></tr>
<tr><td><strong>Integrity</strong></td><td>Optional checksums for validation</td></tr>
<tr><td><strong>Thread Safety</strong></td><td>Fully thread-safe due to immutability</td></tr>
</tbody></table>
</div>
<h3 id="creating-chunks"><a class="header" href="#creating-chunks">Creating Chunks</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::FileChunk;

// Basic chunk creation
let data = vec![1, 2, 3, 4, 5];
let chunk = FileChunk::new(
    0,        // sequence_number
    0,        // offset
    data,     // data
    false,    // is_final
)?;

println!("Chunk ID: {}", chunk.id());
println!("Size: {} bytes", chunk.size().bytes());
<span class="boring">}</span></code></pre></pre>
<h3 id="chunk-with-checksum"><a class="header" href="#chunk-with-checksum">Chunk with Checksum</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Create chunk with checksum
let data = vec![1, 2, 3, 4, 5];
let chunk = FileChunk::new(0, 0, data, false)?
    .with_calculated_checksum();

// Verify checksum
if let Some(checksum) = chunk.checksum() {
    println!("Checksum: {}", checksum);
}

// Verify data integrity
assert!(chunk.verify_checksum());
<span class="boring">}</span></code></pre></pre>
<h3 id="chunk-properties"><a class="header" href="#chunk-properties">Chunk Properties</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Access chunk properties
println!("ID: {}", chunk.id());
println!("Sequence: {}", chunk.sequence_number());
println!("Offset: {}", chunk.offset());
println!("Size: {} bytes", chunk.size().bytes());
println!("Is final: {}", chunk.is_final());
println!("Created: {}", chunk.created_at());

// Access data
let data: &amp;[u8] = chunk.data();
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="filepath-value-object-1"><a class="header" href="#filepath-value-object-1">FilePath Value Object</a></h2>
<p><code>FilePath&lt;T&gt;</code> is a type-safe, validated file path with compile-time category enforcement.</p>
<h3 id="path-categories"><a class="header" href="#path-categories">Path Categories</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Type-safe path categories
pub struct InputPath;      // For input files
pub struct OutputPath;     // For output files
pub struct TempPath;       // For temporary files
pub struct LogPath;        // For log files

// Usage with phantom types
let input: FilePath&lt;InputPath&gt; = FilePath::new("./input.dat")?;
let output: FilePath&lt;OutputPath&gt; = FilePath::new("./output.dat")?;

// ✅ Type system prevents mixing categories
// ❌ Cannot assign input path to output variable
// let wrong: FilePath&lt;OutputPath&gt; = input;  // Compile error!
<span class="boring">}</span></code></pre></pre>
<h3 id="path-validation"><a class="header" href="#path-validation">Path Validation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::value_objects::{FilePath, InputPath};

// Create and validate path
let path = FilePath::&lt;InputPath&gt;::new("/path/to/file.dat")?;

// Path properties
println!("Path: {}", path.as_str());
println!("Exists: {}", path.exists());
println!("Is file: {}", path.is_file());
println!("Is dir: {}", path.is_dir());
println!("Is absolute: {}", path.is_absolute());

// Convert to std::path::Path
let std_path: &amp;Path = path.as_path();
<span class="boring">}</span></code></pre></pre>
<h3 id="path-operations"><a class="header" href="#path-operations">Path Operations</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Get file name
let file_name = path.file_name();

// Get parent directory
let parent = path.parent();

// Get file extension
let extension = path.extension();

// Convert to string
let path_str = path.to_string();
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="chunksize-value-object-1"><a class="header" href="#chunksize-value-object-1">ChunkSize Value Object</a></h2>
<p><code>ChunkSize</code> represents a validated chunk size within system bounds.</p>
<h3 id="size-constraints"><a class="header" href="#size-constraints">Size Constraints</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Chunk size constants
ChunkSize::MIN_SIZE  // 1 byte
ChunkSize::MAX_SIZE  // 512 MB
ChunkSize::DEFAULT   // 1 MB
<span class="boring">}</span></code></pre></pre>
<h3 id="creating-chunk-sizes"><a class="header" href="#creating-chunk-sizes">Creating Chunk Sizes</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::ChunkSize;

// From bytes
let size = ChunkSize::new(1024 * 1024)?;  // 1 MB
assert_eq!(size.bytes(), 1_048_576);

// From kilobytes
let size_kb = ChunkSize::from_kb(512)?;  // 512 KB
assert_eq!(size_kb.kilobytes(), 512.0);

// From megabytes
let size_mb = ChunkSize::from_mb(16)?;  // 16 MB
assert_eq!(size_mb.megabytes(), 16.0);

// Default (1 MB)
let default_size = ChunkSize::default();
assert_eq!(default_size.megabytes(), 1.0);
<span class="boring">}</span></code></pre></pre>
<h3 id="size-validation"><a class="header" href="#size-validation">Size Validation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Valid sizes
let valid = ChunkSize::new(64 * 1024)?;  // 64 KB

// ❌ Invalid: too small
let too_small = ChunkSize::new(0);  // Error: must be ≥ 1 byte
assert!(too_small.is_err());

// ❌ Invalid: too large
let too_large = ChunkSize::new(600 * 1024 * 1024);  // Error: must be ≤ 512 MB
assert!(too_large.is_err());
<span class="boring">}</span></code></pre></pre>
<h3 id="optimal-sizing"><a class="header" href="#optimal-sizing">Optimal Sizing</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Calculate optimal chunk size for file
let file_size = 100 * 1024 * 1024;  // 100 MB
let optimal = ChunkSize::optimal_for_file_size(file_size);

println!("Optimal chunk size: {} MB", optimal.megabytes());

// Size conversions
let size = ChunkSize::from_mb(4)?;
println!("Bytes: {}", size.bytes());
println!("Kilobytes: {}", size.kilobytes());
println!("Megabytes: {}", size.megabytes());
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="fileioservice-interface"><a class="header" href="#fileioservice-interface">FileIOService Interface</a></h2>
<p><code>FileIOService</code> is an async trait defining file I/O operations.</p>
<h3 id="service-interface-1"><a class="header" href="#service-interface-1">Service Interface</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[async_trait]
pub trait FileIOService: Send + Sync {
    /// Read file into chunks
    async fn read_file_chunks(
        &amp;self,
        path: &amp;Path,
        chunk_size: ChunkSize,
    ) -&gt; Result&lt;Vec&lt;FileChunk&gt;, PipelineError&gt;;

    /// Write chunks to file
    async fn write_file_chunks(
        &amp;self,
        path: &amp;Path,
        chunks: Vec&lt;FileChunk&gt;,
    ) -&gt; Result&lt;(), PipelineError&gt;;

    /// Stream chunks for processing
    async fn stream_chunks(
        &amp;self,
        path: &amp;Path,
        chunk_size: ChunkSize,
    ) -&gt; Result&lt;impl Stream&lt;Item = Result&lt;FileChunk, PipelineError&gt;&gt;, PipelineError&gt;;
}
<span class="boring">}</span></code></pre></pre>
<h3 id="why-async"><a class="header" href="#why-async">Why Async?</a></h3>
<p>The service is async because file I/O is <strong>I/O-bound</strong>, not CPU-bound:</p>
<p><strong>Benefits:</strong></p>
<ul>
<li><strong>Non-Blocking</strong>: Doesn't block the async runtime</li>
<li><strong>Concurrent</strong>: Multiple files can be processed concurrently</li>
<li><strong>tokio Integration</strong>: Natural integration with tokio::fs</li>
<li><strong>Performance</strong>: Better throughput for I/O operations</li>
</ul>
<p><strong>Classification:</strong></p>
<ul>
<li>This is an <strong>infrastructure port</strong>, not a domain service</li>
<li>Domain services (compression, encryption) are CPU-bound and sync</li>
<li>Infrastructure ports (file I/O, network, database) are I/O-bound and async</li>
</ul>
<hr />
<h2 id="file-reading"><a class="header" href="#file-reading">File Reading</a></h2>
<p>File reading uses streaming and chunking for memory-efficient processing.</p>
<h3 id="reading-small-files"><a class="header" href="#reading-small-files">Reading Small Files</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::FileIOService;

// Read entire file into chunks
let service: Arc&lt;dyn FileIOService&gt; = /* ... */;
let chunks = service.read_file_chunks(
    Path::new("./input.dat"),
    ChunkSize::from_mb(1)?,
).await?;

println!("Read {} chunks", chunks.len());
for chunk in chunks {
    println!("Chunk {}: {} bytes", chunk.sequence_number(), chunk.size().bytes());
}
<span class="boring">}</span></code></pre></pre>
<h3 id="streaming-large-files"><a class="header" href="#streaming-large-files">Streaming Large Files</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tokio_stream::StreamExt;

// Stream chunks for memory efficiency
let mut stream = service.stream_chunks(
    Path::new("./large_file.dat"),
    ChunkSize::from_mb(4)?,
).await?;

while let Some(chunk_result) = stream.next().await {
    let chunk = chunk_result?;

    // Process chunk without loading entire file
    process_chunk(chunk).await?;
}
<span class="boring">}</span></code></pre></pre>
<h3 id="reading-with-metadata"><a class="header" href="#reading-with-metadata">Reading with Metadata</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::fs::metadata;

// Get file metadata first
let metadata = metadata("./input.dat")?;
let file_size = metadata.len();

// Choose optimal chunk size
let chunk_size = ChunkSize::optimal_for_file_size(file_size);

// Read with optimal settings
let chunks = service.read_file_chunks(
    Path::new("./input.dat"),
    chunk_size,
).await?;
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="file-writing"><a class="header" href="#file-writing">File Writing</a></h2>
<p>File writing assembles processed chunks back into complete files.</p>
<h3 id="writing-chunks-to-file"><a class="header" href="#writing-chunks-to-file">Writing Chunks to File</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Write chunks to output file
let processed_chunks: Vec&lt;FileChunk&gt; = /* ... */;

service.write_file_chunks(
    Path::new("./output.dat"),
    processed_chunks,
).await?;

println!("File written successfully");
<span class="boring">}</span></code></pre></pre>
<h3 id="streaming-write"><a class="header" href="#streaming-write">Streaming Write</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tokio::fs::File;
use tokio::io::AsyncWriteExt;

// Stream chunks directly to file
let mut file = File::create("./output.dat").await?;

for chunk in processed_chunks {
    file.write_all(chunk.data()).await?;
}

file.flush().await?;
println!("File written: {} bytes", file.metadata().await?.len());
<span class="boring">}</span></code></pre></pre>
<h3 id="atomic-writes"><a class="header" href="#atomic-writes">Atomic Writes</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tempfile::NamedTempFile;

// Write to temporary file first
let temp_file = NamedTempFile::new()?;
service.write_file_chunks(
    temp_file.path(),
    chunks,
).await?;

// Atomically move to final location
temp_file.persist("./output.dat")?;
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="memory-management-2"><a class="header" href="#memory-management-2">Memory Management</a></h2>
<p>The system uses several strategies to manage memory efficiently.</p>
<h3 id="bounded-memory-usage"><a class="header" href="#bounded-memory-usage">Bounded Memory Usage</a></h3>
<pre><code class="language-text">File Size: 1 GB
Chunk Size: 4 MB
Memory Usage: ~4 MB (single chunk in memory at a time)

Without chunking: 1 GB in memory
With chunking: 4 MB in memory (250x reduction!)
</code></pre>
<h3 id="memory-mapping-for-large-files"><a class="header" href="#memory-mapping-for-large-files">Memory Mapping for Large Files</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Automatically uses memory mapping for files &gt; threshold
let config = FileIOConfig {
    chunk_size: ChunkSize::from_mb(4)?,
    use_memory_mapping: true,
    memory_mapping_threshold: 100 * 1024 * 1024,  // 100 MB
    ..Default::default()
};

// Files &gt; 100 MB use memory mapping
let chunks = service.read_file_chunks_with_config(
    Path::new("./large_file.dat"),
    &amp;config,
).await?;
<span class="boring">}</span></code></pre></pre>
<h3 id="streaming-vs-buffering"><a class="header" href="#streaming-vs-buffering">Streaming vs. Buffering</a></h3>
<p><strong>Streaming (Memory-Efficient):</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Process one chunk at a time
let mut stream = service.stream_chunks(path, chunk_size).await?;
while let Some(chunk) = stream.next().await {
    process_chunk(chunk?).await?;
}
// Peak memory: 1 chunk size
<span class="boring">}</span></code></pre></pre>
<p><strong>Buffering (Performance):</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Load all chunks into memory
let chunks = service.read_file_chunks(path, chunk_size).await?;
process_all_chunks(chunks).await?;
// Peak memory: all chunks
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="error-handling-5"><a class="header" href="#error-handling-5">Error Handling</a></h2>
<p>The file I/O system handles various error conditions.</p>
<h3 id="common-errors"><a class="header" href="#common-errors">Common Errors</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::PipelineError;

match service.read_file_chunks(path, chunk_size).await {
    Ok(chunks) =&gt; { /* success */ },
    Err(PipelineError::FileNotFound(path)) =&gt; {
        eprintln!("File not found: {}", path);
    },
    Err(PipelineError::PermissionDenied(path)) =&gt; {
        eprintln!("Permission denied: {}", path);
    },
    Err(PipelineError::InsufficientDiskSpace) =&gt; {
        eprintln!("Not enough disk space");
    },
    Err(PipelineError::InvalidChunk(msg)) =&gt; {
        eprintln!("Invalid chunk: {}", msg);
    },
    Err(e) =&gt; {
        eprintln!("I/O error: {}", e);
    },
}
<span class="boring">}</span></code></pre></pre>
<h3 id="retry-logic"><a class="header" href="#retry-logic">Retry Logic</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tokio::time::{sleep, Duration};

async fn read_with_retry(
    service: &amp;dyn FileIOService,
    path: &amp;Path,
    chunk_size: ChunkSize,
    max_retries: u32,
) -&gt; Result&lt;Vec&lt;FileChunk&gt;, PipelineError&gt; {
    let mut retries = 0;

    loop {
        match service.read_file_chunks(path, chunk_size).await {
            Ok(chunks) =&gt; return Ok(chunks),
            Err(e) if retries &lt; max_retries =&gt; {
                retries += 1;
                warn!("Read failed (attempt {}/{}): {}", retries, max_retries, e);
                sleep(Duration::from_secs(1 &lt;&lt; retries)).await;  // Exponential backoff
            },
            Err(e) =&gt; return Err(e),
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="partial-reads"><a class="header" href="#partial-reads">Partial Reads</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Handle partial reads gracefully
async fn read_available_chunks(
    service: &amp;dyn FileIOService,
    path: &amp;Path,
    chunk_size: ChunkSize,
) -&gt; Result&lt;Vec&lt;FileChunk&gt;, PipelineError&gt; {
    let mut chunks = Vec::new();
    let mut stream = service.stream_chunks(path, chunk_size).await?;

    while let Some(chunk_result) = stream.next().await {
        match chunk_result {
            Ok(chunk) =&gt; chunks.push(chunk),
            Err(e) =&gt; {
                warn!("Chunk read error: {}, stopping", e);
                break;  // Return partial results
            },
        }
    }

    Ok(chunks)
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="performance-optimization-2"><a class="header" href="#performance-optimization-2">Performance Optimization</a></h2>
<p>Several strategies optimize file I/O performance.</p>
<h3 id="optimal-chunk-size-selection"><a class="header" href="#optimal-chunk-size-selection">Optimal Chunk Size Selection</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Chunk size recommendations
fn optimal_chunk_size(file_size: u64) -&gt; ChunkSize {
    match file_size {
        0..=10_485_760 =&gt; ChunkSize::from_mb(1).unwrap(),          // &lt; 10 MB: 1 MB chunks
        10_485_761..=104_857_600 =&gt; ChunkSize::from_mb(4).unwrap(), // 10-100 MB: 4 MB chunks
        104_857_601..=1_073_741_824 =&gt; ChunkSize::from_mb(8).unwrap(), // 100 MB - 1 GB: 8 MB chunks
        _ =&gt; ChunkSize::from_mb(16).unwrap(),                       // &gt; 1 GB: 16 MB chunks
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="parallel-processing-4"><a class="header" href="#parallel-processing-4">Parallel Processing</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use futures::future::try_join_all;

// Process chunks in parallel
let futures: Vec&lt;_&gt; = chunks.into_iter()
    .map(|chunk| async move {
        process_chunk(chunk).await
    })
    .collect();

let results = try_join_all(futures).await?;
<span class="boring">}</span></code></pre></pre>
<h3 id="buffered-io"><a class="header" href="#buffered-io">Buffered I/O</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tokio::io::BufReader;

// Use buffered reading for better performance
let file = File::open(path).await?;
let mut reader = BufReader::with_capacity(8 * 1024 * 1024, file);  // 8 MB buffer

// Read chunks with buffering
while let Some(chunk) = read_chunk(&amp;mut reader).await? {
    process_chunk(chunk).await?;
}
<span class="boring">}</span></code></pre></pre>
<h3 id="performance-benchmarks-1"><a class="header" href="#performance-benchmarks-1">Performance Benchmarks</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Small Files (&lt; 10 MB)</th><th>Medium Files (100 MB)</th><th>Large Files (&gt; 1 GB)</th></tr></thead><tbody>
<tr><td><strong>Read</strong></td><td>~50 MB/s</td><td>~200 MB/s</td><td>~300 MB/s</td></tr>
<tr><td><strong>Write</strong></td><td>~40 MB/s</td><td>~180 MB/s</td><td>~280 MB/s</td></tr>
<tr><td><strong>Stream</strong></td><td>~45 MB/s</td><td>~190 MB/s</td><td>~290 MB/s</td></tr>
</tbody></table>
</div>
<p><em>Benchmarks on SSD with 4 MB chunks</em></p>
<hr />
<h2 id="usage-examples-5"><a class="header" href="#usage-examples-5">Usage Examples</a></h2>
<h3 id="example-1-basic-file-processing"><a class="header" href="#example-1-basic-file-processing">Example 1: Basic File Processing</a></h3>
<pre><pre class="playground"><code class="language-rust">use pipeline_domain::{FileIOService, ChunkSize};
use std::path::Path;

#[tokio::main]
async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    let service: Arc&lt;dyn FileIOService&gt; = /* ... */;

    // Read file
    let chunks = service.read_file_chunks(
        Path::new("./input.dat"),
        ChunkSize::from_mb(1)?,
    ).await?;

    println!("Read {} chunks", chunks.len());

    // Process chunks
    let processed: Vec&lt;_&gt; = chunks.into_iter()
        .map(|chunk| process_chunk(chunk))
        .collect();

    // Write output
    service.write_file_chunks(
        Path::new("./output.dat"),
        processed,
    ).await?;

    println!("Processing complete!");
    Ok(())
}

fn process_chunk(chunk: FileChunk) -&gt; FileChunk {
    // Transform chunk data
    let transformed_data = chunk.data().to_vec();
    FileChunk::new(
        chunk.sequence_number(),
        chunk.offset(),
        transformed_data,
        chunk.is_final(),
    ).unwrap()
}</code></pre></pre>
<h3 id="example-2-streaming-large-files"><a class="header" href="#example-2-streaming-large-files">Example 2: Streaming Large Files</a></h3>
<pre><pre class="playground"><code class="language-rust">use tokio_stream::StreamExt;

#[tokio::main]
async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    let service: Arc&lt;dyn FileIOService&gt; = /* ... */;

    // Stream large file
    let mut stream = service.stream_chunks(
        Path::new("./large_file.dat"),
        ChunkSize::from_mb(8)?,
    ).await?;

    let mut processed_chunks = Vec::new();

    while let Some(chunk_result) = stream.next().await {
        let chunk = chunk_result?;

        // Process chunk in streaming fashion
        let processed = process_chunk(chunk);
        processed_chunks.push(processed);

        // Optional: write chunks as they're processed
        // write_chunk_to_file(&amp;processed).await?;
    }

    println!("Processed {} chunks", processed_chunks.len());
    Ok(())
}</code></pre></pre>
<h3 id="example-3-parallel-chunk-processing"><a class="header" href="#example-3-parallel-chunk-processing">Example 3: Parallel Chunk Processing</a></h3>
<pre><pre class="playground"><code class="language-rust">use futures::future::try_join_all;

#[tokio::main]
async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    let service: Arc&lt;dyn FileIOService&gt; = /* ... */;

    // Read all chunks
    let chunks = service.read_file_chunks(
        Path::new("./input.dat"),
        ChunkSize::from_mb(4)?,
    ).await?;

    // Process chunks in parallel
    let futures = chunks.into_iter().map(|chunk| {
        tokio::spawn(async move {
            process_chunk_async(chunk).await
        })
    });

    let results = try_join_all(futures).await?;
    let processed: Vec&lt;_&gt; = results.into_iter()
        .collect::&lt;Result&lt;Vec&lt;_&gt;, _&gt;&gt;()?;

    // Write results
    service.write_file_chunks(
        Path::new("./output.dat"),
        processed,
    ).await?;

    Ok(())
}

async fn process_chunk_async(chunk: FileChunk) -&gt; Result&lt;FileChunk, PipelineError&gt; {
    // Async processing
    tokio::time::sleep(Duration::from_millis(10)).await;
    Ok(process_chunk(chunk))
}</code></pre></pre>
<h3 id="example-4-error-handling-and-retry"><a class="header" href="#example-4-error-handling-and-retry">Example 4: Error Handling and Retry</a></h3>
<pre><pre class="playground"><code class="language-rust">#[tokio::main]
async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    let service: Arc&lt;dyn FileIOService&gt; = /* ... */;
    let path = Path::new("./input.dat");
    let chunk_size = ChunkSize::from_mb(1)?;

    // Retry on failure
    let chunks = read_with_retry(&amp;*service, path, chunk_size, 3).await?;

    println!("Successfully read {} chunks", chunks.len());
    Ok(())
}

async fn read_with_retry(
    service: &amp;dyn FileIOService,
    path: &amp;Path,
    chunk_size: ChunkSize,
    max_retries: u32,
) -&gt; Result&lt;Vec&lt;FileChunk&gt;, PipelineError&gt; {
    for attempt in 1..=max_retries {
        match service.read_file_chunks(path, chunk_size).await {
            Ok(chunks) =&gt; return Ok(chunks),
            Err(e) if attempt &lt; max_retries =&gt; {
                eprintln!("Attempt {}/{} failed: {}", attempt, max_retries, e);
                tokio::time::sleep(Duration::from_secs(2_u64.pow(attempt))).await;
            },
            Err(e) =&gt; return Err(e),
        }
    }
    unreachable!()
}</code></pre></pre>
<hr />
<h2 id="best-practices-7"><a class="header" href="#best-practices-7">Best Practices</a></h2>
<h3 id="1-choose-appropriate-chunk-sizes"><a class="header" href="#1-choose-appropriate-chunk-sizes">1. Choose Appropriate Chunk Sizes</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Optimize chunk size for file
let file_size = metadata(path)?.len();
let chunk_size = ChunkSize::optimal_for_file_size(file_size);

// ❌ Bad: Fixed chunk size for all files
let chunk_size = ChunkSize::from_mb(1)?;  // May be suboptimal
<span class="boring">}</span></code></pre></pre>
<h3 id="2-use-streaming-for-large-files"><a class="header" href="#2-use-streaming-for-large-files">2. Use Streaming for Large Files</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Stream large files
let mut stream = service.stream_chunks(path, chunk_size).await?;
while let Some(chunk) = stream.next().await {
    process_chunk(chunk?).await?;
}

// ❌ Bad: Load entire large file into memory
let chunks = service.read_file_chunks(path, chunk_size).await?;
// Entire file in memory!
<span class="boring">}</span></code></pre></pre>
<h3 id="3-validate-chunk-integrity"><a class="header" href="#3-validate-chunk-integrity">3. Validate Chunk Integrity</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Verify checksums
for chunk in chunks {
    if !chunk.verify_checksum() {
        return Err(PipelineError::ChecK sumMismatch);
    }
    process_chunk(chunk)?;
}
<span class="boring">}</span></code></pre></pre>
<h3 id="4-handle-errors-gracefully"><a class="header" href="#4-handle-errors-gracefully">4. Handle Errors Gracefully</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Specific error handling
match service.read_file_chunks(path, chunk_size).await {
    Ok(chunks) =&gt; process(chunks),
    Err(PipelineError::FileNotFound(_)) =&gt; handle_missing_file(),
    Err(PipelineError::PermissionDenied(_)) =&gt; handle_permissions(),
    Err(e) =&gt; handle_generic_error(e),
}
<span class="boring">}</span></code></pre></pre>
<h3 id="5-use-type-safe-paths"><a class="header" href="#5-use-type-safe-paths">5. Use Type-Safe Paths</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Type-safe paths
let input: FilePath&lt;InputPath&gt; = FilePath::new("./input.dat")?;
let output: FilePath&lt;OutputPath&gt; = FilePath::new("./output.dat")?;

// ❌ Bad: Raw strings
let input = "./input.dat";
let output = "./output.dat";
<span class="boring">}</span></code></pre></pre>
<h3 id="6-clean-up-temporary-files"><a class="header" href="#6-clean-up-temporary-files">6. Clean Up Temporary Files</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Automatic cleanup with tempfile
let temp = NamedTempFile::new()?;
write_chunks(temp.path(), chunks).await?;
// Automatically deleted when dropped

// Or explicit cleanup
temp.close()?;
<span class="boring">}</span></code></pre></pre>
<h3 id="7-monitor-memory-usage"><a class="header" href="#7-monitor-memory-usage">7. Monitor Memory Usage</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Track memory usage
let chunks_in_memory = chunks.len();
let memory_used = chunks_in_memory * chunk_size.bytes();
if memory_used &gt; max_memory {
    // Switch to streaming
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="troubleshooting-3"><a class="header" href="#troubleshooting-3">Troubleshooting</a></h2>
<h3 id="issue-1-out-of-memory"><a class="header" href="#issue-1-out-of-memory">Issue 1: Out of Memory</a></h3>
<p><strong>Symptom:</strong></p>
<pre><code class="language-text">Error: Out of memory
</code></pre>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 1. Reduce chunk size
let chunk_size = ChunkSize::from_mb(1)?;  // Smaller chunks

// 2. Use streaming instead of buffering
let mut stream = service.stream_chunks(path, chunk_size).await?;

// 3. Process chunks one at a time
while let Some(chunk) = stream.next().await {
    process_chunk(chunk?).await?;
    // Chunk dropped, memory freed
}
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-2-slow-file-io"><a class="header" href="#issue-2-slow-file-io">Issue 2: Slow File I/O</a></h3>
<p><strong>Diagnosis:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let start = Instant::now();
let chunks = service.read_file_chunks(path, chunk_size).await?;
let duration = start.elapsed();
println!("Read took: {:?}", duration);
<span class="boring">}</span></code></pre></pre>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 1. Increase chunk size
let chunk_size = ChunkSize::from_mb(8)?;  // Larger chunks = fewer I/O ops

// 2. Use memory mapping for large files
let config = FileIOConfig {
    use_memory_mapping: true,
    memory_mapping_threshold: 50 * 1024 * 1024,  // 50 MB
    ..Default::default()
};

// 3. Use buffered I/O
let reader = BufReader::with_capacity(8 * 1024 * 1024, file);
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-3-checksum-mismatch"><a class="header" href="#issue-3-checksum-mismatch">Issue 3: Checksum Mismatch</a></h3>
<p><strong>Symptom:</strong></p>
<pre><code class="language-text">Error: Checksum mismatch for chunk 42
</code></pre>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 1. Verify during read
let chunk = chunk.with_calculated_checksum();
if !chunk.verify_checksum() {
    // Re-read chunk
}

// 2. Log and skip corrupted chunks
match chunk.verify_checksum() {
    true =&gt; process_chunk(chunk),
    false =&gt; {
        warn!("Chunk {} corrupted, skipping", chunk.sequence_number());
        continue;
    },
}
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-4-file-permission-errors"><a class="header" href="#issue-4-file-permission-errors">Issue 4: File Permission Errors</a></h3>
<p><strong>Symptom:</strong></p>
<pre><code class="language-text">Error: Permission denied: ./output.dat
</code></pre>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 1. Check permissions before writing
use std::fs;
let metadata = fs::metadata(parent_dir)?;
if metadata.permissions().readonly() {
    return Err("Output directory is read-only".into());
}

// 2. Use appropriate path categories
let output: FilePath&lt;OutputPath&gt; = FilePath::new("./output.dat")?;
output.ensure_writable()?;
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="testing-strategies-3"><a class="header" href="#testing-strategies-3">Testing Strategies</a></h2>
<h3 id="unit-testing-with-mock-chunks"><a class="header" href="#unit-testing-with-mock-chunks">Unit Testing with Mock Chunks</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_chunk_creation() {
        let data = vec![1, 2, 3, 4, 5];
        let chunk = FileChunk::new(0, 0, data.clone(), false).unwrap();

        assert_eq!(chunk.sequence_number(), 0);
        assert_eq!(chunk.data(), &amp;data);
        assert!(!chunk.is_final());
    }

    #[test]
    fn test_chunk_checksum() {
        let data = vec![1, 2, 3, 4, 5];
        let chunk = FileChunk::new(0, 0, data, false)
            .unwrap()
            .with_calculated_checksum();

        assert!(chunk.checksum().is_some());
        assert!(chunk.verify_checksum());
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="integration-testing-with-files"><a class="header" href="#integration-testing-with-files">Integration Testing with Files</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_file_round_trip() {
    let service: Arc&lt;dyn FileIOService&gt; = create_test_service();

    // Create test data
    let test_data = vec![0u8; 10 * 1024 * 1024];  // 10 MB
    let input_path = temp_dir().join("test_input.dat");
    std::fs::write(&amp;input_path, &amp;test_data).unwrap();

    // Read chunks
    let chunks = service.read_file_chunks(
        &amp;input_path,
        ChunkSize::from_mb(1).unwrap(),
    ).await.unwrap();

    assert_eq!(chunks.len(), 10);  // 10 x 1MB chunks

    // Write chunks
    let output_path = temp_dir().join("test_output.dat");
    service.write_file_chunks(&amp;output_path, chunks).await.unwrap();

    // Verify
    let output_data = std::fs::read(&amp;output_path).unwrap();
    assert_eq!(test_data, output_data);
}
<span class="boring">}</span></code></pre></pre>
<h3 id="streaming-tests"><a class="header" href="#streaming-tests">Streaming Tests</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_streaming() {
    use tokio_stream::StreamExt;

    let service: Arc&lt;dyn FileIOService&gt; = create_test_service();
    let path = create_test_file(100 * 1024 * 1024);  // 100 MB

    let mut stream = service.stream_chunks(
        &amp;path,
        ChunkSize::from_mb(4).unwrap(),
    ).await.unwrap();

    let mut chunk_count = 0;
    while let Some(chunk_result) = stream.next().await {
        let chunk = chunk_result.unwrap();
        assert!(chunk.size().bytes() &lt;= 4 * 1024 * 1024);
        chunk_count += 1;
    }

    assert_eq!(chunk_count, 25);  // 100 MB / 4 MB = 25 chunks
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="next-steps-17"><a class="header" href="#next-steps-17">Next Steps</a></h2>
<p>After understanding file I/O fundamentals, explore specific implementations:</p>
<h3 id="detailed-file-io-topics"><a class="header" href="#detailed-file-io-topics">Detailed File I/O Topics</a></h3>
<ol>
<li><strong><a href="implementation/chunking.html">Chunking Strategy</a></strong>: Deep dive into chunking algorithms and optimization</li>
<li><strong><a href="implementation/binary-format.html">Binary Format</a></strong>: File format specification and serialization</li>
</ol>
<h3 id="related-topics-2"><a class="header" href="#related-topics-2">Related Topics</a></h3>
<ul>
<li><strong><a href="implementation/stages.html">Stage Processing</a></strong>: How stages process file chunks</li>
<li><strong><a href="implementation/integrity.html">Integrity Checking</a></strong>: Checksums and verification</li>
<li><strong><a href="implementation/../advanced/performance.html">Performance Optimization</a></strong>: I/O performance tuning</li>
</ul>
<h3 id="advanced-topics-2"><a class="header" href="#advanced-topics-2">Advanced Topics</a></h3>
<ul>
<li><strong><a href="implementation/../advanced/concurrency.html">Concurrency Model</a></strong>: Parallel file processing</li>
<li><strong><a href="implementation/../advanced/extending.html">Extending the Pipeline</a></strong>: Custom file formats and I/O</li>
</ul>
<hr />
<h2 id="summary-2"><a class="header" href="#summary-2">Summary</a></h2>
<p><strong>Key Takeaways:</strong></p>
<ol>
<li><strong>FileChunk</strong> is an immutable value object representing file data portions</li>
<li><strong>FilePath<T></strong> provides type-safe, validated file paths with phantom types</li>
<li><strong>ChunkSize</strong> validates chunk sizes within 1 byte to 512 MB bounds</li>
<li><strong>FileIOService</strong> defines async I/O operations for streaming and chunking</li>
<li><strong>Streaming</strong> enables memory-efficient processing of files of any size</li>
<li><strong>Memory Management</strong> uses bounded buffers and optional memory mapping</li>
<li><strong>Error Handling</strong> provides retry logic and graceful degradation</li>
</ol>
<p><strong>Architecture File References:</strong></p>
<ul>
<li><strong>FileChunk:</strong> <code>pipeline-domain/src/value_objects/file_chunk.rs:176</code></li>
<li><strong>FilePath:</strong> <code>pipeline-domain/src/value_objects/file_path.rs:1</code></li>
<li><strong>ChunkSize:</strong> <code>pipeline-domain/src/value_objects/chunk_size.rs:1</code></li>
<li><strong>FileIOService:</strong> <code>pipeline-domain/src/services/file_io_service.rs:185</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chunking-strategy"><a class="header" href="#chunking-strategy">Chunking Strategy</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>This chapter provides a comprehensive overview of the file chunking strategy in the adaptive pipeline system. Learn how files are split into manageable chunks, how chunk sizes are optimized, and how chunking enables efficient parallel processing.</p>
<hr />
<h2 id="table-of-contents-3"><a class="header" href="#table-of-contents-3">Table of Contents</a></h2>
<ul>
<li><a href="implementation/chunking.html#overview">Overview</a></li>
<li><a href="implementation/chunking.html#chunking-architecture">Chunking Architecture</a></li>
<li><a href="implementation/chunking.html#chunk-size-selection">Chunk Size Selection</a></li>
<li><a href="implementation/chunking.html#chunking-algorithm">Chunking Algorithm</a></li>
<li><a href="implementation/chunking.html#optimal-sizing-strategy">Optimal Sizing Strategy</a></li>
<li><a href="implementation/chunking.html#memory-management">Memory Management</a></li>
<li><a href="implementation/chunking.html#parallel-processing">Parallel Processing</a></li>
<li><a href="implementation/chunking.html#adaptive-chunking">Adaptive Chunking</a></li>
<li><a href="implementation/chunking.html#performance-characteristics">Performance Characteristics</a></li>
<li><a href="implementation/chunking.html#usage-examples">Usage Examples</a></li>
<li><a href="implementation/chunking.html#best-practices">Best Practices</a></li>
<li><a href="implementation/chunking.html#troubleshooting">Troubleshooting</a></li>
<li><a href="implementation/chunking.html#testing-strategies">Testing Strategies</a></li>
<li><a href="implementation/chunking.html#next-steps">Next Steps</a></li>
</ul>
<hr />
<h2 id="overview-12"><a class="header" href="#overview-12">Overview</a></h2>
<p><strong>Chunking</strong> is the process of dividing files into smaller, manageable pieces (chunks) that can be processed independently. This strategy enables efficient memory usage, parallel processing, and scalable file handling regardless of file size.</p>
<h3 id="key-benefits"><a class="header" href="#key-benefits">Key Benefits</a></h3>
<ul>
<li><strong>Memory Efficiency</strong>: Process files larger than available RAM</li>
<li><strong>Parallel Processing</strong>: Process multiple chunks concurrently</li>
<li><strong>Fault Tolerance</strong>: Failed chunks can be retried independently</li>
<li><strong>Progress Tracking</strong>: Track progress at chunk granularity</li>
<li><strong>Scalability</strong>: Handle files from bytes to terabytes</li>
</ul>
<h3 id="chunking-workflow"><a class="header" href="#chunking-workflow">Chunking Workflow</a></h3>
<pre><code class="language-text">Input File (100 MB)
        ↓
[Chunking Strategy]
        ↓
┌──────────┬──────────┬──────────┬──────────┐
│ Chunk 0  │ Chunk 1  │ Chunk 2  │ Chunk 3  │
│ (0-25MB) │(25-50MB) │(50-75MB) │(75-100MB)│
└──────────┴──────────┴──────────┴──────────┘
        ↓ parallel processing
┌──────────┬──────────┬──────────┬──────────┐
│Processed │Processed │Processed │Processed │
│ Chunk 0  │ Chunk 1  │ Chunk 2  │ Chunk 3  │
└──────────┴──────────┴──────────┴──────────┘
        ↓
Output File (processed)
</code></pre>
<h3 id="design-principles-3"><a class="header" href="#design-principles-3">Design Principles</a></h3>
<ol>
<li><strong>Predictable Memory</strong>: Bounded memory usage regardless of file size</li>
<li><strong>Optimal Sizing</strong>: Empirically optimized chunk sizes for performance</li>
<li><strong>Independent Processing</strong>: Each chunk can be processed in isolation</li>
<li><strong>Ordered Reassembly</strong>: Chunks maintain sequence for correct reassembly</li>
<li><strong>Adaptive Strategy</strong>: Chunk size adapts to file size and system resources</li>
</ol>
<hr />
<h2 id="chunking-architecture"><a class="header" href="#chunking-architecture">Chunking Architecture</a></h2>
<p>The chunking system uses a combination of value objects and algorithms to efficiently divide files.</p>
<h3 id="chunking-components"><a class="header" href="#chunking-components">Chunking Components</a></h3>
<pre><code class="language-text">┌─────────────────────────────────────────────────────────┐
│                 Chunking Strategy                        │
│                                                          │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐ │
│  │  ChunkSize   │  │ FileChunk    │  │  Chunking    │ │
│  │   (1B-512MB) │  │ (immutable)  │  │  Algorithm   │ │
│  └──────────────┘  └──────────────┘  └──────────────┘ │
│                                                          │
└─────────────────────────────────────────────────────────┘
                         ↓
┌─────────────────────────────────────────────────────────┐
│              Optimal Size Calculation                    │
│                                                          │
│  File Size → optimal_for_file_size() → ChunkSize       │
│                                                          │
│  - Small files  (&lt;10 MB):    64-256 KB chunks          │
│  - Medium files (10-500 MB): 2-16 MB chunks            │
│  - Large files  (500MB-2GB): 64 MB chunks              │
│  - Huge files   (&gt;2 GB):     128 MB chunks             │
└─────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="chunk-lifecycle"><a class="header" href="#chunk-lifecycle">Chunk Lifecycle</a></h3>
<pre><code class="language-text">1. Size Determination
   - Calculate optimal chunk size based on file size
   - Adjust for available memory if needed
   ↓
2. File Division
   - Read file in chunk-sized pieces
   - Create FileChunk with sequence number and offset
   ↓
3. Chunk Processing
   - Apply pipeline stages to each chunk
   - Process chunks in parallel if enabled
   ↓
4. Chunk Reassembly
   - Combine processed chunks by sequence number
   - Write to output file
</code></pre>
<hr />
<h2 id="chunk-size-selection-2"><a class="header" href="#chunk-size-selection-2">Chunk Size Selection</a></h2>
<p>Chunk size is critical for performance and memory efficiency. The system supports validated sizes from 1 byte to 512 MB.</p>
<h3 id="size-constraints-1"><a class="header" href="#size-constraints-1">Size Constraints</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ChunkSize constants
ChunkSize::MIN_SIZE  // 1 byte
ChunkSize::MAX_SIZE  // 512 MB
ChunkSize::DEFAULT   // 1 MB
<span class="boring">}</span></code></pre></pre>
<h3 id="creating-chunk-sizes-1"><a class="header" href="#creating-chunk-sizes-1">Creating Chunk Sizes</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::ChunkSize;

// From bytes
let chunk = ChunkSize::new(1024 * 1024)?;  // 1 MB

// From kilobytes
let chunk_kb = ChunkSize::from_kb(512)?;  // 512 KB

// From megabytes
let chunk_mb = ChunkSize::from_mb(16)?;  // 16 MB

// Default size
let default_chunk = ChunkSize::default();  // 1 MB
<span class="boring">}</span></code></pre></pre>
<h3 id="size-validation-1"><a class="header" href="#size-validation-1">Size Validation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Valid sizes
let valid = ChunkSize::new(64 * 1024)?;  // 64 KB - valid

// ❌ Invalid: too small
let too_small = ChunkSize::new(0);  // Error: must be ≥ 1 byte
assert!(too_small.is_err());

// ❌ Invalid: too large
let too_large = ChunkSize::new(600 * 1024 * 1024);  // Error: must be ≤ 512 MB
assert!(too_large.is_err());
<span class="boring">}</span></code></pre></pre>
<h3 id="size-trade-offs"><a class="header" href="#size-trade-offs">Size Trade-offs</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Chunk Size</th><th>Memory Usage</th><th>I/O Overhead</th><th>Parallelism</th><th>Best For</th></tr></thead><tbody>
<tr><td><strong>Small (64-256 KB)</strong></td><td>Low</td><td>High</td><td>Excellent</td><td>Small files, limited memory</td></tr>
<tr><td><strong>Medium (1-16 MB)</strong></td><td>Moderate</td><td>Moderate</td><td>Good</td><td>Most use cases</td></tr>
<tr><td><strong>Large (64-128 MB)</strong></td><td>High</td><td>Low</td><td>Limited</td><td>Large files, ample memory</td></tr>
</tbody></table>
</div>
<hr />
<h2 id="chunking-algorithm"><a class="header" href="#chunking-algorithm">Chunking Algorithm</a></h2>
<p>The chunking algorithm divides files into sequential chunks with proper metadata.</p>
<h3 id="basic-chunking-process"><a class="header" href="#basic-chunking-process">Basic Chunking Process</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn chunk_file(
    file_path: &amp;Path,
    chunk_size: ChunkSize,
) -&gt; Result&lt;Vec&lt;FileChunk&gt;, PipelineError&gt; {
    let file = File::open(file_path)?;
    let file_size = file.metadata()?.len();
    let mut chunks = Vec::new();
    let mut offset = 0;
    let mut sequence = 0;

    // Read file in chunks
    let mut reader = BufReader::new(file);
    let mut buffer = vec![0u8; chunk_size.bytes()];

    loop {
        let bytes_read = reader.read(&amp;mut buffer)?;
        if bytes_read == 0 {
            break;  // EOF
        }

        let data = buffer[..bytes_read].to_vec();
        let is_final = offset + bytes_read as u64 &gt;= file_size;

        let chunk = FileChunk::new(sequence, offset, data, is_final)?;
        chunks.push(chunk);

        offset += bytes_read as u64;
        sequence += 1;
    }

    Ok(chunks)
}
<span class="boring">}</span></code></pre></pre>
<h3 id="chunk-metadata"><a class="header" href="#chunk-metadata">Chunk Metadata</a></h3>
<p>Each chunk contains essential metadata:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct FileChunk {
    id: Uuid,                 // Unique chunk identifier
    sequence_number: u64,     // Order in file (0-based)
    offset: u64,              // Byte offset in original file
    size: ChunkSize,          // Actual chunk size
    data: Vec&lt;u8&gt;,            // Chunk data
    checksum: Option&lt;String&gt;, // Optional checksum
    is_final: bool,           // Last chunk flag
    created_at: DateTime&lt;Utc&gt;,// Creation timestamp
}
<span class="boring">}</span></code></pre></pre>
<h3 id="calculating-chunk-count"><a class="header" href="#calculating-chunk-count">Calculating Chunk Count</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Calculate number of chunks needed
let file_size = 100 * 1024 * 1024;  // 100 MB
let chunk_size = ChunkSize::from_mb(4)?;  // 4 MB chunks

let num_chunks = chunk_size.chunks_needed_for_file(file_size);
println!("Need {} chunks", num_chunks);  // 25 chunks
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="optimal-sizing-strategy"><a class="header" href="#optimal-sizing-strategy">Optimal Sizing Strategy</a></h2>
<p>The system uses empirically optimized chunk sizes based on comprehensive benchmarking.</p>
<h3 id="optimization-strategy"><a class="header" href="#optimization-strategy">Optimization Strategy</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn optimal_for_file_size(file_size: u64) -&gt; ChunkSize {
    let optimal_size = match file_size {
        // Small files: smaller chunks
        0..=1_048_576 =&gt; 64 * 1024,           // 64KB for ≤ 1MB
        1_048_577..=10_485_760 =&gt; 256 * 1024, // 256KB for ≤ 10MB

        // Medium files: empirically optimized
        10_485_761..=52_428_800 =&gt; 2 * 1024 * 1024,  // 2MB for ≤ 50MB
        52_428_801..=524_288_000 =&gt; 16 * 1024 * 1024, // 16MB for 50-500MB

        // Large files: larger chunks for efficiency
        524_288_001..=2_147_483_648 =&gt; 64 * 1024 * 1024, // 64MB for 500MB-2GB

        // Huge files: maximum throughput
        _ =&gt; 128 * 1024 * 1024, // 128MB for &gt;2GB
    };

    ChunkSize { bytes: optimal_size.clamp(MIN_SIZE, MAX_SIZE) }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="empirical-results"><a class="header" href="#empirical-results">Empirical Results</a></h3>
<p>Benchmarking results that informed this strategy:</p>
<div class="table-wrapper"><table><thead><tr><th>File Size</th><th>Chunk Size</th><th>Throughput</th><th>Improvement</th></tr></thead><tbody>
<tr><td>100 MB</td><td>16 MB</td><td>~300 MB/s</td><td>+43.7% vs 2 MB</td></tr>
<tr><td>500 MB</td><td>16 MB</td><td>~320 MB/s</td><td>+56.2% vs 4 MB</td></tr>
<tr><td>2 GB</td><td>128 MB</td><td>~350 MB/s</td><td>Baseline</td></tr>
</tbody></table>
</div>
<h3 id="using-optimal-sizes"><a class="header" href="#using-optimal-sizes">Using Optimal Sizes</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Automatically select optimal chunk size
let file_size = 100 * 1024 * 1024;  // 100 MB
let optimal = ChunkSize::optimal_for_file_size(file_size);

println!("Optimal chunk size: {} MB", optimal.megabytes());  // 16 MB

// Check if current size is optimal
let current = ChunkSize::from_mb(4)?;
if !current.is_optimal_for_file(file_size) {
    println!("Warning: chunk size may be suboptimal");
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="memory-management-3"><a class="header" href="#memory-management-3">Memory Management</a></h2>
<p>Chunking enables predictable memory usage regardless of file size.</p>
<h3 id="bounded-memory-usage-1"><a class="header" href="#bounded-memory-usage-1">Bounded Memory Usage</a></h3>
<pre><code class="language-text">Without Chunking:
  File: 10 GB
  Memory: 10 GB (entire file in memory)

With Chunking (16 MB chunks):
  File: 10 GB
  Memory: 16 MB (single chunk in memory)
  Reduction: 640x less memory!
</code></pre>
<h3 id="memory-adaptive-chunking"><a class="header" href="#memory-adaptive-chunking">Memory-Adaptive Chunking</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Adjust chunk size based on available memory
let available_memory = 100 * 1024 * 1024;  // 100 MB available
let max_parallel_chunks = 4;

let chunk_size = ChunkSize::from_mb(32)?;  // Desired 32 MB
let adjusted = chunk_size.adjust_for_memory(
    available_memory,
    max_parallel_chunks,
)?;

println!("Adjusted chunk size: {} MB", adjusted.megabytes());
// 25 MB (100 MB / 4 chunks)
<span class="boring">}</span></code></pre></pre>
<h3 id="memory-footprint-calculation"><a class="header" href="#memory-footprint-calculation">Memory Footprint Calculation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn calculate_memory_footprint(
    chunk_size: ChunkSize,
    parallel_chunks: usize,
) -&gt; usize {
    // Base memory per chunk
    let per_chunk = chunk_size.bytes();

    // Additional overhead (metadata, buffers, etc.)
    let overhead_per_chunk = 1024;  // ~1 KB overhead

    // Total memory footprint
    parallel_chunks * (per_chunk + overhead_per_chunk)
}

let chunk_size = ChunkSize::from_mb(4)?;
let memory = calculate_memory_footprint(chunk_size, 4);
println!("Memory footprint: {} MB", memory / (1024 * 1024));
// ~16 MB total
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="parallel-processing-5"><a class="header" href="#parallel-processing-5">Parallel Processing</a></h2>
<p>Chunking enables efficient parallel processing of file data.</p>
<h3 id="parallel-chunk-processing-3"><a class="header" href="#parallel-chunk-processing-3">Parallel Chunk Processing</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use futures::future::try_join_all;

async fn process_chunks_parallel(
    chunks: Vec&lt;FileChunk&gt;,
) -&gt; Result&lt;Vec&lt;FileChunk&gt;, PipelineError&gt; {
    // Process chunks in parallel
    let futures = chunks.into_iter().map(|chunk| {
        tokio::spawn(async move {
            process_chunk(chunk).await
        })
    });

    // Wait for all to complete
    let results = try_join_all(futures).await?;
    Ok(results.into_iter().collect::&lt;Result&lt;Vec&lt;_&gt;, _&gt;&gt;()?)
}
<span class="boring">}</span></code></pre></pre>
<h3 id="parallelism-trade-offs"><a class="header" href="#parallelism-trade-offs">Parallelism Trade-offs</a></h3>
<pre><code class="language-text">Sequential Processing:
  Time = num_chunks × time_per_chunk
  Memory = 1 × chunk_size

Parallel Processing (N threads):
  Time = (num_chunks / N) × time_per_chunk
  Memory = N × chunk_size
</code></pre>
<h3 id="optimal-parallelism"><a class="header" href="#optimal-parallelism">Optimal Parallelism</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Calculate optimal parallelism
fn optimal_parallelism(
    file_size: u64,
    chunk_size: ChunkSize,
    available_memory: usize,
    cpu_cores: usize,
) -&gt; usize {
    let num_chunks = chunk_size.chunks_needed_for_file(file_size) as usize;

    // Memory-based limit
    let memory_limit = available_memory / chunk_size.bytes();

    // CPU-based limit
    let cpu_limit = cpu_cores;

    // Chunk count limit
    let chunk_limit = num_chunks;

    // Take minimum of all limits
    memory_limit.min(cpu_limit).min(chunk_limit).max(1)
}

let file_size = 100 * 1024 * 1024;
let chunk_size = ChunkSize::from_mb(4)?;
let parallelism = optimal_parallelism(
    file_size,
    chunk_size,
    64 * 1024 * 1024,  // 64 MB available
    8,                  // 8 CPU cores
);
println!("Optimal parallelism: {} chunks", parallelism);
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="adaptive-chunking"><a class="header" href="#adaptive-chunking">Adaptive Chunking</a></h2>
<p>The system can adapt chunk sizes dynamically based on conditions.</p>
<h3 id="adaptive-sizing-triggers"><a class="header" href="#adaptive-sizing-triggers">Adaptive Sizing Triggers</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub enum AdaptiveTrigger {
    MemoryPressure,      // Reduce chunk size due to low memory
    SlowPerformance,     // Increase chunk size for better throughput
    NetworkLatency,      // Reduce chunk size for streaming
    CpuUtilization,      // Adjust based on CPU usage
}
<span class="boring">}</span></code></pre></pre>
<h3 id="dynamic-adjustment"><a class="header" href="#dynamic-adjustment">Dynamic Adjustment</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn adaptive_chunking(
    file_path: &amp;Path,
    initial_chunk_size: ChunkSize,
) -&gt; Result&lt;Vec&lt;FileChunk&gt;, PipelineError&gt; {
    let mut chunk_size = initial_chunk_size;
    let mut chunks = Vec::new();
    let mut performance_samples = Vec::new();

    loop {
        let start = Instant::now();

        // Read next chunk
        let chunk = read_next_chunk(file_path, chunk_size)?;
        if chunk.is_none() {
            break;
        }

        let duration = start.elapsed();
        performance_samples.push(duration);

        chunks.push(chunk.unwrap());

        // Adapt chunk size based on performance
        if performance_samples.len() &gt;= 5 {
            let avg_time = performance_samples.iter().sum::&lt;Duration&gt;() / 5;

            if avg_time &gt; Duration::from_millis(100) {
                // Too slow, increase chunk size
                chunk_size = adjust_chunk_size(chunk_size, 1.5)?;
            } else if avg_time &lt; Duration::from_millis(10) {
                // Too fast, reduce overhead by increasing size
                chunk_size = adjust_chunk_size(chunk_size, 1.2)?;
            }

            performance_samples.clear();
        }
    }

    Ok(chunks)
}

fn adjust_chunk_size(
    current: ChunkSize,
    factor: f64,
) -&gt; Result&lt;ChunkSize, PipelineError&gt; {
    let new_size = (current.bytes() as f64 * factor) as usize;
    ChunkSize::new(new_size)
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="performance-characteristics"><a class="header" href="#performance-characteristics">Performance Characteristics</a></h2>
<p>Chunking performance varies based on size and system characteristics.</p>
<h3 id="throughput-by-chunk-size"><a class="header" href="#throughput-by-chunk-size">Throughput by Chunk Size</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Chunk Size</th><th>Read Speed</th><th>Write Speed</th><th>CPU Usage</th><th>Memory</th></tr></thead><tbody>
<tr><td><strong>64 KB</strong></td><td>~40 MB/s</td><td>~35 MB/s</td><td>15%</td><td>Low</td></tr>
<tr><td><strong>1 MB</strong></td><td>~120 MB/s</td><td>~100 MB/s</td><td>20%</td><td>Low</td></tr>
<tr><td><strong>16 MB</strong></td><td>~300 MB/s</td><td>~280 MB/s</td><td>25%</td><td>Medium</td></tr>
<tr><td><strong>64 MB</strong></td><td>~320 MB/s</td><td>~300 MB/s</td><td>30%</td><td>High</td></tr>
<tr><td><strong>128 MB</strong></td><td>~350 MB/s</td><td>~320 MB/s</td><td>35%</td><td>High</td></tr>
</tbody></table>
</div>
<p><em>Benchmarks on NVMe SSD with 8-core CPU</em></p>
<h3 id="latency-characteristics"><a class="header" href="#latency-characteristics">Latency Characteristics</a></h3>
<pre><code class="language-text">Small Chunks (64 KB):
  - Low latency per chunk: ~1-2ms
  - High overhead: many chunks
  - Good for: streaming, low memory

Large Chunks (128 MB):
  - High latency per chunk: ~400ms
  - Low overhead: few chunks
  - Good for: throughput, batch processing
</code></pre>
<h3 id="performance-optimization-3"><a class="header" href="#performance-optimization-3">Performance Optimization</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Benchmark different chunk sizes
async fn benchmark_chunk_sizes(
    file_path: &amp;Path,
    sizes: &amp;[ChunkSize],
) -&gt; Vec&lt;(ChunkSize, Duration)&gt; {
    let mut results = Vec::new();

    for &amp;size in sizes {
        let start = Instant::now();
        let _ = chunk_file(file_path, size).await.unwrap();
        let duration = start.elapsed();

        results.push((size, duration));
    }

    results.sort_by_key(|(_, duration)| *duration);
    results
}

// Usage
let sizes = vec![
    ChunkSize::from_kb(64)?,
    ChunkSize::from_mb(1)?,
    ChunkSize::from_mb(16)?,
    ChunkSize::from_mb(64)?,
];

let results = benchmark_chunk_sizes(Path::new("./test.dat"), &amp;sizes).await;
for (size, duration) in results {
    println!("{} MB: {:?}", size.megabytes(), duration);
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="usage-examples-6"><a class="header" href="#usage-examples-6">Usage Examples</a></h2>
<h3 id="example-1-basic-chunking"><a class="header" href="#example-1-basic-chunking">Example 1: Basic Chunking</a></h3>
<pre><pre class="playground"><code class="language-rust">use pipeline_domain::{ChunkSize, FileChunk};
use std::path::Path;

#[tokio::main]
async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    let file_path = Path::new("./input.dat");

    // Determine optimal chunk size
    let file_size = std::fs::metadata(file_path)?.len();
    let chunk_size = ChunkSize::optimal_for_file_size(file_size);

    println!("File size: {} MB", file_size / (1024 * 1024));
    println!("Chunk size: {} MB", chunk_size.megabytes());

    // Chunk the file
    let chunks = chunk_file(file_path, chunk_size)?;
    println!("Created {} chunks", chunks.len());

    Ok(())
}</code></pre></pre>
<h3 id="example-2-memory-adaptive-chunking"><a class="header" href="#example-2-memory-adaptive-chunking">Example 2: Memory-Adaptive Chunking</a></h3>
<pre><pre class="playground"><code class="language-rust">#[tokio::main]
async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    let file_path = Path::new("./large_file.dat");
    let file_size = std::fs::metadata(file_path)?.len();

    // Start with optimal size
    let optimal = ChunkSize::optimal_for_file_size(file_size);

    // Adjust for available memory
    let available_memory = 100 * 1024 * 1024;  // 100 MB
    let max_parallel = 4;

    let chunk_size = optimal.adjust_for_memory(
        available_memory,
        max_parallel,
    )?;

    println!("Optimal: {} MB", optimal.megabytes());
    println!("Adjusted: {} MB", chunk_size.megabytes());

    let chunks = chunk_file(file_path, chunk_size)?;
    println!("Created {} chunks", chunks.len());

    Ok(())
}</code></pre></pre>
<h3 id="example-3-parallel-chunk-processing-1"><a class="header" href="#example-3-parallel-chunk-processing-1">Example 3: Parallel Chunk Processing</a></h3>
<pre><pre class="playground"><code class="language-rust">use futures::future::try_join_all;

#[tokio::main]
async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    let file_path = Path::new("./input.dat");
    let chunk_size = ChunkSize::from_mb(16)?;

    // Create chunks
    let chunks = chunk_file(file_path, chunk_size)?;
    println!("Processing {} chunks in parallel", chunks.len());

    // Process in parallel
    let futures = chunks.into_iter().map(|chunk| {
        tokio::spawn(async move {
            // Simulate processing
            tokio::time::sleep(Duration::from_millis(10)).await;
            process_chunk(chunk).await
        })
    });

    let results = try_join_all(futures).await?;
    let processed: Vec&lt;_&gt; = results.into_iter()
        .collect::&lt;Result&lt;Vec&lt;_&gt;, _&gt;&gt;()?;

    println!("Processed {} chunks", processed.len());

    Ok(())
}

async fn process_chunk(chunk: FileChunk) -&gt; Result&lt;FileChunk, PipelineError&gt; {
    // Transform chunk data
    let transformed_data = chunk.data().to_vec();
    Ok(FileChunk::new(
        chunk.sequence_number(),
        chunk.offset(),
        transformed_data,
        chunk.is_final(),
    )?)
}</code></pre></pre>
<h3 id="example-4-adaptive-chunking"><a class="header" href="#example-4-adaptive-chunking">Example 4: Adaptive Chunking</a></h3>
<pre><pre class="playground"><code class="language-rust">#[tokio::main]
async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    let file_path = Path::new("./test.dat");
    let initial_size = ChunkSize::from_mb(4)?;

    println!("Starting with {} MB chunks", initial_size.megabytes());

    let chunks = adaptive_chunking(file_path, initial_size).await?;

    println!("Created {} chunks with adaptive sizing", chunks.len());

    // Analyze chunk sizes
    for chunk in &amp;chunks[0..5.min(chunks.len())] {
        println!("Chunk {}: {} bytes",
            chunk.sequence_number(),
            chunk.size().bytes()
        );
    }

    Ok(())
}</code></pre></pre>
<hr />
<h2 id="best-practices-8"><a class="header" href="#best-practices-8">Best Practices</a></h2>
<h3 id="1-use-optimal-chunk-sizes"><a class="header" href="#1-use-optimal-chunk-sizes">1. Use Optimal Chunk Sizes</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Use optimal sizing
let file_size = metadata(path)?.len();
let chunk_size = ChunkSize::optimal_for_file_size(file_size);

// ❌ Bad: Fixed size for all files
let chunk_size = ChunkSize::from_mb(1)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="2-consider-memory-constraints"><a class="header" href="#2-consider-memory-constraints">2. Consider Memory Constraints</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Adjust for available memory
let chunk_size = optimal.adjust_for_memory(
    available_memory,
    max_parallel_chunks,
)?;

// ❌ Bad: Ignore memory limits
let chunk_size = ChunkSize::from_mb(128)?;  // May cause OOM
<span class="boring">}</span></code></pre></pre>
<h3 id="3-validate-chunk-sizes"><a class="header" href="#3-validate-chunk-sizes">3. Validate Chunk Sizes</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Validate user input
let user_size_mb = 32;
let file_size = metadata(path)?.len();

match ChunkSize::validate_user_input(user_size_mb, file_size) {
    Ok(bytes) =&gt; {
        let chunk_size = ChunkSize::new(bytes)?;
        // Use validated size
    },
    Err(msg) =&gt; {
        eprintln!("Invalid chunk size: {}", msg);
        // Use optimal instead
        let chunk_size = ChunkSize::optimal_for_file_size(file_size);
    },
}
<span class="boring">}</span></code></pre></pre>
<h3 id="4-monitor-chunk-processing"><a class="header" href="#4-monitor-chunk-processing">4. Monitor Chunk Processing</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Track progress
for (i, chunk) in chunks.iter().enumerate() {
    let start = Instant::now();
    process_chunk(chunk)?;
    let duration = start.elapsed();

    println!("Chunk {}/{}: {:?}",
        i + 1, chunks.len(), duration);
}
<span class="boring">}</span></code></pre></pre>
<h3 id="5-handle-edge-cases"><a class="header" href="#5-handle-edge-cases">5. Handle Edge Cases</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Handle small files
if file_size &lt; chunk_size.bytes() as u64 {
    // File fits in single chunk
    let chunk_size = ChunkSize::new(file_size as usize)?;
}

// ✅ Good: Handle empty files
if file_size == 0 {
    return Ok(Vec::new());  // No chunks needed
}
<span class="boring">}</span></code></pre></pre>
<h3 id="6-use-checksums-for-integrity"><a class="header" href="#6-use-checksums-for-integrity">6. Use Checksums for Integrity</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Add checksums to chunks
let chunk = FileChunk::new(seq, offset, data, is_final)?
    .with_calculated_checksum();

// Verify before processing
if !chunk.verify_checksum() {
    return Err(PipelineError::ChecksumMismatch);
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="troubleshooting-4"><a class="header" href="#troubleshooting-4">Troubleshooting</a></h2>
<h3 id="issue-1-out-of-memory-with-large-chunks"><a class="header" href="#issue-1-out-of-memory-with-large-chunks">Issue 1: Out of Memory with Large Chunks</a></h3>
<p><strong>Symptom:</strong></p>
<pre><code class="language-text">Error: Out of memory allocating chunk
</code></pre>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 1. Reduce chunk size
let smaller = ChunkSize::from_mb(4)?;  // Instead of 128 MB

// 2. Adjust for available memory
let adjusted = chunk_size.adjust_for_memory(
    available_memory,
    max_parallel,
)?;

// 3. Process sequentially instead of parallel
for chunk in chunks {
    process_chunk(chunk).await?;
    // Chunk dropped, memory freed
}
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-2-poor-performance-with-small-chunks"><a class="header" href="#issue-2-poor-performance-with-small-chunks">Issue 2: Poor Performance with Small Chunks</a></h3>
<p><strong>Symptom:</strong> Processing is slower than expected.</p>
<p><strong>Diagnosis:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let start = Instant::now();
let chunks = chunk_file(path, chunk_size)?;
let duration = start.elapsed();

println!("Chunking took: {:?}", duration);
println!("Chunks created: {}", chunks.len());
println!("Avg per chunk: {:?}", duration / chunks.len() as u32);
<span class="boring">}</span></code></pre></pre>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 1. Use optimal chunk size
let chunk_size = ChunkSize::optimal_for_file_size(file_size);

// 2. Increase chunk size
let larger = ChunkSize::from_mb(16)?;  // Instead of 1 MB

// 3. Benchmark different sizes
let results = benchmark_chunk_sizes(path, &amp;sizes).await;
let (best_size, _) = results.first().unwrap();
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-3-too-many-chunks"><a class="header" href="#issue-3-too-many-chunks">Issue 3: Too Many Chunks</a></h3>
<p><strong>Symptom:</strong></p>
<pre><code class="language-text">Created 10,000 chunks for 1 GB file
</code></pre>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 1. Increase chunk size
let chunk_size = ChunkSize::from_mb(16)?;  // ~63 chunks for 1 GB

// 2. Use optimal sizing
let chunk_size = ChunkSize::optimal_for_file_size(file_size);

// 3. Set maximum chunk count
let max_chunks = 100;
let min_chunk_size = file_size / max_chunks as u64;
let chunk_size = ChunkSize::new(min_chunk_size as usize)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-4-chunk-size-larger-than-file"><a class="header" href="#issue-4-chunk-size-larger-than-file">Issue 4: Chunk Size Larger Than File</a></h3>
<p><strong>Symptom:</strong></p>
<pre><code class="language-text">Error: Chunk size 16 MB is larger than file size (1 MB)
</code></pre>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 1. Validate before chunking
let chunk_size = if file_size &lt; chunk_size.bytes() as u64 {
    ChunkSize::new(file_size as usize)?
} else {
    chunk_size
};

// 2. Use validate_user_input
match ChunkSize::validate_user_input(user_size_mb, file_size) {
    Ok(bytes) =&gt; ChunkSize::new(bytes)?,
    Err(msg) =&gt; {
        eprintln!("Warning: {}", msg);
        ChunkSize::optimal_for_file_size(file_size)
    },
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="testing-strategies-4"><a class="header" href="#testing-strategies-4">Testing Strategies</a></h2>
<h3 id="unit-tests-3"><a class="header" href="#unit-tests-3">Unit Tests</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_chunk_size_validation() {
        // Valid sizes
        assert!(ChunkSize::new(1024).is_ok());
        assert!(ChunkSize::new(1024 * 1024).is_ok());

        // Invalid sizes
        assert!(ChunkSize::new(0).is_err());
        assert!(ChunkSize::new(600 * 1024 * 1024).is_err());
    }

    #[test]
    fn test_optimal_sizing() {
        // Small file
        let small = ChunkSize::optimal_for_file_size(500_000);
        assert_eq!(small.bytes(), 64 * 1024);

        // Medium file
        let medium = ChunkSize::optimal_for_file_size(100 * 1024 * 1024);
        assert_eq!(medium.bytes(), 16 * 1024 * 1024);

        // Large file
        let large = ChunkSize::optimal_for_file_size(1_000_000_000);
        assert_eq!(large.bytes(), 64 * 1024 * 1024);
    }

    #[test]
    fn test_chunks_needed() {
        let chunk_size = ChunkSize::from_mb(4).unwrap();
        let file_size = 100 * 1024 * 1024;  // 100 MB

        let num_chunks = chunk_size.chunks_needed_for_file(file_size);
        assert_eq!(num_chunks, 25);  // 100 MB / 4 MB = 25
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="integration-tests-3"><a class="header" href="#integration-tests-3">Integration Tests</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_file_chunking() {
    // Create test file
    let test_file = create_test_file(10 * 1024 * 1024);  // 10 MB

    // Chunk the file
    let chunk_size = ChunkSize::from_mb(1).unwrap();
    let chunks = chunk_file(&amp;test_file, chunk_size).unwrap();

    assert_eq!(chunks.len(), 10);

    // Verify sequences
    for (i, chunk) in chunks.iter().enumerate() {
        assert_eq!(chunk.sequence_number(), i as u64);
    }

    // Verify last chunk flag
    assert!(chunks.last().unwrap().is_final());
}
<span class="boring">}</span></code></pre></pre>
<h3 id="performance-tests"><a class="header" href="#performance-tests">Performance Tests</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_chunking_performance() {
    let test_file = create_test_file(100 * 1024 * 1024);  // 100 MB

    let sizes = vec![
        ChunkSize::from_mb(1).unwrap(),
        ChunkSize::from_mb(16).unwrap(),
        ChunkSize::from_mb(64).unwrap(),
    ];

    for size in sizes {
        let start = Instant::now();
        let chunks = chunk_file(&amp;test_file, size).unwrap();
        let duration = start.elapsed();

        println!("{} MB chunks: {} chunks in {:?}",
            size.megabytes(), chunks.len(), duration);
    }
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="next-steps-18"><a class="header" href="#next-steps-18">Next Steps</a></h2>
<p>After understanding chunking strategy, explore related topics:</p>
<h3 id="related-implementation-topics"><a class="header" href="#related-implementation-topics">Related Implementation Topics</a></h3>
<ol>
<li><strong><a href="implementation/file-io.html">File I/O</a></strong>: File reading and writing with chunks</li>
<li><strong><a href="implementation/binary-format.html">Binary Format</a></strong>: How chunks are serialized</li>
</ol>
<h3 id="related-topics-3"><a class="header" href="#related-topics-3">Related Topics</a></h3>
<ul>
<li><strong><a href="implementation/stages.html">Stage Processing</a></strong>: How stages process chunks</li>
<li><strong><a href="implementation/compression.html">Compression</a></strong>: Compressing chunk data</li>
<li><strong><a href="implementation/encryption.html">Encryption</a></strong>: Encrypting chunks</li>
</ul>
<h3 id="advanced-topics-3"><a class="header" href="#advanced-topics-3">Advanced Topics</a></h3>
<ul>
<li><strong><a href="implementation/../advanced/performance.html">Performance Optimization</a></strong>: Optimizing chunking performance</li>
<li><strong><a href="implementation/../advanced/concurrency.html">Concurrency Model</a></strong>: Parallel chunk processing</li>
</ul>
<hr />
<h2 id="summary-3"><a class="header" href="#summary-3">Summary</a></h2>
<p><strong>Key Takeaways:</strong></p>
<ol>
<li><strong>Chunking</strong> divides files into manageable pieces for efficient processing</li>
<li><strong>Chunk sizes</strong> range from 1 byte to 512 MB with optimal sizes empirically determined</li>
<li><strong>Optimal sizing</strong> adapts to file size: small files use small chunks, large files use large chunks</li>
<li><strong>Memory management</strong> ensures bounded memory usage regardless of file size</li>
<li><strong>Parallel processing</strong> enables concurrent chunk processing for better performance</li>
<li><strong>Adaptive chunking</strong> can dynamically adjust chunk sizes based on performance</li>
<li><strong>Performance</strong> varies significantly with chunk size (64 KB: ~40 MB/s, 128 MB: ~350 MB/s)</li>
</ol>
<p><strong>Architecture File References:</strong></p>
<ul>
<li><strong>ChunkSize:</strong> <code>pipeline-domain/src/value_objects/chunk_size.rs:169</code></li>
<li><strong>FileChunk:</strong> <code>pipeline-domain/src/value_objects/file_chunk.rs:176</code></li>
<li><strong>Chunking Diagram:</strong> <code>pipeline/docs/diagrams/chunk-processing.puml</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="binary-file-format"><a class="header" href="#binary-file-format">Binary File Format</a></h1>
<p><strong>Version:</strong> 1.0
<strong>Date:</strong> 2025-01-04
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner, Claude Code
<strong>Status:</strong> Active</p>
<h2 id="overview-13"><a class="header" href="#overview-13">Overview</a></h2>
<p>The Adaptive Pipeline uses a custom binary file format (<code>.adapipe</code>) to store processed files with complete recovery metadata and integrity verification. This format enables perfect restoration of original files while maintaining processing history and security.</p>
<p><strong>Key Features:</strong></p>
<ul>
<li><strong>Complete Recovery</strong>: All metadata needed to restore original files</li>
<li><strong>Integrity Verification</strong>: SHA-256 checksums for both input and output</li>
<li><strong>Processing History</strong>: Complete record of all processing steps</li>
<li><strong>Format Versioning</strong>: Backward compatibility through version management</li>
<li><strong>Security</strong>: Supports encryption with nonce management</li>
</ul>
<h2 id="file-format-specification"><a class="header" href="#file-format-specification">File Format Specification</a></h2>
<h3 id="binary-layout"><a class="header" href="#binary-layout">Binary Layout</a></h3>
<p>The <code>.adapipe</code> format uses a reverse-header design for efficient processing:</p>
<pre><code class="language-text">┌─────────────────────────────────────────────┐
│          PROCESSED CHUNK DATA               │
│         (variable length)                   │
│  - Compressed and/or encrypted chunks       │
│  - Each chunk: [NONCE][LENGTH][DATA]        │
└─────────────────────────────────────────────┘
┌─────────────────────────────────────────────┐
│          JSON HEADER                        │
│         (variable length)                   │
│  - Processing metadata                      │
│  - Recovery information                     │
│  - Checksums                                │
└─────────────────────────────────────────────┘
┌─────────────────────────────────────────────┐
│      HEADER_LENGTH (4 bytes, u32 LE)        │
│  - Length of JSON header in bytes           │
└─────────────────────────────────────────────┘
┌─────────────────────────────────────────────┐
│    FORMAT_VERSION (2 bytes, u16 LE)         │
│  - Current version: 1                       │
└─────────────────────────────────────────────┘
┌─────────────────────────────────────────────┐
│      MAGIC_BYTES (8 bytes)                  │
│  - "ADAPIPE\0" (0x4144415049504500)         │
└─────────────────────────────────────────────┘
</code></pre>
<p><strong>Why Reverse Header?</strong></p>
<ul>
<li><strong>Efficient Reading</strong>: Read magic bytes and version first</li>
<li><strong>Validation</strong>: Quickly validate format without reading entire file</li>
<li><strong>Streaming</strong>: Process chunk data while reading header</li>
<li><strong>Metadata Location</strong>: Header location calculated from end of file</li>
</ul>
<h3 id="magic-bytes"><a class="header" href="#magic-bytes">Magic Bytes</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub const MAGIC_BYTES: [u8; 8] = [
    0x41, 0x44, 0x41, 0x50, // "ADAP"
    0x49, 0x50, 0x45, 0x00  // "IPE\0"
];
<span class="boring">}</span></code></pre></pre>
<p><strong>Purpose:</strong></p>
<ul>
<li>Identify files in <code>.adapipe</code> format</li>
<li>Prevent accidental processing of wrong file types</li>
<li>Enable format detection tools</li>
</ul>
<h3 id="format-version"><a class="header" href="#format-version">Format Version</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub const CURRENT_FORMAT_VERSION: u16 = 1;
<span class="boring">}</span></code></pre></pre>
<p><strong>Version History:</strong></p>
<ul>
<li><strong>Version 1</strong>: Initial format with compression, encryption, checksum support</li>
</ul>
<p><strong>Future Versions:</strong></p>
<ul>
<li>Version 2: Enhanced metadata, additional algorithms</li>
<li>Version 3: Streaming optimizations, compression improvements</li>
</ul>
<h2 id="file-header-structure"><a class="header" href="#file-header-structure">File Header Structure</a></h2>
<h3 id="header-fields"><a class="header" href="#header-fields">Header Fields</a></h3>
<p>The JSON header contains comprehensive metadata:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct FileHeader {
    /// Application version (e.g., "0.1.0")
    pub app_version: String,

    /// File format version (1)
    pub format_version: u16,

    /// Original input filename
    pub original_filename: String,

    /// Original file size in bytes
    pub original_size: u64,

    /// SHA-256 checksum of original file
    pub original_checksum: String,

    /// SHA-256 checksum of processed file
    pub output_checksum: String,

    /// Processing steps applied (in order)
    pub processing_steps: Vec&lt;ProcessingStep&gt;,

    /// Chunk size used (bytes)
    pub chunk_size: u32,

    /// Number of chunks
    pub chunk_count: u32,

    /// Processing timestamp (RFC3339)
    pub processed_at: DateTime&lt;Utc&gt;,

    /// Pipeline ID
    pub pipeline_id: String,

    /// Additional metadata
    pub metadata: HashMap&lt;String, String&gt;,
}
<span class="boring">}</span></code></pre></pre>
<h3 id="processing-steps"><a class="header" href="#processing-steps">Processing Steps</a></h3>
<p>Each processing step records transformation details:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct ProcessingStep {
    /// Step type (compression, encryption, etc.)
    pub step_type: ProcessingStepType,

    /// Algorithm used (e.g., "brotli", "aes-256-gcm")
    pub algorithm: String,

    /// Algorithm-specific parameters
    pub parameters: HashMap&lt;String, String&gt;,

    /// Application order (0-based)
    pub order: u32,
}

pub enum ProcessingStepType {
    Compression,
    Encryption,
    Checksum,
    PassThrough,
    Custom(String),
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Example Processing Steps:</strong></p>
<pre><code class="language-json">{
  "processing_steps": [
    {
      "step_type": "Compression",
      "algorithm": "brotli",
      "parameters": {
        "level": "6"
      },
      "order": 0
    },
    {
      "step_type": "Encryption",
      "algorithm": "aes-256-gcm",
      "parameters": {
        "key_derivation": "argon2"
      },
      "order": 1
    },
    {
      "step_type": "Checksum",
      "algorithm": "sha256",
      "parameters": {},
      "order": 2
    }
  ]
}
</code></pre>
<h2 id="chunk-format"><a class="header" href="#chunk-format">Chunk Format</a></h2>
<h3 id="chunk-structure"><a class="header" href="#chunk-structure">Chunk Structure</a></h3>
<p>Each chunk in the processed data section follows this format:</p>
<pre><code class="language-text">┌────────────────────────────────────┐
│   NONCE (12 bytes)                 │
│  - Unique for each chunk           │
│  - Used for encryption IV          │
└────────────────────────────────────┘
┌────────────────────────────────────┐
│   DATA_LENGTH (4 bytes, u32 LE)    │
│  - Length of encrypted data        │
└────────────────────────────────────┘
┌────────────────────────────────────┐
│   ENCRYPTED_DATA (variable)        │
│  - Compressed and encrypted        │
│  - Includes authentication tag     │
└────────────────────────────────────┘
</code></pre>
<p><strong>Rust Structure:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct ChunkFormat {
    /// Encryption nonce (12 bytes for AES-GCM)
    pub nonce: [u8; 12],

    /// Length of encrypted data
    pub data_length: u32,

    /// Encrypted (and possibly compressed) chunk data
    pub encrypted_data: Vec&lt;u8&gt;,
}
<span class="boring">}</span></code></pre></pre>
<h3 id="chunk-processing-1"><a class="header" href="#chunk-processing-1">Chunk Processing</a></h3>
<p><strong>Forward Processing (Compress → Encrypt):</strong></p>
<pre><code class="language-text">1. Read original chunk
2. Compress chunk data
3. Generate unique nonce
4. Encrypt compressed data
5. Write: [NONCE][LENGTH][ENCRYPTED_DATA]
</code></pre>
<p><strong>Reverse Processing (Decrypt → Decompress):</strong></p>
<pre><code class="language-text">1. Read: [NONCE][LENGTH][ENCRYPTED_DATA]
2. Decrypt using nonce
3. Decompress decrypted data
4. Verify checksum
5. Write original chunk
</code></pre>
<h2 id="creating-binary-files"><a class="header" href="#creating-binary-files">Creating Binary Files</a></h2>
<h3 id="basic-file-creation"><a class="header" href="#basic-file-creation">Basic File Creation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::value_objects::{FileHeader, ProcessingStep};
use std::fs::File;
use std::io::Write;

fn create_adapipe_file(
    input_data: &amp;[u8],
    output_path: &amp;str,
    processing_steps: Vec&lt;ProcessingStep&gt;,
) -&gt; Result&lt;(), PipelineError&gt; {
    // Create header
    let original_checksum = calculate_sha256(input_data);
    let mut header = FileHeader::new(
        "input.txt".to_string(),
        input_data.len() as u64,
        original_checksum,
    );

    // Add processing steps
    header.processing_steps = processing_steps;
    header.chunk_count = calculate_chunk_count(input_data.len(), header.chunk_size);

    // Process chunks
    let processed_data = process_chunks(input_data, &amp;header.processing_steps)?;

    // Calculate output checksum
    header.output_checksum = calculate_sha256(&amp;processed_data);

    // Serialize header to JSON
    let json_header = serde_json::to_vec(&amp;header)?;
    let header_length = json_header.len() as u32;

    // Write file in reverse order
    let mut file = File::create(output_path)?;

    // 1. Write processed data
    file.write_all(&amp;processed_data)?;

    // 2. Write JSON header
    file.write_all(&amp;json_header)?;

    // 3. Write header length
    file.write_all(&amp;header_length.to_le_bytes())?;

    // 4. Write format version
    file.write_all(&amp;CURRENT_FORMAT_VERSION.to_le_bytes())?;

    // 5. Write magic bytes
    file.write_all(&amp;MAGIC_BYTES)?;

    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h3 id="adding-processing-steps"><a class="header" href="#adding-processing-steps">Adding Processing Steps</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl FileHeader {
    /// Add compression step
    pub fn add_compression_step(mut self, algorithm: &amp;str, level: u32) -&gt; Self {
        let mut parameters = HashMap::new();
        parameters.insert("level".to_string(), level.to_string());

        self.processing_steps.push(ProcessingStep {
            step_type: ProcessingStepType::Compression,
            algorithm: algorithm.to_string(),
            parameters,
            order: self.processing_steps.len() as u32,
        });

        self
    }

    /// Add encryption step
    pub fn add_encryption_step(
        mut self,
        algorithm: &amp;str,
        key_derivation: &amp;str
    ) -&gt; Self {
        let mut parameters = HashMap::new();
        parameters.insert("key_derivation".to_string(), key_derivation.to_string());

        self.processing_steps.push(ProcessingStep {
            step_type: ProcessingStepType::Encryption,
            algorithm: algorithm.to_string(),
            parameters,
            order: self.processing_steps.len() as u32,
        });

        self
    }

    /// Add checksum step
    pub fn add_checksum_step(mut self, algorithm: &amp;str) -&gt; Self {
        self.processing_steps.push(ProcessingStep {
            step_type: ProcessingStepType::Checksum,
            algorithm: algorithm.to_string(),
            parameters: HashMap::new(),
            order: self.processing_steps.len() as u32,
        });

        self
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="reading-binary-files"><a class="header" href="#reading-binary-files">Reading Binary Files</a></h2>
<h3 id="basic-file-reading"><a class="header" href="#basic-file-reading">Basic File Reading</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::fs::File;
use std::io::{Read, Seek, SeekFrom};

fn read_adapipe_file(path: &amp;str) -&gt; Result&lt;FileHeader, PipelineError&gt; {
    let mut file = File::open(path)?;

    // Read from end of file (reverse header)
    file.seek(SeekFrom::End(-8))?;

    // 1. Read and validate magic bytes
    let mut magic = [0u8; 8];
    file.read_exact(&amp;mut magic)?;

    if magic != MAGIC_BYTES {
        return Err(PipelineError::InvalidFormat(
            "Not an .adapipe file".to_string()
        ));
    }

    // 2. Read format version
    file.seek(SeekFrom::End(-10))?;
    let mut version_bytes = [0u8; 2];
    file.read_exact(&amp;mut version_bytes)?;
    let version = u16::from_le_bytes(version_bytes);

    if version &gt; CURRENT_FORMAT_VERSION {
        return Err(PipelineError::UnsupportedVersion(version));
    }

    // 3. Read header length
    file.seek(SeekFrom::End(-14))?;
    let mut length_bytes = [0u8; 4];
    file.read_exact(&amp;mut length_bytes)?;
    let header_length = u32::from_le_bytes(length_bytes) as usize;

    // 4. Read JSON header
    file.seek(SeekFrom::End(-(14 + header_length as i64)))?;
    let mut json_data = vec![0u8; header_length];
    file.read_exact(&amp;mut json_data)?;

    // 5. Deserialize header
    let header: FileHeader = serde_json::from_slice(&amp;json_data)?;

    Ok(header)
}
<span class="boring">}</span></code></pre></pre>
<h3 id="reading-chunk-data"><a class="header" href="#reading-chunk-data">Reading Chunk Data</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn read_chunks(
    file: &amp;mut File,
    header: &amp;FileHeader
) -&gt; Result&lt;Vec&lt;ChunkFormat&gt;, PipelineError&gt; {
    let mut chunks = Vec::with_capacity(header.chunk_count as usize);

    // Seek to start of chunk data
    file.seek(SeekFrom::Start(0))?;

    for _ in 0..header.chunk_count {
        // Read nonce
        let mut nonce = [0u8; 12];
        file.read_exact(&amp;mut nonce)?;

        // Read data length
        let mut length_bytes = [0u8; 4];
        file.read_exact(&amp;mut length_bytes)?;
        let data_length = u32::from_le_bytes(length_bytes);

        // Read encrypted data
        let mut encrypted_data = vec![0u8; data_length as usize];
        file.read_exact(&amp;mut encrypted_data)?;

        chunks.push(ChunkFormat {
            nonce,
            data_length,
            encrypted_data,
        });
    }

    Ok(chunks)
}
<span class="boring">}</span></code></pre></pre>
<h2 id="file-recovery"><a class="header" href="#file-recovery">File Recovery</a></h2>
<h3 id="complete-recovery-process"><a class="header" href="#complete-recovery-process">Complete Recovery Process</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn restore_original_file(
    input_path: &amp;str,
    output_path: &amp;str,
    password: Option&lt;&amp;str&gt;,
) -&gt; Result&lt;(), PipelineError&gt; {
    // 1. Read header
    let header = read_adapipe_file(input_path)?;

    // 2. Read chunks
    let mut file = File::open(input_path)?;
    let chunks = read_chunks(&amp;mut file, &amp;header)?;

    // 3. Process chunks in reverse order
    let mut restored_data = Vec::new();

    for chunk in chunks {
        let mut chunk_data = chunk.encrypted_data;

        // Reverse processing steps
        for step in header.processing_steps.iter().rev() {
            chunk_data = match step.step_type {
                ProcessingStepType::Encryption =&gt; {
                    decrypt_chunk(chunk_data, &amp;chunk.nonce, &amp;step, password)?
                }
                ProcessingStepType::Compression =&gt; {
                    decompress_chunk(chunk_data, &amp;step)?
                }
                ProcessingStepType::Checksum =&gt; {
                    verify_chunk_checksum(&amp;chunk_data, &amp;step)?;
                    chunk_data
                }
                _ =&gt; chunk_data,
            };
        }

        restored_data.extend_from_slice(&amp;chunk_data);
    }

    // 4. Verify restored data
    let restored_checksum = calculate_sha256(&amp;restored_data);
    if restored_checksum != header.original_checksum {
        return Err(PipelineError::IntegrityError(
            "Restored data checksum mismatch".to_string()
        ));
    }

    // 5. Write restored file
    let mut output = File::create(output_path)?;
    output.write_all(&amp;restored_data)?;

    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h3 id="processing-step-reversal"><a class="header" href="#processing-step-reversal">Processing Step Reversal</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn reverse_processing_step(
    data: Vec&lt;u8&gt;,
    step: &amp;ProcessingStep,
    password: Option&lt;&amp;str&gt;,
) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
    match step.step_type {
        ProcessingStepType::Compression =&gt; {
            // Decompress
            match step.algorithm.as_str() {
                "brotli" =&gt; decompress_brotli(data),
                "gzip" =&gt; decompress_gzip(data),
                "zstd" =&gt; decompress_zstd(data),
                "lz4" =&gt; decompress_lz4(data),
                _ =&gt; Err(PipelineError::UnsupportedAlgorithm(
                    step.algorithm.clone()
                )),
            }
        }
        ProcessingStepType::Encryption =&gt; {
            // Decrypt
            let password = password.ok_or(PipelineError::MissingPassword)?;
            match step.algorithm.as_str() {
                "aes-256-gcm" =&gt; decrypt_aes_256_gcm(data, password, step),
                "chacha20-poly1305" =&gt; decrypt_chacha20(data, password, step),
                _ =&gt; Err(PipelineError::UnsupportedAlgorithm(
                    step.algorithm.clone()
                )),
            }
        }
        ProcessingStepType::Checksum =&gt; {
            // Verify checksum (no transformation)
            verify_checksum(&amp;data, step)?;
            Ok(data)
        }
        _ =&gt; Ok(data),
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="integrity-verification-3"><a class="header" href="#integrity-verification-3">Integrity Verification</a></h2>
<h3 id="file-validation"><a class="header" href="#file-validation">File Validation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn validate_adapipe_file(path: &amp;str) -&gt; Result&lt;ValidationReport, PipelineError&gt; {
    let mut report = ValidationReport::new();

    // 1. Read and validate header
    let header = match read_adapipe_file(path) {
        Ok(h) =&gt; {
            report.add_check("Header format", true, "Valid");
            h
        }
        Err(e) =&gt; {
            report.add_check("Header format", false, &amp;e.to_string());
            return Ok(report);
        }
    };

    // 2. Validate format version
    if header.format_version &lt;= CURRENT_FORMAT_VERSION {
        report.add_check("Format version", true, &amp;format!("v{}", header.format_version));
    } else {
        report.add_check(
            "Format version",
            false,
            &amp;format!("Unsupported: v{}", header.format_version)
        );
    }

    // 3. Validate processing steps
    for (i, step) in header.processing_steps.iter().enumerate() {
        let is_supported = match step.step_type {
            ProcessingStepType::Compression =&gt; {
                matches!(step.algorithm.as_str(), "brotli" | "gzip" | "zstd" | "lz4")
            }
            ProcessingStepType::Encryption =&gt; {
                matches!(step.algorithm.as_str(), "aes-256-gcm" | "chacha20-poly1305")
            }
            _ =&gt; true,
        };

        report.add_check(
            &amp;format!("Step {} ({:?})", i, step.step_type),
            is_supported,
            &amp;step.algorithm
        );
    }

    // 4. Verify output checksum
    let mut file = File::open(path)?;
    let data_length = file.metadata()?.len() - 14 - header.json_size() as u64;
    let mut processed_data = vec![0u8; data_length as usize];
    file.read_exact(&amp;mut processed_data)?;

    let calculated_checksum = calculate_sha256(&amp;processed_data);
    let checksums_match = calculated_checksum == header.output_checksum;

    report.add_check(
        "Output checksum",
        checksums_match,
        if checksums_match { "Valid" } else { "Mismatch" }
    );

    Ok(report)
}

pub struct ValidationReport {
    checks: Vec&lt;(String, bool, String)&gt;,
}

impl ValidationReport {
    pub fn new() -&gt; Self {
        Self { checks: Vec::new() }
    }

    pub fn add_check(&amp;mut self, name: &amp;str, passed: bool, message: &amp;str) {
        self.checks.push((name.to_string(), passed, message.to_string()));
    }

    pub fn is_valid(&amp;self) -&gt; bool {
        self.checks.iter().all(|(_, passed, _)| *passed)
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="checksum-verification"><a class="header" href="#checksum-verification">Checksum Verification</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn verify_file_integrity(path: &amp;str) -&gt; Result&lt;bool, PipelineError&gt; {
    let header = read_adapipe_file(path)?;

    // Calculate actual checksum
    let mut file = File::open(path)?;
    let data_length = file.metadata()?.len() - 14 - header.json_size() as u64;
    let mut data = vec![0u8; data_length as usize];
    file.read_exact(&amp;mut data)?;

    let calculated = calculate_sha256(&amp;data);

    // Compare with stored checksum
    Ok(calculated == header.output_checksum)
}

fn calculate_sha256(data: &amp;[u8]) -&gt; String {
    let mut hasher = Sha256::new();
    hasher.update(data);
    format!("{:x}", hasher.finalize())
}
<span class="boring">}</span></code></pre></pre>
<h2 id="version-management"><a class="header" href="#version-management">Version Management</a></h2>
<h3 id="format-versioning"><a class="header" href="#format-versioning">Format Versioning</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn check_format_compatibility(version: u16) -&gt; Result&lt;(), PipelineError&gt; {
    match version {
        1 =&gt; Ok(()), // Current version
        v if v &lt; CURRENT_FORMAT_VERSION =&gt; {
            // Older version - attempt migration
            migrate_format(v, CURRENT_FORMAT_VERSION)
        }
        v =&gt; Err(PipelineError::UnsupportedVersion(v)),
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="format-migration"><a class="header" href="#format-migration">Format Migration</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn migrate_format(from: u16, to: u16) -&gt; Result&lt;(), PipelineError&gt; {
    match (from, to) {
        (1, 2) =&gt; {
            // Migration from v1 to v2
            // Add new fields with defaults
            Ok(())
        }
        _ =&gt; Err(PipelineError::MigrationUnsupported(from, to)),
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="backward-compatibility"><a class="header" href="#backward-compatibility">Backward Compatibility</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn read_any_version(path: &amp;str) -&gt; Result&lt;FileHeader, PipelineError&gt; {
    let version = read_format_version(path)?;

    match version {
        1 =&gt; read_v1_format(path),
        2 =&gt; read_v2_format(path),
        v =&gt; Err(PipelineError::UnsupportedVersion(v)),
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="best-practices-9"><a class="header" href="#best-practices-9">Best Practices</a></h2>
<h3 id="file-creation"><a class="header" href="#file-creation">File Creation</a></h3>
<p><strong>Always set checksums:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Set both checksums
let original_checksum = calculate_sha256(&amp;input_data);
let header = FileHeader::new(filename, size, original_checksum);
// ... process data ...
header.output_checksum = calculate_sha256(&amp;processed_data);
<span class="boring">}</span></code></pre></pre>
<p><strong>Record all processing steps:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Record every transformation
header = header
    .add_compression_step("brotli", 6)
    .add_encryption_step("aes-256-gcm", "argon2")
    .add_checksum_step("sha256");
<span class="boring">}</span></code></pre></pre>
<h3 id="file-reading-1"><a class="header" href="#file-reading-1">File Reading</a></h3>
<p><strong>Always validate format:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Validate before processing
let header = read_adapipe_file(path)?;

if header.format_version &gt; CURRENT_FORMAT_VERSION {
    return Err(PipelineError::UnsupportedVersion(
        header.format_version
    ));
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Verify checksums:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Verify integrity
let restored_checksum = calculate_sha256(&amp;restored_data);
if restored_checksum != header.original_checksum {
    return Err(PipelineError::IntegrityError(
        "Checksum mismatch".to_string()
    ));
}
<span class="boring">}</span></code></pre></pre>
<h3 id="error-handling-6"><a class="header" href="#error-handling-6">Error Handling</a></h3>
<p><strong>Handle all error cases:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>match read_adapipe_file(path) {
    Ok(header) =&gt; process_file(header),
    Err(PipelineError::InvalidFormat(msg)) =&gt; {
        eprintln!("Not a valid .adapipe file: {}", msg);
    }
    Err(PipelineError::UnsupportedVersion(v)) =&gt; {
        eprintln!("Unsupported format version: {}", v);
    }
    Err(e) =&gt; {
        eprintln!("Error reading file: {}", e);
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="security-considerations-2"><a class="header" href="#security-considerations-2">Security Considerations</a></h2>
<h3 id="nonce-management-2"><a class="header" href="#nonce-management-2">Nonce Management</a></h3>
<p><strong>Never reuse nonces:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Generate unique nonce per chunk
fn generate_nonce() -&gt; [u8; 12] {
    let mut nonce = [0u8; 12];
    use rand::RngCore;
    rand::thread_rng().fill_bytes(&amp;mut nonce);
    nonce
}
<span class="boring">}</span></code></pre></pre>
<h3 id="key-derivation-1"><a class="header" href="#key-derivation-1">Key Derivation</a></h3>
<p><strong>Use strong key derivation:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Argon2 for password-based encryption
fn derive_key(password: &amp;str, salt: &amp;[u8]) -&gt; Vec&lt;u8&gt; {
    use argon2::{Argon2, PasswordHasher};

    let argon2 = Argon2::default();
    let hash = argon2.hash_password(password.as_bytes(), salt)
        .unwrap();

    hash.hash.unwrap().as_bytes().to_vec()
}
<span class="boring">}</span></code></pre></pre>
<h3 id="integrity-protection"><a class="header" href="#integrity-protection">Integrity Protection</a></h3>
<p><strong>Verify at every step:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Verify after each transformation
fn process_with_verification(
    data: Vec&lt;u8&gt;,
    step: &amp;ProcessingStep
) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
    let processed = apply_transformation(data, step)?;
    verify_transformation(&amp;processed, step)?;
    Ok(processed)
}
<span class="boring">}</span></code></pre></pre>
<h2 id="next-steps-19"><a class="header" href="#next-steps-19">Next Steps</a></h2>
<p>Now that you understand the binary file format:</p>
<ul>
<li><a href="implementation/chunking.html">Chunking Strategy</a> - Efficient chunk processing</li>
<li><a href="implementation/file-io.html">File I/O</a> - File reading and writing patterns</li>
<li><a href="implementation/integrity.html">Integrity Verification</a> - Checksum algorithms</li>
<li><a href="implementation/encryption.html">Encryption</a> - Encryption implementation details</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="observability-overview"><a class="header" href="#observability-overview">Observability Overview</a></h1>
<p><strong>Version</strong>: 1.0
<strong>Date</strong>: 2025-10-04
<strong>License</strong>: BSD-3-Clause
<strong>Copyright</strong>: (c) 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors</strong>: Michael Gardner
<strong>Status</strong>: Active</p>
<hr />
<h2 id="overview-14"><a class="header" href="#overview-14">Overview</a></h2>
<p>Observability is the ability to understand the internal state of a system by examining its external outputs. The Optimized Adaptive Pipeline implements a comprehensive observability strategy that combines <strong>metrics</strong>, <strong>logging</strong>, and <strong>health monitoring</strong> to provide complete system visibility.</p>
<h3 id="key-principles-1"><a class="header" href="#key-principles-1">Key Principles</a></h3>
<ul>
<li><strong>Three Pillars</strong>: Metrics, Logs, and Traces (health monitoring)</li>
<li><strong>Comprehensive Coverage</strong>: Monitor all aspects of system operation</li>
<li><strong>Real-Time Insights</strong>: Live performance tracking and alerting</li>
<li><strong>Low Overhead</strong>: Minimal performance impact on pipeline processing</li>
<li><strong>Integration Ready</strong>: Compatible with external monitoring systems (Prometheus, Grafana)</li>
<li><strong>Actionable</strong>: Designed to support debugging, optimization, and operations</li>
</ul>
<hr />
<h2 id="the-three-pillars"><a class="header" href="#the-three-pillars">The Three Pillars</a></h2>
<h3 id="1-metrics---quantitative-measurements"><a class="header" href="#1-metrics---quantitative-measurements">1. Metrics - Quantitative Measurements</a></h3>
<p><strong>What</strong>: Numerical measurements aggregated over time</p>
<p><strong>Purpose</strong>: Track system performance, identify trends, detect anomalies</p>
<p><strong>Implementation</strong>:</p>
<ul>
<li>Domain layer: <code>ProcessingMetrics</code> entity</li>
<li>Infrastructure layer: <code>MetricsService</code> with Prometheus integration</li>
<li>HTTP <code>/metrics</code> endpoint for scraping</li>
</ul>
<p><strong>Key Metrics</strong>:</p>
<ul>
<li><strong>Counters</strong>: Total pipelines processed, bytes processed, errors</li>
<li><strong>Gauges</strong>: Active pipelines, current throughput, memory usage</li>
<li><strong>Histograms</strong>: Processing duration, latency distribution</li>
</ul>
<p><strong>See</strong>: <a href="implementation/metrics.html">Metrics Collection</a></p>
<h3 id="2-logging---contextual-events"><a class="header" href="#2-logging---contextual-events">2. Logging - Contextual Events</a></h3>
<p><strong>What</strong>: Timestamped records of discrete events with structured context</p>
<p><strong>Purpose</strong>: Understand what happened, when, and why</p>
<p><strong>Implementation</strong>:</p>
<ul>
<li>Bootstrap phase: <code>BootstrapLogger</code> trait</li>
<li>Application phase: <code>tracing</code> crate with structured logging</li>
<li>Multiple log levels: ERROR, WARN, INFO, DEBUG, TRACE</li>
</ul>
<p><strong>Key Features</strong>:</p>
<ul>
<li>Structured fields for filtering and analysis</li>
<li>Correlation IDs for request tracing</li>
<li>Integration with ObservabilityService for alerts</li>
</ul>
<p><strong>See</strong>: <a href="implementation/logging.html">Logging Implementation</a></p>
<h3 id="3-health-monitoring---system-status"><a class="header" href="#3-health-monitoring---system-status">3. Health Monitoring - System Status</a></h3>
<p><strong>What</strong>: Aggregated health scores and status indicators</p>
<p><strong>Purpose</strong>: Quickly assess system health and detect degradation</p>
<p><strong>Implementation</strong>:</p>
<ul>
<li><code>ObservabilityService</code> with real-time health scoring</li>
<li><code>SystemHealth</code> status reporting</li>
<li>Alert generation for threshold violations</li>
</ul>
<p><strong>Key Components</strong>:</p>
<ul>
<li>Performance health (throughput, latency)</li>
<li>Error health (error rates, failure patterns)</li>
<li>Resource health (CPU, memory, I/O)</li>
<li>Overall health score (weighted composite)</li>
</ul>
<hr />
<h2 id="architecture-5"><a class="header" href="#architecture-5">Architecture</a></h2>
<h3 id="layered-observability"><a class="header" href="#layered-observability">Layered Observability</a></h3>
<pre><code class="language-text">┌─────────────────────────────────────────────────────────────┐
│                    Application Layer                         │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  ┌─────────────────────────────────────────────────────┐   │
│  │           ObservabilityService                      │   │
│  │  (Orchestrates monitoring, alerting, health)        │   │
│  └──────────┬────────────────┬──────────────┬──────────┘   │
│             │                │              │               │
│             ▼                ▼              ▼               │
│  ┌──────────────┐  ┌─────────────┐  ┌─────────────┐       │
│  │ Performance  │  │   Alert     │  │   Health    │       │
│  │   Tracker    │  │  Manager    │  │  Monitor    │       │
│  └──────────────┘  └─────────────┘  └─────────────┘       │
│                                                              │
└─────────────────────────────────────────────────────────────┘
                           │
                           │ Uses
                           ▼
┌─────────────────────────────────────────────────────────────┐
│                  Infrastructure Layer                        │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  ┌──────────────────┐              ┌──────────────────┐    │
│  │ MetricsService   │              │ Logging (tracing)│    │
│  │ (Prometheus)     │              │ (Structured logs)│    │
│  └──────────────────┘              └──────────────────┘    │
│           │                                 │               │
│           │                                 │               │
│           ▼                                 ▼               │
│  ┌──────────────────┐              ┌──────────────────┐    │
│  │ /metrics HTTP    │              │ Log Subscribers  │    │
│  │ endpoint         │              │ (console, file)  │    │
│  └──────────────────┘              └──────────────────┘    │
│                                                              │
└─────────────────────────────────────────────────────────────┘
                           │
                           │ Exposes
                           ▼
┌─────────────────────────────────────────────────────────────┐
│                    External Systems                          │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐ │
│  │  Prometheus  │    │    Grafana   │    │ Log Analysis │ │
│  │   (Scraper)  │    │ (Dashboards) │    │    Tools     │ │
│  └──────────────┘    └──────────────┘    └──────────────┘ │
│                                                              │
└─────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="component-integration"><a class="header" href="#component-integration">Component Integration</a></h3>
<p>The observability components are tightly integrated:</p>
<ol>
<li><strong>ObservabilityService</strong> orchestrates monitoring</li>
<li><strong>MetricsService</strong> records quantitative data</li>
<li><strong>Logging</strong> records contextual events</li>
<li><strong>PerformanceTracker</strong> maintains real-time state</li>
<li><strong>AlertManager</strong> checks thresholds and generates alerts</li>
<li><strong>HealthMonitor</strong> computes system health scores</li>
</ol>
<hr />
<h2 id="observabilityservice"><a class="header" href="#observabilityservice">ObservabilityService</a></h2>
<h3 id="core-responsibilities"><a class="header" href="#core-responsibilities">Core Responsibilities</a></h3>
<p>The <code>ObservabilityService</code> is the central orchestrator for monitoring:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct ObservabilityService {
    metrics_service: Arc&lt;MetricsService&gt;,
    performance_tracker: Arc&lt;RwLock&lt;PerformanceTracker&gt;&gt;,
    alert_thresholds: AlertThresholds,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Key Methods</strong>:</p>
<ul>
<li><code>start_operation()</code> - Begin tracking an operation</li>
<li><code>complete_operation()</code> - End tracking with metrics</li>
<li><code>get_system_health()</code> - Get current health status</li>
<li><code>record_processing_metrics()</code> - Record pipeline metrics</li>
<li><code>check_alerts()</code> - Evaluate alert conditions</li>
</ul>
<h3 id="performancetracker"><a class="header" href="#performancetracker">PerformanceTracker</a></h3>
<p>Maintains real-time performance state:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct PerformanceTracker {
    pub active_operations: u32,
    pub total_operations: u64,
    pub average_throughput_mbps: f64,
    pub peak_throughput_mbps: f64,
    pub error_rate_percent: f64,
    pub system_health_score: f64,
    pub last_update: Instant,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Tracked Metrics</strong>:</p>
<ul>
<li>Active operation count</li>
<li>Total operation count</li>
<li>Average and peak throughput</li>
<li>Error rate percentage</li>
<li>Overall health score</li>
<li>Last update timestamp</li>
</ul>
<h3 id="operationtracker"><a class="header" href="#operationtracker">OperationTracker</a></h3>
<p>Automatic operation lifecycle tracking:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct OperationTracker {
    operation_name: String,
    start_time: Instant,
    observability_service: ObservabilityService,
    completed: AtomicBool,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Lifecycle</strong>:</p>
<ol>
<li>Created via <code>start_operation()</code></li>
<li>Increments active operation count</li>
<li>Logs operation start</li>
<li>On completion: Records duration, throughput, success/failure</li>
<li>On drop (if not completed): Marks as failed</li>
</ol>
<p><strong>Drop Safety</strong>: If the tracker is dropped without explicit completion (e.g., due to panic), it automatically marks the operation as failed.</p>
<hr />
<h2 id="health-monitoring"><a class="header" href="#health-monitoring">Health Monitoring</a></h2>
<h3 id="systemhealth-structure"><a class="header" href="#systemhealth-structure">SystemHealth Structure</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct SystemHealth {
    pub status: HealthStatus,
    pub score: f64,
    pub active_operations: u32,
    pub throughput_mbps: f64,
    pub error_rate_percent: f64,
    pub uptime_seconds: u64,
    pub alerts: Vec&lt;Alert&gt;,
}

pub enum HealthStatus {
    Healthy,   // Score &gt;= 90.0
    Warning,   // Score &gt;= 70.0 &amp;&amp; &lt; 90.0
    Critical,  // Score &lt; 70.0
    Unknown,   // Unable to determine health
}
<span class="boring">}</span></code></pre></pre>
<h3 id="health-score-calculation"><a class="header" href="#health-score-calculation">Health Score Calculation</a></h3>
<p>The health score starts at 100 and deductions are applied:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut score = 100.0;

// Deduct for high error rate
if error_rate_percent &gt; max_error_rate_percent {
    score -= 30.0;  // Error rate is critical
}

// Deduct for low throughput
if average_throughput_mbps &lt; min_throughput_mbps {
    score -= 20.0;  // Performance degradation
}

// Additional deductions for other factors...
<span class="boring">}</span></code></pre></pre>
<p><strong>Health Score Ranges</strong>:</p>
<ul>
<li><strong>100-90</strong>: Healthy - System operating normally</li>
<li><strong>89-70</strong>: Warning - Degraded performance, investigation needed</li>
<li><strong>69-0</strong>: Critical - System in distress, immediate action required</li>
</ul>
<h3 id="alert-structure"><a class="header" href="#alert-structure">Alert Structure</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct Alert {
    pub severity: AlertSeverity,
    pub message: String,
    pub timestamp: String,
    pub metric_name: String,
    pub current_value: f64,
    pub threshold: f64,
}

pub enum AlertSeverity {
    Info,
    Warning,
    Critical,
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="alert-thresholds"><a class="header" href="#alert-thresholds">Alert Thresholds</a></h2>
<h3 id="configuration-3"><a class="header" href="#configuration-3">Configuration</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct AlertThresholds {
    pub max_error_rate_percent: f64,
    pub min_throughput_mbps: f64,
    pub max_processing_duration_seconds: f64,
    pub max_memory_usage_mb: f64,
}

impl Default for AlertThresholds {
    fn default() -&gt; Self {
        Self {
            max_error_rate_percent: 5.0,
            min_throughput_mbps: 1.0,
            max_processing_duration_seconds: 300.0,
            max_memory_usage_mb: 1024.0,
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="alert-generation"><a class="header" href="#alert-generation">Alert Generation</a></h3>
<p>Alerts are generated when thresholds are violated:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn check_alerts(&amp;self, tracker: &amp;PerformanceTracker) {
    // High error rate alert
    if tracker.error_rate_percent &gt; self.alert_thresholds.max_error_rate_percent {
        warn!(
            "🚨 Alert: High error rate {:.1}% (threshold: {:.1}%)",
            tracker.error_rate_percent,
            self.alert_thresholds.max_error_rate_percent
        );
    }

    // Low throughput alert
    if tracker.average_throughput_mbps &lt; self.alert_thresholds.min_throughput_mbps {
        warn!(
            "🚨 Alert: Low throughput {:.2} MB/s (threshold: {:.2} MB/s)",
            tracker.average_throughput_mbps,
            self.alert_thresholds.min_throughput_mbps
        );
    }

    // High concurrent operations alert
    if tracker.active_operations &gt; 10 {
        warn!("🚨 Alert: High concurrent operations: {}", tracker.active_operations);
    }
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="usage-patterns"><a class="header" href="#usage-patterns">Usage Patterns</a></h2>
<h3 id="basic-operation-tracking"><a class="header" href="#basic-operation-tracking">Basic Operation Tracking</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Start operation tracking
let tracker = observability_service
    .start_operation("file_processing")
    .await;

// Do work
let result = process_file(&amp;input_path).await?;

// Complete tracking with success/failure
tracker.complete(true, result.bytes_processed).await;
<span class="boring">}</span></code></pre></pre>
<h3 id="automatic-tracking-with-drop-safety"><a class="header" href="#automatic-tracking-with-drop-safety">Automatic Tracking with Drop Safety</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn process_pipeline(id: &amp;PipelineId) -&gt; Result&lt;()&gt; {
    // Tracker automatically handles failure if function panics or returns Err
    let tracker = observability_service
        .start_operation("pipeline_execution")
        .await;

    // If this fails, tracker is dropped and marks operation as failed
    let result = execute_stages(id).await?;

    // Explicit success
    tracker.complete(true, result.bytes_processed).await;
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h3 id="recording-pipeline-metrics"><a class="header" href="#recording-pipeline-metrics">Recording Pipeline Metrics</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// After pipeline completion
let metrics = pipeline.processing_metrics();

// Record to both Prometheus and performance tracker
observability_service
    .record_processing_metrics(&amp;metrics)
    .await;

// This automatically:
// - Updates Prometheus counters/gauges/histograms
// - Updates PerformanceTracker state
// - Checks alert thresholds
// - Logs completion with metrics
<span class="boring">}</span></code></pre></pre>
<h3 id="health-check-endpoint"><a class="header" href="#health-check-endpoint">Health Check Endpoint</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn health_check() -&gt; Result&lt;SystemHealth&gt; {
    let health = observability_service.get_system_health().await;

    match health.status {
        HealthStatus::Healthy =&gt; {
            info!("System health: HEALTHY (score: {:.1})", health.score);
        }
        HealthStatus::Warning =&gt; {
            warn!(
                "System health: WARNING (score: {:.1}, {} alerts)",
                health.score,
                health.alerts.len()
            );
        }
        HealthStatus::Critical =&gt; {
            error!(
                "System health: CRITICAL (score: {:.1}, {} alerts)",
                health.score,
                health.alerts.len()
            );
        }
        HealthStatus::Unknown =&gt; {
            warn!("System health: UNKNOWN");
        }
    }

    Ok(health)
}
<span class="boring">}</span></code></pre></pre>
<h3 id="performance-summary"><a class="header" href="#performance-summary">Performance Summary</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Get human-readable performance summary
let summary = observability_service
    .get_performance_summary()
    .await;

println!("{}", summary);
<span class="boring">}</span></code></pre></pre>
<p><strong>Output</strong>:</p>
<pre><code class="language-text">📊 Performance Summary:
Active Operations: 3
Total Operations: 1247
Average Throughput: 45.67 MB/s
Peak Throughput: 89.23 MB/s
Error Rate: 2.1%
System Health: 88.5/100 (Warning)
Alerts: 1
</code></pre>
<hr />
<h2 id="integration-with-external-systems"><a class="header" href="#integration-with-external-systems">Integration with External Systems</a></h2>
<h3 id="prometheus-integration"><a class="header" href="#prometheus-integration">Prometheus Integration</a></h3>
<p>The system exposes metrics via HTTP endpoint:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// HTTP /metrics endpoint
use axum::{routing::get, Router};

let app = Router::new()
    .route("/metrics", get(metrics_handler));

async fn metrics_handler() -&gt; String {
    metrics_service.get_metrics()
        .unwrap_or_else(|_| "# Error generating metrics\n".to_string())
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Prometheus Configuration</strong>:</p>
<pre><code class="language-yaml">scrape_configs:
  - job_name: 'pipeline'
    static_configs:
      - targets: ['localhost:9090']
    scrape_interval: 15s
    scrape_timeout: 10s
</code></pre>
<h3 id="grafana-dashboards"><a class="header" href="#grafana-dashboards">Grafana Dashboards</a></h3>
<p>Create dashboards to visualize:</p>
<ul>
<li><strong>Pipeline Throughput</strong>: Line graph of MB/s over time</li>
<li><strong>Active Operations</strong>: Gauge of current active count</li>
<li><strong>Error Rate</strong>: Line graph of error percentage</li>
<li><strong>Processing Duration</strong>: Histogram of completion times</li>
<li><strong>System Health</strong>: Gauge with color thresholds</li>
</ul>
<p><strong>Example PromQL Queries</strong>:</p>
<pre><code class="language-promql"># Average throughput over 5 minutes
rate(pipeline_bytes_processed_total[5m]) / 1024 / 1024

# Error rate percentage
100 * (
  rate(pipeline_errors_total[5m]) /
  rate(pipeline_processed_total[5m])
)

# P99 processing duration
histogram_quantile(0.99, pipeline_processing_duration_seconds_bucket)
</code></pre>
<h3 id="log-aggregation"><a class="header" href="#log-aggregation">Log Aggregation</a></h3>
<p>Send logs to external systems:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tracing_subscriber::{fmt, layer::SubscriberExt, EnvFilter, Registry};
use tracing_appender::{non_blocking, rolling};

// JSON logs for shipping to ELK/Splunk
let file_appender = rolling::daily("./logs", "pipeline.json");
let (non_blocking_appender, _guard) = non_blocking(file_appender);

let file_layer = fmt::layer()
    .with_writer(non_blocking_appender)
    .json()
    .with_target(true)
    .with_thread_ids(true);

let subscriber = Registry::default()
    .with(EnvFilter::new("info"))
    .with(file_layer);

tracing::subscriber::set_global_default(subscriber)?;
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="performance-considerations-1"><a class="header" href="#performance-considerations-1">Performance Considerations</a></h2>
<h3 id="low-overhead-design"><a class="header" href="#low-overhead-design">Low Overhead Design</a></h3>
<p><strong>Atomic Operations</strong>: Metrics use atomic types to avoid locks:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct MetricsService {
    pipelines_processed: Arc&lt;AtomicU64&gt;,
    bytes_processed: Arc&lt;AtomicU64&gt;,
    // ...
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Async RwLock</strong>: PerformanceTracker uses async RwLock for concurrent reads:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>performance_tracker: Arc&lt;RwLock&lt;PerformanceTracker&gt;&gt;
<span class="boring">}</span></code></pre></pre>
<p><strong>Lazy Evaluation</strong>: Expensive calculations only performed when health is queried</p>
<p><strong>Compile-Time Filtering</strong>: Debug/trace logs have zero overhead in release builds</p>
<h3 id="benchmark-results"><a class="header" href="#benchmark-results">Benchmark Results</a></h3>
<p>Observability overhead on Intel i7-10700K @ 3.8 GHz:</p>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Time</th><th>Overhead</th></tr></thead><tbody>
<tr><td><code>start_operation()</code></td><td>~500 ns</td><td>Negligible</td></tr>
<tr><td><code>complete_operation()</code></td><td>~1.2 μs</td><td>Minimal</td></tr>
<tr><td><code>record_processing_metrics()</code></td><td>~2.5 μs</td><td>Low</td></tr>
<tr><td><code>get_system_health()</code></td><td>~8 μs</td><td>Moderate (infrequent)</td></tr>
<tr><td><code>info!()</code> log</td><td>~80 ns</td><td>Negligible</td></tr>
<tr><td><code>debug!()</code> log (disabled)</td><td>~0 ns</td><td>Zero</td></tr>
</tbody></table>
</div>
<p><strong>Total overhead</strong>: &lt; 0.1% of pipeline processing time</p>
<hr />
<h2 id="best-practices-10"><a class="header" href="#best-practices-10">Best Practices</a></h2>
<h3 id="-do-1"><a class="header" href="#-do-1">✅ DO</a></h3>
<p><strong>Track all significant operations</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let tracker = observability.start_operation("file_compression").await;
let result = compress_file(&amp;path).await?;
tracker.complete(true, result.compressed_size).await;
<span class="boring">}</span></code></pre></pre>
<p><strong>Use structured logging</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>info!(
    pipeline_id = %id,
    bytes = total_bytes,
    duration_ms = elapsed.as_millis(),
    "Pipeline completed"
);
<span class="boring">}</span></code></pre></pre>
<p><strong>Record domain metrics</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>observability.record_processing_metrics(&amp;pipeline.metrics()).await;
<span class="boring">}</span></code></pre></pre>
<p><strong>Check health regularly</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// In health check endpoint
let health = observability.get_system_health().await;
<span class="boring">}</span></code></pre></pre>
<p><strong>Configure thresholds appropriately</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let observability = ObservabilityService::new_with_config(metrics_service).await;
<span class="boring">}</span></code></pre></pre>
<h3 id="-dont-1"><a class="header" href="#-dont-1">❌ DON'T</a></h3>
<p><strong>Don't track trivial operations</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// BAD: Too fine-grained
let tracker = observability.start_operation("allocate_vec").await;
let vec = Vec::with_capacity(100);
tracker.complete(true, 0).await; // Overhead &gt; value
<span class="boring">}</span></code></pre></pre>
<p><strong>Don't log in hot loops without rate limiting</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// BAD: Excessive logging
for chunk in chunks {
    debug!("Processing chunk {}", chunk.id); // Called millions of times!
}

// GOOD: Log summary
debug!(chunk_count = chunks.len(), "Processing chunks");
info!(chunks_processed = chunks.len(), "Chunk processing complete");
<span class="boring">}</span></code></pre></pre>
<p><strong>Don't forget to complete trackers</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// BAD: Leaks active operation count
let tracker = observability.start_operation("process").await;
process().await?;
// Forgot to call tracker.complete()!

// GOOD: Explicit completion
let tracker = observability.start_operation("process").await;
let result = process().await?;
tracker.complete(true, result.bytes).await;
<span class="boring">}</span></code></pre></pre>
<p><strong>Don't block on observability operations</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// BAD: Blocking in async context
tokio::task::block_in_place(|| {
    observability.get_system_health().await // Won't compile anyway!
});

// GOOD: Await directly
let health = observability.get_system_health().await;
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="testing-strategies-5"><a class="header" href="#testing-strategies-5">Testing Strategies</a></h2>
<h3 id="unit-testing-observabilityservice"><a class="header" href="#unit-testing-observabilityservice">Unit Testing ObservabilityService</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_operation_tracking() {
    let metrics_service = Arc::new(MetricsService::new().unwrap());
    let observability = ObservabilityService::new(metrics_service);

    // Start operation
    let tracker = observability.start_operation("test").await;

    // Check active count increased
    let health = observability.get_system_health().await;
    assert_eq!(health.active_operations, 1);

    // Complete operation
    tracker.complete(true, 1000).await;

    // Check active count decreased
    let health = observability.get_system_health().await;
    assert_eq!(health.active_operations, 0);
}
<span class="boring">}</span></code></pre></pre>
<h3 id="testing-alert-generation"><a class="header" href="#testing-alert-generation">Testing Alert Generation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_high_error_rate_alert() {
    let metrics_service = Arc::new(MetricsService::new().unwrap());
    let mut observability = ObservabilityService::new(metrics_service);

    // Set low threshold
    observability.alert_thresholds.max_error_rate_percent = 1.0;

    // Simulate high error rate
    for _ in 0..10 {
        let tracker = observability.start_operation("test").await;
        tracker.complete(false, 0).await; // All failures
    }

    // Check health has alerts
    let health = observability.get_system_health().await;
    assert!(!health.alerts.is_empty());
    assert_eq!(health.status, HealthStatus::Critical);
}
<span class="boring">}</span></code></pre></pre>
<h3 id="integration-testing"><a class="header" href="#integration-testing">Integration Testing</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_end_to_end_observability() {
    // Setup
    let metrics_service = Arc::new(MetricsService::new().unwrap());
    let observability = Arc::new(ObservabilityService::new(metrics_service.clone()));

    // Run pipeline with tracking
    let tracker = observability.start_operation("pipeline").await;
    let result = run_test_pipeline().await.unwrap();
    tracker.complete(true, result.bytes_processed).await;

    // Verify metrics recorded
    let metrics_output = metrics_service.get_metrics().unwrap();
    assert!(metrics_output.contains("pipeline_processed_total"));

    // Verify health is good
    let health = observability.get_system_health().await;
    assert_eq!(health.status, HealthStatus::Healthy);
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="common-issues-and-solutions"><a class="header" href="#common-issues-and-solutions">Common Issues and Solutions</a></h2>
<h3 id="issue-active-operations-count-stuck"><a class="header" href="#issue-active-operations-count-stuck">Issue: Active operations count stuck</a></h3>
<p><strong>Symptom</strong>: <code>active_operations</code> never decreases</p>
<p><strong>Cause</strong>: <code>OperationTracker</code> not completed or dropped</p>
<p><strong>Solution</strong>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Ensure tracker is completed in all code paths
let tracker = observability.start_operation("op").await;
let result = match dangerous_operation().await {
    Ok(r) =&gt; {
        tracker.complete(true, r.bytes).await;
        Ok(r)
    }
    Err(e) =&gt; {
        tracker.complete(false, 0).await;
        Err(e)
    }
};
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-health-score-always-100"><a class="header" href="#issue-health-score-always-100">Issue: Health score always 100</a></h3>
<p><strong>Symptom</strong>: Health never degrades despite errors</p>
<p><strong>Cause</strong>: Metrics not being recorded</p>
<p><strong>Solution</strong>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Always record processing metrics
observability.record_processing_metrics(&amp;metrics).await;
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-alerts-not-firing"><a class="header" href="#issue-alerts-not-firing">Issue: Alerts not firing</a></h3>
<p><strong>Symptom</strong>: Thresholds violated but no alerts logged</p>
<p><strong>Cause</strong>: Log level filtering out WARN messages</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># Enable WARN level
export RUST_LOG=warn

# Or per-module
export RUST_LOG=pipeline::infrastructure::logging=warn
</code></pre>
<hr />
<h2 id="next-steps-20"><a class="header" href="#next-steps-20">Next Steps</a></h2>
<ul>
<li><strong><a href="implementation/metrics.html">Metrics Collection</a></strong>: Deep dive into Prometheus metrics</li>
<li><strong><a href="implementation/logging.html">Logging Implementation</a></strong>: Structured logging with tracing</li>
<li><strong><a href="implementation/configuration.html">Configuration</a></strong>: Configure alert thresholds and settings</li>
<li><strong><a href="implementation/../testing/integration-tests.html">Testing</a></strong>: Integration testing strategies</li>
</ul>
<hr />
<h2 id="references-1"><a class="header" href="#references-1">References</a></h2>
<ul>
<li>Source: <code>pipeline/src/infrastructure/logging/observability_service.rs</code> (lines 1-716)</li>
<li><a href="https://prometheus.io/docs/">Prometheus Documentation</a></li>
<li><a href="https://grafana.com/docs/grafana/latest/dashboards/">Grafana Dashboards</a></li>
<li><a href="https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/ch04.html">The Three Pillars of Observability</a></li>
<li><a href="https://sre.google/sre-book/monitoring-distributed-systems/">Site Reliability Engineering</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="metrics-collection"><a class="header" href="#metrics-collection">Metrics Collection</a></h1>
<p><strong>Version:</strong> 1.0
<strong>Date:</strong> 2025-01-04
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner, Claude Code
<strong>Status:</strong> Active</p>
<h2 id="overview-15"><a class="header" href="#overview-15">Overview</a></h2>
<p>The pipeline system implements comprehensive metrics collection for monitoring, observability, and performance analysis. Metrics are collected in real-time, exported in Prometheus format, and can be visualized using Grafana dashboards.</p>
<p><strong>Key Features:</strong></p>
<ul>
<li><strong>Prometheus Integration</strong>: Native Prometheus metrics export</li>
<li><strong>Real-Time Collection</strong>: Low-overhead metric updates</li>
<li><strong>Comprehensive Coverage</strong>: Performance, system, and business metrics</li>
<li><strong>Dimensional Data</strong>: Labels for multi-dimensional analysis</li>
<li><strong>Thread-Safe</strong>: Safe concurrent metric updates</li>
</ul>
<h2 id="metrics-architecture"><a class="header" href="#metrics-architecture">Metrics Architecture</a></h2>
<h3 id="two-level-metrics-system"><a class="header" href="#two-level-metrics-system">Two-Level Metrics System</a></h3>
<p>The system uses a two-level metrics architecture:</p>
<pre><code class="language-text">┌─────────────────────────────────────────────┐
│         Application Layer                   │
│  - ProcessingMetrics (domain entity)        │
│  - Per-pipeline performance tracking        │
│  - Business metrics and analytics           │
└─────────────────┬───────────────────────────┘
                  │
                  ↓
┌─────────────────────────────────────────────┐
│       Infrastructure Layer                  │
│  - MetricsService (Prometheus)              │
│  - System-wide aggregation                  │
│  - HTTP export endpoint                     │
└─────────────────────────────────────────────┘
</code></pre>
<p><strong>Domain Metrics (ProcessingMetrics):</strong></p>
<ul>
<li>Attached to processing context</li>
<li>Track individual pipeline execution</li>
<li>Support detailed analytics</li>
<li>Persist to database</li>
</ul>
<p><strong>Infrastructure Metrics (MetricsService):</strong></p>
<ul>
<li>System-wide aggregation</li>
<li>Prometheus counters, gauges, histograms</li>
<li>HTTP /metrics endpoint</li>
<li>Real-time monitoring</li>
</ul>
<h2 id="domain-metrics"><a class="header" href="#domain-metrics">Domain Metrics</a></h2>
<h3 id="processingmetrics-entity"><a class="header" href="#processingmetrics-entity">ProcessingMetrics Entity</a></h3>
<p>Tracks performance data for individual pipeline executions:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::entities::ProcessingMetrics;
use std::time::{Duration, Instant};
use std::collections::HashMap;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProcessingMetrics {
    // Progress tracking
    bytes_processed: u64,
    bytes_total: u64,
    chunks_processed: u64,
    chunks_total: u64,

    // Timing (high-resolution internal, RFC3339 for export)
    #[serde(skip)]
    start_time: Option&lt;Instant&gt;,
    #[serde(skip)]
    end_time: Option&lt;Instant&gt;,
    start_time_rfc3339: Option&lt;String&gt;,
    end_time_rfc3339: Option&lt;String&gt;,
    processing_duration: Option&lt;Duration&gt;,

    // Performance metrics
    throughput_bytes_per_second: f64,
    compression_ratio: Option&lt;f64&gt;,

    // Error tracking
    error_count: u64,
    warning_count: u64,

    // File information
    input_file_size_bytes: u64,
    output_file_size_bytes: u64,
    input_file_checksum: Option&lt;String&gt;,
    output_file_checksum: Option&lt;String&gt;,

    // Per-stage metrics
    stage_metrics: HashMap&lt;String, StageMetrics&gt;,
}
<span class="boring">}</span></code></pre></pre>
<h3 id="creating-and-using-processingmetrics"><a class="header" href="#creating-and-using-processingmetrics">Creating and Using ProcessingMetrics</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl ProcessingMetrics {
    /// Create new metrics for pipeline execution
    pub fn new(total_bytes: u64, total_chunks: u64) -&gt; Self {
        Self {
            bytes_processed: 0,
            bytes_total: total_bytes,
            chunks_processed: 0,
            chunks_total: total_chunks,
            start_time: Some(Instant::now()),
            start_time_rfc3339: Some(Utc::now().to_rfc3339()),
            end_time: None,
            end_time_rfc3339: None,
            processing_duration: None,
            throughput_bytes_per_second: 0.0,
            compression_ratio: None,
            error_count: 0,
            warning_count: 0,
            input_file_size_bytes: total_bytes,
            output_file_size_bytes: 0,
            input_file_checksum: None,
            output_file_checksum: None,
            stage_metrics: HashMap::new(),
        }
    }

    /// Update progress
    pub fn update_progress(&amp;mut self, bytes: u64, chunks: u64) {
        self.bytes_processed += bytes;
        self.chunks_processed += chunks;
        self.update_throughput();
    }

    /// Calculate throughput
    fn update_throughput(&amp;mut self) {
        if let Some(start) = self.start_time {
            let elapsed = start.elapsed().as_secs_f64();
            if elapsed &gt; 0.0 {
                self.throughput_bytes_per_second =
                    self.bytes_processed as f64 / elapsed;
            }
        }
    }

    /// Complete processing
    pub fn complete(&amp;mut self) {
        self.end_time = Some(Instant::now());
        self.end_time_rfc3339 = Some(Utc::now().to_rfc3339());

        if let (Some(start), Some(end)) = (self.start_time, self.end_time) {
            self.processing_duration = Some(end - start);
        }

        self.update_throughput();
        self.calculate_compression_ratio();
    }

    /// Calculate compression ratio
    fn calculate_compression_ratio(&amp;mut self) {
        if self.output_file_size_bytes &gt; 0 &amp;&amp; self.input_file_size_bytes &gt; 0 {
            self.compression_ratio = Some(
                self.output_file_size_bytes as f64 /
                self.input_file_size_bytes as f64
            );
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="stagemetrics"><a class="header" href="#stagemetrics">StageMetrics</a></h3>
<p>Track performance for individual pipeline stages:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StageMetrics {
    /// Stage name
    stage_name: String,

    /// Bytes processed by this stage
    bytes_processed: u64,

    /// Processing time for this stage
    #[serde(skip)]
    processing_time: Option&lt;Duration&gt;,

    /// Throughput (bytes per second)
    throughput_bps: f64,

    /// Error count for this stage
    error_count: u64,

    /// Memory usage (optional)
    memory_usage_bytes: Option&lt;u64&gt;,
}

impl ProcessingMetrics {
    /// Record stage metrics
    pub fn record_stage_metrics(
        &amp;mut self,
        stage_name: String,
        bytes: u64,
        duration: Duration,
    ) {
        let throughput = if duration.as_secs_f64() &gt; 0.0 {
            bytes as f64 / duration.as_secs_f64()
        } else {
            0.0
        };

        let stage_metrics = StageMetrics {
            stage_name: stage_name.clone(),
            bytes_processed: bytes,
            processing_time: Some(duration),
            throughput_bps: throughput,
            error_count: 0,
            memory_usage_bytes: None,
        };

        self.stage_metrics.insert(stage_name, stage_metrics);
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="prometheus-metrics"><a class="header" href="#prometheus-metrics">Prometheus Metrics</a></h2>
<h3 id="metricsservice"><a class="header" href="#metricsservice">MetricsService</a></h3>
<p>Infrastructure service for Prometheus metrics:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use prometheus::{
    IntCounter, IntGauge, Gauge, Histogram,
    HistogramOpts, Opts, Registry
};
use std::sync::Arc;

#[derive(Clone)]
pub struct MetricsService {
    registry: Arc&lt;Registry&gt;,

    // Pipeline execution metrics
    pipelines_processed_total: IntCounter,
    pipeline_processing_duration: Histogram,
    pipeline_bytes_processed_total: IntCounter,
    pipeline_chunks_processed_total: IntCounter,
    pipeline_errors_total: IntCounter,
    pipeline_warnings_total: IntCounter,

    // Performance metrics
    throughput_mbps: Gauge,
    compression_ratio: Gauge,
    active_pipelines: IntGauge,

    // System metrics
    memory_usage_bytes: IntGauge,
    cpu_utilization_percent: Gauge,
}

impl MetricsService {
    pub fn new() -&gt; Result&lt;Self, PipelineError&gt; {
        let registry = Arc::new(Registry::new());

        // Create counters
        let pipelines_processed_total = IntCounter::new(
            "pipeline_processed_total",
            "Total number of pipelines processed"
        )?;

        let pipeline_bytes_processed_total = IntCounter::new(
            "pipeline_bytes_processed_total",
            "Total bytes processed by pipelines"
        )?;

        let pipeline_errors_total = IntCounter::new(
            "pipeline_errors_total",
            "Total number of processing errors"
        )?;

        // Create histograms
        let pipeline_processing_duration = Histogram::with_opts(
            HistogramOpts::new(
                "pipeline_processing_duration_seconds",
                "Pipeline processing duration in seconds"
            )
            .buckets(vec![0.1, 0.5, 1.0, 5.0, 10.0, 30.0, 60.0])
        )?;

        // Create gauges
        let throughput_mbps = Gauge::new(
            "pipeline_throughput_mbps",
            "Current processing throughput in MB/s"
        )?;

        let compression_ratio = Gauge::new(
            "pipeline_compression_ratio",
            "Current compression ratio"
        )?;

        let active_pipelines = IntGauge::new(
            "pipeline_active_count",
            "Number of currently active pipelines"
        )?;

        // Register all metrics
        registry.register(Box::new(pipelines_processed_total.clone()))?;
        registry.register(Box::new(pipeline_bytes_processed_total.clone()))?;
        registry.register(Box::new(pipeline_errors_total.clone()))?;
        registry.register(Box::new(pipeline_processing_duration.clone()))?;
        registry.register(Box::new(throughput_mbps.clone()))?;
        registry.register(Box::new(compression_ratio.clone()))?;
        registry.register(Box::new(active_pipelines.clone()))?;

        Ok(Self {
            registry,
            pipelines_processed_total,
            pipeline_processing_duration,
            pipeline_bytes_processed_total,
            pipeline_chunks_processed_total: /* ... */,
            pipeline_errors_total,
            pipeline_warnings_total: /* ... */,
            throughput_mbps,
            compression_ratio,
            active_pipelines,
            memory_usage_bytes: /* ... */,
            cpu_utilization_percent: /* ... */,
        })
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="recording-metrics"><a class="header" href="#recording-metrics">Recording Metrics</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl MetricsService {
    /// Record pipeline completion
    pub fn record_pipeline_completion(
        &amp;self,
        metrics: &amp;ProcessingMetrics
    ) -&gt; Result&lt;(), PipelineError&gt; {
        // Increment counters
        self.pipelines_processed_total.inc();
        self.pipeline_bytes_processed_total
            .inc_by(metrics.bytes_processed());
        self.pipeline_chunks_processed_total
            .inc_by(metrics.chunks_processed());

        // Record duration
        if let Some(duration) = metrics.processing_duration() {
            self.pipeline_processing_duration
                .observe(duration.as_secs_f64());
        }

        // Update gauges
        self.throughput_mbps.set(
            metrics.throughput_bytes_per_second() / 1_000_000.0
        );

        if let Some(ratio) = metrics.compression_ratio() {
            self.compression_ratio.set(ratio);
        }

        // Record errors
        if metrics.error_count() &gt; 0 {
            self.pipeline_errors_total
                .inc_by(metrics.error_count());
        }

        Ok(())
    }

    /// Record pipeline start
    pub fn record_pipeline_start(&amp;self) {
        self.active_pipelines.inc();
    }

    /// Record pipeline end
    pub fn record_pipeline_end(&amp;self) {
        self.active_pipelines.dec();
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="available-metrics"><a class="header" href="#available-metrics">Available Metrics</a></h2>
<h3 id="counter-metrics"><a class="header" href="#counter-metrics">Counter Metrics</a></h3>
<p>Monotonically increasing values:</p>
<div class="table-wrapper"><table><thead><tr><th>Metric Name</th><th>Type</th><th>Description</th></tr></thead><tbody>
<tr><td><code>pipeline_processed_total</code></td><td>Counter</td><td>Total pipelines processed</td></tr>
<tr><td><code>pipeline_bytes_processed_total</code></td><td>Counter</td><td>Total bytes processed</td></tr>
<tr><td><code>pipeline_chunks_processed_total</code></td><td>Counter</td><td>Total chunks processed</td></tr>
<tr><td><code>pipeline_errors_total</code></td><td>Counter</td><td>Total processing errors</td></tr>
<tr><td><code>pipeline_warnings_total</code></td><td>Counter</td><td>Total warnings</td></tr>
</tbody></table>
</div>
<h3 id="gauge-metrics"><a class="header" href="#gauge-metrics">Gauge Metrics</a></h3>
<p>Values that can increase or decrease:</p>
<div class="table-wrapper"><table><thead><tr><th>Metric Name</th><th>Type</th><th>Description</th></tr></thead><tbody>
<tr><td><code>pipeline_active_count</code></td><td>Gauge</td><td>Currently active pipelines</td></tr>
<tr><td><code>pipeline_throughput_mbps</code></td><td>Gauge</td><td>Current throughput (MB/s)</td></tr>
<tr><td><code>pipeline_compression_ratio</code></td><td>Gauge</td><td>Current compression ratio</td></tr>
<tr><td><code>pipeline_memory_usage_bytes</code></td><td>Gauge</td><td>Memory usage in bytes</td></tr>
<tr><td><code>pipeline_cpu_utilization_percent</code></td><td>Gauge</td><td>CPU utilization percentage</td></tr>
</tbody></table>
</div>
<h3 id="histogram-metrics"><a class="header" href="#histogram-metrics">Histogram Metrics</a></h3>
<p>Distribution of values with buckets:</p>
<div class="table-wrapper"><table><thead><tr><th>Metric Name</th><th>Type</th><th>Buckets</th><th>Description</th></tr></thead><tbody>
<tr><td><code>pipeline_processing_duration_seconds</code></td><td>Histogram</td><td>0.1, 0.5, 1, 5, 10, 30, 60</td><td>Processing duration</td></tr>
<tr><td><code>pipeline_chunk_size_bytes</code></td><td>Histogram</td><td>1K, 10K, 100K, 1M, 10M</td><td>Chunk size distribution</td></tr>
<tr><td><code>pipeline_stage_duration_seconds</code></td><td>Histogram</td><td>0.01, 0.1, 0.5, 1, 5</td><td>Stage processing time</td></tr>
</tbody></table>
</div>
<h2 id="metrics-export"><a class="header" href="#metrics-export">Metrics Export</a></h2>
<h3 id="http-endpoint"><a class="header" href="#http-endpoint">HTTP Endpoint</a></h3>
<p>Export metrics via HTTP for Prometheus scraping:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use warp::Filter;

pub fn metrics_endpoint(
    metrics_service: Arc&lt;MetricsService&gt;
) -&gt; impl Filter&lt;Extract = impl warp::Reply, Error = warp::Rejection&gt; + Clone {
    warp::path!("metrics")
        .and(warp::get())
        .map(move || {
            let encoder = prometheus::TextEncoder::new();
            let metric_families = metrics_service.registry.gather();
            let mut buffer = Vec::new();

            encoder.encode(&amp;metric_families, &amp;mut buffer)
                .unwrap_or_else(|e| {
                    eprintln!("Failed to encode metrics: {}", e);
                });

            warp::reply::with_header(
                buffer,
                "Content-Type",
                encoder.format_type()
            )
        })
}
<span class="boring">}</span></code></pre></pre>
<h3 id="prometheus-configuration"><a class="header" href="#prometheus-configuration">Prometheus Configuration</a></h3>
<p>Configure Prometheus to scrape metrics:</p>
<pre><code class="language-yaml"># prometheus.yml
scrape_configs:
  - job_name: 'pipeline'
    scrape_interval: 15s
    static_configs:
      - targets: ['localhost:9090']
    metrics_path: '/metrics'
</code></pre>
<h3 id="example-metrics-output"><a class="header" href="#example-metrics-output">Example Metrics Output</a></h3>
<pre><code class="language-text"># HELP pipeline_processed_total Total number of pipelines processed
# TYPE pipeline_processed_total counter
pipeline_processed_total 1234

# HELP pipeline_bytes_processed_total Total bytes processed
# TYPE pipeline_bytes_processed_total counter
pipeline_bytes_processed_total 1073741824

# HELP pipeline_processing_duration_seconds Pipeline processing duration
# TYPE pipeline_processing_duration_seconds histogram
pipeline_processing_duration_seconds_bucket{le="0.1"} 45
pipeline_processing_duration_seconds_bucket{le="0.5"} 120
pipeline_processing_duration_seconds_bucket{le="1.0"} 280
pipeline_processing_duration_seconds_bucket{le="5.0"} 450
pipeline_processing_duration_seconds_bucket{le="10.0"} 500
pipeline_processing_duration_seconds_bucket{le="+Inf"} 520
pipeline_processing_duration_seconds_sum 2340.5
pipeline_processing_duration_seconds_count 520

# HELP pipeline_throughput_mbps Current processing throughput
# TYPE pipeline_throughput_mbps gauge
pipeline_throughput_mbps 125.7

# HELP pipeline_compression_ratio Current compression ratio
# TYPE pipeline_compression_ratio gauge
pipeline_compression_ratio 0.35
</code></pre>
<h2 id="integration-with-processing"><a class="header" href="#integration-with-processing">Integration with Processing</a></h2>
<h3 id="automatic-metric-collection"><a class="header" href="#automatic-metric-collection">Automatic Metric Collection</a></h3>
<p>Metrics are automatically collected during processing:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline::MetricsService;

async fn process_file_with_metrics(
    input_path: &amp;str,
    output_path: &amp;str,
    metrics_service: Arc&lt;MetricsService&gt;,
) -&gt; Result&lt;(), PipelineError&gt; {
    // Create domain metrics
    let mut metrics = ProcessingMetrics::new(
        file_size,
        chunk_count
    );

    // Record start
    metrics_service.record_pipeline_start();

    // Process file
    for chunk in chunks {
        let start = Instant::now();

        let processed = process_chunk(chunk)?;

        metrics.update_progress(
            processed.len() as u64,
            1
        );

        metrics.record_stage_metrics(
            "compression".to_string(),
            processed.len() as u64,
            start.elapsed()
        );
    }

    // Complete metrics
    metrics.complete();

    // Export to Prometheus
    metrics_service.record_pipeline_completion(&amp;metrics)?;
    metrics_service.record_pipeline_end();

    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h3 id="observer-pattern-1"><a class="header" href="#observer-pattern-1">Observer Pattern</a></h3>
<p>Use observer pattern for automatic metric updates:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct MetricsObserver {
    metrics_service: Arc&lt;MetricsService&gt;,
}

impl MetricsObserver {
    pub fn observe_processing(
        &amp;self,
        event: ProcessingEvent
    ) {
        match event {
            ProcessingEvent::PipelineStarted =&gt; {
                self.metrics_service.record_pipeline_start();
            }
            ProcessingEvent::ChunkProcessed { bytes, duration } =&gt; {
                self.metrics_service.pipeline_bytes_processed_total
                    .inc_by(bytes);
                self.metrics_service.pipeline_processing_duration
                    .observe(duration.as_secs_f64());
            }
            ProcessingEvent::PipelineCompleted { metrics } =&gt; {
                self.metrics_service
                    .record_pipeline_completion(&amp;metrics)
                    .ok();
                self.metrics_service.record_pipeline_end();
            }
            ProcessingEvent::Error =&gt; {
                self.metrics_service.pipeline_errors_total.inc();
            }
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="visualization-with-grafana"><a class="header" href="#visualization-with-grafana">Visualization with Grafana</a></h2>
<h3 id="dashboard-configuration"><a class="header" href="#dashboard-configuration">Dashboard Configuration</a></h3>
<p>Create Grafana dashboard for visualization:</p>
<pre><code class="language-json">{
  "dashboard": {
    "title": "Pipeline Metrics",
    "panels": [
      {
        "title": "Throughput",
        "targets": [{
          "expr": "rate(pipeline_bytes_processed_total[5m]) / 1000000",
          "legendFormat": "Throughput (MB/s)"
        }]
      },
      {
        "title": "Processing Duration (P95)",
        "targets": [{
          "expr": "histogram_quantile(0.95, rate(pipeline_processing_duration_seconds_bucket[5m]))",
          "legendFormat": "P95 Duration"
        }]
      },
      {
        "title": "Active Pipelines",
        "targets": [{
          "expr": "pipeline_active_count",
          "legendFormat": "Active"
        }]
      },
      {
        "title": "Error Rate",
        "targets": [{
          "expr": "rate(pipeline_errors_total[5m])",
          "legendFormat": "Errors/sec"
        }]
      }
    ]
  }
}
</code></pre>
<h3 id="common-queries"><a class="header" href="#common-queries">Common Queries</a></h3>
<p><strong>Throughput over time:</strong></p>
<pre><code class="language-promql">rate(pipeline_bytes_processed_total[5m]) / 1000000
</code></pre>
<p><strong>Average processing duration:</strong></p>
<pre><code class="language-promql">rate(pipeline_processing_duration_seconds_sum[5m]) /
rate(pipeline_processing_duration_seconds_count[5m])
</code></pre>
<p><strong>P99 latency:</strong></p>
<pre><code class="language-promql">histogram_quantile(0.99,
  rate(pipeline_processing_duration_seconds_bucket[5m])
)
</code></pre>
<p><strong>Error rate:</strong></p>
<pre><code class="language-promql">rate(pipeline_errors_total[5m])
</code></pre>
<p><strong>Compression effectiveness:</strong></p>
<pre><code class="language-promql">avg(pipeline_compression_ratio)
</code></pre>
<h2 id="performance-considerations-2"><a class="header" href="#performance-considerations-2">Performance Considerations</a></h2>
<h3 id="low-overhead-updates"><a class="header" href="#low-overhead-updates">Low-Overhead Updates</a></h3>
<p>Metrics use atomic operations for minimal overhead:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Atomic increment
self.counter.inc();

// ❌ BAD: Locking for simple increment
let mut guard = self.counter.lock().unwrap();
*guard += 1;
<span class="boring">}</span></code></pre></pre>
<h3 id="batch-updates"><a class="header" href="#batch-updates">Batch Updates</a></h3>
<p>Batch metric updates when possible:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Batch update
self.pipeline_bytes_processed_total.inc_by(total_bytes);

// ❌ BAD: Multiple individual updates
for _ in 0..total_bytes {
    self.pipeline_bytes_processed_total.inc();
}
<span class="boring">}</span></code></pre></pre>
<h3 id="efficient-labels"><a class="header" href="#efficient-labels">Efficient Labels</a></h3>
<p>Use labels judiciously to avoid cardinality explosion:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Limited cardinality
let counter = register_int_counter_vec!(
    "pipeline_processed_total",
    "Total pipelines processed",
    &amp;["algorithm", "stage"]  // ~10 algorithms × ~5 stages = 50 series
)?;

// ❌ BAD: High cardinality
let counter = register_int_counter_vec!(
    "pipeline_processed_total",
    "Total pipelines processed",
    &amp;["pipeline_id", "user_id"]  // Could be millions of series!
)?;
<span class="boring">}</span></code></pre></pre>
<h2 id="alerting"><a class="header" href="#alerting">Alerting</a></h2>
<h3 id="alert-rules"><a class="header" href="#alert-rules">Alert Rules</a></h3>
<p>Define Prometheus alert rules:</p>
<pre><code class="language-yaml"># alerts.yml
groups:
  - name: pipeline_alerts
    rules:
      - alert: HighErrorRate
        expr: rate(pipeline_errors_total[5m]) &gt; 0.1
        for: 5m
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} errors/sec"

      - alert: LowThroughput
        expr: rate(pipeline_bytes_processed_total[5m]) &lt; 1000000
        for: 10m
        annotations:
          summary: "Low throughput detected"
          description: "Throughput is {{ $value }} bytes/sec"

      - alert: HighLatency
        expr: |
          histogram_quantile(0.95,
            rate(pipeline_processing_duration_seconds_bucket[5m])
          ) &gt; 60
        for: 5m
        annotations:
          summary: "High P95 latency"
          description: "P95 latency is {{ $value }}s"
</code></pre>
<h2 id="best-practices-11"><a class="header" href="#best-practices-11">Best Practices</a></h2>
<h3 id="metric-naming"><a class="header" href="#metric-naming">Metric Naming</a></h3>
<p>Follow Prometheus naming conventions:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Clear, consistent names
pipeline_bytes_processed_total      // Counter with _total suffix
pipeline_processing_duration_seconds // Time in base unit (seconds)
pipeline_active_count               // Gauge without suffix

// ❌ BAD: Inconsistent naming
processed_bytes                     // Missing namespace
duration_ms                        // Wrong unit (use seconds)
active                             // Too vague
<span class="boring">}</span></code></pre></pre>
<h3 id="unit-consistency"><a class="header" href="#unit-consistency">Unit Consistency</a></h3>
<p>Always use base units:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Base units
duration_seconds: f64              // Seconds, not milliseconds
size_bytes: u64                    // Bytes, not KB/MB
ratio: f64                         // Unitless ratio 0.0-1.0

// ❌ BAD: Non-standard units
duration_ms: u64
size_mb: f64
percentage: u8
<span class="boring">}</span></code></pre></pre>
<h3 id="documentation"><a class="header" href="#documentation">Documentation</a></h3>
<p>Document all metrics:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Well documented
let counter = IntCounter::with_opts(
    Opts::new(
        "pipeline_processed_total",
        "Total number of pipelines processed successfully. \
         Incremented on completion of each pipeline execution."
    )
)?;
<span class="boring">}</span></code></pre></pre>
<h2 id="next-steps-21"><a class="header" href="#next-steps-21">Next Steps</a></h2>
<p>Now that you understand metrics collection:</p>
<ul>
<li><a href="implementation/logging.html">Logging</a> - Structured logging implementation</li>
<li><a href="implementation/observability.html">Observability</a> - Complete observability strategy</li>
<li><a href="implementation/../advanced/performance.html">Performance</a> - Performance optimization</li>
<li><a href="implementation/../advanced/monitoring.html">Monitoring</a> - Production monitoring setup</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="logging-implementation"><a class="header" href="#logging-implementation">Logging Implementation</a></h1>
<p><strong>Version</strong>: 1.0
<strong>Date</strong>: 2025-10-04
<strong>License</strong>: BSD-3-Clause
<strong>Copyright</strong>: (c) 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors</strong>: Michael Gardner
<strong>Status</strong>: Active</p>
<hr />
<h2 id="overview-16"><a class="header" href="#overview-16">Overview</a></h2>
<p>The Optimized Adaptive Pipeline uses <strong>structured logging</strong> via the <a href="https://docs.rs/tracing">tracing</a> crate to provide comprehensive observability throughout the system. This chapter details the logging architecture, implementation patterns, and best practices.</p>
<h3 id="key-features-3"><a class="header" href="#key-features-3">Key Features</a></h3>
<ul>
<li><strong>Structured Logging</strong>: Rich, structured log events with contextual metadata</li>
<li><strong>Two-Phase Architecture</strong>: Separate bootstrap and application logging</li>
<li><strong>Hierarchical Levels</strong>: Traditional log levels (ERROR, WARN, INFO, DEBUG, TRACE)</li>
<li><strong>Targeted Filtering</strong>: Fine-grained control via log targets</li>
<li><strong>Integration</strong>: Seamless integration with ObservabilityService and metrics</li>
<li><strong>Performance</strong>: Low-overhead logging with compile-time filtering</li>
<li><strong>Testability</strong>: Trait-based abstractions for testing</li>
</ul>
<hr />
<h2 id="architecture-6"><a class="header" href="#architecture-6">Architecture</a></h2>
<h3 id="two-phase-logging-system"><a class="header" href="#two-phase-logging-system">Two-Phase Logging System</a></h3>
<p>The system employs a two-phase logging approach to handle different initialization stages:</p>
<pre><code class="language-text">┌─────────────────────────────────────────────────────────────┐
│                    Application Lifecycle                     │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  Phase 1: Bootstrap                Phase 2: Application     │
│  ┌───────────────────┐             ┌──────────────────────┐ │
│  │ BootstrapLogger   │────────────&gt;│ Tracing Subscriber   │ │
│  │ (Early init)      │             │ (Full featured)      │ │
│  ├───────────────────┤             ├──────────────────────┤ │
│  │ - Simple API      │             │ - Structured events  │ │
│  │ - No dependencies │             │ - Span tracking      │ │
│  │ - Testable        │             │ - Context propagation│ │
│  │ - Minimal overhead│             │ - External outputs   │ │
│  └───────────────────┘             └──────────────────────┘ │
│                                                              │
└─────────────────────────────────────────────────────────────┘
</code></pre>
<h4 id="phase-1-bootstrap-logger"><a class="header" href="#phase-1-bootstrap-logger">Phase 1: Bootstrap Logger</a></h4>
<p>Located in <code>bootstrap/src/logger.rs</code>, the bootstrap logger provides minimal logging during early initialization:</p>
<ul>
<li><strong>Minimal dependencies</strong>: No heavy tracing infrastructure</li>
<li><strong>Trait-based abstraction</strong>: <code>BootstrapLogger</code> trait for testability</li>
<li><strong>Simple API</strong>: Only 4 log levels (error, warn, info, debug)</li>
<li><strong>Early availability</strong>: Available before tracing subscriber initialization</li>
</ul>
<h4 id="phase-2-application-logging"><a class="header" href="#phase-2-application-logging">Phase 2: Application Logging</a></h4>
<p>Once the application is fully initialized, the full tracing infrastructure is used:</p>
<ul>
<li><strong>Rich structured logging</strong>: Fields, spans, and events</li>
<li><strong>Multiple subscribers</strong>: Console, file, JSON, external systems</li>
<li><strong>Performance tracking</strong>: Integration with ObservabilityService</li>
<li><strong>Distributed tracing</strong>: Context propagation for async operations</li>
</ul>
<hr />
<h2 id="log-levels"><a class="header" href="#log-levels">Log Levels</a></h2>
<p>The system uses five hierarchical log levels, from most to least severe:</p>
<div class="table-wrapper"><table><thead><tr><th>Level</th><th>Macro</th><th>Use Case</th><th>Example</th></tr></thead><tbody>
<tr><td><strong>ERROR</strong></td><td><code>error!()</code></td><td>Fatal errors, unrecoverable failures</td><td>Database connection failure, file corruption</td></tr>
<tr><td><strong>WARN</strong></td><td><code>warn!()</code></td><td>Non-fatal issues, degraded performance</td><td>High error rate alert, configuration warning</td></tr>
<tr><td><strong>INFO</strong></td><td><code>info!()</code></td><td>Normal operations, key milestones</td><td>Pipeline started, file processed successfully</td></tr>
<tr><td><strong>DEBUG</strong></td><td><code>debug!()</code></td><td>Detailed diagnostic information</td><td>Stage execution details, chunk processing</td></tr>
<tr><td><strong>TRACE</strong></td><td><code>trace!()</code></td><td>Very verbose debugging</td><td>Function entry/exit, detailed state dumps</td></tr>
</tbody></table>
</div>
<h3 id="level-guidelines"><a class="header" href="#level-guidelines">Level Guidelines</a></h3>
<p><strong>ERROR</strong>: Use sparingly for genuine failures that require attention</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>error!("Failed to connect to database: {}", err);
error!(pipeline_id = %id, "Pipeline execution failed: {}", err);
<span class="boring">}</span></code></pre></pre>
<p><strong>WARN</strong>: Use for concerning situations that don't prevent operation</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>warn!("High error rate: {:.1}% (threshold: {:.1}%)", rate, threshold);
warn!(stage = %name, "Stage processing slower than expected");
<span class="boring">}</span></code></pre></pre>
<p><strong>INFO</strong>: Use for important operational events</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>info!("Started pipeline processing: {}", pipeline_name);
info!(bytes = %bytes_processed, duration = ?elapsed, "File processing completed");
<span class="boring">}</span></code></pre></pre>
<p><strong>DEBUG</strong>: Use for detailed diagnostic information during development</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>debug!("Preparing stage: {}", stage.name());
debug!(chunk_count = chunks, size = bytes, "Processing chunk batch");
<span class="boring">}</span></code></pre></pre>
<p><strong>TRACE</strong>: Use for extremely detailed debugging (usually disabled in production)</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>trace!("Entering function with args: {:?}", args);
trace!(state = ?current_state, "State transition complete");
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="bootstrap-logger"><a class="header" href="#bootstrap-logger">Bootstrap Logger</a></h2>
<h3 id="bootstraplogger-trait"><a class="header" href="#bootstraplogger-trait">BootstrapLogger Trait</a></h3>
<p>The bootstrap logger abstraction allows for different implementations:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Bootstrap logging abstraction
pub trait BootstrapLogger: Send + Sync {
    /// Log an error message (fatal errors during bootstrap)
    fn error(&amp;self, message: &amp;str);

    /// Log a warning message (non-fatal issues)
    fn warn(&amp;self, message: &amp;str);

    /// Log an info message (normal bootstrap progress)
    fn info(&amp;self, message: &amp;str);

    /// Log a debug message (detailed diagnostic information)
    fn debug(&amp;self, message: &amp;str);
}
<span class="boring">}</span></code></pre></pre>
<h3 id="consolelogger-implementation"><a class="header" href="#consolelogger-implementation">ConsoleLogger Implementation</a></h3>
<p>The production implementation wraps the tracing crate:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Console logger implementation using tracing
pub struct ConsoleLogger {
    prefix: String,
}

impl ConsoleLogger {
    /// Create a new console logger with default prefix
    pub fn new() -&gt; Self {
        Self::with_prefix("bootstrap")
    }

    /// Create a new console logger with custom prefix
    pub fn with_prefix(prefix: impl Into&lt;String&gt;) -&gt; Self {
        Self { prefix: prefix.into() }
    }
}

impl BootstrapLogger for ConsoleLogger {
    fn error(&amp;self, message: &amp;str) {
        tracing::error!(target: "bootstrap", "[{}] {}", self.prefix, message);
    }

    fn warn(&amp;self, message: &amp;str) {
        tracing::warn!(target: "bootstrap", "[{}] {}", self.prefix, message);
    }

    fn info(&amp;self, message: &amp;str) {
        tracing::info!(target: "bootstrap", "[{}] {}", self.prefix, message);
    }

    fn debug(&amp;self, message: &amp;str) {
        tracing::debug!(target: "bootstrap", "[{}] {}", self.prefix, message);
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="nooplogger-for-testing"><a class="header" href="#nooplogger-for-testing">NoOpLogger for Testing</a></h3>
<p>A no-op implementation for testing scenarios where logging should be silent:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// No-op logger for testing
pub struct NoOpLogger;

impl NoOpLogger {
    pub fn new() -&gt; Self {
        Self
    }
}

impl BootstrapLogger for NoOpLogger {
    fn error(&amp;self, _message: &amp;str) {}
    fn warn(&amp;self, _message: &amp;str) {}
    fn info(&amp;self, _message: &amp;str) {}
    fn debug(&amp;self, _message: &amp;str) {}
}
<span class="boring">}</span></code></pre></pre>
<h3 id="usage-example-1"><a class="header" href="#usage-example-1">Usage Example</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use bootstrap::logger::{BootstrapLogger, ConsoleLogger};

fn bootstrap_application() -&gt; Result&lt;()&gt; {
    let logger = ConsoleLogger::new();

    logger.info("Starting application bootstrap");
    logger.debug("Parsing command line arguments");

    match parse_config() {
        Ok(config) =&gt; {
            logger.info("Configuration loaded successfully");
            Ok(())
        }
        Err(e) =&gt; {
            logger.error(&amp;format!("Failed to load configuration: {}", e));
            Err(e)
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="application-logging-with-tracing"><a class="header" href="#application-logging-with-tracing">Application Logging with Tracing</a></h2>
<h3 id="basic-logging-macros"><a class="header" href="#basic-logging-macros">Basic Logging Macros</a></h3>
<p>Once the tracing subscriber is initialized, use the standard tracing macros:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tracing::{trace, debug, info, warn, error};

fn process_pipeline(pipeline_id: &amp;str) -&gt; Result&lt;()&gt; {
    info!("Starting pipeline processing: {}", pipeline_id);

    debug!(pipeline_id = %pipeline_id, "Loading pipeline configuration");

    match execute_pipeline(pipeline_id) {
        Ok(result) =&gt; {
            info!(
                pipeline_id = %pipeline_id,
                bytes_processed = result.bytes,
                duration = ?result.duration,
                "Pipeline completed successfully"
            );
            Ok(result)
        }
        Err(e) =&gt; {
            error!(
                pipeline_id = %pipeline_id,
                error = %e,
                "Pipeline execution failed"
            );
            Err(e)
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="structured-fields"><a class="header" href="#structured-fields">Structured Fields</a></h3>
<p>Add structured fields to log events for better searchability and filtering:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Simple field
info!(stage = "compression", "Stage started");

// Display formatting with %
info!(pipeline_id = %id, "Processing pipeline");

// Debug formatting with ?
debug!(config = ?pipeline_config, "Loaded configuration");

// Multiple fields
info!(
    pipeline_id = %id,
    stage = "encryption",
    bytes_processed = total_bytes,
    duration_ms = elapsed.as_millis(),
    "Stage completed"
);
<span class="boring">}</span></code></pre></pre>
<h3 id="log-targets"><a class="header" href="#log-targets">Log Targets</a></h3>
<p>Use targets to categorize and filter log events:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Bootstrap logs
tracing::info!(target: "bootstrap", "Application starting");

// Domain events
tracing::debug!(target: "domain::pipeline", "Creating pipeline entity");

// Infrastructure events
tracing::debug!(target: "infrastructure::database", "Executing query");

// Metrics events
tracing::info!(target: "metrics", "Recording pipeline completion");
<span class="boring">}</span></code></pre></pre>
<h3 id="example-from-codebase"><a class="header" href="#example-from-codebase">Example from Codebase</a></h3>
<p>From <code>pipeline/src/infrastructure/repositories/stage_executor.rs</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>tracing::debug!(
    "Processing {} chunks with {} workers",
    chunks.len(),
    worker_count
);

tracing::info!(
    "Processed {} bytes in {:.2}s ({:.2} MB/s)",
    total_bytes,
    duration.as_secs_f64(),
    throughput_mbps
);

tracing::debug!(
    "Stage {} processed {} chunks successfully",
    stage_name,
    chunk_count
);
<span class="boring">}</span></code></pre></pre>
<p>From <code>bootstrap/src/signals.rs</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>tracing::info!("Received SIGTERM, initiating graceful shutdown");
tracing::info!("Received SIGINT (Ctrl+C), initiating graceful shutdown");
tracing::error!("Failed to register SIGTERM handler: {}", e);
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="integration-with-observabilityservice"><a class="header" href="#integration-with-observabilityservice">Integration with ObservabilityService</a></h2>
<p>The logging system integrates with the ObservabilityService for comprehensive monitoring.</p>
<h3 id="automatic-logging-in-operations"><a class="header" href="#automatic-logging-in-operations">Automatic Logging in Operations</a></h3>
<p>From <code>pipeline/src/infrastructure/logging/observability_service.rs</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl ObservabilityService {
    /// Complete operation tracking
    pub async fn complete_operation(
        &amp;self,
        operation_name: &amp;str,
        duration: Duration,
        success: bool,
        throughput_mbps: f64,
    ) {
        // ... update metrics ...

        info!(
            "Completed operation: {} in {:.2}s (throughput: {:.2} MB/s, success: {})",
            operation_name,
            duration.as_secs_f64(),
            throughput_mbps,
            success
        );

        // Check for alerts
        self.check_alerts(&amp;tracker).await;
    }

    /// Check for alerts based on current metrics
    async fn check_alerts(&amp;self, tracker: &amp;PerformanceTracker) {
        if tracker.error_rate_percent &gt; self.alert_thresholds.max_error_rate_percent {
            warn!(
                "🚨 Alert: High error rate {:.1}% (threshold: {:.1}%)",
                tracker.error_rate_percent,
                self.alert_thresholds.max_error_rate_percent
            );
        }

        if tracker.average_throughput_mbps &lt; self.alert_thresholds.min_throughput_mbps {
            warn!(
                "🚨 Alert: Low throughput {:.2} MB/s (threshold: {:.2} MB/s)",
                tracker.average_throughput_mbps,
                self.alert_thresholds.min_throughput_mbps
            );
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="operationtracker-with-logging"><a class="header" href="#operationtracker-with-logging">OperationTracker with Logging</a></h3>
<p>The <code>OperationTracker</code> automatically logs operation lifecycle:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct OperationTracker {
    operation_name: String,
    start_time: Instant,
    observability_service: ObservabilityService,
    completed: std::sync::atomic::AtomicBool,
}

impl OperationTracker {
    pub async fn complete(self, success: bool, bytes_processed: u64) {
        self.completed.store(true, std::sync::atomic::Ordering::Relaxed);

        let duration = self.start_time.elapsed();
        let throughput_mbps = calculate_throughput(bytes_processed, duration);

        // Logs completion via ObservabilityService.complete_operation()
        self.observability_service
            .complete_operation(&amp;self.operation_name, duration, success, throughput_mbps)
            .await;
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="usage-pattern"><a class="header" href="#usage-pattern">Usage Pattern</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Start operation (increments active count, logs start)
let tracker = observability_service
    .start_operation("file_processing")
    .await;

// Do work...
let result = process_file(&amp;input_path).await?;

// Complete operation (logs completion with metrics)
tracker.complete(true, result.bytes_processed).await;
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="logging-configuration"><a class="header" href="#logging-configuration">Logging Configuration</a></h2>
<h3 id="environment-variables-1"><a class="header" href="#environment-variables-1">Environment Variables</a></h3>
<p>Control logging behavior via environment variables:</p>
<pre><code class="language-bash"># Set log level (error, warn, info, debug, trace)
export RUST_LOG=info

# Set level per module
export RUST_LOG=pipeline=debug,bootstrap=info

# Set level per target
export RUST_LOG=infrastructure::database=debug

# Complex filtering
export RUST_LOG="info,pipeline::domain=debug,sqlx=warn"
</code></pre>
<h3 id="subscriber-initialization"><a class="header" href="#subscriber-initialization">Subscriber Initialization</a></h3>
<p>In <code>main.rs</code> or bootstrap code:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tracing_subscriber::{fmt, EnvFilter};

fn init_logging() -&gt; Result&lt;()&gt; {
    // Initialize tracing subscriber with environment filter
    tracing_subscriber::fmt()
        .with_env_filter(
            EnvFilter::try_from_default_env()
                .unwrap_or_else(|_| EnvFilter::new("info"))
        )
        .with_target(true)
        .with_thread_ids(true)
        .with_line_number(true)
        .init();

    info!("Logging initialized");
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h3 id="advanced-subscriber-configuration"><a class="header" href="#advanced-subscriber-configuration">Advanced Subscriber Configuration</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tracing_subscriber::{fmt, layer::SubscriberExt, EnvFilter, Registry};
use tracing_appender::{non_blocking, rolling};

fn init_advanced_logging() -&gt; Result&lt;()&gt; {
    // Console output
    let console_layer = fmt::layer()
        .with_target(true)
        .with_thread_ids(true);

    // File output with daily rotation
    let file_appender = rolling::daily("./logs", "pipeline.log");
    let (non_blocking_appender, _guard) = non_blocking(file_appender);
    let file_layer = fmt::layer()
        .with_writer(non_blocking_appender)
        .with_ansi(false)
        .json();

    // Combine layers
    let subscriber = Registry::default()
        .with(EnvFilter::try_from_default_env().unwrap_or_else(|_| EnvFilter::new("info")))
        .with(console_layer)
        .with(file_layer);

    tracing::subscriber::set_global_default(subscriber)?;

    info!("Advanced logging initialized");
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="performance-considerations-3"><a class="header" href="#performance-considerations-3">Performance Considerations</a></h2>
<h3 id="compile-time-filtering"><a class="header" href="#compile-time-filtering">Compile-Time Filtering</a></h3>
<p>Tracing macros are compile-time filtered at the <code>trace!</code> and <code>debug!</code> levels in release builds:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// This has ZERO overhead in release builds if max_level_debug is not set
debug!("Expensive computation result: {:?}", expensive_calculation());
<span class="boring">}</span></code></pre></pre>
<p>To enable debug/trace in release builds, add to <code>Cargo.toml</code>:</p>
<pre><code class="language-toml">[dependencies]
tracing = { version = "0.1", features = ["max_level_debug"] }
</code></pre>
<h3 id="lazy-evaluation"><a class="header" href="#lazy-evaluation">Lazy Evaluation</a></h3>
<p>Use closures for expensive operations:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// BAD: Always evaluates format_large_struct() even if debug is disabled
debug!("Large struct: {}", format_large_struct(&amp;data));

// GOOD: Only evaluates if debug level is enabled
debug!(data = ?data, "Processing large struct");
<span class="boring">}</span></code></pre></pre>
<h3 id="structured-vs-formatted"><a class="header" href="#structured-vs-formatted">Structured vs. Formatted</a></h3>
<p>Prefer structured fields over formatted strings:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Less efficient: String formatting always happens
info!("Processed {} bytes in {}ms", bytes, duration_ms);

// More efficient: Fields recorded directly
info!(bytes = bytes, duration_ms = duration_ms, "Processed data");
<span class="boring">}</span></code></pre></pre>
<h3 id="async-performance"><a class="header" href="#async-performance">Async Performance</a></h3>
<p>In async contexts, avoid blocking operations in log statements:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// BAD: Blocking call in log statement
info!("Config: {}", read_config_file_sync()); // Blocks async task!

// GOOD: Log after async operation completes
let config = read_config_file().await?;
info!(config = ?config, "Configuration loaded");
<span class="boring">}</span></code></pre></pre>
<h3 id="benchmark-results-1"><a class="header" href="#benchmark-results-1">Benchmark Results</a></h3>
<p>Logging overhead on Intel i7-10700K @ 3.8 GHz:</p>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Time</th><th>Overhead</th></tr></thead><tbody>
<tr><td><code>info!()</code> with 3 fields</td><td>~80 ns</td><td>Negligible</td></tr>
<tr><td><code>debug!()</code> (disabled)</td><td>~0 ns</td><td>Zero</td></tr>
<tr><td><code>debug!()</code> (enabled) with 5 fields</td><td>~120 ns</td><td>Minimal</td></tr>
<tr><td>JSON serialization</td><td>~500 ns</td><td>Low</td></tr>
<tr><td>File I/O (async)</td><td>~10 μs</td><td>Moderate</td></tr>
</tbody></table>
</div>
<p><strong>Recommendation</strong>: Log freely at <code>info!</code> and above. Use <code>debug!</code> and <code>trace!</code> judiciously.</p>
<hr />
<h2 id="best-practices-12"><a class="header" href="#best-practices-12">Best Practices</a></h2>
<h3 id="-do-2"><a class="header" href="#-do-2">✅ DO</a></h3>
<p><strong>Use structured fields for important data</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>info!(
    pipeline_id = %id,
    bytes = total_bytes,
    duration_ms = elapsed.as_millis(),
    "Pipeline completed"
);
<span class="boring">}</span></code></pre></pre>
<p><strong>Use display formatting (%) for user-facing types</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>info!(pipeline_id = %id, stage = %stage_name, "Processing stage");
<span class="boring">}</span></code></pre></pre>
<p><strong>Use debug formatting (?) for complex types</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>debug!(config = ?pipeline_config, "Loaded configuration");
<span class="boring">}</span></code></pre></pre>
<p><strong>Log errors with context</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>error!(
    pipeline_id = %id,
    stage = "encryption",
    error = %err,
    "Stage execution failed"
);
<span class="boring">}</span></code></pre></pre>
<p><strong>Use targets for filtering</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>tracing::debug!(target: "infrastructure::database", "Executing query: {}", sql);
<span class="boring">}</span></code></pre></pre>
<p><strong>Log at appropriate levels</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Error: Genuine failures
error!("Database connection lost: {}", err);

// Warn: Concerning but recoverable
warn!("Retry attempt {} of {}", attempt, max_attempts);

// Info: Important operational events
info!("Pipeline started successfully");

// Debug: Detailed diagnostic information
debug!("Stage prepared with {} workers", worker_count);
<span class="boring">}</span></code></pre></pre>
<h3 id="-dont-2"><a class="header" href="#-dont-2">❌ DON'T</a></h3>
<p><strong>Don't log sensitive information</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// BAD: Logging encryption keys
debug!("Encryption key: {}", key); // SECURITY RISK!

// GOOD: Log that operation happened without exposing secrets
debug!("Encryption key loaded from configuration");
<span class="boring">}</span></code></pre></pre>
<p><strong>Don't use println! or eprintln! in production code</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// BAD
println!("Processing file: {}", path);

// GOOD
info!(path = %path, "Processing file");
<span class="boring">}</span></code></pre></pre>
<p><strong>Don't log in hot loops without rate limiting</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// BAD: Logs millions of times
for chunk in chunks {
    debug!("Processing chunk {}", chunk.id); // Too verbose!
}

// GOOD: Log summary
debug!(chunk_count = chunks.len(), "Processing chunks");
// ... process chunks ...
info!(chunks_processed = chunks.len(), "Chunk processing complete");
<span class="boring">}</span></code></pre></pre>
<p><strong>Don't perform expensive operations in log statements</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// BAD
debug!("Data: {}", expensive_serialization(&amp;data));

// GOOD: Use debug formatting for lazy evaluation
debug!(data = ?data, "Processing data");
<span class="boring">}</span></code></pre></pre>
<p><strong>Don't duplicate metrics in logs</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// BAD: Metrics already tracked separately
info!("Throughput: {} MB/s", throughput); // Redundant with MetricsService!

// GOOD: Log events, not metrics
info!("File processing completed successfully");
// MetricsService already tracks throughput
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="testing-strategies-6"><a class="header" href="#testing-strategies-6">Testing Strategies</a></h2>
<h3 id="capturinglogger-for-tests"><a class="header" href="#capturinglogger-for-tests">CapturingLogger for Tests</a></h3>
<p>The bootstrap layer provides a capturing logger for tests:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(test)]
pub struct CapturingLogger {
    messages: Arc&lt;Mutex&lt;Vec&lt;LogMessage&gt;&gt;&gt;,
}

#[cfg(test)]
impl CapturingLogger {
    pub fn new() -&gt; Self {
        Self {
            messages: Arc::new(Mutex::new(Vec::new())),
        }
    }

    pub fn messages(&amp;self) -&gt; Vec&lt;LogMessage&gt; {
        self.messages.lock().unwrap().clone()
    }

    pub fn clear(&amp;self) {
        self.messages.lock().unwrap().clear();
    }
}

#[cfg(test)]
impl BootstrapLogger for CapturingLogger {
    fn error(&amp;self, message: &amp;str) {
        self.log(LogLevel::Error, message);
    }

    fn warn(&amp;self, message: &amp;str) {
        self.log(LogLevel::Warn, message);
    }

    fn info(&amp;self, message: &amp;str) {
        self.log(LogLevel::Info, message);
    }

    fn debug(&amp;self, message: &amp;str) {
        self.log(LogLevel::Debug, message);
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="test-usage"><a class="header" href="#test-usage">Test Usage</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[test]
fn test_bootstrap_logging() {
    let logger = CapturingLogger::new();

    logger.info("Starting operation");
    logger.debug("Detailed diagnostic");
    logger.error("Operation failed");

    let messages = logger.messages();
    assert_eq!(messages.len(), 3);
    assert_eq!(messages[0].level, LogLevel::Info);
    assert_eq!(messages[0].message, "Starting operation");
    assert_eq!(messages[2].level, LogLevel::Error);
}
<span class="boring">}</span></code></pre></pre>
<h3 id="testing-with-tracing-subscriber"><a class="header" href="#testing-with-tracing-subscriber">Testing with tracing-subscriber</a></h3>
<p>For testing application-level logging:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tracing_subscriber::{fmt, EnvFilter};

#[test]
fn test_application_logging() {
    // Initialize test subscriber
    let subscriber = fmt()
        .with_test_writer()
        .with_env_filter(EnvFilter::new("debug"))
        .finish();

    tracing::subscriber::with_default(subscriber, || {
        // Test code that produces logs
        process_pipeline("test-pipeline");
    });
}
<span class="boring">}</span></code></pre></pre>
<h3 id="noop-logger-for-silent-tests"><a class="header" href="#noop-logger-for-silent-tests">NoOp Logger for Silent Tests</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[test]
fn test_without_logging() {
    let logger = NoOpLogger::new();

    // Run tests without any log output
    let result = bootstrap_with_logger(&amp;logger);

    assert!(result.is_ok());
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="common-patterns"><a class="header" href="#common-patterns">Common Patterns</a></h2>
<h3 id="requestresponse-logging"><a class="header" href="#requestresponse-logging">Request/Response Logging</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub async fn process_file(path: &amp;Path) -&gt; Result&lt;ProcessingResult&gt; {
    let request_id = Uuid::new_v4();

    info!(
        request_id = %request_id,
        path = %path.display(),
        "Processing file"
    );

    match do_processing(path).await {
        Ok(result) =&gt; {
            info!(
                request_id = %request_id,
                bytes_processed = result.bytes,
                duration_ms = result.duration.as_millis(),
                "File processed successfully"
            );
            Ok(result)
        }
        Err(e) =&gt; {
            error!(
                request_id = %request_id,
                error = %e,
                "File processing failed"
            );
            Err(e)
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="progress-logging"><a class="header" href="#progress-logging">Progress Logging</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub async fn process_chunks(chunks: Vec&lt;Chunk&gt;) -&gt; Result&lt;()&gt; {
    let total = chunks.len();

    info!(total_chunks = total, "Starting chunk processing");

    for (i, chunk) in chunks.iter().enumerate() {
        process_chunk(chunk).await?;

        // Log progress every 10%
        if (i + 1) % (total / 10).max(1) == 0 {
            let percent = ((i + 1) * 100) / total;
            info!(
                processed = i + 1,
                total = total,
                percent = percent,
                "Chunk processing progress"
            );
        }
    }

    info!(total_chunks = total, "Chunk processing completed");
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h3 id="error-context-logging"><a class="header" href="#error-context-logging">Error Context Logging</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub async fn execute_pipeline(id: &amp;PipelineId) -&gt; Result&lt;()&gt; {
    debug!(pipeline_id = %id, "Loading pipeline from database");

    let pipeline = repository.find_by_id(id).await
        .map_err(|e| {
            error!(
                pipeline_id = %id,
                error = %e,
                "Failed to load pipeline from database"
            );
            e
        })?;

    debug!(
        pipeline_id = %id,
        stage_count = pipeline.stages().len(),
        "Pipeline loaded successfully"
    );

    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h3 id="conditional-logging"><a class="header" href="#conditional-logging">Conditional Logging</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn process_with_validation(data: &amp;Data) -&gt; Result&lt;()&gt; {
    if let Err(e) = validate(data) {
        warn!(
            validation_error = %e,
            "Data validation failed, attempting recovery"
        );

        return recover_from_validation_error(data, e);
    }

    debug!("Data validation passed");
    process(data)
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="next-steps-22"><a class="header" href="#next-steps-22">Next Steps</a></h2>
<ul>
<li><strong><a href="implementation/observability.html">Observability Overview</a></strong>: Complete observability strategy</li>
<li><strong><a href="implementation/metrics.html">Metrics Collection</a></strong>: Prometheus metrics integration</li>
<li><strong><a href="implementation/../architecture/error-handling.html">Error Handling</a></strong>: Error handling patterns</li>
<li><strong><a href="implementation/../testing/unit-tests.html">Testing</a></strong>: Testing strategies and practices</li>
</ul>
<hr />
<h2 id="references-2"><a class="header" href="#references-2">References</a></h2>
<ul>
<li><a href="https://docs.rs/tracing">tracing Documentation</a></li>
<li><a href="https://docs.rs/tracing-subscriber">tracing-subscriber Documentation</a></li>
<li><a href="https://www.honeycomb.io/blog/structured-logging-and-your-team">Structured Logging Best Practices</a></li>
<li>Source: <code>bootstrap/src/logger.rs</code> (lines 1-292)</li>
<li>Source: <code>pipeline/src/infrastructure/logging/observability_service.rs</code> (lines 1-716)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="concurrency-model"><a class="header" href="#concurrency-model">Concurrency Model</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>This chapter provides a comprehensive overview of the concurrency model in the adaptive pipeline system. Learn how async/await, Tokio runtime, and concurrent patterns enable high-performance, scalable file processing.</p>
<hr />
<h2 id="table-of-contents-4"><a class="header" href="#table-of-contents-4">Table of Contents</a></h2>
<ul>
<li><a href="advanced/concurrency.html#overview">Overview</a></li>
<li><a href="advanced/concurrency.html#concurrency-architecture">Concurrency Architecture</a></li>
<li><a href="advanced/concurrency.html#asyncawait-model">Async/Await Model</a></li>
<li><a href="advanced/concurrency.html#tokio-runtime">Tokio Runtime</a></li>
<li><a href="advanced/concurrency.html#parallel-chunk-processing">Parallel Chunk Processing</a></li>
<li><a href="advanced/concurrency.html#concurrency-primitives">Concurrency Primitives</a></li>
<li><a href="advanced/concurrency.html#thread-pools-and-workers">Thread Pools and Workers</a></li>
<li><a href="advanced/concurrency.html#resource-management">Resource Management</a></li>
<li><a href="advanced/concurrency.html#concurrency-patterns">Concurrency Patterns</a></li>
<li><a href="advanced/concurrency.html#performance-considerations">Performance Considerations</a></li>
<li><a href="advanced/concurrency.html#best-practices">Best Practices</a></li>
<li><a href="advanced/concurrency.html#troubleshooting">Troubleshooting</a></li>
<li><a href="advanced/concurrency.html#next-steps">Next Steps</a></li>
</ul>
<hr />
<h2 id="overview-17"><a class="header" href="#overview-17">Overview</a></h2>
<p>The <strong>concurrency model</strong> enables the adaptive pipeline to process files efficiently through parallel processing, async I/O, and concurrent chunk handling. The system uses Rust's async/await with the Tokio runtime for high-performance, scalable concurrency.</p>
<h3 id="key-features-4"><a class="header" href="#key-features-4">Key Features</a></h3>
<ul>
<li><strong>Async/Await</strong>: Non-blocking asynchronous operations</li>
<li><strong>Tokio Runtime</strong>: Multi-threaded async runtime</li>
<li><strong>Parallel Processing</strong>: Concurrent chunk processing</li>
<li><strong>Worker Pools</strong>: Configurable thread pools</li>
<li><strong>Resource Management</strong>: Efficient resource allocation and cleanup</li>
<li><strong>Thread Safety</strong>: Safe concurrent access through Rust's type system</li>
</ul>
<h3 id="concurrency-stack"><a class="header" href="#concurrency-stack">Concurrency Stack</a></h3>
<pre><code class="language-text">┌─────────────────────────────────────────────────────────────┐
│                  Application Layer                          │
│  - Pipeline orchestration                                   │
│  - File processing coordination                             │
└─────────────────────────────────────────────────────────────┘
                         ↓ async
┌─────────────────────────────────────────────────────────────┐
│                   Tokio Runtime                             │
│  - Multi-threaded work-stealing scheduler                   │
│  - Async task execution                                     │
│  - I/O reactor                                              │
└─────────────────────────────────────────────────────────────┘
                         ↓
┌─────────────────────────────────────────────────────────────┐
│              Concurrency Primitives                         │
│  ┌─────────┬──────────┬──────────┬─────────────┐           │
│  │ Mutex   │ RwLock   │ Semaphore│  Channel    │           │
│  │ (Sync)  │ (Shared) │ (Limit)  │ (Message)   │           │
│  └─────────┴──────────┴──────────┴─────────────┘           │
└─────────────────────────────────────────────────────────────┘
                         ↓
┌─────────────────────────────────────────────────────────────┐
│                  Worker Threads                             │
│  - Chunk processing workers                                 │
│  - I/O workers                                              │
│  - Background tasks                                         │
└─────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="design-principles-4"><a class="header" href="#design-principles-4">Design Principles</a></h3>
<ol>
<li><strong>Async-First</strong>: All I/O operations are asynchronous</li>
<li><strong>Structured Concurrency</strong>: Clear task ownership and lifetimes</li>
<li><strong>Safe Sharing</strong>: Thread-safe sharing through Arc and sync primitives</li>
<li><strong>Resource Bounded</strong>: Limited resource usage with semaphores</li>
<li><strong>Zero-Cost Abstractions</strong>: Minimal overhead from async runtime</li>
</ol>
<hr />
<h2 id="concurrency-architecture"><a class="header" href="#concurrency-architecture">Concurrency Architecture</a></h2>
<p>The system uses a layered concurrency architecture with clear separation between sync and async code.</p>
<h3 id="architectural-layers-1"><a class="header" href="#architectural-layers-1">Architectural Layers</a></h3>
<pre><code class="language-text">┌─────────────────────────────────────────────────────────────┐
│ Async Layer (I/O Bound)                                     │
│  ┌──────────────────────────────────────────────────┐      │
│  │  File I/O Service (async)                        │      │
│  │  - tokio::fs file operations                     │      │
│  │  - Async read/write                              │      │
│  └──────────────────────────────────────────────────┘      │
│  ┌──────────────────────────────────────────────────┐      │
│  │  Pipeline Service (async orchestration)          │      │
│  │  - Async workflow coordination                   │      │
│  │  - Task spawning and management                  │      │
│  └──────────────────────────────────────────────────┘      │
└─────────────────────────────────────────────────────────────┘
                         ↓
┌─────────────────────────────────────────────────────────────┐
│ Sync Layer (CPU Bound)                                      │
│  ┌──────────────────────────────────────────────────┐      │
│  │  Compression Service (sync)                      │      │
│  │  - CPU-bound compression algorithms              │      │
│  │  - No async overhead                             │      │
│  └──────────────────────────────────────────────────┘      │
│  ┌──────────────────────────────────────────────────┐      │
│  │  Encryption Service (sync)                       │      │
│  │  - CPU-bound encryption algorithms               │      │
│  │  - No async overhead                             │      │
│  └──────────────────────────────────────────────────┘      │
└─────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="async-vs-sync-decision"><a class="header" href="#async-vs-sync-decision">Async vs Sync Decision</a></h3>
<p><strong>Async for:</strong></p>
<ul>
<li>File I/O (tokio::fs)</li>
<li>Network I/O</li>
<li>Database operations</li>
<li>Long-running waits</li>
</ul>
<p><strong>Sync for:</strong></p>
<ul>
<li>CPU-bound compression</li>
<li>CPU-bound encryption</li>
<li>Hash calculations</li>
<li>Pure computation</li>
</ul>
<hr />
<h2 id="asyncawait-model"><a class="header" href="#asyncawait-model">Async/Await Model</a></h2>
<p>The system uses Rust's async/await for non-blocking concurrency.</p>
<h3 id="async-functions"><a class="header" href="#async-functions">Async Functions</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Async function definition
async fn process_file(
    path: &amp;Path,
    chunk_size: ChunkSize,
) -&gt; Result&lt;Vec&lt;FileChunk&gt;, PipelineError&gt; {
    // Await async operations
    let chunks = read_file_chunks(path, chunk_size).await?;

    // Process chunks
    let results = process_chunks_parallel(chunks).await?;

    Ok(results)
}
<span class="boring">}</span></code></pre></pre>
<h3 id="awaiting-futures"><a class="header" href="#awaiting-futures">Awaiting Futures</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Sequential awaits
let chunks = service.read_file_chunks(path, chunk_size).await?;
let processed = process_chunks(chunks).await?;
service.write_file_chunks(output_path, processed).await?;

// Parallel awaits with join
use tokio::try_join;

let (chunks1, chunks2) = try_join!(
    service.read_file_chunks(path1, chunk_size),
    service.read_file_chunks(path2, chunk_size),
)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="async-traits"><a class="header" href="#async-traits">Async Traits</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use async_trait::async_trait;

#[async_trait]
pub trait FileIOService: Send + Sync {
    async fn read_file_chunks(
        &amp;self,
        path: &amp;Path,
        chunk_size: ChunkSize,
    ) -&gt; Result&lt;Vec&lt;FileChunk&gt;, PipelineError&gt;;

    async fn write_file_chunks(
        &amp;self,
        path: &amp;Path,
        chunks: Vec&lt;FileChunk&gt;,
    ) -&gt; Result&lt;(), PipelineError&gt;;
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="tokio-runtime"><a class="header" href="#tokio-runtime">Tokio Runtime</a></h2>
<p>The system uses Tokio's multi-threaded runtime for async execution.</p>
<h3 id="runtime-configuration"><a class="header" href="#runtime-configuration">Runtime Configuration</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tokio::runtime::Runtime;

// Multi-threaded runtime (default)
let runtime = Runtime::new()?;

// Custom configuration
let runtime = tokio::runtime::Builder::new_multi_thread()
    .worker_threads(8)              // 8 worker threads
    .thread_name("pipeline-worker")
    .thread_stack_size(3 * 1024 * 1024)  // 3 MB stack
    .enable_all()                   // Enable I/O and time drivers
    .build()?;

// Execute async work
runtime.block_on(async {
    process_file(path, chunk_size).await?;
});
<span class="boring">}</span></code></pre></pre>
<h3 id="runtime-selection"><a class="header" href="#runtime-selection">Runtime Selection</a></h3>
<pre><pre class="playground"><code class="language-rust">// Multi-threaded runtime (CPU-bound + I/O)
#[tokio::main]
async fn main() {
    // Automatically uses multi-threaded runtime
    process_pipeline().await;
}

// Current-thread runtime (testing, single-threaded)
#[tokio::main(flavor = "current_thread")]
async fn main() {
    // Single-threaded runtime
    process_pipeline().await;
}</code></pre></pre>
<h3 id="work-stealing-scheduler"><a class="header" href="#work-stealing-scheduler">Work-Stealing Scheduler</a></h3>
<p>Tokio uses a work-stealing scheduler for load balancing:</p>
<pre><code class="language-text">Thread 1: [Task A] [Task B] ────────&gt; (idle, steals Task D)
Thread 2: [Task C] [Task D] [Task E]  (busy)
Thread 3: [Task F] ────────────────&gt;  (idle, steals Task E)
</code></pre>
<hr />
<h2 id="parallel-chunk-processing-4"><a class="header" href="#parallel-chunk-processing-4">Parallel Chunk Processing</a></h2>
<p>Chunks are processed concurrently for maximum throughput.</p>
<h3 id="parallel-processing-with-try_join_all"><a class="header" href="#parallel-processing-with-try_join_all">Parallel Processing with try_join_all</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use futures::future::try_join_all;

async fn process_chunks_parallel(
    chunks: Vec&lt;FileChunk&gt;,
) -&gt; Result&lt;Vec&lt;FileChunk&gt;, PipelineError&gt; {
    // Spawn tasks for each chunk
    let futures = chunks.into_iter().map(|chunk| {
        tokio::spawn(async move {
            process_chunk(chunk).await
        })
    });

    // Wait for all to complete
    let results = try_join_all(futures).await?;

    // Collect results
    Ok(results.into_iter().collect::&lt;Result&lt;Vec&lt;_&gt;, _&gt;&gt;()?)
}
<span class="boring">}</span></code></pre></pre>
<h3 id="bounded-parallelism"><a class="header" href="#bounded-parallelism">Bounded Parallelism</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tokio::sync::Semaphore;
use std::sync::Arc;

async fn process_with_limit(
    chunks: Vec&lt;FileChunk&gt;,
    max_parallel: usize,
) -&gt; Result&lt;Vec&lt;FileChunk&gt;, PipelineError&gt; {
    let semaphore = Arc::new(Semaphore::new(max_parallel));
    let futures = chunks.into_iter().map(|chunk| {
        let permit = semaphore.clone();
        async move {
            let _guard = permit.acquire().await.unwrap();
            process_chunk(chunk).await
        }
    });

    try_join_all(futures).await
}
<span class="boring">}</span></code></pre></pre>
<h3 id="pipeline-parallelism"><a class="header" href="#pipeline-parallelism">Pipeline Parallelism</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Stage 1: Read chunks
let chunks = read_chunks_stream(path, chunk_size);

// Stage 2: Process chunks (parallel)
let processed = chunks
    .map(|chunk| async move {
        tokio::spawn(async move {
            compress_chunk(chunk).await
        }).await
    })
    .buffer_unordered(8);  // Up to 8 chunks in flight

// Stage 3: Write chunks
write_chunks_stream(processed).await?;
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="concurrency-primitives"><a class="header" href="#concurrency-primitives">Concurrency Primitives</a></h2>
<p>The system uses Tokio's async-aware concurrency primitives.</p>
<h3 id="async-mutex"><a class="header" href="#async-mutex">Async Mutex</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tokio::sync::Mutex;
use std::sync::Arc;

let shared_state = Arc::new(Mutex::new(HashMap::new()));

// Acquire lock asynchronously
let mut state = shared_state.lock().await;
state.insert(key, value);
// Lock automatically released when dropped
<span class="boring">}</span></code></pre></pre>
<h3 id="async-rwlock"><a class="header" href="#async-rwlock">Async RwLock</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tokio::sync::RwLock;
use std::sync::Arc;

let config = Arc::new(RwLock::new(PipelineConfig::default()));

// Multiple readers
let config_read = config.read().await;
let chunk_size = config_read.chunk_size;

// Single writer
let mut config_write = config.write().await;
config_write.chunk_size = ChunkSize::from_mb(16)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="channels-mpsc"><a class="header" href="#channels-mpsc">Channels (mpsc)</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tokio::sync::mpsc;

// Create channel
let (tx, mut rx) = mpsc::channel::&lt;FileChunk&gt;(100);

// Send chunks
tokio::spawn(async move {
    for chunk in chunks {
        tx.send(chunk).await.unwrap();
    }
});

// Receive chunks
while let Some(chunk) = rx.recv().await {
    process_chunk(chunk).await?;
}
<span class="boring">}</span></code></pre></pre>
<h3 id="semaphores"><a class="header" href="#semaphores">Semaphores</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tokio::sync::Semaphore;

// Limit concurrent operations
let semaphore = Arc::new(Semaphore::new(4));  // Max 4 concurrent

for chunk in chunks {
    let permit = semaphore.clone().acquire_owned().await.unwrap();
    tokio::spawn(async move {
        let result = process_chunk(chunk).await;
        drop(permit);  // Release permit
        result
    });
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="thread-pools-and-workers"><a class="header" href="#thread-pools-and-workers">Thread Pools and Workers</a></h2>
<p>Worker pools manage concurrent task execution.</p>
<h3 id="worker-pool-configuration"><a class="header" href="#worker-pool-configuration">Worker Pool Configuration</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct WorkerPool {
    max_workers: usize,
    semaphore: Arc&lt;Semaphore&gt;,
}

impl WorkerPool {
    pub fn new(max_workers: usize) -&gt; Self {
        Self {
            max_workers,
            semaphore: Arc::new(Semaphore::new(max_workers)),
        }
    }

    pub async fn execute&lt;F, T&gt;(&amp;self, task: F) -&gt; Result&lt;T, PipelineError&gt;
    where
        F: Future&lt;Output = Result&lt;T, PipelineError&gt;&gt; + Send + 'static,
        T: Send + 'static,
    {
        let _permit = self.semaphore.acquire().await.unwrap();
        tokio::spawn(task).await.unwrap()
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="adaptive-worker-count"><a class="header" href="#adaptive-worker-count">Adaptive Worker Count</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn optimal_worker_count() -&gt; usize {
    let cpu_count = num_cpus::get();

    // For I/O-bound: 2x CPU count
    // For CPU-bound: 1x CPU count
    // For mixed: 1.5x CPU count
    (cpu_count as f64 * 1.5) as usize
}

let worker_pool = WorkerPool::new(optimal_worker_count());
<span class="boring">}</span></code></pre></pre>
<p>For detailed worker pool implementation, see <a href="advanced/thread-pooling.html">Thread Pooling</a>.</p>
<hr />
<h2 id="resource-management-1"><a class="header" href="#resource-management-1">Resource Management</a></h2>
<p>Efficient resource management is critical for concurrent systems.</p>
<h3 id="resource-limits"><a class="header" href="#resource-limits">Resource Limits</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct ResourceLimits {
    max_memory: usize,
    max_file_handles: usize,
    max_concurrent_tasks: usize,
}

impl ResourceLimits {
    pub fn calculate_max_parallel_chunks(&amp;self, chunk_size: ChunkSize) -&gt; usize {
        let memory_limit = self.max_memory / chunk_size.bytes();
        let task_limit = self.max_concurrent_tasks;

        memory_limit.min(task_limit)
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="resource-tracking"><a class="header" href="#resource-tracking">Resource Tracking</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::sync::atomic::{AtomicUsize, Ordering};

pub struct ResourceTracker {
    active_tasks: AtomicUsize,
    memory_used: AtomicUsize,
}

impl ResourceTracker {
    pub fn acquire_task(&amp;self) -&gt; TaskGuard {
        self.active_tasks.fetch_add(1, Ordering::SeqCst);
        TaskGuard { tracker: self }
    }
}

pub struct TaskGuard&lt;'a&gt; {
    tracker: &amp;'a ResourceTracker,
}

impl Drop for TaskGuard&lt;'_&gt; {
    fn drop(&amp;mut self) {
        self.tracker.active_tasks.fetch_sub(1, Ordering::SeqCst);
    }
}
<span class="boring">}</span></code></pre></pre>
<p>For detailed resource management, see <a href="advanced/resources.html">Resource Management</a>.</p>
<hr />
<h2 id="concurrency-patterns"><a class="header" href="#concurrency-patterns">Concurrency Patterns</a></h2>
<p>Common concurrency patterns used in the pipeline.</p>
<h3 id="pattern-1-fan-outfan-in"><a class="header" href="#pattern-1-fan-outfan-in">Pattern 1: Fan-Out/Fan-In</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Fan-out: Distribute work to multiple workers
let futures = chunks.into_iter().map(|chunk| {
    tokio::spawn(async move {
        process_chunk(chunk).await
    })
});

// Fan-in: Collect results
let results = try_join_all(futures).await?;
<span class="boring">}</span></code></pre></pre>
<h3 id="pattern-2-pipeline-pattern"><a class="header" href="#pattern-2-pipeline-pattern">Pattern 2: Pipeline Pattern</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tokio_stream::StreamExt;

// Stage 1 → Stage 2 → Stage 3
let result = read_stream(path)
    .map(|chunk| compress_chunk(chunk))
    .buffer_unordered(8)
    .map(|chunk| encrypt_chunk(chunk))
    .buffer_unordered(8)
    .collect::&lt;Vec&lt;_&gt;&gt;()
    .await;
<span class="boring">}</span></code></pre></pre>
<h3 id="pattern-3-worker-pool-pattern"><a class="header" href="#pattern-3-worker-pool-pattern">Pattern 3: Worker Pool Pattern</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let pool = WorkerPool::new(8);

for chunk in chunks {
    pool.execute(async move {
        process_chunk(chunk).await
    }).await?;
}
<span class="boring">}</span></code></pre></pre>
<h3 id="pattern-4-rate-limiting"><a class="header" href="#pattern-4-rate-limiting">Pattern 4: Rate Limiting</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tokio::time::{interval, Duration};

let mut interval = interval(Duration::from_millis(100));

for chunk in chunks {
    interval.tick().await;  // Rate limit: 10 chunks/sec
    process_chunk(chunk).await?;
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="performance-considerations-4"><a class="header" href="#performance-considerations-4">Performance Considerations</a></h2>
<h3 id="tokio-task-overhead"><a class="header" href="#tokio-task-overhead">Tokio Task Overhead</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Cost</th><th>Notes</th></tr></thead><tbody>
<tr><td><strong>Spawn task</strong></td><td>~1-2 μs</td><td>Very lightweight</td></tr>
<tr><td><strong>Context switch</strong></td><td>~100 ns</td><td>Work-stealing scheduler</td></tr>
<tr><td><strong>Mutex lock</strong></td><td>~50 ns</td><td>Uncontended case</td></tr>
<tr><td><strong>Channel send</strong></td><td>~100-200 ns</td><td>Depends on channel type</td></tr>
</tbody></table>
</div>
<h3 id="choosing-concurrency-level"><a class="header" href="#choosing-concurrency-level">Choosing Concurrency Level</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn optimal_concurrency(
    file_size: u64,
    chunk_size: ChunkSize,
    available_memory: usize,
) -&gt; usize {
    let num_chunks = (file_size / chunk_size.bytes() as u64) as usize;
    let memory_limit = available_memory / chunk_size.bytes();
    let cpu_limit = num_cpus::get() * 2;

    num_chunks.min(memory_limit).min(cpu_limit)
}
<span class="boring">}</span></code></pre></pre>
<h3 id="avoiding-contention"><a class="header" href="#avoiding-contention">Avoiding Contention</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ❌ Bad: High contention
let counter = Arc::new(Mutex::new(0));
for _ in 0..1000 {
    let c = counter.clone();
    tokio::spawn(async move {
        *c.lock().await += 1;  // Lock contention!
    });
}

// ✅ Good: Reduce contention
let counter = Arc::new(AtomicUsize::new(0));
for _ in 0..1000 {
    let c = counter.clone();
    tokio::spawn(async move {
        c.fetch_add(1, Ordering::Relaxed);  // Lock-free!
    });
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="best-practices-13"><a class="header" href="#best-practices-13">Best Practices</a></h2>
<h3 id="1-use-async-for-io-sync-for-cpu"><a class="header" href="#1-use-async-for-io-sync-for-cpu">1. Use Async for I/O, Sync for CPU</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Async I/O
async fn read_file(path: &amp;Path) -&gt; Result&lt;Vec&lt;u8&gt;, Error&gt; {
    tokio::fs::read(path).await
}

// ✅ Good: Sync CPU-bound
fn compress_data(data: &amp;[u8]) -&gt; Result&lt;Vec&lt;u8&gt;, Error&gt; {
    brotli::compress(data)  // Sync, CPU-bound
}

// ❌ Bad: Async for CPU-bound
async fn compress_data_async(data: &amp;[u8]) -&gt; Result&lt;Vec&lt;u8&gt;, Error&gt; {
    // Unnecessary async overhead
    brotli::compress(data)
}
<span class="boring">}</span></code></pre></pre>
<h3 id="2-spawn-blocking-for-sync-code"><a class="header" href="#2-spawn-blocking-for-sync-code">2. Spawn Blocking for Sync Code</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Spawn blocking task
async fn process_chunk(chunk: FileChunk) -&gt; Result&lt;FileChunk, Error&gt; {
    tokio::task::spawn_blocking(move || {
        // CPU-bound compression in blocking thread
        compress_sync(chunk)
    }).await?
}
<span class="boring">}</span></code></pre></pre>
<h3 id="3-limit-concurrent-tasks"><a class="header" href="#3-limit-concurrent-tasks">3. Limit Concurrent Tasks</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Bounded parallelism
let semaphore = Arc::new(Semaphore::new(max_concurrent));
for chunk in chunks {
    let permit = semaphore.clone();
    tokio::spawn(async move {
        let _guard = permit.acquire().await.unwrap();
        process_chunk(chunk).await
    });
}

// ❌ Bad: Unbounded parallelism
for chunk in chunks {
    tokio::spawn(async move {
        process_chunk(chunk).await  // May spawn thousands of tasks!
    });
}
<span class="boring">}</span></code></pre></pre>
<h3 id="4-use-channels-for-communication"><a class="header" href="#4-use-channels-for-communication">4. Use Channels for Communication</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Channel communication
let (tx, mut rx) = mpsc::channel(100);

tokio::spawn(async move {
    while let Some(chunk) = rx.recv().await {
        process_chunk(chunk).await;
    }
});

tx.send(chunk).await?;
<span class="boring">}</span></code></pre></pre>
<h3 id="5-handle-errors-properly"><a class="header" href="#5-handle-errors-properly">5. Handle Errors Properly</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Proper error handling
let results: Result&lt;Vec&lt;_&gt;, _&gt; = try_join_all(futures).await;
match results {
    Ok(chunks) =&gt; { /* success */ },
    Err(e) =&gt; {
        error!("Processing failed: {}", e);
        // Cleanup resources
        return Err(e);
    }
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="troubleshooting-5"><a class="header" href="#troubleshooting-5">Troubleshooting</a></h2>
<h3 id="issue-1-too-many-tokio-tasks"><a class="header" href="#issue-1-too-many-tokio-tasks">Issue 1: Too Many Tokio Tasks</a></h3>
<p><strong>Symptom:</strong></p>
<pre><code class="language-text">thread 'tokio-runtime-worker' stack overflow
</code></pre>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 1. Limit concurrent tasks
let semaphore = Arc::new(Semaphore::new(100));

// 2. Use buffer_unordered
stream.buffer_unordered(10).collect().await

// 3. Increase stack size
tokio::runtime::Builder::new_multi_thread()
    .thread_stack_size(4 * 1024 * 1024)  // 4 MB
    .build()?;
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-2-mutex-deadlock"><a class="header" href="#issue-2-mutex-deadlock">Issue 2: Mutex Deadlock</a></h3>
<p><strong>Symptom:</strong> Tasks hang indefinitely.</p>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 1. Always acquire locks in same order
async fn transfer(from: &amp;Mutex&lt;u64&gt;, to: &amp;Mutex&lt;u64&gt;, amount: u64) {
    let (first, second) = if ptr::eq(from, to) {
        panic!("Same account");
    } else if (from as *const _ as usize) &lt; (to as *const _ as usize) {
        (from, to)
    } else {
        (to, from)
    };

    let mut a = first.lock().await;
    let mut b = second.lock().await;
    // Transfer logic
}

// 2. Use try_lock with timeout
tokio::time::timeout(Duration::from_secs(5), mutex.lock()).await??;
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-3-channel-backpressure"><a class="header" href="#issue-3-channel-backpressure">Issue 3: Channel Backpressure</a></h3>
<p><strong>Symptom:</strong></p>
<pre><code class="language-text">Producer overwhelms consumer
</code></pre>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 1. Bounded channel
let (tx, rx) = mpsc::channel::&lt;FileChunk&gt;(100);  // Max 100 in flight

// 2. Apply backpressure
match tx.try_send(chunk) {
    Ok(()) =&gt; { /* sent */ },
    Err(TrySendError::Full(_)) =&gt; {
        // Wait and retry
        tokio::time::sleep(Duration::from_millis(10)).await;
    },
    Err(e) =&gt; return Err(e.into()),
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="next-steps-23"><a class="header" href="#next-steps-23">Next Steps</a></h2>
<p>After understanding the concurrency model, explore specific implementations:</p>
<h3 id="related-advanced-topics"><a class="header" href="#related-advanced-topics">Related Advanced Topics</a></h3>
<ol>
<li><strong><a href="advanced/thread-pooling.html">Thread Pooling</a></strong>: Worker pool implementation and optimization</li>
<li><strong><a href="advanced/resources.html">Resource Management</a></strong>: Memory and resource tracking</li>
</ol>
<h3 id="related-topics-4"><a class="header" href="#related-topics-4">Related Topics</a></h3>
<ul>
<li><strong><a href="advanced/performance.html">Performance Optimization</a></strong>: Optimizing concurrent code</li>
<li><strong><a href="advanced/../implementation/file-io.html">File I/O</a></strong>: Async file operations</li>
</ul>
<hr />
<h2 id="summary-4"><a class="header" href="#summary-4">Summary</a></h2>
<p><strong>Key Takeaways:</strong></p>
<ol>
<li><strong>Async/Await</strong> provides non-blocking concurrency for I/O operations</li>
<li><strong>Tokio Runtime</strong> uses work-stealing for efficient task scheduling</li>
<li><strong>Parallel Processing</strong> enables concurrent chunk processing for throughput</li>
<li><strong>Concurrency Primitives</strong> (Mutex, RwLock, Semaphore) enable safe sharing</li>
<li><strong>Worker Pools</strong> manage bounded concurrent task execution</li>
<li><strong>Resource Management</strong> tracks and limits resource usage</li>
<li><strong>Patterns</strong> (fan-out/fan-in, pipeline, worker pool) structure concurrent code</li>
</ol>
<p><strong>Architecture File References:</strong></p>
<ul>
<li><strong>Pipeline Service:</strong> <code>pipeline/src/application/services/pipeline_service.rs:189</code></li>
<li><strong>File Processor:</strong> <code>pipeline/src/application/services/file_processor_service.rs:1</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="thread-pooling"><a class="header" href="#thread-pooling">Thread Pooling</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>This chapter explores the pipeline's thread pool architecture, configuration strategies, and tuning guidelines for optimal performance across different workload types.</p>
<h2 id="overview-18"><a class="header" href="#overview-18">Overview</a></h2>
<p>The pipeline employs a <strong>dual thread pool</strong> architecture that separates async I/O operations from sync CPU-bound computations:</p>
<ul>
<li><strong>Tokio Runtime</strong>: Handles async I/O operations (file reads/writes, database queries, network)</li>
<li><strong>Rayon Thread Pools</strong>: Handles parallel CPU-bound operations (compression, encryption, hashing)</li>
</ul>
<p>This separation prevents CPU-intensive work from blocking async tasks and ensures optimal resource utilization.</p>
<h2 id="thread-pool-architecture"><a class="header" href="#thread-pool-architecture">Thread Pool Architecture</a></h2>
<h3 id="dual-pool-design"><a class="header" href="#dual-pool-design">Dual Pool Design</a></h3>
<pre><code class="language-text">┌─────────────────────────────────────────────────────────────┐
│                    Async Layer (Tokio)                      │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  Multi-threaded Tokio Runtime                        │   │
│  │  - Work-stealing scheduler                           │   │
│  │  - Async I/O operations                              │   │
│  │  - Task coordination                                 │   │
│  │  - Event loop management                             │   │
│  └──────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
                                │
                                │ spawn_blocking()
                                ▼
┌─────────────────────────────────────────────────────────────┐
│                  Sync Layer (Rayon Pools)                   │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  CPU-Bound Pool (rayon-cpu-{N})                      │   │
│  │  - Compression operations                            │   │
│  │  - Encryption/decryption                             │   │
│  │  - Checksum calculation                              │   │
│  │  - Complex transformations                           │   │
│  └──────────────────────────────────────────────────────┘   │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  Mixed Workload Pool (rayon-mixed-{N})               │   │
│  │  - File processing with transformations              │   │
│  │  - Database operations with calculations             │   │
│  │  - Network operations with data processing           │   │
│  └──────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="thread-naming-convention"><a class="header" href="#thread-naming-convention">Thread Naming Convention</a></h3>
<p>All worker threads are named for debugging and profiling:</p>
<ul>
<li><strong>Tokio threads</strong>: Default Tokio naming (<code>tokio-runtime-worker-{N}</code>)</li>
<li><strong>Rayon CPU pool</strong>: <code>rayon-cpu-{N}</code> where N is the thread index</li>
<li><strong>Rayon mixed pool</strong>: <code>rayon-mixed-{N}</code> where N is the thread index</li>
</ul>
<p>This naming convention enables:</p>
<ul>
<li>Easy identification in profilers (e.g., <code>perf</code>, <code>flamegraph</code>)</li>
<li>Clear thread attribution in stack traces</li>
<li>Simplified debugging of concurrency issues</li>
</ul>
<h2 id="tokio-runtime-configuration"><a class="header" href="#tokio-runtime-configuration">Tokio Runtime Configuration</a></h2>
<h3 id="default-configuration-1"><a class="header" href="#default-configuration-1">Default Configuration</a></h3>
<p>The pipeline uses Tokio's default multi-threaded runtime:</p>
<pre><pre class="playground"><code class="language-rust">#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    // Tokio runtime initialized with default settings
    run_pipeline().await
}</code></pre></pre>
<p><strong>Default Tokio Settings:</strong></p>
<ul>
<li><strong>Worker threads</strong>: Equal to number of CPU cores (via <code>std::thread::available_parallelism()</code>)</li>
<li><strong>Thread stack size</strong>: 2 MiB per thread</li>
<li><strong>Work-stealing</strong>: Enabled (automatic task balancing)</li>
<li><strong>I/O driver</strong>: Enabled (async file I/O, network)</li>
<li><strong>Time driver</strong>: Enabled (async timers, delays)</li>
</ul>
<h3 id="custom-runtime-configuration"><a class="header" href="#custom-runtime-configuration">Custom Runtime Configuration</a></h3>
<p>For advanced use cases, you can configure the Tokio runtime explicitly:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tokio::runtime::Runtime;

fn create_custom_runtime() -&gt; Runtime {
    Runtime::builder()
        .worker_threads(8)          // Override thread count
        .thread_stack_size(3 * 1024 * 1024) // 3 MiB stack
        .thread_name("pipeline-async")
        .enable_all()               // Enable I/O and time drivers
        .build()
        .expect("Failed to create Tokio runtime")
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Configuration Parameters:</strong></p>
<ul>
<li><code>worker_threads(usize)</code>: Number of worker threads (defaults to CPU cores)</li>
<li><code>thread_stack_size(usize)</code>: Stack size per thread in bytes (default: 2 MiB)</li>
<li><code>thread_name(impl Into&lt;String&gt;)</code>: Thread name prefix for debugging</li>
<li><code>enable_all()</code>: Enable both I/O and time drivers</li>
</ul>
<h2 id="rayon-thread-pool-configuration"><a class="header" href="#rayon-thread-pool-configuration">Rayon Thread Pool Configuration</a></h2>
<h3 id="global-pool-manager"><a class="header" href="#global-pool-manager">Global Pool Manager</a></h3>
<p>The pipeline uses a global <code>RAYON_POOLS</code> manager with two specialized pools:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline::infrastructure::config::rayon_config::RAYON_POOLS;

// Access CPU-bound pool
let cpu_pool = RAYON_POOLS.cpu_bound_pool();

// Access mixed workload pool
let mixed_pool = RAYON_POOLS.mixed_workload_pool();
<span class="boring">}</span></code></pre></pre>
<p><strong>Implementation</strong> (<code>pipeline/src/infrastructure/config/rayon_config.rs</code>):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct RayonPoolManager {
    cpu_bound_pool: Arc&lt;rayon::ThreadPool&gt;,
    mixed_workload_pool: Arc&lt;rayon::ThreadPool&gt;,
}

impl RayonPoolManager {
    pub fn new() -&gt; Result&lt;Self, PipelineError&gt; {
        let available_cores = std::thread::available_parallelism()
            .map(|n| n.get())
            .unwrap_or(WorkerCount::DEFAULT_WORKERS);

        // CPU-bound pool: Use optimal worker count for CPU-intensive ops
        let cpu_worker_count = WorkerCount::optimal_for_processing_type(
            100 * 1024 * 1024,  // Assume 100MB default file size
            available_cores,
            true,               // CPU-intensive = true
        );

        let cpu_bound_pool = rayon::ThreadPoolBuilder::new()
            .num_threads(cpu_worker_count.count())
            .thread_name(|i| format!("rayon-cpu-{}", i))
            .build()?;

        // Mixed workload pool: Use fewer threads to avoid contention
        let mixed_worker_count = (available_cores / 2).max(WorkerCount::MIN_WORKERS);

        let mixed_workload_pool = rayon::ThreadPoolBuilder::new()
            .num_threads(mixed_worker_count)
            .thread_name(|i| format!("rayon-mixed-{}", i))
            .build()?;

        Ok(Self {
            cpu_bound_pool: Arc::new(cpu_bound_pool),
            mixed_workload_pool: Arc::new(mixed_workload_pool),
        })
    }
}

// Global static instance
pub static RAYON_POOLS: std::sync::LazyLock&lt;RayonPoolManager&gt; =
    std::sync::LazyLock::new(|| RayonPoolManager::new()
        .expect("Failed to initialize Rayon pools"));
<span class="boring">}</span></code></pre></pre>
<h3 id="cpu-bound-pool"><a class="header" href="#cpu-bound-pool">CPU-Bound Pool</a></h3>
<p><strong>Purpose</strong>: Maximize throughput for CPU-intensive operations</p>
<p><strong>Workload Types:</strong></p>
<ul>
<li>Data compression (Brotli, LZ4, Zstandard)</li>
<li>Data encryption/decryption (AES-256-GCM, ChaCha20-Poly1305)</li>
<li>Checksum calculation (SHA-256, BLAKE3)</li>
<li>Complex data transformations</li>
</ul>
<p><strong>Thread Count Strategy:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Uses WorkerCount::optimal_for_processing_type()
// For CPU-intensive work, allocates up to available_cores threads
let cpu_worker_count = WorkerCount::optimal_for_processing_type(
    file_size,
    available_cores,
    true  // CPU-intensive
);
<span class="boring">}</span></code></pre></pre>
<p><strong>Typical Thread Counts</strong> (on 8-core system):</p>
<ul>
<li>Small files (&lt; 50 MB): 6-14 workers (aggressive parallelism)</li>
<li>Medium files (50-500 MB): 5-12 workers (balanced approach)</li>
<li>Large files (&gt; 500 MB): 8-12 workers (moderate parallelism)</li>
<li>Huge files (&gt; 2 GB): 3-6 workers (conservative, avoid overhead)</li>
</ul>
<h3 id="mixed-workload-pool"><a class="header" href="#mixed-workload-pool">Mixed Workload Pool</a></h3>
<p><strong>Purpose</strong>: Handle operations with both CPU and I/O components</p>
<p><strong>Workload Types:</strong></p>
<ul>
<li>File processing with transformations</li>
<li>Database operations with calculations</li>
<li>Network operations with data processing</li>
</ul>
<p><strong>Thread Count Strategy:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Uses half the cores to avoid contention with I/O operations
let mixed_worker_count = (available_cores / 2).max(WorkerCount::MIN_WORKERS);
<span class="boring">}</span></code></pre></pre>
<p><strong>Typical Thread Counts</strong> (on 8-core system):</p>
<ul>
<li>4 threads (half of 8 cores)</li>
<li>Minimum: 1 thread (WorkerCount::MIN_WORKERS)</li>
<li>Maximum: 16 threads (half of MAX_WORKERS = 32)</li>
</ul>
<h2 id="the-spawn_blocking-pattern"><a class="header" href="#the-spawn_blocking-pattern">The spawn_blocking Pattern</a></h2>
<h3 id="purpose-4"><a class="header" href="#purpose-4">Purpose</a></h3>
<p><code>tokio::task::spawn_blocking</code> bridges async and sync code by running synchronous CPU-bound operations on a dedicated blocking thread pool <strong>without blocking the Tokio async runtime</strong>.</p>
<h3 id="when-to-use-spawn_blocking"><a class="header" href="#when-to-use-spawn_blocking">When to Use spawn_blocking</a></h3>
<p><strong>Always use for:</strong></p>
<ul>
<li>✅ CPU-intensive operations (compression, encryption, hashing)</li>
<li>✅ Blocking I/O that can't be made async</li>
<li>✅ Long-running computations (&gt; 10-100 μs)</li>
<li>✅ Sync library calls that may block</li>
</ul>
<p><strong>Never use for:</strong></p>
<ul>
<li>❌ Quick operations (&lt; 10 μs)</li>
<li>❌ Already async operations</li>
<li>❌ Pure data transformations (use inline instead)</li>
</ul>
<h3 id="usage-pattern-1"><a class="header" href="#usage-pattern-1">Usage Pattern</a></h3>
<p><strong>Basic spawn_blocking:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tokio::task;

async fn compress_chunk_async(
    chunk: FileChunk,
    config: &amp;CompressionConfig,
) -&gt; Result&lt;FileChunk, PipelineError&gt; {
    let config = config.clone();

    // Move CPU-bound work to blocking thread pool
    task::spawn_blocking(move || {
        // This runs on a dedicated blocking thread
        compress_chunk_sync(chunk, &amp;config)
    })
    .await
    .map_err(|e| PipelineError::InternalError(format!("Task join error: {}", e)))?
}

fn compress_chunk_sync(
    chunk: FileChunk,
    config: &amp;CompressionConfig,
) -&gt; Result&lt;FileChunk, PipelineError&gt; {
    // Expensive CPU-bound operation
    // This will NOT block the Tokio async runtime
    let compressed_data = brotli::compress(&amp;chunk.data, config.level)?;
    Ok(FileChunk::new(compressed_data))
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Rayon inside spawn_blocking</strong> (recommended for batch parallelism):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn compress_chunks_parallel(
    chunks: Vec&lt;FileChunk&gt;,
    config: &amp;CompressionConfig,
) -&gt; Result&lt;Vec&lt;FileChunk&gt;, PipelineError&gt; {
    use rayon::prelude::*;

    let config = config.clone();

    // Entire Rayon batch runs on blocking thread pool
    tokio::task::spawn_blocking(move || {
        // Use CPU-bound pool for compression
        RAYON_POOLS.cpu_bound_pool().install(|| {
            chunks
                .into_par_iter()
                .map(|chunk| compress_chunk_sync(chunk, &amp;config))
                .collect::&lt;Result&lt;Vec&lt;_&gt;, _&gt;&gt;()
        })
    })
    .await
    .map_err(|e| PipelineError::InternalError(format!("Task join error: {}", e)))?
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Key Points:</strong></p>
<ul>
<li><code>spawn_blocking</code> returns a <code>JoinHandle&lt;T&gt;</code></li>
<li>Must <code>.await</code> the handle to get the result</li>
<li>Errors are wrapped in <code>JoinError</code> (handle with <code>map_err</code>)</li>
<li>Data must be <code>Send + 'static</code> (use <code>clone()</code> or <code>move</code>)</li>
</ul>
<h3 id="spawn_blocking-thread-pool"><a class="header" href="#spawn_blocking-thread-pool">spawn_blocking Thread Pool</a></h3>
<p>Tokio maintains a <strong>separate blocking thread pool</strong> for <code>spawn_blocking</code>:</p>
<ul>
<li><strong>Default size</strong>: 512 threads (very large to handle many blocking operations)</li>
<li><strong>Thread creation</strong>: On-demand (lazy initialization)</li>
<li><strong>Thread lifetime</strong>: Threads terminate after being idle for 10 seconds</li>
<li><strong>Stack size</strong>: 2 MiB per thread (same as Tokio async threads)</li>
</ul>
<p><strong>Why 512 threads?</strong></p>
<ul>
<li>Blocking operations may wait indefinitely (file I/O, database queries)</li>
<li>Need many threads to prevent starvation</li>
<li>Threads are cheap (created on-demand, destroyed when idle)</li>
</ul>
<h2 id="worker-count-optimization"><a class="header" href="#worker-count-optimization">Worker Count Optimization</a></h2>
<h3 id="workercount-value-object"><a class="header" href="#workercount-value-object">WorkerCount Value Object</a></h3>
<p>The pipeline uses a sophisticated <code>WorkerCount</code> value object for adaptive thread allocation:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::value_objects::WorkerCount;

// Optimal for file size (empirically validated)
let workers = WorkerCount::optimal_for_file_size(file_size);

// Optimal for file size + system resources
let workers = WorkerCount::optimal_for_file_and_system(file_size, available_cores);

// Optimal for processing type (CPU-intensive vs I/O-intensive)
let workers = WorkerCount::optimal_for_processing_type(
    file_size,
    available_cores,
    is_cpu_intensive,
);
<span class="boring">}</span></code></pre></pre>
<h3 id="optimization-strategies"><a class="header" href="#optimization-strategies">Optimization Strategies</a></h3>
<p><strong>Empirically Validated</strong> (based on benchmarks):</p>
<div class="table-wrapper"><table><thead><tr><th>File Size</th><th>Worker Count</th><th>Strategy</th><th>Performance Gain</th></tr></thead><tbody>
<tr><td>&lt; 1 MB (tiny)</td><td>1-2</td><td>Minimal parallelism</td><td>N/A</td></tr>
<tr><td>1-50 MB (small)</td><td>6-14</td><td>Aggressive parallelism</td><td>+102% (5 MB)</td></tr>
<tr><td>50-500 MB (medium)</td><td>5-12</td><td>Balanced approach</td><td>+70% (50 MB)</td></tr>
<tr><td>500 MB-2 GB (large)</td><td>8-12</td><td>Moderate parallelism</td><td>Balanced</td></tr>
<tr><td>&gt; 2 GB (huge)</td><td>3-6</td><td>Conservative strategy</td><td>+76% (2 GB)</td></tr>
</tbody></table>
</div>
<p><strong>Why these strategies?</strong></p>
<ul>
<li><strong>Small files</strong>: High parallelism amortizes task overhead quickly</li>
<li><strong>Medium files</strong>: Balanced approach for consistent performance</li>
<li><strong>Large files</strong>: Moderate parallelism to manage memory and coordination overhead</li>
<li><strong>Huge files</strong>: Conservative to avoid excessive memory pressure and thread contention</li>
</ul>
<h3 id="configuration-options"><a class="header" href="#configuration-options">Configuration Options</a></h3>
<p><strong>Via CLI:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use bootstrap::config::AppConfig;

let config = AppConfig::builder()
    .app_name("pipeline")
    .worker_threads(8)  // Override automatic detection
    .build();
<span class="boring">}</span></code></pre></pre>
<p><strong>Via Environment Variable:</strong></p>
<pre><code class="language-bash">export ADAPIPE_WORKER_COUNT=8
./pipeline process input.txt
</code></pre>
<p><strong>Programmatic:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let worker_count = config.worker_threads()
    .map(WorkerCount::new)
    .unwrap_or_else(|| WorkerCount::default_for_system());
<span class="boring">}</span></code></pre></pre>
<h2 id="tuning-guidelines"><a class="header" href="#tuning-guidelines">Tuning Guidelines</a></h2>
<h3 id="1-start-with-defaults"><a class="header" href="#1-start-with-defaults">1. Start with Defaults</a></h3>
<p><strong>Recommendation</strong>: Use default settings for most workloads.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Let WorkerCount choose optimal values
let workers = WorkerCount::optimal_for_file_size(file_size);
<span class="boring">}</span></code></pre></pre>
<p><strong>Why?</strong></p>
<ul>
<li>Empirically validated strategies</li>
<li>Adapts to system resources</li>
<li>Handles edge cases (tiny files, huge files)</li>
</ul>
<h3 id="2-cpu-intensive-workloads"><a class="header" href="#2-cpu-intensive-workloads">2. CPU-Intensive Workloads</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>High CPU utilization (&gt; 80%)</li>
<li>Low I/O wait time</li>
<li>Operations: Compression, encryption, hashing</li>
</ul>
<p><strong>Tuning:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Use CPU-bound pool
RAYON_POOLS.cpu_bound_pool().install(|| {
    // Parallel CPU-intensive work
});

// Or increase worker count
let workers = WorkerCount::new(available_cores);
<span class="boring">}</span></code></pre></pre>
<p><strong>Guidelines:</strong></p>
<ul>
<li>Match worker count to CPU cores (1x to 1.5x)</li>
<li>Avoid excessive oversubscription (&gt; 2x cores)</li>
<li>Monitor context switching with <code>perf stat</code></li>
</ul>
<h3 id="3-io-intensive-workloads"><a class="header" href="#3-io-intensive-workloads">3. I/O-Intensive Workloads</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>Low CPU utilization (&lt; 50%)</li>
<li>High I/O wait time</li>
<li>Operations: File reads, database queries, network</li>
</ul>
<p><strong>Tuning:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Use mixed workload pool or reduce workers
let workers = WorkerCount::optimal_for_processing_type(
    file_size,
    available_cores,
    false,  // Not CPU-intensive
);
<span class="boring">}</span></code></pre></pre>
<p><strong>Guidelines:</strong></p>
<ul>
<li>Use fewer workers (0.5x to 1x cores)</li>
<li>Rely on async I/O instead of parallelism</li>
<li>Consider increasing I/O buffer sizes</li>
</ul>
<h3 id="4-memory-constrained-systems"><a class="header" href="#4-memory-constrained-systems">4. Memory-Constrained Systems</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>High memory usage</li>
<li>Swapping or OOM errors</li>
<li>Large files (&gt; 1 GB)</li>
</ul>
<p><strong>Tuning:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Reduce worker count to limit memory
let workers = WorkerCount::new(3);  // Conservative

// Or increase chunk size to reduce memory overhead
let chunk_size = ChunkSize::new(64 * 1024 * 1024);  // 64 MB chunks
<span class="boring">}</span></code></pre></pre>
<p><strong>Guidelines:</strong></p>
<ul>
<li>Limit workers to 3-6 for huge files (&gt; 2 GB)</li>
<li>Increase chunk size to reduce parallel memory allocations</li>
<li>Monitor RSS memory with <code>htop</code> or <code>ps</code></li>
</ul>
<h3 id="5-profiling-and-measurement"><a class="header" href="#5-profiling-and-measurement">5. Profiling and Measurement</a></h3>
<p><strong>Tools:</strong></p>
<ul>
<li><strong>perf</strong>: CPU profiling, context switches, cache misses</li>
<li><strong>flamegraph</strong>: Visual CPU time breakdown</li>
<li><strong>htop</strong>: Real-time CPU and memory usage</li>
<li><strong>tokio-console</strong>: Async task monitoring</li>
</ul>
<p><strong>Example: perf stat:</strong></p>
<pre><code class="language-bash">perf stat -e cycles,instructions,context-switches,cache-misses \
    ./pipeline process large-file.bin
</code></pre>
<p><strong>Metrics to monitor:</strong></p>
<ul>
<li><strong>CPU utilization</strong>: Should be 70-95% for CPU-bound work</li>
<li><strong>Context switches</strong>: High (&gt; 10k/sec) indicates oversubscription</li>
<li><strong>Cache misses</strong>: High indicates memory contention</li>
<li><strong>Task throughput</strong>: Measure chunks processed per second</li>
</ul>
<h2 id="common-patterns-1"><a class="header" href="#common-patterns-1">Common Patterns</a></h2>
<h3 id="pattern-1-fan-out-cpu-work"><a class="header" href="#pattern-1-fan-out-cpu-work">Pattern 1: Fan-Out CPU Work</a></h3>
<p>Distribute CPU-bound work across Rayon pool:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use rayon::prelude::*;

async fn process_chunks_parallel(
    chunks: Vec&lt;FileChunk&gt;,
) -&gt; Result&lt;Vec&lt;FileChunk&gt;, PipelineError&gt; {
    tokio::task::spawn_blocking(move || {
        RAYON_POOLS.cpu_bound_pool().install(|| {
            chunks
                .into_par_iter()
                .map(|chunk| {
                    // CPU-intensive per-chunk work
                    compress_and_encrypt(chunk)
                })
                .collect::&lt;Result&lt;Vec&lt;_&gt;, _&gt;&gt;()
        })
    })
    .await??
}
<span class="boring">}</span></code></pre></pre>
<h3 id="pattern-2-bounded-parallelism"><a class="header" href="#pattern-2-bounded-parallelism">Pattern 2: Bounded Parallelism</a></h3>
<p>Limit concurrent CPU-bound tasks with semaphore:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tokio::sync::Semaphore;

async fn process_with_limit(
    chunks: Vec&lt;FileChunk&gt;,
    max_parallel: usize,
) -&gt; Result&lt;Vec&lt;FileChunk&gt;, PipelineError&gt; {
    let semaphore = Arc::new(Semaphore::new(max_parallel));

    let futures = chunks.into_iter().map(|chunk| {
        let permit = semaphore.clone();
        async move {
            let _guard = permit.acquire().await.unwrap();

            // Run CPU-bound work on blocking pool
            tokio::task::spawn_blocking(move || {
                compress_chunk_sync(chunk)
            })
            .await?
        }
    });

    futures::future::try_join_all(futures).await
}
<span class="boring">}</span></code></pre></pre>
<h3 id="pattern-3-mixed-asyncsync-pipeline"><a class="header" href="#pattern-3-mixed-asyncsync-pipeline">Pattern 3: Mixed Async/Sync Pipeline</a></h3>
<p>Combine async I/O with sync CPU-bound stages:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn process_file_pipeline(path: &amp;Path) -&gt; Result&lt;(), PipelineError&gt; {
    // Stage 1: Async file read
    let chunks = read_file_chunks(path).await?;

    // Stage 2: Sync CPU-bound processing (spawn_blocking + Rayon)
    let processed = tokio::task::spawn_blocking(move || {
        RAYON_POOLS.cpu_bound_pool().install(|| {
            chunks.into_par_iter()
                .map(|chunk| compress_and_encrypt(chunk))
                .collect::&lt;Result&lt;Vec&lt;_&gt;, _&gt;&gt;()
        })
    })
    .await??;

    // Stage 3: Async file write
    write_chunks(path, processed).await?;

    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h2 id="performance-characteristics-1"><a class="header" href="#performance-characteristics-1">Performance Characteristics</a></h2>
<h3 id="thread-creation-overhead"><a class="header" href="#thread-creation-overhead">Thread Creation Overhead</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Time</th><th>Notes</th></tr></thead><tbody>
<tr><td>Tokio runtime initialization</td><td>~1-5 ms</td><td>One-time cost at startup</td></tr>
<tr><td>Rayon pool creation</td><td>~500 μs</td><td>One-time cost (global static)</td></tr>
<tr><td>spawn_blocking task</td><td>~10-50 μs</td><td>Per-task overhead</td></tr>
<tr><td>Rayon parallel iteration</td><td>~5-20 μs</td><td>Per-iteration overhead</td></tr>
<tr><td>Thread context switch</td><td>~1-5 μs</td><td>Depends on workload and OS</td></tr>
</tbody></table>
</div>
<p><strong>Guidelines:</strong></p>
<ul>
<li>Amortize overhead with work units &gt; 100 μs</li>
<li>Batch small operations to reduce per-task overhead</li>
<li>Avoid spawning tasks for trivial work (&lt; 10 μs)</li>
</ul>
<h3 id="scalability-1"><a class="header" href="#scalability-1">Scalability</a></h3>
<p><strong>Linear Scaling</strong> (ideal):</p>
<ul>
<li>CPU-bound operations with independent chunks</li>
<li>Small files (&lt; 50 MB) with 6-14 workers</li>
<li>Minimal memory contention</li>
</ul>
<p><strong>Sub-Linear Scaling</strong> (common):</p>
<ul>
<li>Large files (&gt; 500 MB) due to memory bandwidth limits</li>
<li>Mixed workloads with I/O contention</li>
<li>High worker counts (&gt; 12) with coordination overhead</li>
</ul>
<p><strong>Performance Cliff</strong> (avoid):</p>
<ul>
<li>Excessive worker count (&gt; 2x CPU cores)</li>
<li>Memory pressure causing swapping</li>
<li>Lock contention in shared data structures</li>
</ul>
<h2 id="best-practices-14"><a class="header" href="#best-practices-14">Best Practices</a></h2>
<h3 id="1-use-appropriate-thread-pool"><a class="header" href="#1-use-appropriate-thread-pool">1. Use Appropriate Thread Pool</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: CPU-intensive work on CPU-bound pool
RAYON_POOLS.cpu_bound_pool().install(|| {
    chunks.par_iter().map(|c| compress(c)).collect()
});

// ❌ Bad: Using wrong pool or no pool
chunks.iter().map(|c| compress(c)).collect()  // Sequential!
<span class="boring">}</span></code></pre></pre>
<h3 id="2-wrap-rayon-with-spawn_blocking"><a class="header" href="#2-wrap-rayon-with-spawn_blocking">2. Wrap Rayon with spawn_blocking</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Rayon work inside spawn_blocking
tokio::task::spawn_blocking(move || {
    RAYON_POOLS.cpu_bound_pool().install(|| {
        // Parallel work
    })
})
.await?

// ❌ Bad: Rayon work directly in async context
RAYON_POOLS.cpu_bound_pool().install(|| {
    // This blocks the async runtime!
})
<span class="boring">}</span></code></pre></pre>
<h3 id="3-let-workercount-optimize"><a class="header" href="#3-let-workercount-optimize">3. Let WorkerCount Optimize</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Use empirically validated strategies
let workers = WorkerCount::optimal_for_file_size(file_size);

// ❌ Bad: Arbitrary fixed count
let workers = 8;  // May be too many or too few!
<span class="boring">}</span></code></pre></pre>
<h3 id="4-monitor-and-measure"><a class="header" href="#4-monitor-and-measure">4. Monitor and Measure</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Measure actual performance
let start = Instant::now();
let result = process_chunks(chunks).await?;
let duration = start.elapsed();
info!("Processed {} chunks in {:?}", chunks.len(), duration);

// ❌ Bad: Assume defaults are optimal without measurement
<span class="boring">}</span></code></pre></pre>
<h3 id="5-avoid-oversubscription"><a class="header" href="#5-avoid-oversubscription">5. Avoid Oversubscription</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Bounded parallelism based on cores
let max_workers = available_cores.min(WorkerCount::MAX_WORKERS);

// ❌ Bad: Unbounded parallelism
let workers = chunks.len();  // Could be thousands!
<span class="boring">}</span></code></pre></pre>
<h2 id="troubleshooting-6"><a class="header" href="#troubleshooting-6">Troubleshooting</a></h2>
<h3 id="issue-1-high-context-switching"><a class="header" href="#issue-1-high-context-switching">Issue 1: High Context Switching</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li><code>perf stat</code> shows &gt; 10k context switches/sec</li>
<li>High CPU usage but low throughput</li>
</ul>
<p><strong>Causes:</strong></p>
<ul>
<li>Too many worker threads (&gt; 2x cores)</li>
<li>Rayon pool size exceeds optimal</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Reduce Rayon pool size
let workers = WorkerCount::new(available_cores);  // Not 2x

// Or use sequential processing for small workloads
if chunks.len() &lt; 10 {
    chunks.into_iter().map(process).collect()
} else {
    chunks.into_par_iter().map(process).collect()
}
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-2-spawn_blocking-blocking-async-runtime"><a class="header" href="#issue-2-spawn_blocking-blocking-async-runtime">Issue 2: spawn_blocking Blocking Async Runtime</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>Async tasks become slow</li>
<li>Other async operations stall</li>
</ul>
<p><strong>Causes:</strong></p>
<ul>
<li>Long-running CPU work directly in async fn (no spawn_blocking)</li>
<li>Blocking I/O in async context</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Use spawn_blocking for CPU work
async fn process_chunk(chunk: FileChunk) -&gt; Result&lt;FileChunk&gt; {
    tokio::task::spawn_blocking(move || {
        // CPU-intensive work here
        compress_chunk_sync(chunk)
    })
    .await?
}

// ❌ Bad: CPU work blocking async runtime
async fn process_chunk(chunk: FileChunk) -&gt; Result&lt;FileChunk&gt; {
    compress_chunk_sync(chunk)  // Blocks entire runtime!
}
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-3-memory-pressure-with-many-workers"><a class="header" href="#issue-3-memory-pressure-with-many-workers">Issue 3: Memory Pressure with Many Workers</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>High memory usage (&gt; 80% RAM)</li>
<li>Swapping or OOM errors</li>
</ul>
<p><strong>Causes:</strong></p>
<ul>
<li>Too many concurrent chunks allocated</li>
<li>Large chunk size × high worker count</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Reduce worker count for large files
let workers = if file_size &gt; 2 * GB {
    WorkerCount::new(3)  // Conservative for huge files
} else {
    WorkerCount::optimal_for_file_size(file_size)
};

// Or reduce chunk size
let chunk_size = ChunkSize::new(16 * MB);  // Smaller chunks
<span class="boring">}</span></code></pre></pre>
<h2 id="related-topics-5"><a class="header" href="#related-topics-5">Related Topics</a></h2>
<ul>
<li>See <a href="advanced/concurrency.html">Concurrency Model</a> for async/await patterns</li>
<li>See <a href="advanced/resources.html">Resource Management</a> for memory and task limits</li>
<li>See <a href="advanced/../advanced/performance.html">Performance Optimization</a> for benchmarking strategies</li>
<li>See <a href="advanced/../advanced/profiling.html">Profiling</a> for detailed performance analysis</li>
</ul>
<h2 id="summary-5"><a class="header" href="#summary-5">Summary</a></h2>
<p>The pipeline's dual thread pool architecture provides:</p>
<ol>
<li><strong>Tokio Runtime</strong>: Async I/O operations with work-stealing scheduler</li>
<li><strong>Rayon Pools</strong>: Parallel CPU-bound work with specialized pools</li>
<li><strong>spawn_blocking</strong>: Bridge between async and sync without blocking</li>
<li><strong>WorkerCount</strong>: Empirically validated thread allocation strategies</li>
<li><strong>Tuning</strong>: Guidelines for CPU-intensive, I/O-intensive, and memory-constrained workloads</li>
</ol>
<p><strong>Key Takeaways:</strong></p>
<ul>
<li>Use CPU-bound pool for compression, encryption, hashing</li>
<li>Wrap Rayon work with <code>spawn_blocking</code> in async contexts</li>
<li>Let <code>WorkerCount</code> optimize based on file size and system resources</li>
<li>Monitor performance with <code>perf</code>, <code>flamegraph</code>, and metrics</li>
<li>Tune based on actual measurements, not assumptions</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="resource-management-2"><a class="header" href="#resource-management-2">Resource Management</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>This chapter explores the pipeline's resource management system, including CPU and I/O token governance, memory tracking, and resource optimization strategies for different workload types.</p>
<h2 id="overview-19"><a class="header" href="#overview-19">Overview</a></h2>
<p>The pipeline employs a <strong>two-level resource governance</strong> architecture that prevents system oversubscription when processing multiple files concurrently:</p>
<ol>
<li><strong>Global Resource Manager</strong>: Caps total system resources (CPU tokens, I/O tokens, memory)</li>
<li><strong>Local Resource Limits</strong>: Caps per-file concurrency (semaphores within each file's processing)</li>
</ol>
<p>This two-level approach ensures optimal resource utilization without overwhelming the system.</p>
<h3 id="why-resource-governance"><a class="header" href="#why-resource-governance">Why Resource Governance?</a></h3>
<p><strong>Problem without limits:</strong></p>
<pre><code class="language-text">10 files × 8 workers/file = 80 concurrent tasks on an 8-core machine
Result: CPU oversubscription, cache thrashing, poor throughput
</code></pre>
<p><strong>Solution with resource governance:</strong></p>
<pre><code class="language-text">Global CPU tokens: 7 (cores - 1)
Maximum 7 concurrent CPU-intensive tasks across all files
Result: Optimal CPU utilization, consistent performance
</code></pre>
<h2 id="resource-management-architecture"><a class="header" href="#resource-management-architecture">Resource Management Architecture</a></h2>
<pre><code class="language-text">┌─────────────────────────────────────────────────────────────┐
│              Global Resource Manager                        │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  CPU Tokens (Semaphore)                              │   │
│  │  - Default: cores - 1                                │   │
│  │  - Purpose: Prevent CPU oversubscription             │   │
│  │  - Used by: Rayon workers, CPU-bound operations      │   │
│  └──────────────────────────────────────────────────────┘   │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  I/O Tokens (Semaphore)                              │   │
│  │  - Default: Device-specific (NVMe:24, SSD:12, HDD:4) │   │
│  │  - Purpose: Prevent I/O queue overrun                │   │
│  │  - Used by: File reads/writes                        │   │
│  └──────────────────────────────────────────────────────┘   │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  Memory Tracking (AtomicUsize)                       │   │
│  │  - Default: 40 GB capacity                           │   │
│  │  - Purpose: Monitor memory pressure (gauge only)     │   │
│  │  - Used by: Memory allocation/deallocation           │   │
│  └──────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
                                │
                                │ acquire_cpu() / acquire_io()
                                ▼
┌─────────────────────────────────────────────────────────────┐
│              File-Level Processing                          │
│  ┌──────────────────────────────────────────────────────┐   │
│  │  Per-File Semaphore                                  │   │
│  │  - Local concurrency limit (e.g., 8 workers/file)    │   │
│  │  - Prevents single file from saturating system       │   │
│  └──────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
</code></pre>
<h2 id="cpu-token-management"><a class="header" href="#cpu-token-management">CPU Token Management</a></h2>
<h3 id="overview-20"><a class="header" href="#overview-20">Overview</a></h3>
<p><strong>CPU tokens</strong> limit the total number of concurrent CPU-bound operations across all files being processed. This prevents CPU oversubscription and cache thrashing.</p>
<p><strong>Implementation:</strong> Semaphore-based with RAII (Resource Acquisition Is Initialization)</p>
<h3 id="configuration-4"><a class="header" href="#configuration-4">Configuration</a></h3>
<p><strong>Default CPU Token Count:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Automatic detection: cores - 1
let available_cores = std::thread::available_parallelism().map(|n| n.get()).unwrap_or(4);
let cpu_token_count = (available_cores - 1).max(1);

// Example on 8-core machine: 7 CPU tokens
<span class="boring">}</span></code></pre></pre>
<p><strong>Why <code>cores - 1</code>?</strong></p>
<ul>
<li>Leave one core for OS, I/O threads, and system tasks</li>
<li>Prevents complete CPU saturation</li>
<li>Improves overall system responsiveness</li>
<li>Reduces context switching overhead</li>
</ul>
<p><strong>Custom Configuration:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline::infrastructure::runtime::{init_resource_manager, ResourceConfig};

// Initialize with custom CPU token count
let config = ResourceConfig {
    cpu_tokens: Some(6),  // Override: use 6 CPU workers
    ..Default::default()
};

init_resource_manager(config)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="usage-pattern-2"><a class="header" href="#usage-pattern-2">Usage Pattern</a></h3>
<p><strong>Acquire CPU token before CPU-intensive work:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline::infrastructure::runtime::RESOURCE_MANAGER;

async fn process_chunk(chunk: FileChunk) -&gt; Result&lt;FileChunk, PipelineError&gt; {
    // 1. Acquire global CPU token (waits if system is saturated)
    let _cpu_permit = RESOURCE_MANAGER.acquire_cpu().await?;

    // 2. Do CPU-intensive work (compression, encryption, hashing)
    tokio::task::spawn_blocking(move || {
        RAYON_POOLS.cpu_bound_pool().install(|| {
            compress_and_encrypt(chunk)
        })
    })
    .await?

    // 3. Permit released automatically when _cpu_permit goes out of scope (RAII)
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Key Points:</strong></p>
<ul>
<li><code>acquire_cpu()</code> returns a <code>SemaphorePermit</code> guard</li>
<li>If all CPU tokens are in use, the call <strong>waits</strong> (backpressure)</li>
<li>Permit is auto-released when dropped (RAII pattern)</li>
<li>This creates natural flow control and prevents oversubscription</li>
</ul>
<h3 id="backpressure-mechanism"><a class="header" href="#backpressure-mechanism">Backpressure Mechanism</a></h3>
<p>When all CPU tokens are in use:</p>
<pre><code class="language-text">Timeline:
─────────────────────────────────────────────────────────────
Task 1: [===CPU work===]         (acquired token)
Task 2:   [===CPU work===]       (acquired token)
Task 3:     [===CPU work===]     (acquired token)
...
Task 7:             [===CPU===]  (acquired token - last one!)
Task 8:               ⏳⏳⏳     (waiting for token...)
Task 9:                 ⏳⏳⏳   (waiting for token...)
─────────────────────────────────────────────────────────────

When Task 1 completes → Task 8 acquires the released token
When Task 2 completes → Task 9 acquires the released token
</code></pre>
<p>This backpressure prevents overwhelming the CPU with too many concurrent tasks.</p>
<h3 id="monitoring-cpu-saturation"><a class="header" href="#monitoring-cpu-saturation">Monitoring CPU Saturation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline::infrastructure::metrics::CONCURRENCY_METRICS;

// Check CPU saturation percentage
let saturation = CONCURRENCY_METRICS.cpu_saturation_percent();

if saturation &gt; 80.0 {
    println!("CPU-saturated: {}%", saturation);
    println!("Available tokens: {}", RESOURCE_MANAGER.cpu_tokens_available());
    println!("Total tokens: {}", RESOURCE_MANAGER.cpu_tokens_total());
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Saturation Interpretation:</strong></p>
<ul>
<li><strong>0-50%</strong>: Underutilized, could increase worker count</li>
<li><strong>50-80%</strong>: Good utilization, healthy balance</li>
<li><strong>80-95%</strong>: High utilization, approaching saturation</li>
<li><strong>95-100%</strong>: Fully saturated, tasks frequently waiting</li>
</ul>
<h2 id="io-token-management"><a class="header" href="#io-token-management">I/O Token Management</a></h2>
<h3 id="overview-21"><a class="header" href="#overview-21">Overview</a></h3>
<p><strong>I/O tokens</strong> limit the total number of concurrent I/O operations to prevent overwhelming the storage device's queue depth.</p>
<p><strong>Why separate from CPU tokens?</strong></p>
<ul>
<li>I/O and CPU have different characteristics</li>
<li>Storage devices have specific optimal queue depths</li>
<li>Prevents I/O queue saturation independent of CPU usage</li>
</ul>
<h3 id="device-specific-optimization"><a class="header" href="#device-specific-optimization">Device-Specific Optimization</a></h3>
<p>Different storage devices have different optimal I/O queue depths:</p>
<div class="table-wrapper"><table><thead><tr><th>Storage Type</th><th>Queue Depth</th><th>Rationale</th></tr></thead><tbody>
<tr><td><strong>NVMe</strong></td><td>24</td><td>Multiple parallel channels, low latency</td></tr>
<tr><td><strong>SSD</strong></td><td>12</td><td>Medium parallelism, good random access</td></tr>
<tr><td><strong>HDD</strong></td><td>4</td><td>Sequential access preferred, high seek latency</td></tr>
<tr><td><strong>Auto</strong></td><td>12</td><td>Conservative default (assumes SSD)</td></tr>
</tbody></table>
</div>
<p><strong>Implementation</strong> (<code>pipeline/src/infrastructure/runtime/resource_manager.rs</code>):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub enum StorageType {
    NVMe,   // 24 tokens
    SSD,    // 12 tokens
    HDD,    // 4 tokens
    Auto,   // 12 tokens (SSD default)
    Custom(usize),
}

fn detect_optimal_io_tokens(storage_type: StorageType) -&gt; usize {
    match storage_type {
        StorageType::NVMe =&gt; 24,
        StorageType::SSD =&gt; 12,
        StorageType::HDD =&gt; 4,
        StorageType::Auto =&gt; 12,  // Conservative default
        StorageType::Custom(n) =&gt; n,
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="configuration-5"><a class="header" href="#configuration-5">Configuration</a></h3>
<p><strong>Custom I/O token count:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let config = ResourceConfig {
    io_tokens: Some(24),              // Override: NVMe-optimized
    storage_type: StorageType::NVMe,
    ..Default::default()
};

init_resource_manager(config)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="usage-pattern-3"><a class="header" href="#usage-pattern-3">Usage Pattern</a></h3>
<p><strong>Acquire I/O token before file operations:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn read_file_chunk(path: &amp;Path, offset: u64, size: usize)
    -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt;
{
    // 1. Acquire global I/O token (waits if I/O queue is full)
    let _io_permit = RESOURCE_MANAGER.acquire_io().await?;

    // 2. Perform I/O operation
    let mut file = tokio::fs::File::open(path).await?;
    file.seek(SeekFrom::Start(offset)).await?;

    let mut buffer = vec![0u8; size];
    file.read_exact(&amp;mut buffer).await?;

    Ok(buffer)

    // 3. Permit auto-released when _io_permit goes out of scope
}
<span class="boring">}</span></code></pre></pre>
<h3 id="monitoring-io-saturation"><a class="header" href="#monitoring-io-saturation">Monitoring I/O Saturation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Check I/O saturation
let io_saturation = CONCURRENCY_METRICS.io_saturation_percent();

if io_saturation &gt; 80.0 {
    println!("I/O-saturated: {}%", io_saturation);
    println!("Available tokens: {}", RESOURCE_MANAGER.io_tokens_available());
    println!("Consider reducing concurrent I/O or optimizing chunk size");
}
<span class="boring">}</span></code></pre></pre>
<h2 id="memory-management-4"><a class="header" href="#memory-management-4">Memory Management</a></h2>
<h3 id="overview-22"><a class="header" href="#overview-22">Overview</a></h3>
<p>The pipeline uses <strong>memory tracking</strong> (not enforcement) to monitor memory pressure and provide visibility into resource usage.</p>
<p><strong>Design Philosophy:</strong></p>
<ul>
<li><strong>Phase 1</strong>: Monitor memory usage (current implementation)</li>
<li><strong>Phase 2</strong>: Soft limits with warnings</li>
<li><strong>Phase 3</strong>: Hard limits with enforcement (future)</li>
</ul>
<p><strong>Why start with monitoring only?</strong></p>
<ul>
<li>Memory is harder to predict and control than CPU/I/O</li>
<li>Avoids complexity in initial implementation</li>
<li>Allows gathering real-world usage data before adding limits</li>
</ul>
<h3 id="memory-tracking"><a class="header" href="#memory-tracking">Memory Tracking</a></h3>
<p><strong>Atomic counter</strong> (<code>AtomicUsize</code>) tracks allocated memory:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline::infrastructure::runtime::RESOURCE_MANAGER;

// Track memory allocation
let chunk_size = 1024 * 1024;  // 1 MB
RESOURCE_MANAGER.allocate_memory(chunk_size);

// ... use memory ...

// Track memory deallocation
RESOURCE_MANAGER.deallocate_memory(chunk_size);
<span class="boring">}</span></code></pre></pre>
<p><strong>Usage with RAII guard:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct MemoryGuard {
    size: usize,
}

impl MemoryGuard {
    pub fn new(size: usize) -&gt; Self {
        RESOURCE_MANAGER.allocate_memory(size);
        Self { size }
    }
}

impl Drop for MemoryGuard {
    fn drop(&amp;mut self) {
        RESOURCE_MANAGER.deallocate_memory(self.size);
    }
}

// Usage
let _guard = MemoryGuard::new(chunk_size);
// Memory automatically tracked on allocation and deallocation
<span class="boring">}</span></code></pre></pre>
<h3 id="configuration-6"><a class="header" href="#configuration-6">Configuration</a></h3>
<p><strong>Set memory capacity:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let config = ResourceConfig {
    memory_limit: Some(40 * 1024 * 1024 * 1024),  // 40 GB capacity
    ..Default::default()
};

init_resource_manager(config)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="monitoring-memory-usage"><a class="header" href="#monitoring-memory-usage">Monitoring Memory Usage</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline::infrastructure::metrics::CONCURRENCY_METRICS;

// Check current memory usage
let used_bytes = RESOURCE_MANAGER.memory_used();
let used_mb = CONCURRENCY_METRICS.memory_used_mb();
let utilization = CONCURRENCY_METRICS.memory_utilization_percent();

println!("Memory: {:.2} MB / {} MB ({:.1}%)",
    used_mb,
    RESOURCE_MANAGER.memory_capacity() / (1024 * 1024),
    utilization
);

// Alert on high memory usage
if utilization &gt; 80.0 {
    println!("⚠️  High memory usage: {:.1}%", utilization);
    println!("Consider reducing chunk size or worker count");
}
<span class="boring">}</span></code></pre></pre>
<h2 id="concurrency-metrics"><a class="header" href="#concurrency-metrics">Concurrency Metrics</a></h2>
<h3 id="overview-23"><a class="header" href="#overview-23">Overview</a></h3>
<p>The pipeline provides comprehensive metrics for monitoring resource utilization, wait times, and saturation levels.</p>
<p><strong>Metric Types:</strong></p>
<ul>
<li><strong>Gauges</strong>: Instant values (e.g., CPU tokens available)</li>
<li><strong>Counters</strong>: Cumulative values (e.g., total wait time)</li>
<li><strong>Histograms</strong>: Distribution of values (e.g., P50/P95/P99 wait times)</li>
</ul>
<h3 id="cpu-metrics"><a class="header" href="#cpu-metrics">CPU Metrics</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline::infrastructure::metrics::CONCURRENCY_METRICS;

// Instant metrics (gauges)
let cpu_available = CONCURRENCY_METRICS.cpu_tokens_available();
let cpu_saturation = CONCURRENCY_METRICS.cpu_saturation_percent();

// Wait time percentiles (histograms)
let p50 = CONCURRENCY_METRICS.cpu_wait_p50();  // Median wait time (ms)
let p95 = CONCURRENCY_METRICS.cpu_wait_p95();  // 95th percentile (ms)
let p99 = CONCURRENCY_METRICS.cpu_wait_p99();  // 99th percentile (ms)

println!("CPU Saturation: {:.1}%", cpu_saturation);
println!("Wait times: P50={} ms, P95={} ms, P99={} ms", p50, p95, p99);
<span class="boring">}</span></code></pre></pre>
<p><strong>Why percentiles matter:</strong></p>
<p>Averages hide problems:</p>
<ul>
<li>Average wait: 10 ms (looks fine)</li>
<li>P99 wait: 500 ms (users experience terrible latency!)</li>
</ul>
<p>Histograms show the full distribution and reveal tail latencies.</p>
<h3 id="io-metrics"><a class="header" href="#io-metrics">I/O Metrics</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// I/O saturation and wait times
let io_saturation = CONCURRENCY_METRICS.io_saturation_percent();
let io_p95 = CONCURRENCY_METRICS.io_wait_p95();

if io_saturation &gt; 80.0 &amp;&amp; io_p95 &gt; 50 {
    println!("⚠️  I/O bottleneck detected:");
    println!("  Saturation: {:.1}%", io_saturation);
    println!("  P95 wait time: {} ms", io_p95);
}
<span class="boring">}</span></code></pre></pre>
<h3 id="worker-metrics"><a class="header" href="#worker-metrics">Worker Metrics</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Worker tracking
let active = CONCURRENCY_METRICS.active_workers();
let spawned = CONCURRENCY_METRICS.tasks_spawned();
let completed = CONCURRENCY_METRICS.tasks_completed();

println!("Workers: {} active, {} spawned, {} completed",
    active, spawned, completed);

// Queue depth (backpressure indicator)
let queue_depth = CONCURRENCY_METRICS.cpu_queue_depth();
let queue_max = CONCURRENCY_METRICS.cpu_queue_depth_max();

if queue_depth &gt; 100 {
    println!("⚠️  High queue depth: {} (max: {})", queue_depth, queue_max);
    println!("Workers can't keep up with reader - increase workers or optimize stages");
}
<span class="boring">}</span></code></pre></pre>
<h3 id="queue-metrics"><a class="header" href="#queue-metrics">Queue Metrics</a></h3>
<p><strong>Queue depth</strong> reveals whether workers can keep up with the reader:</p>
<ul>
<li><strong>Depth near 0</strong>: Workers are faster than reader (good!)</li>
<li><strong>Depth near capacity</strong>: Workers are bottleneck (increase workers or optimize stages)</li>
<li><strong>Depth at capacity</strong>: Reader is blocked (severe backpressure)</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Queue wait time distribution
let queue_p50 = CONCURRENCY_METRICS.cpu_queue_wait_p50();
let queue_p95 = CONCURRENCY_METRICS.cpu_queue_wait_p95();
let queue_p99 = CONCURRENCY_METRICS.cpu_queue_wait_p99();

println!("Queue wait: P50={} ms, P95={} ms, P99={} ms",
    queue_p50, queue_p95, queue_p99);
<span class="boring">}</span></code></pre></pre>
<h2 id="resource-configuration"><a class="header" href="#resource-configuration">Resource Configuration</a></h2>
<h3 id="resourceconfig-structure"><a class="header" href="#resourceconfig-structure">ResourceConfig Structure</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct ResourceConfig {
    /// Number of CPU worker tokens (default: cores - 1)
    pub cpu_tokens: Option&lt;usize&gt;,

    /// Number of I/O tokens (default: device-specific)
    pub io_tokens: Option&lt;usize&gt;,

    /// Storage device type for I/O optimization
    pub storage_type: StorageType,

    /// Soft memory limit in bytes (gauge only, no enforcement)
    pub memory_limit: Option&lt;usize&gt;,
}
<span class="boring">}</span></code></pre></pre>
<h3 id="initialization-pattern"><a class="header" href="#initialization-pattern">Initialization Pattern</a></h3>
<p><strong>In <code>main()</code>, before any operations:</strong></p>
<pre><pre class="playground"><code class="language-rust">use pipeline::infrastructure::runtime::{init_resource_manager, ResourceConfig, StorageType};

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    // 1. Initialize resource manager with configuration
    let config = ResourceConfig {
        cpu_tokens: Some(6),              // 6 CPU workers
        io_tokens: Some(24),              // NVMe-optimized
        storage_type: StorageType::NVMe,
        memory_limit: Some(40 * 1024 * 1024 * 1024),  // 40 GB
    };

    init_resource_manager(config)
        .map_err(|e| anyhow!("Failed to initialize resources: {}", e))?;

    // 2. Now safe to use RESOURCE_MANAGER anywhere
    run_pipeline().await
}</code></pre></pre>
<p><strong>Global access pattern:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn my_function() {
    // Access global resource manager
    let _cpu_permit = RESOURCE_MANAGER.acquire_cpu().await?;
    // ... CPU work ...
}
<span class="boring">}</span></code></pre></pre>
<h2 id="tuning-guidelines-1"><a class="header" href="#tuning-guidelines-1">Tuning Guidelines</a></h2>
<h3 id="1-cpu-bound-workloads"><a class="header" href="#1-cpu-bound-workloads">1. CPU-Bound Workloads</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>High CPU saturation (&gt; 80%)</li>
<li>Low CPU wait times (&lt; 10 ms P95)</li>
<li>Heavy compression, encryption, or hashing</li>
</ul>
<p><strong>Tuning:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Increase CPU tokens to match cores (remove safety margin)
let config = ResourceConfig {
    cpu_tokens: Some(available_cores),  // Use all cores
    ..Default::default()
};
<span class="boring">}</span></code></pre></pre>
<p><strong>Trade-offs:</strong></p>
<ul>
<li>✅ Higher throughput for CPU-bound work</li>
<li>❌ Reduced system responsiveness</li>
<li>❌ Higher context switching overhead</li>
</ul>
<h3 id="2-io-bound-workloads"><a class="header" href="#2-io-bound-workloads">2. I/O-Bound Workloads</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>High I/O saturation (&gt; 80%)</li>
<li>High I/O wait times (&gt; 50 ms P95)</li>
<li>Many concurrent file operations</li>
</ul>
<p><strong>Tuning:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Increase I/O tokens for high-throughput storage
let config = ResourceConfig {
    io_tokens: Some(32),              // Higher than default
    storage_type: StorageType::Custom(32),
    ..Default::default()
};
<span class="boring">}</span></code></pre></pre>
<p><strong>Monitoring:</strong></p>
<pre><code class="language-bash"># Check I/O queue depth on Linux
iostat -x 1

# Look for:
# - avgqu-sz (average queue size) - should be &lt; I/O tokens
# - %util (device utilization) - should be 70-90%
</code></pre>
<h3 id="3-memory-constrained-systems"><a class="header" href="#3-memory-constrained-systems">3. Memory-Constrained Systems</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>High memory utilization (&gt; 80%)</li>
<li>Swapping or OOM errors</li>
<li>Processing large files</li>
</ul>
<p><strong>Tuning:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Reduce chunk size to lower memory usage
let chunk_size = ChunkSize::new(16 * 1024 * 1024);  // 16 MB (smaller)

// Reduce worker count to limit concurrent chunks
let config = ResourceConfig {
    cpu_tokens: Some(3),  // Fewer workers = less memory
    ..Default::default()
};
<span class="boring">}</span></code></pre></pre>
<p><strong>Formula:</strong></p>
<pre><code class="language-text">Peak Memory ≈ chunk_size × cpu_tokens × files_processed_concurrently

Example:
  chunk_size = 64 MB
  cpu_tokens = 7
  files = 3
  Peak Memory ≈ 64 MB × 7 × 3 = 1.3 GB
</code></pre>
<h3 id="4-mixed-workloads"><a class="header" href="#4-mixed-workloads">4. Mixed Workloads</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>Both CPU and I/O saturation</li>
<li>Variable chunk processing times</li>
<li>Compression + file I/O operations</li>
</ul>
<p><strong>Tuning:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Balanced configuration
let config = ResourceConfig {
    cpu_tokens: Some(available_cores - 1),  // Leave headroom
    io_tokens: Some(12),                    // Moderate I/O concurrency
    storage_type: StorageType::SSD,
    ..Default::default()
};
<span class="boring">}</span></code></pre></pre>
<p><strong>Best practices:</strong></p>
<ul>
<li>Monitor both CPU and I/O saturation</li>
<li>Adjust based on bottleneck (CPU vs I/O)</li>
<li>Use Rayon's mixed workload pool for hybrid operations</li>
</ul>
<h3 id="5-multi-file-processing"><a class="header" href="#5-multi-file-processing">5. Multi-File Processing</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>Processing many files concurrently</li>
<li>High queue depths</li>
<li>Resource contention between files</li>
</ul>
<p><strong>Tuning:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Global limits prevent oversubscription
let config = ResourceConfig {
    cpu_tokens: Some(7),   // Total across ALL files
    io_tokens: Some(12),   // Total across ALL files
    ..Default::default()
};

// Per-file semaphores limit individual file's concurrency
async fn process_file(path: &amp;Path) -&gt; Result&lt;()&gt; {
    let file_semaphore = Arc::new(Semaphore::new(4));  // Max 4 workers/file

    for chunk in chunks {
        let _global_cpu = RESOURCE_MANAGER.acquire_cpu().await?;  // Global limit
        let _local = file_semaphore.acquire().await?;             // Local limit

        // Process chunk...
    }

    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Two-level governance:</strong></p>
<ul>
<li><strong>Global</strong>: Prevents system oversubscription (7 CPU tokens total)</li>
<li><strong>Local</strong>: Prevents single file from monopolizing resources (4 workers/file)</li>
</ul>
<h2 id="performance-characteristics-2"><a class="header" href="#performance-characteristics-2">Performance Characteristics</a></h2>
<h3 id="resource-acquisition-overhead"><a class="header" href="#resource-acquisition-overhead">Resource Acquisition Overhead</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Time</th><th>Notes</th></tr></thead><tbody>
<tr><td>acquire_cpu() (fast path)</td><td>~100 ns</td><td>Token immediately available</td></tr>
<tr><td>acquire_cpu() (slow path)</td><td>~1-50 ms</td><td>Must wait for token</td></tr>
<tr><td>acquire_io() (fast path)</td><td>~100 ns</td><td>Token immediately available</td></tr>
<tr><td>allocate_memory() tracking</td><td>~10 ns</td><td>Atomic increment (Relaxed)</td></tr>
<tr><td>Metrics update</td><td>~50 ns</td><td>Atomic operations</td></tr>
</tbody></table>
</div>
<p><strong>Guidelines:</strong></p>
<ul>
<li>Fast path is negligible overhead</li>
<li>Slow path (waiting) creates backpressure (intentional)</li>
<li>Memory tracking is extremely low overhead</li>
</ul>
<h3 id="scalability-2"><a class="header" href="#scalability-2">Scalability</a></h3>
<p><strong>Linear scaling</strong> (ideal):</p>
<ul>
<li>CPU tokens = available cores</li>
<li>I/O tokens matched to device queue depth</li>
<li>Minimal waiting for resources</li>
</ul>
<p><strong>Sub-linear scaling</strong> (common with oversubscription):</p>
<ul>
<li>Too many CPU tokens (&gt; 2x cores)</li>
<li>Excessive context switching</li>
<li>Cache thrashing</li>
</ul>
<p><strong>Performance cliff</strong> (avoid):</p>
<ul>
<li>No resource limits</li>
<li>Uncontrolled parallelism</li>
<li>System thrashing (swapping, CPU saturation)</li>
</ul>
<h2 id="best-practices-15"><a class="header" href="#best-practices-15">Best Practices</a></h2>
<h3 id="1-always-acquire-resources-before-work"><a class="header" href="#1-always-acquire-resources-before-work">1. Always Acquire Resources Before Work</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Acquire global resource token before work
async fn process_chunk(chunk: FileChunk) -&gt; Result&lt;FileChunk&gt; {
    let _cpu_permit = RESOURCE_MANAGER.acquire_cpu().await?;

    tokio::task::spawn_blocking(move || {
        // CPU-intensive work
    }).await?
}

// ❌ Bad: No resource governance
async fn process_chunk(chunk: FileChunk) -&gt; Result&lt;FileChunk&gt; {
    tokio::task::spawn_blocking(move || {
        // Uncontrolled parallelism!
    }).await?
}
<span class="boring">}</span></code></pre></pre>
<h3 id="2-use-raii-for-automatic-release"><a class="header" href="#2-use-raii-for-automatic-release">2. Use RAII for Automatic Release</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: RAII guard auto-releases
let _permit = RESOURCE_MANAGER.acquire_cpu().await?;
// Work here...
// Permit released automatically when _permit goes out of scope

// ❌ Bad: Manual release (error-prone)
// Don't do this - no manual release API
<span class="boring">}</span></code></pre></pre>
<h3 id="3-monitor-saturation-regularly"><a class="header" href="#3-monitor-saturation-regularly">3. Monitor Saturation Regularly</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Periodic monitoring
tokio::spawn(async {
    let mut interval = tokio::time::interval(Duration::from_secs(10));
    loop {
        interval.tick().await;

        let cpu_sat = CONCURRENCY_METRICS.cpu_saturation_percent();
        let io_sat = CONCURRENCY_METRICS.io_saturation_percent();
        let mem_util = CONCURRENCY_METRICS.memory_utilization_percent();

        info!("Resources: CPU={:.1}%, I/O={:.1}%, Mem={:.1}%",
            cpu_sat, io_sat, mem_util);
    }
});
<span class="boring">}</span></code></pre></pre>
<h3 id="4-configure-based-on-workload-type"><a class="header" href="#4-configure-based-on-workload-type">4. Configure Based on Workload Type</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Workload-specific configuration
let config = if is_cpu_intensive {
    ResourceConfig {
        cpu_tokens: Some(available_cores),
        io_tokens: Some(8),  // Lower I/O concurrency
        ..Default::default()
    }
} else {
    ResourceConfig {
        cpu_tokens: Some(available_cores / 2),
        io_tokens: Some(24),  // Higher I/O concurrency
        ..Default::default()
    }
};
<span class="boring">}</span></code></pre></pre>
<h3 id="5-track-memory-with-guards"><a class="header" href="#5-track-memory-with-guards">5. Track Memory with Guards</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: RAII memory guard
pub struct ChunkBuffer {
    data: Vec&lt;u8&gt;,
    _memory_guard: MemoryGuard,
}

impl ChunkBuffer {
    pub fn new(size: usize) -&gt; Self {
        let data = vec![0u8; size];
        let _memory_guard = MemoryGuard::new(size);
        Self { data, _memory_guard }
    }
}
// Memory automatically tracked on allocation and freed on drop
<span class="boring">}</span></code></pre></pre>
<h2 id="troubleshooting-7"><a class="header" href="#troubleshooting-7">Troubleshooting</a></h2>
<h3 id="issue-1-high-cpu-wait-times"><a class="header" href="#issue-1-high-cpu-wait-times">Issue 1: High CPU Wait Times</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>P95 CPU wait time &gt; 50 ms</li>
<li>Low CPU saturation (&lt; 50%)</li>
<li>Many tasks waiting for CPU tokens</li>
</ul>
<p><strong>Causes:</strong></p>
<ul>
<li>Too few CPU tokens configured</li>
<li>CPU tokens not matching actual cores</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Check current configuration
println!("CPU tokens: {}", RESOURCE_MANAGER.cpu_tokens_total());
println!("CPU saturation: {:.1}%", CONCURRENCY_METRICS.cpu_saturation_percent());
println!("CPU wait P95: {} ms", CONCURRENCY_METRICS.cpu_wait_p95());

// Increase CPU tokens
let config = ResourceConfig {
    cpu_tokens: Some(available_cores),  // Increase from cores-1
    ..Default::default()
};
init_resource_manager(config)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-2-io-queue-saturation"><a class="header" href="#issue-2-io-queue-saturation">Issue 2: I/O Queue Saturation</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>I/O saturation &gt; 90%</li>
<li>High I/O wait times (&gt; 100 ms P95)</li>
<li><code>iostat</code> shows high <code>avgqu-sz</code></li>
</ul>
<p><strong>Causes:</strong></p>
<ul>
<li>Too many I/O tokens for storage device</li>
<li>Storage device can't handle queue depth</li>
<li>Sequential I/O on HDD with high concurrency</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Reduce I/O tokens for HDD
let config = ResourceConfig {
    storage_type: StorageType::HDD,  // Sets I/O tokens = 4
    ..Default::default()
};

// Or manually configure
let config = ResourceConfig {
    io_tokens: Some(4),  // Lower concurrency
    ..Default::default()
};
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-3-memory-pressure"><a class="header" href="#issue-3-memory-pressure">Issue 3: Memory Pressure</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>Memory utilization &gt; 80%</li>
<li>Swapping (check <code>vmstat</code>)</li>
<li>OOM killer activated</li>
</ul>
<p><strong>Causes:</strong></p>
<ul>
<li>Too many concurrent chunks allocated</li>
<li>Large chunk size × high worker count</li>
<li>Memory leaks (not tracked properly)</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Reduce memory usage
let chunk_size = ChunkSize::new(16 * 1024 * 1024);  // Smaller chunks

let config = ResourceConfig {
    cpu_tokens: Some(3),  // Fewer concurrent chunks
    ..Default::default()
};

// Monitor memory closely
let mem_mb = CONCURRENCY_METRICS.memory_used_mb();
let mem_pct = CONCURRENCY_METRICS.memory_utilization_percent();

if mem_pct &gt; 80.0 {
    error!("⚠️  High memory usage: {:.1}% ({:.2} MB)", mem_pct, mem_mb);
}
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-4-high-queue-depth"><a class="header" href="#issue-4-high-queue-depth">Issue 4: High Queue Depth</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>CPU queue depth &gt; 100</li>
<li>High queue wait times (&gt; 50 ms P95)</li>
<li>Reader blocked waiting for workers</li>
</ul>
<p><strong>Causes:</strong></p>
<ul>
<li>Workers can't keep up with reader</li>
<li>Slow compression/encryption stages</li>
<li>Insufficient worker count</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Check queue metrics
let depth = CONCURRENCY_METRICS.cpu_queue_depth();
let max_depth = CONCURRENCY_METRICS.cpu_queue_depth_max();
let wait_p95 = CONCURRENCY_METRICS.cpu_queue_wait_p95();

println!("Queue depth: {} (max: {})", depth, max_depth);
println!("Queue wait P95: {} ms", wait_p95);

// Increase workers to drain queue faster
let config = ResourceConfig {
    cpu_tokens: Some(available_cores),  // More workers
    ..Default::default()
};

// Or optimize stages (faster compression, fewer stages)
<span class="boring">}</span></code></pre></pre>
<h2 id="related-topics-6"><a class="header" href="#related-topics-6">Related Topics</a></h2>
<ul>
<li>See <a href="advanced/thread-pooling.html">Thread Pooling</a> for worker pool configuration</li>
<li>See <a href="advanced/concurrency.html">Concurrency Model</a> for async/await patterns</li>
<li>See <a href="advanced/../advanced/performance.html">Performance Optimization</a> for benchmarking</li>
<li>See <a href="advanced/../advanced/profiling.html">Profiling</a> for performance analysis</li>
</ul>
<h2 id="summary-6"><a class="header" href="#summary-6">Summary</a></h2>
<p>The pipeline's resource management system provides:</p>
<ol>
<li><strong>CPU Token Management</strong>: Prevent CPU oversubscription with semaphore-based limits</li>
<li><strong>I/O Token Management</strong>: Device-specific I/O queue depth optimization</li>
<li><strong>Memory Tracking</strong>: Monitor memory usage with atomic counters (gauge only)</li>
<li><strong>Concurrency Metrics</strong>: Comprehensive observability (gauges, counters, histograms)</li>
<li><strong>Two-Level Governance</strong>: Global + local limits prevent system saturation</li>
</ol>
<p><strong>Key Takeaways:</strong></p>
<ul>
<li>Always acquire resource tokens before CPU/I/O work</li>
<li>Use RAII guards for automatic resource release</li>
<li>Monitor saturation percentages and wait time percentiles</li>
<li>Configure based on workload type (CPU-intensive vs I/O-intensive)</li>
<li>Start with defaults, tune based on actual measurements</li>
<li>Memory tracking is informational only (no enforcement yet)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="performance-optimization-4"><a class="header" href="#performance-optimization-4">Performance Optimization</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>This chapter explores performance optimization strategies for the adaptive pipeline, including benchmarking methodologies, tuning parameters, and common performance bottlenecks with their solutions.</p>
<h2 id="overview-24"><a class="header" href="#overview-24">Overview</a></h2>
<p>The pipeline is designed for high-performance file processing with several optimization strategies:</p>
<ol>
<li><strong>Adaptive Configuration</strong>: Automatically selects optimal settings based on file characteristics</li>
<li><strong>Parallel Processing</strong>: Leverages multi-core systems with Tokio and Rayon</li>
<li><strong>Resource Management</strong>: Prevents oversubscription with CPU/I/O token governance</li>
<li><strong>Memory Efficiency</strong>: Streaming processing with bounded memory usage</li>
<li><strong>I/O Optimization</strong>: Memory mapping, chunked I/O, and device-specific tuning</li>
</ol>
<p><strong>Performance Goals:</strong></p>
<ul>
<li><strong>Throughput</strong>: 100-500 MB/s for compression/encryption pipelines</li>
<li><strong>Latency</strong>: &lt; 100 ms overhead for small files (&lt; 10 MB)</li>
<li><strong>Memory</strong>: Bounded memory usage regardless of file size</li>
<li><strong>Scalability</strong>: Linear scaling up to available CPU cores</li>
</ul>
<h2 id="performance-metrics-1"><a class="header" href="#performance-metrics-1">Performance Metrics</a></h2>
<h3 id="throughput"><a class="header" href="#throughput">Throughput</a></h3>
<p><strong>Definition</strong>: Bytes processed per second</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::entities::ProcessingMetrics;

let metrics = ProcessingMetrics::new();
metrics.start();

// ... process data ...
metrics.add_bytes_processed(file_size);

metrics.end();

println!("Throughput: {:.2} MB/s", metrics.throughput_mb_per_second());
<span class="boring">}</span></code></pre></pre>
<p><strong>Typical Values:</strong></p>
<ul>
<li><strong>Uncompressed I/O</strong>: 500-2000 MB/s (limited by storage device)</li>
<li><strong>LZ4 compression</strong>: 300-600 MB/s (fast, low compression)</li>
<li><strong>Brotli compression</strong>: 50-150 MB/s (slow, high compression)</li>
<li><strong>AES-256-GCM encryption</strong>: 400-800 MB/s (hardware-accelerated)</li>
<li><strong>ChaCha20-Poly1305</strong>: 200-400 MB/s (software)</li>
</ul>
<h3 id="latency"><a class="header" href="#latency">Latency</a></h3>
<p><strong>Definition</strong>: Time from start to completion</p>
<p><strong>Components:</strong></p>
<ul>
<li><strong>Setup overhead</strong>: File opening, thread pool initialization (1-5 ms)</li>
<li><strong>I/O time</strong>: Reading/writing chunks (varies by device and size)</li>
<li><strong>Processing time</strong>: Compression, encryption, hashing (varies by algorithm)</li>
<li><strong>Coordination overhead</strong>: Task spawning, semaphore acquisition (&lt; 1 ms)</li>
</ul>
<p><strong>Optimization Strategies:</strong></p>
<ul>
<li>Minimize setup overhead by reusing resources</li>
<li>Use memory mapping for large files to reduce I/O time</li>
<li>Choose faster algorithms (LZ4 vs Brotli, ChaCha20 vs AES)</li>
<li>Batch small operations to amortize coordination overhead</li>
</ul>
<h3 id="memory-usage-1"><a class="header" href="#memory-usage-1">Memory Usage</a></h3>
<p><strong>Formula:</strong></p>
<pre><code class="language-text">Peak Memory ≈ chunk_size × active_workers × files_concurrent
</code></pre>
<p><strong>Example:</strong></p>
<pre><code class="language-text">chunk_size = 64 MB
active_workers = 7
files_concurrent = 1
Peak Memory ≈ 64 MB × 7 × 1 = 448 MB
</code></pre>
<p><strong>Monitoring:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline::infrastructure::metrics::CONCURRENCY_METRICS;

let mem_mb = CONCURRENCY_METRICS.memory_used_mb();
let mem_pct = CONCURRENCY_METRICS.memory_utilization_percent();

println!("Memory: {:.2} MB ({:.1}%)", mem_mb, mem_pct);
<span class="boring">}</span></code></pre></pre>
<h2 id="optimization-strategies-1"><a class="header" href="#optimization-strategies-1">Optimization Strategies</a></h2>
<h3 id="1-chunk-size-optimization"><a class="header" href="#1-chunk-size-optimization">1. Chunk Size Optimization</a></h3>
<p><strong>Impact</strong>: Chunk size affects memory usage, I/O efficiency, and parallelism.</p>
<p><strong>Adaptive Chunk Sizing:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::value_objects::ChunkSize;

// Automatically selects optimal chunk size based on file size
let chunk_size = ChunkSize::optimal_for_file_size(file_size);

println!("Optimal chunk size: {}", chunk_size);  // e.g., "4.0MB"
<span class="boring">}</span></code></pre></pre>
<p><strong>Guidelines:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>File Size</th><th>Chunk Size</th><th>Rationale</th></tr></thead><tbody>
<tr><td>&lt; 10 MB (small)</td><td>64-256 KB</td><td>Minimize memory, enable fine-grained parallelism</td></tr>
<tr><td>10-100 MB (medium)</td><td>256 KB-1 MB</td><td>Balance memory and I/O efficiency</td></tr>
<tr><td>100 MB-1 GB (large)</td><td>1-4 MB</td><td>Reduce I/O overhead, acceptable memory usage</td></tr>
<tr><td>&gt; 1 GB (huge)</td><td>4-16 MB</td><td>Maximize I/O throughput, still bounded memory</td></tr>
</tbody></table>
</div>
<p><strong>Trade-offs:</strong></p>
<ul>
<li><strong>Small chunks</strong>: ✅ Lower memory, better parallelism ❌ Higher I/O overhead</li>
<li><strong>Large chunks</strong>: ✅ Lower I/O overhead ❌ Higher memory, less parallelism</li>
</ul>
<h3 id="2-worker-count-optimization"><a class="header" href="#2-worker-count-optimization">2. Worker Count Optimization</a></h3>
<p><strong>Impact</strong>: Worker count affects CPU utilization and resource contention.</p>
<p><strong>Adaptive Worker Count:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::value_objects::WorkerCount;

// File size + system resources + processing type
let workers = WorkerCount::optimal_for_processing_type(
    file_size,
    available_cores,
    is_cpu_intensive,  // true for compression/encryption
);

println!("Optimal workers: {}", workers);  // e.g., "8 workers"
<span class="boring">}</span></code></pre></pre>
<p><strong>Empirically Validated Strategies:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>File Size</th><th>Worker Count</th><th>Strategy</th><th>Benchmark Result</th></tr></thead><tbody>
<tr><td>5 MB (small)</td><td>9</td><td>Aggressive parallelism</td><td>+102% speedup</td></tr>
<tr><td>50 MB (medium)</td><td>5</td><td>Balanced approach</td><td>+70% speedup</td></tr>
<tr><td>2 GB (huge)</td><td>3</td><td>Conservative (avoid overhead)</td><td>+76% speedup</td></tr>
</tbody></table>
</div>
<p><strong>Why these strategies work:</strong></p>
<ul>
<li><strong>Small files</strong>: Task overhead is amortized quickly with many workers</li>
<li><strong>Medium files</strong>: Balanced to avoid both under-utilization and over-subscription</li>
<li><strong>Huge files</strong>: Fewer workers prevent memory pressure and coordination overhead</li>
</ul>
<h3 id="3-memory-mapping-vs-regular-io"><a class="header" href="#3-memory-mapping-vs-regular-io">3. Memory Mapping vs Regular I/O</a></h3>
<p><strong>When to use memory mapping:</strong></p>
<ul>
<li>✅ Files &gt; 100 MB (amortizes setup cost)</li>
<li>✅ Random access patterns (page cache efficiency)</li>
<li>✅ Read-heavy workloads (no write overhead)</li>
</ul>
<p><strong>When to use regular I/O:</strong></p>
<ul>
<li>✅ Files &lt; 10 MB (lower setup cost)</li>
<li>✅ Sequential access patterns (streaming)</li>
<li>✅ Write-heavy workloads (buffered writes)</li>
</ul>
<p><strong>Configuration:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline::infrastructure::services::FileIOServiceImpl;
use pipeline_domain::services::FileIOConfig;

let config = FileIOConfig {
    enable_memory_mapping: true,
    max_mmap_size: 1024 * 1024 * 1024,  // 1 GB threshold
    default_chunk_size: 64 * 1024,       // 64 KB chunks
    ..Default::default()
};

let service = FileIOServiceImpl::new(config);
<span class="boring">}</span></code></pre></pre>
<p><strong>Benchmark Results</strong> (from <code>pipeline/benches/file_io_benchmark.rs</code>):</p>
<div class="table-wrapper"><table><thead><tr><th>File Size</th><th>Regular I/O</th><th>Memory Mapping</th><th>Winner</th></tr></thead><tbody>
<tr><td>1 MB</td><td>2000 MB/s</td><td>1500 MB/s</td><td>Regular I/O</td></tr>
<tr><td>10 MB</td><td>1800 MB/s</td><td>1900 MB/s</td><td>Comparable</td></tr>
<tr><td>50 MB</td><td>1500 MB/s</td><td>2200 MB/s</td><td>Memory Mapping</td></tr>
<tr><td>100 MB</td><td>1400 MB/s</td><td>2500 MB/s</td><td>Memory Mapping</td></tr>
</tbody></table>
</div>
<h3 id="4-compression-algorithm-selection"><a class="header" href="#4-compression-algorithm-selection">4. Compression Algorithm Selection</a></h3>
<p><strong>Performance vs Compression Ratio:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>Algorithm</th><th>Compression Speed</th><th>Decompression Speed</th><th>Ratio</th><th>Use Case</th></tr></thead><tbody>
<tr><td><strong>LZ4</strong></td><td>500-700 MB/s</td><td>2000-3000 MB/s</td><td>2-3x</td><td>Real-time, low latency</td></tr>
<tr><td><strong>Zstd</strong></td><td>200-400 MB/s</td><td>600-800 MB/s</td><td>3-5x</td><td>Balanced, general use</td></tr>
<tr><td><strong>Brotli</strong></td><td>50-150 MB/s</td><td>300-500 MB/s</td><td>4-8x</td><td>Storage, high compression</td></tr>
</tbody></table>
</div>
<p><strong>Adaptive Selection:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::services::CompressionPriority;

// Automatic algorithm selection
let config = service.get_optimal_config(
    "data.bin",
    &amp;sample_data,
    CompressionPriority::Speed,  // or CompressionPriority::Ratio
)?;

println!("Selected: {:?}", config.algorithm);
<span class="boring">}</span></code></pre></pre>
<p><strong>Guidelines:</strong></p>
<ul>
<li><strong>Speed priority</strong>: LZ4 for streaming, real-time processing</li>
<li><strong>Balanced</strong>: Zstandard for general-purpose use</li>
<li><strong>Ratio priority</strong>: Brotli for archival, storage optimization</li>
</ul>
<h3 id="5-encryption-algorithm-selection"><a class="header" href="#5-encryption-algorithm-selection">5. Encryption Algorithm Selection</a></h3>
<p><strong>Performance Characteristics:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>Algorithm</th><th>Throughput</th><th>Security</th><th>Hardware Support</th></tr></thead><tbody>
<tr><td><strong>AES-256-GCM</strong></td><td>400-800 MB/s</td><td>Excellent</td><td>Yes (AES-NI)</td></tr>
<tr><td><strong>ChaCha20-Poly1305</strong></td><td>200-400 MB/s</td><td>Excellent</td><td>No</td></tr>
<tr><td><strong>XChaCha20-Poly1305</strong></td><td>180-350 MB/s</td><td>Excellent</td><td>No</td></tr>
</tbody></table>
</div>
<p><strong>Configuration:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::services::EncryptionAlgorithm;

// Use AES-256-GCM if hardware support available
let algorithm = if has_aes_ni() {
    EncryptionAlgorithm::Aes256Gcm  // 2-4x faster with AES-NI
} else {
    EncryptionAlgorithm::ChaCha20Poly1305  // Software fallback
};
<span class="boring">}</span></code></pre></pre>
<h2 id="common-bottlenecks"><a class="header" href="#common-bottlenecks">Common Bottlenecks</a></h2>
<h3 id="1-cpu-bottleneck"><a class="header" href="#1-cpu-bottleneck">1. CPU Bottleneck</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>CPU saturation &gt; 80%</li>
<li>High CPU wait times (P95 &gt; 50 ms)</li>
<li>Low I/O utilization</li>
</ul>
<p><strong>Causes:</strong></p>
<ul>
<li>Too many CPU-intensive operations (compression, encryption)</li>
<li>Insufficient worker count for CPU-bound work</li>
<li>Slow algorithms (Brotli on large files)</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Increase CPU tokens to match cores
let config = ResourceConfig {
    cpu_tokens: Some(available_cores),  // Use all cores
    ..Default::default()
};

// Use faster algorithms
let compression = CompressionAlgorithm::Lz4;  // Instead of Brotli
let encryption = EncryptionAlgorithm::Aes256Gcm;  // With AES-NI

// Optimize worker count
let workers = WorkerCount::optimal_for_processing_type(
    file_size,
    available_cores,
    true,  // CPU-intensive = true
);
<span class="boring">}</span></code></pre></pre>
<h3 id="2-io-bottleneck"><a class="header" href="#2-io-bottleneck">2. I/O Bottleneck</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>I/O saturation &gt; 80%</li>
<li>High I/O wait times (P95 &gt; 100 ms)</li>
<li>Low CPU utilization</li>
</ul>
<p><strong>Causes:</strong></p>
<ul>
<li>Too many concurrent I/O operations</li>
<li>Small chunk sizes causing excessive syscalls</li>
<li>Storage device queue depth exceeded</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Increase chunk size to reduce I/O overhead
let chunk_size = ChunkSize::from_mb(4)?;  // 4 MB chunks

// Reduce I/O concurrency for HDD
let config = ResourceConfig {
    storage_type: StorageType::HDD,  // 4 I/O tokens
    ..Default::default()
};

// Use memory mapping for large files
let use_mmap = file_size &gt; 100 * 1024 * 1024;  // &gt; 100 MB
<span class="boring">}</span></code></pre></pre>
<p><strong>I/O Optimization by Device:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>Device Type</th><th>Optimal Chunk Size</th><th>I/O Tokens</th><th>Strategy</th></tr></thead><tbody>
<tr><td><strong>HDD</strong></td><td>1-4 MB</td><td>4</td><td>Sequential, large chunks</td></tr>
<tr><td><strong>SSD</strong></td><td>256 KB-1 MB</td><td>12</td><td>Balanced</td></tr>
<tr><td><strong>NVMe</strong></td><td>64 KB-256 KB</td><td>24</td><td>Parallel, small chunks</td></tr>
</tbody></table>
</div>
<h3 id="3-memory-bottleneck"><a class="header" href="#3-memory-bottleneck">3. Memory Bottleneck</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>Memory utilization &gt; 80%</li>
<li>Swapping (check <code>vmstat</code>)</li>
<li>OOM errors</li>
</ul>
<p><strong>Causes:</strong></p>
<ul>
<li>Too many concurrent chunks allocated</li>
<li>Large chunk size × high worker count</li>
<li>Memory leaks or unbounded buffers</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Reduce chunk size
let chunk_size = ChunkSize::from_mb(16)?;  // Smaller chunks

// Limit concurrent workers
let config = ResourceConfig {
    cpu_tokens: Some(3),  // Fewer workers = less memory
    ..Default::default()
};

// Monitor memory closely
if CONCURRENCY_METRICS.memory_utilization_percent() &gt; 80.0 {
    warn!("High memory usage, reducing chunk size");
    chunk_size = ChunkSize::from_mb(8)?;
}
<span class="boring">}</span></code></pre></pre>
<h3 id="4-coordination-overhead"><a class="header" href="#4-coordination-overhead">4. Coordination Overhead</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>High task spawn latency</li>
<li>Context switching &gt; 10k/sec</li>
<li>Low overall throughput despite low resource usage</li>
</ul>
<p><strong>Causes:</strong></p>
<ul>
<li>Too many small tasks (excessive spawn_blocking calls)</li>
<li>High semaphore contention</li>
<li>Channel backpressure</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Batch small operations
if chunks.len() &lt; 10 {
    // Sequential for small batches (avoid spawn overhead)
    for chunk in chunks {
        process_chunk_sync(chunk)?;
    }
} else {
    // Parallel for large batches
    tokio::task::spawn_blocking(move || {
        RAYON_POOLS.cpu_bound_pool().install(|| {
            chunks.into_par_iter().map(process_chunk_sync).collect()
        })
    }).await??
}

// Reduce worker count to lower contention
let workers = WorkerCount::new(available_cores / 2);
<span class="boring">}</span></code></pre></pre>
<h2 id="tuning-parameters"><a class="header" href="#tuning-parameters">Tuning Parameters</a></h2>
<h3 id="chunk-size-tuning"><a class="header" href="#chunk-size-tuning">Chunk Size Tuning</a></h3>
<p><strong>Parameters:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct ChunkSize {
    pub const MIN_SIZE: usize = 1;              // 1 byte
    pub const MAX_SIZE: usize = 512 * 1024 * 1024;  // 512 MB
    pub const DEFAULT_SIZE: usize = 1024 * 1024;    // 1 MB
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Configuration:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Via ChunkSize value object
let chunk_size = ChunkSize::from_mb(4)?;

// Via CLI/config file
let chunk_size_mb = 4;
let chunk_size = ChunkSize::from_mb(chunk_size_mb)?;

// Adaptive (recommended)
let chunk_size = ChunkSize::optimal_for_file_size(file_size);
<span class="boring">}</span></code></pre></pre>
<p><strong>Impact:</strong></p>
<ul>
<li><strong>Memory</strong>: Directly proportional (2x chunk = 2x memory per worker)</li>
<li><strong>I/O overhead</strong>: Inversely proportional (2x chunk = 0.5x syscalls)</li>
<li><strong>Parallelism</strong>: Inversely proportional (2x chunk = 0.5x parallel units)</li>
</ul>
<h3 id="worker-count-tuning"><a class="header" href="#worker-count-tuning">Worker Count Tuning</a></h3>
<p><strong>Parameters:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct WorkerCount {
    pub const MIN_WORKERS: usize = 1;
    pub const MAX_WORKERS: usize = 32;
    pub const DEFAULT_WORKERS: usize = 4;
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Configuration:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Manual
let workers = WorkerCount::new(8);

// Adaptive (recommended)
let workers = WorkerCount::optimal_for_file_size(file_size);

// With system resources
let workers = WorkerCount::optimal_for_file_and_system(
    file_size,
    available_cores,
);

// With processing type
let workers = WorkerCount::optimal_for_processing_type(
    file_size,
    available_cores,
    is_cpu_intensive,
);
<span class="boring">}</span></code></pre></pre>
<p><strong>Impact:</strong></p>
<ul>
<li><strong>Throughput</strong>: Generally increases with workers (up to cores)</li>
<li><strong>Memory</strong>: Directly proportional (2x workers = 2x memory)</li>
<li><strong>Context switching</strong>: Increases with workers (diminishing returns &gt; 2x cores)</li>
</ul>
<h3 id="resource-token-tuning"><a class="header" href="#resource-token-tuning">Resource Token Tuning</a></h3>
<p><strong>CPU Tokens:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let config = ResourceConfig {
    cpu_tokens: Some(7),  // cores - 1 (default)
    ..Default::default()
};
<span class="boring">}</span></code></pre></pre>
<p><strong>I/O Tokens:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let config = ResourceConfig {
    io_tokens: Some(24),          // Device-specific
    storage_type: StorageType::NVMe,
    ..Default::default()
};
<span class="boring">}</span></code></pre></pre>
<p><strong>Impact:</strong></p>
<ul>
<li><strong>CPU tokens</strong>: Limits total CPU-bound parallelism across all files</li>
<li><strong>I/O tokens</strong>: Limits total I/O concurrency across all files</li>
<li><strong>Both</strong>: Prevent system oversubscription</li>
</ul>
<h2 id="performance-monitoring"><a class="header" href="#performance-monitoring">Performance Monitoring</a></h2>
<h3 id="real-time-metrics"><a class="header" href="#real-time-metrics">Real-Time Metrics</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline::infrastructure::metrics::CONCURRENCY_METRICS;
use std::time::Duration;

// Spawn monitoring task
tokio::spawn(async {
    let mut interval = tokio::time::interval(Duration::from_secs(5));
    loop {
        interval.tick().await;

        // Resource saturation
        let cpu_sat = CONCURRENCY_METRICS.cpu_saturation_percent();
        let io_sat = CONCURRENCY_METRICS.io_saturation_percent();
        let mem_util = CONCURRENCY_METRICS.memory_utilization_percent();

        // Wait time percentiles
        let cpu_p95 = CONCURRENCY_METRICS.cpu_wait_p95();
        let io_p95 = CONCURRENCY_METRICS.io_wait_p95();

        info!(
            "Resources: CPU={:.1}%, I/O={:.1}%, Mem={:.1}% | Wait: CPU={}ms, I/O={}ms",
            cpu_sat, io_sat, mem_util, cpu_p95, io_p95
        );

        // Alert on issues
        if cpu_sat &gt; 90.0 {
            warn!("CPU saturated - consider increasing workers or faster algorithms");
        }
        if mem_util &gt; 80.0 {
            warn!("High memory - consider reducing chunk size or workers");
        }
    }
});
<span class="boring">}</span></code></pre></pre>
<h3 id="processing-metrics"><a class="header" href="#processing-metrics">Processing Metrics</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::entities::ProcessingMetrics;

let metrics = ProcessingMetrics::new();
metrics.start();

// Process file...
for chunk in chunks {
    metrics.add_bytes_processed(chunk.data.len() as u64);
}

metrics.end();

// Report performance
println!("Throughput: {:.2} MB/s", metrics.throughput_mb_per_second());
println!("Duration: {:.2}s", metrics.duration().as_secs_f64());
println!("Processed: {} MB", metrics.bytes_processed() / (1024 * 1024));

// Stage-specific metrics
for stage_metrics in metrics.stage_metrics() {
    println!("  {}: {:.2} MB/s", stage_metrics.stage_name, stage_metrics.throughput);
}
<span class="boring">}</span></code></pre></pre>
<h2 id="performance-best-practices"><a class="header" href="#performance-best-practices">Performance Best Practices</a></h2>
<h3 id="1-use-adaptive-configuration"><a class="header" href="#1-use-adaptive-configuration">1. Use Adaptive Configuration</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Let the system optimize
let chunk_size = ChunkSize::optimal_for_file_size(file_size);
let workers = WorkerCount::optimal_for_processing_type(
    file_size,
    available_cores,
    is_cpu_intensive,
);

// ❌ Bad: Fixed values
let chunk_size = ChunkSize::from_mb(1)?;
let workers = WorkerCount::new(8);
<span class="boring">}</span></code></pre></pre>
<h3 id="2-choose-appropriate-algorithms"><a class="header" href="#2-choose-appropriate-algorithms">2. Choose Appropriate Algorithms</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Algorithm selection based on priority
let compression_config = service.get_optimal_config(
    file_extension,
    &amp;sample_data,
    CompressionPriority::Speed,  // or Ratio
)?;

// ❌ Bad: Always use same algorithm
let compression_config = CompressionConfig {
    algorithm: CompressionAlgorithm::Brotli,  // Slow!
    ..Default::default()
};
<span class="boring">}</span></code></pre></pre>
<h3 id="3-monitor-and-measure"><a class="header" href="#3-monitor-and-measure">3. Monitor and Measure</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Measure actual performance
let start = Instant::now();
let result = process_file(path).await?;
let duration = start.elapsed();

let throughput_mb_s = (file_size as f64 / duration.as_secs_f64()) / (1024.0 * 1024.0);
info!("Throughput: {:.2} MB/s", throughput_mb_s);

// ❌ Bad: Assume performance without measurement
let result = process_file(path).await?;
<span class="boring">}</span></code></pre></pre>
<h3 id="4-batch-small-operations"><a class="header" href="#4-batch-small-operations">4. Batch Small Operations</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Batch to amortize overhead
tokio::task::spawn_blocking(move || {
    RAYON_POOLS.cpu_bound_pool().install(|| {
        chunks.into_par_iter()
            .map(|chunk| process_chunk(chunk))
            .collect::&lt;Result&lt;Vec&lt;_&gt;, _&gt;&gt;()
    })
}).await??

// ❌ Bad: Spawn for each small operation
for chunk in chunks {
    tokio::task::spawn_blocking(move || {
        process_chunk(chunk)  // Excessive spawn overhead!
    }).await??
}
<span class="boring">}</span></code></pre></pre>
<h3 id="5-use-device-specific-settings"><a class="header" href="#5-use-device-specific-settings">5. Use Device-Specific Settings</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Configure for storage type
let config = ResourceConfig {
    storage_type: StorageType::NVMe,  // 24 I/O tokens
    io_tokens: Some(24),
    ..Default::default()
};

// ❌ Bad: One size fits all
let config = ResourceConfig {
    io_tokens: Some(12),  // May be suboptimal
    ..Default::default()
};
<span class="boring">}</span></code></pre></pre>
<h2 id="troubleshooting-performance-issues"><a class="header" href="#troubleshooting-performance-issues">Troubleshooting Performance Issues</a></h2>
<h3 id="issue-1-low-throughput-despite-low-resource-usage"><a class="header" href="#issue-1-low-throughput-despite-low-resource-usage">Issue 1: Low Throughput Despite Low Resource Usage</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>Throughput &lt; 100 MB/s</li>
<li>CPU usage &lt; 50%</li>
<li>I/O usage &lt; 50%</li>
</ul>
<p><strong>Diagnosis:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Check coordination overhead
let queue_depth = CONCURRENCY_METRICS.cpu_queue_depth();
let active_workers = CONCURRENCY_METRICS.active_workers();

println!("Queue: {}, Active: {}", queue_depth, active_workers);
<span class="boring">}</span></code></pre></pre>
<p><strong>Causes:</strong></p>
<ul>
<li>Too few workers (underutilization)</li>
<li>Small batch sizes (high spawn overhead)</li>
<li>Synchronous bottlenecks</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Increase workers
let workers = WorkerCount::new(available_cores);

// Batch operations
let batch_size = 100;
for batch in chunks.chunks(batch_size) {
    process_batch(batch).await?;
}
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-2-inconsistent-performance"><a class="header" href="#issue-2-inconsistent-performance">Issue 2: Inconsistent Performance</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>Performance varies widely between runs</li>
<li>High P99 latencies (&gt; 10x P50)</li>
</ul>
<p><strong>Diagnosis:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Check wait time distribution
let p50 = CONCURRENCY_METRICS.cpu_wait_p50();
let p95 = CONCURRENCY_METRICS.cpu_wait_p95();
let p99 = CONCURRENCY_METRICS.cpu_wait_p99();

println!("Wait times: P50={}ms, P95={}ms, P99={}ms", p50, p95, p99);
<span class="boring">}</span></code></pre></pre>
<p><strong>Causes:</strong></p>
<ul>
<li>Resource contention (high wait times)</li>
<li>GC pauses or memory pressure</li>
<li>External system interference</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Reduce contention
let config = ResourceConfig {
    cpu_tokens: Some(available_cores - 2),  // Leave headroom
    ..Default::default()
};

// Monitor memory
if mem_util &gt; 70.0 {
    chunk_size = ChunkSize::from_mb(chunk_size_mb / 2)?;
}
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-3-memory-growth"><a class="header" href="#issue-3-memory-growth">Issue 3: Memory Growth</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>Memory usage grows over time</li>
<li>Eventually triggers OOM or swapping</li>
</ul>
<p><strong>Diagnosis:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Track memory trends
let mem_start = CONCURRENCY_METRICS.memory_used_mb();
// ... process files ...
let mem_end = CONCURRENCY_METRICS.memory_used_mb();

if mem_end &gt; mem_start * 1.5 {
    warn!("Memory grew {:.1}%", ((mem_end - mem_start) / mem_start) * 100.0);
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Causes:</strong></p>
<ul>
<li>Memory leaks (improper cleanup)</li>
<li>Unbounded queues or buffers</li>
<li>Large chunk size with many workers</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Use RAII guards for cleanup
struct ChunkBuffer {
    data: Vec&lt;u8&gt;,
    _guard: MemoryGuard,
}

// Limit queue depth
let (tx, rx) = tokio::sync::mpsc::channel(100);  // Bounded channel

// Reduce chunk size
let chunk_size = ChunkSize::from_mb(16)?;  // Smaller
<span class="boring">}</span></code></pre></pre>
<h2 id="related-topics-7"><a class="header" href="#related-topics-7">Related Topics</a></h2>
<ul>
<li>See <a href="advanced/benchmarking.html">Benchmarking</a> for detailed benchmark methodology</li>
<li>See <a href="advanced/profiling.html">Profiling</a> for CPU and memory profiling techniques</li>
<li>See <a href="advanced/thread-pooling.html">Thread Pooling</a> for worker configuration</li>
<li>See <a href="advanced/resources.html">Resource Management</a> for token governance</li>
</ul>
<h2 id="summary-7"><a class="header" href="#summary-7">Summary</a></h2>
<p>The pipeline's performance optimization system provides:</p>
<ol>
<li><strong>Adaptive Configuration</strong>: Automatic chunk size and worker count optimization</li>
<li><strong>Algorithm Selection</strong>: Choose algorithms based on speed/ratio priority</li>
<li><strong>Resource Governance</strong>: Prevent oversubscription with token limits</li>
<li><strong>Memory Efficiency</strong>: Bounded memory usage with streaming processing</li>
<li><strong>Comprehensive Monitoring</strong>: Real-time metrics and performance tracking</li>
</ol>
<p><strong>Key Takeaways:</strong></p>
<ul>
<li>Use adaptive configuration (ChunkSize::optimal_for_file_size, WorkerCount::optimal_for_processing_type)</li>
<li>Choose algorithms based on workload (LZ4 for speed, Brotli for ratio)</li>
<li>Monitor metrics regularly (CPU/I/O saturation, wait times, throughput)</li>
<li>Tune based on bottleneck (CPU: increase workers/faster algorithms, I/O: increase chunk size, Memory: reduce chunk/workers)</li>
<li>Benchmark and measure actual performance (don't assume)</li>
</ul>
<p><strong>Performance Goals Achieved:</strong></p>
<ul>
<li>✅ Throughput: 100-500 MB/s (algorithm-dependent)</li>
<li>✅ Latency: &lt; 100 ms overhead for small files</li>
<li>✅ Memory: Bounded usage (chunk_size × workers × files)</li>
<li>✅ Scalability: Linear scaling up to available cores</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="benchmarking"><a class="header" href="#benchmarking">Benchmarking</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>This chapter covers the pipeline's benchmark suite, methodologies for measuring performance, and techniques for interpreting benchmark results to guide optimization decisions.</p>
<h2 id="overview-25"><a class="header" href="#overview-25">Overview</a></h2>
<p>The pipeline uses <a href="https://github.com/bheisler/criterion.rs">Criterion.rs</a> for rigorous, statistical benchmarking. Criterion provides:</p>
<ul>
<li><strong>Statistical Analysis</strong>: Measures mean, standard deviation, and percentiles</li>
<li><strong>Regression Detection</strong>: Identifies performance regressions automatically</li>
<li><strong>HTML Reports</strong>: Generates detailed visualizations and comparisons</li>
<li><strong>Outlier Detection</strong>: Filters statistical outliers for consistent results</li>
<li><strong>Iterative Measurement</strong>: Automatically determines iteration counts</li>
</ul>
<p><strong>Benchmark Location:</strong> <code>pipeline/benches/file_io_benchmark.rs</code></p>
<h2 id="benchmark-suite"><a class="header" href="#benchmark-suite">Benchmark Suite</a></h2>
<p>The benchmark suite covers four main categories:</p>
<h3 id="1-read-method-benchmarks"><a class="header" href="#1-read-method-benchmarks">1. Read Method Benchmarks</a></h3>
<p><strong>Purpose</strong>: Compare regular file I/O vs memory-mapped I/O across different file sizes.</p>
<p><strong>File Sizes Tested:</strong></p>
<ul>
<li>1 MB (small files)</li>
<li>10 MB (medium files)</li>
<li>50 MB (large files)</li>
<li>100 MB (very large files)</li>
</ul>
<p><strong>Methods Compared:</strong></p>
<ul>
<li><code>regular_io</code>: Traditional buffered file reading</li>
<li><code>memory_mapped</code>: Memory-mapped file access (mmap)</li>
</ul>
<p><strong>Configuration:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let service = FileIOServiceImpl::new(FileIOConfig {
    default_chunk_size: 64 * 1024,     // 64KB chunks
    max_mmap_size: 1024 * 1024 * 1024, // 1GB threshold
    enable_memory_mapping: true,
    ..Default::default()
});
<span class="boring">}</span></code></pre></pre>
<p><strong>Expected Results:</strong></p>
<ul>
<li><strong>Small files (&lt; 10 MB)</strong>: Regular I/O faster (lower setup cost)</li>
<li><strong>Large files (&gt; 50 MB)</strong>: Memory mapping faster (reduced copying)</li>
</ul>
<h3 id="2-chunk-size-benchmarks"><a class="header" href="#2-chunk-size-benchmarks">2. Chunk Size Benchmarks</a></h3>
<p><strong>Purpose</strong>: Determine optimal chunk sizes for different workloads.</p>
<p><strong>Chunk Sizes Tested:</strong></p>
<ul>
<li>4 KB (4096 bytes)</li>
<li>8 KB (8192 bytes)</li>
<li>16 KB (16384 bytes)</li>
<li>32 KB (32768 bytes)</li>
<li>64 KB (65536 bytes)</li>
<li>128 KB (131072 bytes)</li>
</ul>
<p><strong>File Size:</strong> 10 MB (representative medium file)</p>
<p><strong>Methods:</strong></p>
<ul>
<li><code>regular_io</code>: Chunked reading with various sizes</li>
<li><code>memory_mapped</code>: Memory mapping with logical chunk sizes</li>
</ul>
<p><strong>Expected Results:</strong></p>
<ul>
<li><strong>Smaller chunks (4-16 KB)</strong>: Higher overhead, more syscalls</li>
<li><strong>Medium chunks (32-64 KB)</strong>: Good balance for most workloads</li>
<li><strong>Larger chunks (128 KB+)</strong>: Lower syscall overhead, higher memory usage</li>
</ul>
<h3 id="3-checksum-calculation-benchmarks"><a class="header" href="#3-checksum-calculation-benchmarks">3. Checksum Calculation Benchmarks</a></h3>
<p><strong>Purpose</strong>: Measure overhead of integrity verification.</p>
<p><strong>Benchmarks:</strong></p>
<ul>
<li><code>with_checksums</code>: File reading with SHA-256 checksum calculation</li>
<li><code>without_checksums</code>: File reading without checksums</li>
<li><code>checksum_only</code>: Standalone checksum calculation</li>
</ul>
<p><strong>File Size:</strong> 10 MB</p>
<p><strong>Expected Results:</strong></p>
<ul>
<li>Checksum overhead: 10-30% depending on CPU and chunk size</li>
<li>Larger chunks reduce relative overhead (fewer per-chunk hash operations)</li>
</ul>
<h3 id="4-write-operation-benchmarks"><a class="header" href="#4-write-operation-benchmarks">4. Write Operation Benchmarks</a></h3>
<p><strong>Purpose</strong>: Compare write performance across data sizes and options.</p>
<p><strong>Data Sizes Tested:</strong></p>
<ul>
<li>1 KB (tiny writes)</li>
<li>10 KB (small writes)</li>
<li>100 KB (medium writes)</li>
<li>1000 KB / 1 MB (large writes)</li>
</ul>
<p><strong>Write Options:</strong></p>
<ul>
<li><code>write_data</code>: Buffered write without checksum</li>
<li><code>write_data_with_checksum</code>: Buffered write with SHA-256 checksum</li>
<li><code>write_data_sync</code>: Buffered write with fsync</li>
</ul>
<p><strong>Expected Results:</strong></p>
<ul>
<li>Write throughput: 100-1000 MB/s (buffered, no sync)</li>
<li>Checksum overhead: 10-30%</li>
<li>Sync overhead: 10-100x depending on storage device</li>
</ul>
<h2 id="running-benchmarks"><a class="header" href="#running-benchmarks">Running Benchmarks</a></h2>
<h3 id="basic-usage"><a class="header" href="#basic-usage">Basic Usage</a></h3>
<p><strong>Run all benchmarks:</strong></p>
<pre><code class="language-bash">cargo bench --bench file_io_benchmark
</code></pre>
<p><strong>Output:</strong></p>
<pre><code class="language-text">file_read_methods/regular_io/1    time:   [523.45 µs 528.12 µs 533.28 µs]
file_read_methods/regular_io/10   time:   [5.1234 ms 5.2187 ms 5.3312 ms]
file_read_methods/memory_mapped/1 time:   [612.34 µs 618.23 µs 624.91 µs]
file_read_methods/memory_mapped/10 time: [4.8923 ms 4.9721 ms 5.0584 ms]
...
</code></pre>
<h3 id="benchmark-groups"><a class="header" href="#benchmark-groups">Benchmark Groups</a></h3>
<p><strong>Run specific benchmark group:</strong></p>
<pre><code class="language-bash"># Read methods only
cargo bench --bench file_io_benchmark -- "file_read_methods"

# Chunk sizes only
cargo bench --bench file_io_benchmark -- "chunk_sizes"

# Checksums only
cargo bench --bench file_io_benchmark -- "checksum_calculation"

# Write operations only
cargo bench --bench file_io_benchmark -- "write_operations"
</code></pre>
<h3 id="specific-benchmarks"><a class="header" href="#specific-benchmarks">Specific Benchmarks</a></h3>
<p><strong>Run benchmarks matching a pattern:</strong></p>
<pre><code class="language-bash"># All regular_io benchmarks
cargo bench --bench file_io_benchmark -- "regular_io"

# Only 50MB file benchmarks
cargo bench --bench file_io_benchmark -- "/50"

# Memory-mapped with 64KB chunks
cargo bench --bench file_io_benchmark -- "memory_mapped/64"
</code></pre>
<h3 id="output-formats"><a class="header" href="#output-formats">Output Formats</a></h3>
<p><strong>HTML report (default):</strong></p>
<pre><code class="language-bash">cargo bench --bench file_io_benchmark

# Opens: target/criterion/report/index.html
open target/criterion/report/index.html
</code></pre>
<p><strong>Save baseline for comparison:</strong></p>
<pre><code class="language-bash"># Save current performance as baseline
cargo bench --bench file_io_benchmark -- --save-baseline main

# Compare against baseline after changes
cargo bench --bench file_io_benchmark -- --baseline main
</code></pre>
<p><strong>Example comparison output:</strong></p>
<pre><code class="language-text">file_read_methods/regular_io/50
                        time:   [24.512 ms 24.789 ms 25.091 ms]
                        change: [-5.2341% -4.1234% -2.9876%] (p = 0.00 &lt; 0.05)
                        Performance has improved.
</code></pre>
<h2 id="interpreting-results"><a class="header" href="#interpreting-results">Interpreting Results</a></h2>
<h3 id="understanding-output-1"><a class="header" href="#understanding-output-1">Understanding Output</a></h3>
<p><strong>Criterion output format:</strong></p>
<pre><code class="language-text">benchmark_name          time:   [lower_bound estimate upper_bound]
                        change: [lower% estimate% upper%] (p = X.XX &lt; 0.05)
</code></pre>
<p><strong>Components:</strong></p>
<ul>
<li><strong>lower_bound</strong>: 95% confidence interval lower bound</li>
<li><strong>estimate</strong>: Best estimate (typically median)</li>
<li><strong>upper_bound</strong>: 95% confidence interval upper bound</li>
<li><strong>change</strong>: Performance change vs baseline (if available)</li>
<li><strong>p-value</strong>: Statistical significance (&lt; 0.05 = significant)</li>
</ul>
<p><strong>Example:</strong></p>
<pre><code class="language-text">file_read_methods/regular_io/50
                        time:   [24.512 ms 24.789 ms 25.091 ms]
                        change: [-5.2341% -4.1234% -2.9876%] (p = 0.00 &lt; 0.05)
</code></pre>
<p><strong>Interpretation:</strong></p>
<ul>
<li><strong>Time</strong>: File reading takes ~24.79 ms (median)</li>
<li><strong>Confidence</strong>: 95% certain actual time is between 24.51-25.09 ms</li>
<li><strong>Change</strong>: 4.12% faster than baseline (statistically significant)</li>
<li><strong>Conclusion</strong>: Performance improvement confirmed</li>
</ul>
<h3 id="throughput-calculation"><a class="header" href="#throughput-calculation">Throughput Calculation</a></h3>
<p><strong>Calculate MB/s from benchmark time:</strong></p>
<pre><code class="language-text">File size: 50 MB
Time: 24.789 ms = 0.024789 seconds

Throughput = 50 MB / 0.024789 s = 2017 MB/s
</code></pre>
<p><strong>Rust code:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let file_size_mb = 50.0;
let time_ms = 24.789;
let throughput_mb_s = file_size_mb / (time_ms / 1000.0);

println!("Throughput: {:.2} MB/s", throughput_mb_s);  // 2017 MB/s
<span class="boring">}</span></code></pre></pre>
<h3 id="regression-detection"><a class="header" href="#regression-detection">Regression Detection</a></h3>
<p><strong>Performance regression indicators:</strong></p>
<p>✅ <strong>Improvement</strong> (faster):</p>
<pre><code class="language-text">change: [-10.234% -8.123% -6.012%] (p = 0.00 &lt; 0.05)
Performance has improved.
</code></pre>
<p>❌ <strong>Regression</strong> (slower):</p>
<pre><code class="language-text">change: [+6.234% +8.456% +10.891%] (p = 0.00 &lt; 0.05)
Performance has regressed.
</code></pre>
<p>⚠️ <strong>No significant change</strong>:</p>
<pre><code class="language-text">change: [-1.234% +0.456% +2.123%] (p = 0.42 &gt; 0.05)
No change in performance detected.
</code></pre>
<p><strong>Statistical significance:</strong></p>
<ul>
<li><strong>p &lt; 0.05</strong>: Change is statistically significant (95% confidence)</li>
<li><strong>p &gt; 0.05</strong>: Change could be noise (not statistically significant)</li>
</ul>
<h3 id="html-report-navigation"><a class="header" href="#html-report-navigation">HTML Report Navigation</a></h3>
<p>Criterion generates interactive HTML reports at <code>target/criterion/report/index.html</code>.</p>
<p><strong>Report Sections:</strong></p>
<ol>
<li><strong>Summary</strong>: All benchmarks with comparisons</li>
<li><strong>Individual Reports</strong>: Detailed analysis per benchmark</li>
<li><strong>Violin Plots</strong>: Distribution visualization</li>
<li><strong>History</strong>: Performance over time</li>
</ol>
<p><strong>Key Metrics in Report:</strong></p>
<ul>
<li><strong>Mean</strong>: Average execution time</li>
<li><strong>Median</strong>: 50th percentile (typical case)</li>
<li><strong>Std Dev</strong>: Variability in measurements</li>
<li><strong>MAD</strong>: Median Absolute Deviation (robust to outliers)</li>
<li><strong>Slope</strong>: Linear regression slope (for iteration scaling)</li>
</ul>
<h2 id="performance-baselines"><a class="header" href="#performance-baselines">Performance Baselines</a></h2>
<h3 id="establishing-baselines"><a class="header" href="#establishing-baselines">Establishing Baselines</a></h3>
<p><strong>Save baseline before optimizations:</strong></p>
<pre><code class="language-bash"># 1. Run benchmarks on main branch
git checkout main
cargo bench --bench file_io_benchmark -- --save-baseline main

# 2. Switch to feature branch
git checkout feature/optimize-io

# 3. Run benchmarks and compare
cargo bench --bench file_io_benchmark -- --baseline main
</code></pre>
<p><strong>Baseline management:</strong></p>
<pre><code class="language-bash"># List all baselines
ls target/criterion/*/base/

# Delete old baseline
rm -rf target/criterion/*/baseline_name/

# Compare two baselines
cargo bench -- --baseline old_baseline --save-baseline new_baseline
</code></pre>
<h3 id="baseline-strategy"><a class="header" href="#baseline-strategy">Baseline Strategy</a></h3>
<p><strong>Recommended baselines:</strong></p>
<ol>
<li><strong>main</strong>: Current production performance</li>
<li><strong>release-X.Y.Z</strong>: Tagged release versions</li>
<li><strong>pre-optimization</strong>: Before major optimization work</li>
<li><strong>target</strong>: Performance goals</li>
</ol>
<p><strong>Example workflow:</strong></p>
<pre><code class="language-bash"># Establish target baseline (goals)
cargo bench -- --save-baseline target

# Work on optimizations...
# (make changes, run benchmarks)

# Compare to target
cargo bench -- --baseline target

# If goals met, update main baseline
cargo bench -- --save-baseline main
</code></pre>
<h2 id="continuous-integration"><a class="header" href="#continuous-integration">Continuous Integration</a></h2>
<h3 id="cicd-integration"><a class="header" href="#cicd-integration">CI/CD Integration</a></h3>
<p><strong>GitHub Actions example:</strong></p>
<pre><code class="language-yaml">name: Benchmarks

on:
  pull_request:
    branches: [main]

jobs:
  benchmark:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Install Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable

      - name: Run benchmarks
        run: |
          cargo bench --bench file_io_benchmark -- --save-baseline pr

      - name: Compare to main
        run: |
          git fetch origin main:main
          git checkout main
          cargo bench --bench file_io_benchmark -- --save-baseline main
          git checkout -
          cargo bench --bench file_io_benchmark -- --baseline main

      - name: Upload results
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results
          path: target/criterion/
</code></pre>
<h3 id="regression-alerts"><a class="header" href="#regression-alerts">Regression Alerts</a></h3>
<p><strong>Detect regressions in CI:</strong></p>
<pre><code class="language-bash">#!/bin/bash
# scripts/check_benchmarks.sh

# Run benchmarks and save output
cargo bench --bench file_io_benchmark -- --baseline main &gt; bench_output.txt

# Check for regressions
if grep -q "Performance has regressed" bench_output.txt; then
    echo "❌ Performance regression detected!"
    grep "Performance has regressed" bench_output.txt
    exit 1
else
    echo "✅ No performance regressions detected"
    exit 0
fi
</code></pre>
<h2 id="benchmark-best-practices"><a class="header" href="#benchmark-best-practices">Benchmark Best Practices</a></h2>
<h3 id="1-use-representative-workloads"><a class="header" href="#1-use-representative-workloads">1. Use Representative Workloads</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Test realistic file sizes
for size_mb in [1, 10, 50, 100, 500, 1000].iter() {
    let test_file = create_test_file(*size_mb);
    benchmark_file_io(&amp;test_file);
}

// ❌ Bad: Only tiny files
let test_file = create_test_file(1);  // Not representative!
<span class="boring">}</span></code></pre></pre>
<h3 id="2-control-variables"><a class="header" href="#2-control-variables">2. Control Variables</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Isolate what you're measuring
group.bench_function("compression_only", |b| {
    b.iter(|| {
        // Only benchmark compression, not I/O
        compress_data(black_box(&amp;test_data))
    });
});

// ❌ Bad: Measuring multiple things
group.bench_function("compression_and_io", |b| {
    b.iter(|| {
        let data = read_file(path);  // I/O overhead!
        compress_data(&amp;data)         // Compression
    });
});
<span class="boring">}</span></code></pre></pre>
<h3 id="3-use-black_box-to-prevent-optimization"><a class="header" href="#3-use-black_box-to-prevent-optimization">3. Use <code>black_box</code> to Prevent Optimization</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Prevent compiler from eliminating code
b.iter(|| {
    let result = expensive_operation();
    black_box(result);  // Ensures result is not optimized away
});

// ❌ Bad: Compiler may optimize away the work
b.iter(|| {
    expensive_operation();  // Result unused, may be eliminated!
});
<span class="boring">}</span></code></pre></pre>
<h3 id="4-warm-up-before-measuring"><a class="header" href="#4-warm-up-before-measuring">4. Warm Up Before Measuring</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Criterion handles warmup automatically
c.bench_function("my_benchmark", |b| {
    // Criterion runs warmup iterations automatically
    b.iter(|| expensive_operation());
});

// ❌ Bad: Manual warmup (unnecessary with Criterion)
for _ in 0..100 {
    expensive_operation();  // Criterion does this for you!
}
<span class="boring">}</span></code></pre></pre>
<h3 id="5-measure-multiple-configurations"><a class="header" href="#5-measure-multiple-configurations">5. Measure Multiple Configurations</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Test parameter space
let mut group = c.benchmark_group("chunk_sizes");
for chunk_size in [4096, 8192, 16384, 32768, 65536].iter() {
    group.bench_with_input(
        BenchmarkId::from_parameter(chunk_size),
        chunk_size,
        |b, &amp;size| b.iter(|| process_with_chunk_size(size))
    );
}

// ❌ Bad: Single configuration
c.bench_function("process", |b| {
    b.iter(|| process_with_chunk_size(65536));  // Only one size!
});
<span class="boring">}</span></code></pre></pre>
<h2 id="example-benchmark-analysis"><a class="header" href="#example-benchmark-analysis">Example Benchmark Analysis</a></h2>
<h3 id="scenario-optimizing-chunk-size"><a class="header" href="#scenario-optimizing-chunk-size">Scenario: Optimizing Chunk Size</a></h3>
<p><strong>1. Run baseline benchmarks:</strong></p>
<pre><code class="language-bash">cargo bench --bench file_io_benchmark -- "chunk_sizes" --save-baseline before
</code></pre>
<p><strong>Results:</strong></p>
<pre><code class="language-text">chunk_sizes/regular_io/4096   time:   [82.341 ms 83.129 ms 83.987 ms]
chunk_sizes/regular_io/16384  time:   [62.123 ms 62.891 ms 63.712 ms]
chunk_sizes/regular_io/65536  time:   [52.891 ms 53.523 ms 54.201 ms]
</code></pre>
<p><strong>2. Calculate throughput:</strong></p>
<pre><code class="language-text">File size: 10 MB
4KB chunks:   10 / 0.083129 = 120.3 MB/s
16KB chunks:  10 / 0.062891 = 159.0 MB/s
64KB chunks:  10 / 0.053523 = 186.8 MB/s
</code></pre>
<p><strong>3. Analysis:</strong></p>
<ul>
<li><strong>4 KB chunks</strong>: Slow (120 MB/s) due to syscall overhead</li>
<li><strong>16 KB chunks</strong>: Better (159 MB/s), balanced</li>
<li><strong>64 KB chunks</strong>: Best (187 MB/s), amortizes overhead</li>
</ul>
<p><strong>4. Recommendation:</strong></p>
<p>Use 64 KB chunks for medium files (10-100 MB).</p>
<h3 id="scenario-memory-mapping-threshold"><a class="header" href="#scenario-memory-mapping-threshold">Scenario: Memory Mapping Threshold</a></h3>
<p><strong>1. Run benchmarks across file sizes:</strong></p>
<pre><code class="language-bash">cargo bench --bench file_io_benchmark -- "file_read_methods"
</code></pre>
<p><strong>Results:</strong></p>
<pre><code class="language-text">regular_io/1      time:   [523.45 µs 528.12 µs 533.28 µs]  → 1894 MB/s
memory_mapped/1   time:   [612.34 µs 618.23 µs 624.91 µs]  → 1618 MB/s

regular_io/50     time:   [24.512 ms 24.789 ms 25.091 ms]  → 2017 MB/s
memory_mapped/50  time:   [19.234 ms 19.512 ms 19.812 ms]  → 2563 MB/s

regular_io/100    time:   [52.123 ms 52.891 ms 53.712 ms]  → 1891 MB/s
memory_mapped/100 time:   [38.234 ms 38.712 ms 39.234 ms]  → 2584 MB/s
</code></pre>
<p><strong>2. Crossover analysis:</strong></p>
<ul>
<li><strong>1 MB</strong>: Regular I/O faster (1894 vs 1618 MB/s)</li>
<li><strong>50 MB</strong>: Memory mapping faster (2563 vs 2017 MB/s) - <strong>27% improvement</strong></li>
<li><strong>100 MB</strong>: Memory mapping faster (2584 vs 1891 MB/s) - <strong>37% improvement</strong></li>
</ul>
<p><strong>3. Recommendation:</strong></p>
<p>Use memory mapping for files &gt; 10 MB (threshold between 1-50 MB).</p>
<h2 id="related-topics-8"><a class="header" href="#related-topics-8">Related Topics</a></h2>
<ul>
<li>See <a href="advanced/performance.html">Performance Optimization</a> for optimization strategies</li>
<li>See <a href="advanced/profiling.html">Profiling</a> for CPU and memory profiling</li>
<li>See <a href="advanced/thread-pooling.html">Thread Pooling</a> for concurrency tuning</li>
<li>See <a href="advanced/resources.html">Resource Management</a> for resource limits</li>
</ul>
<h2 id="summary-8"><a class="header" href="#summary-8">Summary</a></h2>
<p>The pipeline's benchmarking suite provides:</p>
<ol>
<li><strong>Comprehensive Coverage</strong>: Read/write operations, chunk sizes, checksums</li>
<li><strong>Statistical Rigor</strong>: Criterion.rs with confidence intervals and regression detection</li>
<li><strong>Baseline Comparison</strong>: Track performance changes over time</li>
<li><strong>CI/CD Integration</strong>: Automated regression detection in pull requests</li>
<li><strong>HTML Reports</strong>: Interactive visualizations and detailed analysis</li>
</ol>
<p><strong>Key Takeaways:</strong></p>
<ul>
<li>Run benchmarks before and after optimizations (<code>--save-baseline</code>)</li>
<li>Use representative workloads (realistic file sizes and configurations)</li>
<li>Look for statistically significant changes (p &lt; 0.05)</li>
<li>Calculate throughput (MB/s) for intuitive performance comparison</li>
<li>Integrate benchmarks into CI/CD for regression prevention</li>
<li>Use HTML reports for detailed analysis and visualization</li>
</ul>
<p><strong>Benchmark Commands:</strong></p>
<pre><code class="language-bash"># Run all benchmarks
cargo bench --bench file_io_benchmark

# Save baseline
cargo bench --bench file_io_benchmark -- --save-baseline main

# Compare to baseline
cargo bench --bench file_io_benchmark -- --baseline main

# Specific group
cargo bench --bench file_io_benchmark -- "file_read_methods"
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="profiling"><a class="header" href="#profiling">Profiling</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>This chapter covers profiling tools and techniques for identifying performance bottlenecks, analyzing CPU usage, memory allocation patterns, and optimizing the pipeline's runtime characteristics.</p>
<h2 id="overview-26"><a class="header" href="#overview-26">Overview</a></h2>
<p><strong>Profiling</strong> is the process of measuring where your program spends time and allocates memory. Unlike benchmarking (which measures aggregate performance), profiling provides detailed insights into:</p>
<ul>
<li><strong>CPU hotspots</strong>: Which functions consume the most CPU time</li>
<li><strong>Memory allocation</strong>: Where and how much memory is allocated</li>
<li><strong>Lock contention</strong>: Where threads wait for synchronization</li>
<li><strong>Cache misses</strong>: Memory access patterns affecting performance</li>
</ul>
<p><strong>When to profile:</strong></p>
<ul>
<li>✅ After identifying performance issues in benchmarks</li>
<li>✅ When optimizing critical paths</li>
<li>✅ When investigating unexplained slowness</li>
<li>✅ Before and after major architectural changes</li>
</ul>
<p><strong>When NOT to profile:</strong></p>
<ul>
<li>❌ Before establishing performance goals</li>
<li>❌ On trivial workloads (profile representative cases)</li>
<li>❌ Without benchmarks (profile after quantifying the issue)</li>
</ul>
<h2 id="profiling-tools"><a class="header" href="#profiling-tools">Profiling Tools</a></h2>
<h3 id="cpu-profiling-tools"><a class="header" href="#cpu-profiling-tools">CPU Profiling Tools</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Tool</th><th>Platform</th><th>Sampling</th><th>Instrumentation</th><th>Overhead</th><th>Output</th></tr></thead><tbody>
<tr><td><strong>perf</strong></td><td>Linux</td><td>Yes</td><td>No</td><td>Low (1-5%)</td><td>Text/perf.data</td></tr>
<tr><td><strong>flamegraph</strong></td><td>All</td><td>Yes</td><td>No</td><td>Low (1-5%)</td><td>SVG</td></tr>
<tr><td><strong>samply</strong></td><td>All</td><td>Yes</td><td>No</td><td>Low (1-5%)</td><td>Firefox Profiler</td></tr>
<tr><td><strong>Instruments</strong></td><td>macOS</td><td>Yes</td><td>Optional</td><td>Low-Med</td><td>GUI</td></tr>
<tr><td><strong>VTune</strong></td><td>Linux/Windows</td><td>Yes</td><td>Optional</td><td>Med</td><td>GUI</td></tr>
</tbody></table>
</div>
<p><strong>Recommended for Rust:</strong></p>
<ul>
<li><strong>Linux</strong>: <code>perf</code> + <code>flamegraph</code></li>
<li><strong>macOS</strong>: <code>samply</code> or Instruments</li>
<li><strong>Windows</strong>: VTune or Windows Performance Analyzer</li>
</ul>
<h3 id="memory-profiling-tools"><a class="header" href="#memory-profiling-tools">Memory Profiling Tools</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Tool</th><th>Platform</th><th>Heap</th><th>Leaks</th><th>Peak Usage</th><th>Output</th></tr></thead><tbody>
<tr><td><strong>valgrind (massif)</strong></td><td>Linux/macOS</td><td>Yes</td><td>No</td><td>Yes</td><td>Text/ms_print</td></tr>
<tr><td><strong>heaptrack</strong></td><td>Linux</td><td>Yes</td><td>Yes</td><td>Yes</td><td>GUI</td></tr>
<tr><td><strong>dhat</strong></td><td>Linux/macOS</td><td>Yes</td><td>No</td><td>Yes</td><td>JSON/Web UI</td></tr>
<tr><td><strong>Instruments</strong></td><td>macOS</td><td>Yes</td><td>Yes</td><td>Yes</td><td>GUI</td></tr>
</tbody></table>
</div>
<p><strong>Recommended for Rust:</strong></p>
<ul>
<li><strong>Linux</strong>: <code>heaptrack</code> or <code>dhat</code></li>
<li><strong>macOS</strong>: Instruments (Allocations template)</li>
<li><strong>Memory leaks</strong>: <code>valgrind (memcheck)</code></li>
</ul>
<h2 id="cpu-profiling"><a class="header" href="#cpu-profiling">CPU Profiling</a></h2>
<h3 id="using-perf-linux"><a class="header" href="#using-perf-linux">Using <code>perf</code> (Linux)</a></h3>
<p><strong>Setup:</strong></p>
<pre><code class="language-bash"># Install perf
sudo apt-get install linux-tools-common linux-tools-generic  # Ubuntu/Debian
sudo dnf install perf  # Fedora

# Build with debug symbols
cargo build --release --bin pipeline

# Enable perf for non-root users
echo -1 | sudo tee /proc/sys/kernel/perf_event_paranoid
</code></pre>
<p><strong>Record profile:</strong></p>
<pre><code class="language-bash"># Profile pipeline execution
perf record --call-graph dwarf \
    ./target/release/pipeline process testdata/large-file.bin

# Output: perf.data
</code></pre>
<p><strong>Analyze results:</strong></p>
<pre><code class="language-bash"># Interactive TUI
perf report

# Text summary
perf report --stdio

# Function call graph
perf report --stdio --no-children

# Top functions
perf report --stdio | head -20
</code></pre>
<p><strong>Example output:</strong></p>
<pre><code class="language-text"># Overhead  Command   Shared Object     Symbol
# ........  ........  ................  .......................
    45.23%  pipeline  libbrotli.so      BrotliEncoderCompress
    18.91%  pipeline  pipeline          rayon::iter::collect
    12.34%  pipeline  libcrypto.so      AES_encrypt
     8.72%  pipeline  pipeline          tokio::runtime::task
     6.45%  pipeline  libc.so           memcpy
     3.21%  pipeline  pipeline          std::io::Write::write_all
</code></pre>
<p><strong>Interpretation:</strong></p>
<ul>
<li><strong>45% in BrotliEncoderCompress</strong>: Compression is the hotspot</li>
<li><strong>19% in rayon::iter::collect</strong>: Parallel iteration overhead</li>
<li><strong>12% in AES_encrypt</strong>: Encryption is expensive but expected</li>
<li><strong>9% in Tokio task</strong>: Async runtime overhead</li>
</ul>
<p><strong>Optimization targets:</strong></p>
<ol>
<li>Use faster compression (LZ4 instead of Brotli)</li>
<li>Reduce Rayon parallelism overhead (larger chunks)</li>
<li>Use AES-NI hardware acceleration</li>
</ol>
<h3 id="using-flamegraph-all-platforms"><a class="header" href="#using-flamegraph-all-platforms">Using Flamegraph (All Platforms)</a></h3>
<p><strong>Setup:</strong></p>
<pre><code class="language-bash"># Install cargo-flamegraph
cargo install flamegraph

# Linux: Install perf (see above)

# macOS: Install DTrace (built-in)

# Windows: Install WPA (Windows Performance Analyzer)
</code></pre>
<p><strong>Generate flamegraph:</strong></p>
<pre><code class="language-bash"># Profile and generate SVG
cargo flamegraph --bin pipeline -- process testdata/large-file.bin

# Output: flamegraph.svg
open flamegraph.svg
</code></pre>
<p><strong>Flamegraph structure:</strong></p>
<pre><code class="language-text">┌─────────────────────────────────────────────────────────────┐
│                     main (100%)                             │ ← Root
├─────────────────────┬───────────────────┬───────────────────┤
│ tokio::runtime (50%)│ rayon::iter (30%) │ other (20%)       │
├──────┬──────┬───────┼─────────┬─────────┴───────────────────┤
│ I/O  │ task │ ... │compress │ encrypt │ hash │ ...          │
│ 20%  │ 15%  │  15%│   18%   │   8%    │  4%  │              │
└──────┴──────┴───────┴─────────┴─────────┴──────┴─────────────┘
</code></pre>
<p><strong>Reading flamegraphs:</strong></p>
<ul>
<li><strong>Width</strong>: Percentage of CPU time (wider = more time)</li>
<li><strong>Height</strong>: Call stack depth (bottom = entry point, top = leaf functions)</li>
<li><strong>Color</strong>: Random (for visual distinction, not meaningful)</li>
<li><strong>Interactive</strong>: Click to zoom, search functions</li>
</ul>
<p><strong>Example analysis:</strong></p>
<p>If you see:</p>
<pre><code class="language-text">main → tokio::runtime → spawn_blocking → rayon::par_iter → compress → brotli
                                                               45%
</code></pre>
<p><strong>Interpretation:</strong></p>
<ul>
<li>45% of time spent in Brotli compression</li>
<li>Called through Rayon parallel iteration</li>
<li>Spawned from Tokio's blocking thread pool</li>
</ul>
<p><strong>Action:</strong></p>
<ul>
<li>Evaluate if 45% compression time is acceptable</li>
<li>If too slow, switch to LZ4 or reduce compression level</li>
</ul>
<h3 id="using-samply-all-platforms"><a class="header" href="#using-samply-all-platforms">Using Samply (All Platforms)</a></h3>
<p><strong>Setup:</strong></p>
<pre><code class="language-bash"># Install samply
cargo install samply
</code></pre>
<p><strong>Profile and view:</strong></p>
<pre><code class="language-bash"># Profile and open in Firefox Profiler
samply record --release -- cargo run --bin pipeline -- process testdata/large-file.bin

# Automatically opens in Firefox Profiler web UI
</code></pre>
<p><strong>Firefox Profiler features:</strong></p>
<ul>
<li><strong>Timeline view</strong>: See CPU usage over time</li>
<li><strong>Call tree</strong>: Hierarchical function breakdown</li>
<li><strong>Flame graph</strong>: Interactive flamegraph</li>
<li><strong>Marker timeline</strong>: Custom events and annotations</li>
<li><strong>Thread activity</strong>: Per-thread CPU usage</li>
</ul>
<p><strong>Advantages over static flamegraphs:</strong></p>
<ul>
<li>✅ Interactive (zoom, filter, search)</li>
<li>✅ Timeline view (see activity over time)</li>
<li>✅ Multi-threaded visualization</li>
<li>✅ Easy sharing (web-based)</li>
</ul>
<h2 id="memory-profiling"><a class="header" href="#memory-profiling">Memory Profiling</a></h2>
<h3 id="using-valgrind-massif-linuxmacos"><a class="header" href="#using-valgrind-massif-linuxmacos">Using Valgrind Massif (Linux/macOS)</a></h3>
<p><strong>Setup:</strong></p>
<pre><code class="language-bash"># Install valgrind
sudo apt-get install valgrind  # Ubuntu/Debian
brew install valgrind  # macOS (Intel only)
</code></pre>
<p><strong>Profile heap usage:</strong></p>
<pre><code class="language-bash"># Run with Massif
valgrind --tool=massif --massif-out-file=massif.out \
    ./target/release/pipeline process testdata/large-file.bin

# Visualize results
ms_print massif.out
</code></pre>
<p><strong>Example output:</strong></p>
<pre><code class="language-text">--------------------------------------------------------------------------------
  n        time(i)         total(B)   useful-heap(B) extra-heap(B)    stacks(B)
--------------------------------------------------------------------------------
  0              0                0                0             0            0
  1        123,456       64,000,000       64,000,000             0            0
  2        234,567      128,000,000      128,000,000             0            0
  3        345,678      192,000,000      192,000,000             0            0  ← Peak
  4        456,789      128,000,000      128,000,000             0            0
  5        567,890       64,000,000       64,000,000             0            0
  6        678,901                0                0             0            0

Peak memory: 192 MB

Top allocations:
    45.2%  (87 MB)  Vec::with_capacity (chunk buffer)
    23.1%  (44 MB)  Vec::with_capacity (compressed data)
    15.7%  (30 MB)  Box::new (encryption context)
    ...
</code></pre>
<p><strong>Interpretation:</strong></p>
<ul>
<li><strong>Peak memory</strong>: 192 MB</li>
<li><strong>Main contributor</strong>: 87 MB chunk buffers (45%)</li>
<li><strong>Growth pattern</strong>: Linear increase then decrease (expected for streaming)</li>
</ul>
<p><strong>Optimization:</strong></p>
<ul>
<li>Reduce chunk size to lower peak memory</li>
<li>Reuse buffers instead of allocating new ones</li>
<li>Use <code>SmallVec</code> for small allocations</li>
</ul>
<h3 id="using-heaptrack-linux"><a class="header" href="#using-heaptrack-linux">Using Heaptrack (Linux)</a></h3>
<p><strong>Setup:</strong></p>
<pre><code class="language-bash"># Install heaptrack
sudo apt-get install heaptrack heaptrack-gui  # Ubuntu/Debian
</code></pre>
<p><strong>Profile and analyze:</strong></p>
<pre><code class="language-bash"># Record heap usage
heaptrack ./target/release/pipeline process testdata/large-file.bin

# Output: heaptrack.pipeline.12345.gz

# Analyze with GUI
heaptrack_gui heaptrack.pipeline.12345.gz
</code></pre>
<p><strong>Heaptrack GUI features:</strong></p>
<ul>
<li><strong>Summary</strong>: Total allocations, peak memory, leaked memory</li>
<li><strong>Top allocators</strong>: Functions allocating the most</li>
<li><strong>Flame graph</strong>: Allocation call chains</li>
<li><strong>Timeline</strong>: Memory usage over time</li>
<li><strong>Leak detection</strong>: Allocated but never freed</li>
</ul>
<p><strong>Example metrics:</strong></p>
<pre><code class="language-text">Total allocations: 1,234,567
Total allocated: 15.2 GB
Peak heap: 192 MB
Peak RSS: 256 MB
Leaked: 0 bytes
</code></pre>
<p><strong>Top allocators:</strong></p>
<pre><code class="language-text">1. Vec::with_capacity (chunk buffer)        5.4 GB  (35%)
2. tokio::spawn (task allocation)           2.1 GB  (14%)
3. Vec::with_capacity (compressed data)     1.8 GB  (12%)
</code></pre>
<h3 id="using-dhat-linuxmacos"><a class="header" href="#using-dhat-linuxmacos">Using DHAT (Linux/macOS)</a></h3>
<p><strong>Setup:</strong></p>
<pre><code class="language-bash"># Install valgrind with DHAT
sudo apt-get install valgrind
</code></pre>
<p><strong>Profile with DHAT:</strong></p>
<pre><code class="language-bash"># Run with DHAT
valgrind --tool=dhat --dhat-out-file=dhat.out \
    ./target/release/pipeline process testdata/large-file.bin

# Output: dhat.out.12345

# View in web UI
dhat_viewer dhat.out.12345
# Or upload to https://nnethercote.github.io/dh_view/dh_view.html
</code></pre>
<p><strong>DHAT metrics:</strong></p>
<ul>
<li><strong>Total bytes allocated</strong>: Cumulative allocation size</li>
<li><strong>Total blocks allocated</strong>: Number of allocations</li>
<li><strong>Peak bytes</strong>: Maximum heap size</li>
<li><strong>Average block size</strong>: Typical allocation size</li>
<li><strong>Short-lived</strong>: Allocations freed quickly</li>
</ul>
<p><strong>Example output:</strong></p>
<pre><code class="language-text">Total:     15.2 GB in 1,234,567 blocks
Peak:      192 MB
At t-gmax: 187 MB in 145 blocks
At t-end:  0 B in 0 blocks

Top allocation sites:
  1. 35.4% (5.4 GB in 98,765 blocks)
     Vec::with_capacity (file_io.rs:123)
     ← chunk buffer allocation

  2. 14.2% (2.1 GB in 567,890 blocks)
     tokio::spawn (runtime.rs:456)
     ← task overhead

  3. 11.8% (1.8 GB in 45,678 blocks)
     Vec::with_capacity (compression.rs:789)
     ← compressed buffer
</code></pre>
<h2 id="profiling-workflows"><a class="header" href="#profiling-workflows">Profiling Workflows</a></h2>
<h3 id="workflow-1-identify-cpu-hotspot"><a class="header" href="#workflow-1-identify-cpu-hotspot">Workflow 1: Identify CPU Hotspot</a></h3>
<p><strong>1. Establish baseline (benchmark):</strong></p>
<pre><code class="language-bash">cargo bench --bench file_io_benchmark -- --save-baseline before
</code></pre>
<p><strong>2. Profile with perf + flamegraph:</strong></p>
<pre><code class="language-bash">cargo flamegraph --bin pipeline -- process testdata/large-file.bin
open flamegraph.svg
</code></pre>
<p><strong>3. Identify hotspot:</strong></p>
<p>Look for wide bars in flamegraph (e.g., 45% in <code>BrotliEncoderCompress</code>).</p>
<p><strong>4. Optimize:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Before: Brotli (slow, high compression)
let compression = CompressionAlgorithm::Brotli;

// After: LZ4 (fast, lower compression)
let compression = CompressionAlgorithm::Lz4;
<span class="boring">}</span></code></pre></pre>
<p><strong>5. Verify improvement:</strong></p>
<pre><code class="language-bash">cargo flamegraph --bin pipeline -- process testdata/large-file.bin
# Check if Brotli bar shrunk

cargo bench --bench file_io_benchmark -- --baseline before
# Expect: Performance has improved
</code></pre>
<h3 id="workflow-2-reduce-memory-usage"><a class="header" href="#workflow-2-reduce-memory-usage">Workflow 2: Reduce Memory Usage</a></h3>
<p><strong>1. Profile heap usage:</strong></p>
<pre><code class="language-bash">heaptrack ./target/release/pipeline process testdata/large-file.bin
heaptrack_gui heaptrack.pipeline.12345.gz
</code></pre>
<p><strong>2. Identify large allocations:</strong></p>
<p>Look for top allocators (e.g., 87 MB chunk buffers).</p>
<p><strong>3. Calculate optimal size:</strong></p>
<pre><code class="language-text">Current: 64 MB chunks × 8 workers = 512 MB peak
Target: &lt; 256 MB peak
Solution: 16 MB chunks × 8 workers = 128 MB peak
</code></pre>
<p><strong>4. Reduce chunk size:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Before
let chunk_size = ChunkSize::from_mb(64)?;

// After
let chunk_size = ChunkSize::from_mb(16)?;
<span class="boring">}</span></code></pre></pre>
<p><strong>5. Verify reduction:</strong></p>
<pre><code class="language-bash">heaptrack ./target/release/pipeline process testdata/large-file.bin
# Check peak memory: should be &lt; 256 MB
</code></pre>
<h3 id="workflow-3-optimize-parallel-code"><a class="header" href="#workflow-3-optimize-parallel-code">Workflow 3: Optimize Parallel Code</a></h3>
<p><strong>1. Profile with perf:</strong></p>
<pre><code class="language-bash">perf record --call-graph dwarf \
    ./target/release/pipeline process testdata/large-file.bin

perf report --stdio
</code></pre>
<p><strong>2. Check for synchronization overhead:</strong></p>
<pre><code class="language-text">12.3%  pipeline  [kernel]     futex_wait    ← Lock contention
 8.7%  pipeline  pipeline     rayon::join    ← Coordination
 6.5%  pipeline  pipeline     Arc::clone     ← Reference counting
</code></pre>
<p><strong>3. Reduce contention:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Before: Shared mutex (high contention)
let counter = Arc::new(Mutex::new(0));

// After: Per-thread counters (no contention)
let counter = Arc::new(AtomicUsize::new(0));
counter.fetch_add(1, Ordering::Relaxed);
<span class="boring">}</span></code></pre></pre>
<p><strong>4. Verify improvement:</strong></p>
<pre><code class="language-bash">perf record ./target/release/pipeline process testdata/large-file.bin
perf report --stdio
# Check if futex_wait reduced
</code></pre>
<h2 id="interpreting-results-1"><a class="header" href="#interpreting-results-1">Interpreting Results</a></h2>
<h3 id="cpu-profile-patterns"><a class="header" href="#cpu-profile-patterns">CPU Profile Patterns</a></h3>
<p><strong>Pattern 1: Single Hotspot</strong></p>
<pre><code class="language-text">compress_chunk: 65%  ← Dominant function
encrypt_chunk:  15%
write_chunk:    10%
other:          10%
</code></pre>
<p><strong>Action</strong>: Optimize the 65% hotspot (use faster algorithm or optimize implementation).</p>
<p><strong>Pattern 2: Distributed Cost</strong></p>
<pre><code class="language-text">compress: 20%
encrypt:  18%
hash:     15%
io:       22%
other:    25%
</code></pre>
<p><strong>Action</strong>: No single hotspot. Profile deeper or optimize multiple functions.</p>
<p><strong>Pattern 3: Framework Overhead</strong></p>
<pre><code class="language-text">tokio::runtime: 35%  ← High async overhead
rayon::iter:    25%  ← High parallel overhead
actual_work:    40%
</code></pre>
<p><strong>Action</strong>: Reduce task spawning frequency, batch operations, or use sync code for CPU-bound work.</p>
<h3 id="memory-profile-patterns"><a class="header" href="#memory-profile-patterns">Memory Profile Patterns</a></h3>
<p><strong>Pattern 1: Linear Growth</strong></p>
<pre><code class="language-text">Memory over time:
  0s:   0 MB
 10s:  64 MB
 20s: 128 MB  ← Growing linearly
 30s: 192 MB
</code></pre>
<p><strong>Likely cause</strong>: Streaming processing with bounded buffers (normal).</p>
<p><strong>Pattern 2: Sawtooth</strong></p>
<pre><code class="language-text">Memory over time:
  0-5s:   0 → 128 MB  ← Allocate
  5s:   128 →   0 MB  ← Free
  6-11s:  0 → 128 MB  ← Allocate again
</code></pre>
<p><strong>Likely cause</strong>: Batch processing with periodic flushing (normal).</p>
<p><strong>Pattern 3: Unbounded Growth</strong></p>
<pre><code class="language-text">Memory over time:
  0s:   0 MB
 10s:  64 MB
 20s: 158 MB  ← Growing faster than linear
 30s: 312 MB
</code></pre>
<p><strong>Likely cause</strong>: Memory leak (allocated but never freed).</p>
<p><strong>Action</strong>: Use heaptrack to identify leak source, ensure proper cleanup with RAII.</p>
<h2 id="common-performance-issues"><a class="header" href="#common-performance-issues">Common Performance Issues</a></h2>
<h3 id="issue-1-lock-contention"><a class="header" href="#issue-1-lock-contention">Issue 1: Lock Contention</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>High <code>futex_wait</code> in perf</li>
<li>Threads spending time in synchronization</li>
</ul>
<p><strong>Diagnosis:</strong></p>
<pre><code class="language-bash">perf record --call-graph dwarf ./target/release/pipeline ...
perf report | grep futex
</code></pre>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ❌ Bad: Shared mutex
let counter = Arc::new(Mutex::new(0));

// ✅ Good: Atomic
let counter = Arc::new(AtomicUsize::new(0));
counter.fetch_add(1, Ordering::Relaxed);
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-2-excessive-allocations"><a class="header" href="#issue-2-excessive-allocations">Issue 2: Excessive Allocations</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>High <code>malloc</code> / <code>free</code> in flamegraph</li>
<li>Poor cache performance</li>
</ul>
<p><strong>Diagnosis:</strong></p>
<pre><code class="language-bash">heaptrack ./target/release/pipeline ...
# Check "Total allocations" count
</code></pre>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ❌ Bad: Allocate per iteration
for chunk in chunks {
    let buffer = vec![0; size];  // New allocation every time!
    process(buffer);
}

// ✅ Good: Reuse buffer
let mut buffer = vec![0; size];
for chunk in chunks {
    process(&amp;mut buffer);
    buffer.clear();
}
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-3-small-task-overhead"><a class="header" href="#issue-3-small-task-overhead">Issue 3: Small Task Overhead</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>High <code>tokio::spawn</code> or <code>rayon::spawn</code> overhead</li>
<li>More framework time than actual work</li>
</ul>
<p><strong>Diagnosis:</strong></p>
<pre><code class="language-bash">cargo flamegraph --bin pipeline ...
# Check width of spawn-related functions
</code></pre>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ❌ Bad: Spawn per chunk
for chunk in chunks {
    tokio::spawn(async move { process(chunk).await });
}

// ✅ Good: Batch chunks
tokio::task::spawn_blocking(move || {
    RAYON_POOLS.cpu_bound_pool().install(|| {
        chunks.into_par_iter().map(process).collect()
    })
})
<span class="boring">}</span></code></pre></pre>
<h2 id="profiling-best-practices"><a class="header" href="#profiling-best-practices">Profiling Best Practices</a></h2>
<h3 id="1-profile-release-builds"><a class="header" href="#1-profile-release-builds">1. Profile Release Builds</a></h3>
<pre><code class="language-bash"># ✅ Good: Release mode
cargo build --release
perf record ./target/release/pipeline ...

# ❌ Bad: Debug mode (10-100x slower, misleading results)
cargo build
perf record ./target/debug/pipeline ...
</code></pre>
<h3 id="2-use-representative-workloads"><a class="header" href="#2-use-representative-workloads">2. Use Representative Workloads</a></h3>
<pre><code class="language-bash"># ✅ Good: Large realistic file
perf record ./target/release/pipeline process testdata/100mb-file.bin

# ❌ Bad: Tiny file (setup overhead dominates)
perf record ./target/release/pipeline process testdata/1kb-file.bin
</code></pre>
<h3 id="3-profile-multiple-scenarios"><a class="header" href="#3-profile-multiple-scenarios">3. Profile Multiple Scenarios</a></h3>
<pre><code class="language-bash"># Profile different file sizes
perf record -o perf-small.data ./target/release/pipeline process small.bin
perf record -o perf-large.data ./target/release/pipeline process large.bin

# Compare results
perf report -i perf-small.data
perf report -i perf-large.data
</code></pre>
<h3 id="4-combine-profiling-tools"><a class="header" href="#4-combine-profiling-tools">4. Combine Profiling Tools</a></h3>
<pre><code class="language-bash"># 1. Flamegraph for overview
cargo flamegraph --bin pipeline -- process test.bin

# 2. perf for detailed analysis
perf record --call-graph dwarf ./target/release/pipeline process test.bin
perf report

# 3. Heaptrack for memory
heaptrack ./target/release/pipeline process test.bin
</code></pre>
<h3 id="5-profile-before-and-after-optimizations"><a class="header" href="#5-profile-before-and-after-optimizations">5. Profile Before and After Optimizations</a></h3>
<pre><code class="language-bash"># Before
cargo flamegraph -o before.svg --bin pipeline -- process test.bin
heaptrack -o before.gz ./target/release/pipeline process test.bin

# Make changes...

# After
cargo flamegraph -o after.svg --bin pipeline -- process test.bin
heaptrack -o after.gz ./target/release/pipeline process test.bin

# Compare visually
open before.svg after.svg
</code></pre>
<h2 id="related-topics-9"><a class="header" href="#related-topics-9">Related Topics</a></h2>
<ul>
<li>See <a href="advanced/performance.html">Performance Optimization</a> for optimization strategies</li>
<li>See <a href="advanced/benchmarking.html">Benchmarking</a> for performance measurement</li>
<li>See <a href="advanced/thread-pooling.html">Thread Pooling</a> for concurrency tuning</li>
<li>See <a href="advanced/resources.html">Resource Management</a> for resource limits</li>
</ul>
<h2 id="summary-9"><a class="header" href="#summary-9">Summary</a></h2>
<p>Profiling tools and techniques provide:</p>
<ol>
<li><strong>CPU Profiling</strong>: Identify hotspots with perf, flamegraph, samply</li>
<li><strong>Memory Profiling</strong>: Track allocations with heaptrack, DHAT, Massif</li>
<li><strong>Workflows</strong>: Systematic approaches to optimization</li>
<li><strong>Pattern Recognition</strong>: Understand common performance issues</li>
<li><strong>Best Practices</strong>: Profile release builds with representative workloads</li>
</ol>
<p><strong>Key Takeaways:</strong></p>
<ul>
<li>Use <code>cargo flamegraph</code> for quick CPU profiling (all platforms)</li>
<li>Use <code>heaptrack</code> for comprehensive memory analysis (Linux)</li>
<li>Profile release builds (<code>cargo build --release</code>)</li>
<li>Use representative workloads (large realistic files)</li>
<li>Combine benchmarking (quantify) with profiling (diagnose)</li>
<li>Profile before and after optimizations to verify improvements</li>
</ul>
<p><strong>Quick Start:</strong></p>
<pre><code class="language-bash"># CPU profiling
cargo install flamegraph
cargo flamegraph --bin pipeline -- process large-file.bin

# Memory profiling (Linux)
sudo apt-get install heaptrack
heaptrack ./target/release/pipeline process large-file.bin
heaptrack_gui heaptrack.pipeline.*.gz
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="extending-the-pipeline"><a class="header" href="#extending-the-pipeline">Extending the Pipeline</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>This chapter explains how to extend the pipeline with custom functionality, including custom stages, algorithms, and services while maintaining architectural integrity and type safety.</p>
<h2 id="overview-27"><a class="header" href="#overview-27">Overview</a></h2>
<p>The pipeline is designed for extensibility through well-defined extension points:</p>
<ol>
<li><strong>Custom Stages</strong>: Add new processing stages with custom logic</li>
<li><strong>Custom Algorithms</strong>: Implement new compression, encryption, or hashing algorithms</li>
<li><strong>Custom Services</strong>: Create new domain services for specialized operations</li>
<li><strong>Custom Adapters</strong>: Add infrastructure adapters for external systems</li>
<li><strong>Custom Metrics</strong>: Extend observability with custom metrics collectors</li>
</ol>
<p><strong>Design Principles:</strong></p>
<ul>
<li><strong>Open/Closed</strong>: Open for extension, closed for modification</li>
<li><strong>Dependency Inversion</strong>: Depend on abstractions (traits), not concretions</li>
<li><strong>Single Responsibility</strong>: Each extension has one clear purpose</li>
<li><strong>Type Safety</strong>: Use strong typing to prevent errors</li>
</ul>
<h2 id="extension-points"><a class="header" href="#extension-points">Extension Points</a></h2>
<h3 id="1-custom-stage-types"><a class="header" href="#1-custom-stage-types">1. Custom Stage Types</a></h3>
<p>Add new stage types by extending the <code>StageType</code> enum.</p>
<p><strong>Current stage types:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub enum StageType {
    Compression,    // Data compression/decompression
    Encryption,     // Data encryption/decryption
    Transform,      // Data transformation
    Checksum,       // Integrity verification
    PassThrough,    // No modification
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Adding a new stage type:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 1. Extend StageType enum (in pipeline-domain/src/entities/pipeline_stage.rs)
pub enum StageType {
    Compression,
    Encryption,
    Transform,
    Checksum,
    PassThrough,

    // Custom stage types
    Sanitization,   // Data sanitization (e.g., PII removal)
    Validation,     // Data validation
    Deduplication,  // Remove duplicate chunks
}

// 2. Update Display trait
impl std::fmt::Display for StageType {
    fn fmt(&amp;self, f: &amp;mut std::fmt::Formatter&lt;'_&gt;) -&gt; std::fmt::Result {
        match self {
            // ... existing types ...
            StageType::Sanitization =&gt; write!(f, "sanitization"),
            StageType::Validation =&gt; write!(f, "validation"),
            StageType::Deduplication =&gt; write!(f, "deduplication"),
        }
    }
}

// 3. Update FromStr trait
impl std::str::FromStr for StageType {
    type Err = PipelineError;

    fn from_str(s: &amp;str) -&gt; Result&lt;Self, Self::Err&gt; {
        match s.to_lowercase().as_str() {
            // ... existing types ...
            "sanitization" =&gt; Ok(StageType::Sanitization),
            "validation" =&gt; Ok(StageType::Validation),
            "deduplication" =&gt; Ok(StageType::Deduplication),
            _ =&gt; Err(PipelineError::InvalidConfiguration(format!(
                "Unknown stage type: {}",
                s
            ))),
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="2-custom-algorithms"><a class="header" href="#2-custom-algorithms">2. Custom Algorithms</a></h3>
<p>Extend algorithm enums to support new implementations.</p>
<p><strong>Example: Custom Compression Algorithm</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 1. Extend CompressionAlgorithm enum
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum CompressionAlgorithm {
    Brotli,
    Gzip,
    Zstd,
    Lz4,

    // Custom algorithms
    Snappy,    // Google's Snappy compression
    Lzma,      // LZMA/XZ compression
    Custom(u8), // Custom algorithm identifier
}

// 2. Implement Display trait
impl std::fmt::Display for CompressionAlgorithm {
    fn fmt(&amp;self, f: &amp;mut std::fmt::Formatter&lt;'_&gt;) -&gt; std::fmt::Result {
        match self {
            // ... existing algorithms ...
            CompressionAlgorithm::Snappy =&gt; write!(f, "snappy"),
            CompressionAlgorithm::Lzma =&gt; write!(f, "lzma"),
            CompressionAlgorithm::Custom(id) =&gt; write!(f, "custom-{}", id),
        }
    }
}

// 3. Update FromStr trait
impl std::str::FromStr for CompressionAlgorithm {
    type Err = PipelineError;

    fn from_str(s: &amp;str) -&gt; Result&lt;Self, Self::Err&gt; {
        match s.to_lowercase().as_str() {
            // ... existing algorithms ...
            "snappy" =&gt; Ok(CompressionAlgorithm::Snappy),
            "lzma" =&gt; Ok(CompressionAlgorithm::Lzma),
            s if s.starts_with("custom-") =&gt; {
                let id = s.strip_prefix("custom-")
                    .and_then(|id| id.parse::&lt;u8&gt;().ok())
                    .ok_or_else(|| PipelineError::InvalidConfiguration(
                        format!("Invalid custom algorithm ID: {}", s)
                    ))?;
                Ok(CompressionAlgorithm::Custom(id))
            }
            _ =&gt; Err(PipelineError::InvalidConfiguration(format!(
                "Unknown compression algorithm: {}",
                s
            ))),
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="3-custom-services"><a class="header" href="#3-custom-services">3. Custom Services</a></h3>
<p>Implement domain service traits for custom functionality.</p>
<p><strong>Example: Custom Compression Service</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::services::CompressionService;
use pipeline_domain::{FileChunk, PipelineError, ProcessingContext};

/// Custom compression service using Snappy algorithm
pub struct SnappyCompressionService;

impl SnappyCompressionService {
    pub fn new() -&gt; Self {
        Self
    }
}

impl CompressionService for SnappyCompressionService {
    fn compress(
        &amp;self,
        chunk: FileChunk,
        context: &amp;mut ProcessingContext,
    ) -&gt; Result&lt;FileChunk, PipelineError&gt; {
        use snap::raw::Encoder;

        let start = std::time::Instant::now();

        // Compress data using Snappy
        let mut encoder = Encoder::new();
        let compressed = encoder
            .compress_vec(chunk.data())
            .map_err(|e| PipelineError::CompressionError(format!("Snappy error: {}", e)))?;

        // Update context
        let duration = start.elapsed();
        context.add_bytes_processed(chunk.data().len() as u64);
        context.record_stage_duration(duration);

        // Create compressed chunk
        let mut result = FileChunk::new(
            chunk.sequence_number(),
            chunk.file_offset(),
            compressed,
        );

        // Preserve metadata
        result.set_metadata(chunk.metadata().clone());

        Ok(result)
    }

    fn decompress(
        &amp;self,
        chunk: FileChunk,
        context: &amp;mut ProcessingContext,
    ) -&gt; Result&lt;FileChunk, PipelineError&gt; {
        use snap::raw::Decoder;

        let start = std::time::Instant::now();

        // Decompress data using Snappy
        let mut decoder = Decoder::new();
        let decompressed = decoder
            .decompress_vec(chunk.data())
            .map_err(|e| PipelineError::DecompressionError(format!("Snappy error: {}", e)))?;

        // Update context
        let duration = start.elapsed();
        context.add_bytes_processed(decompressed.len() as u64);
        context.record_stage_duration(duration);

        // Create decompressed chunk
        let mut result = FileChunk::new(
            chunk.sequence_number(),
            chunk.file_offset(),
            decompressed,
        );

        result.set_metadata(chunk.metadata().clone());

        Ok(result)
    }

    fn estimate_compressed_size(&amp;self, chunk: &amp;FileChunk) -&gt; usize {
        // Snappy typically compresses to ~50-70% of original
        (chunk.data().len() as f64 * 0.6) as usize
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="4-custom-adapters"><a class="header" href="#4-custom-adapters">4. Custom Adapters</a></h3>
<p>Create infrastructure adapters for external systems.</p>
<p><strong>Example: Custom Cloud Storage Adapter</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use async_trait::async_trait;
use pipeline_domain::services::FileIOService;
use std::path::Path;

/// Custom adapter for cloud storage (e.g., S3, Azure Blob)
pub struct CloudStorageAdapter {
    client: CloudClient,
    bucket: String,
}

impl CloudStorageAdapter {
    pub fn new(endpoint: &amp;str, bucket: &amp;str) -&gt; Result&lt;Self, PipelineError&gt; {
        let client = CloudClient::connect(endpoint)?;
        Ok(Self {
            client,
            bucket: bucket.to_string(),
        })
    }
}

#[async_trait]
impl FileIOService for CloudStorageAdapter {
    async fn read_file_chunks(
        &amp;self,
        path: &amp;Path,
        options: ReadOptions,
    ) -&gt; Result&lt;Vec&lt;FileChunk&gt;, PipelineError&gt; {
        // 1. Convert local path to cloud path
        let cloud_path = self.to_cloud_path(path)?;

        // 2. Read file from cloud storage
        let data = self.client
            .get_object(&amp;self.bucket, &amp;cloud_path)
            .await
            .map_err(|e| PipelineError::IOError(format!("Cloud read error: {}", e)))?;

        // 3. Chunk the data
        let chunk_size = options.chunk_size.unwrap_or(64 * 1024);
        let chunks = data
            .chunks(chunk_size)
            .enumerate()
            .map(|(seq, chunk)| {
                FileChunk::new(seq, seq * chunk_size, chunk.to_vec())
            })
            .collect();

        Ok(chunks)
    }

    async fn write_file_data(
        &amp;self,
        path: &amp;Path,
        data: &amp;[u8],
        options: WriteOptions,
    ) -&gt; Result&lt;(), PipelineError&gt; {
        // 1. Convert local path to cloud path
        let cloud_path = self.to_cloud_path(path)?;

        // 2. Write to cloud storage
        self.client
            .put_object(&amp;self.bucket, &amp;cloud_path, data)
            .await
            .map_err(|e| PipelineError::IOError(format!("Cloud write error: {}", e)))?;

        Ok(())
    }

    // ... implement other methods ...
}
<span class="boring">}</span></code></pre></pre>
<h3 id="5-custom-metrics"><a class="header" href="#5-custom-metrics">5. Custom Metrics</a></h3>
<p>Extend observability with custom metrics collectors.</p>
<p><strong>Example: Custom Metrics Collector</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline::infrastructure::metrics::MetricsCollector;
use std::sync::atomic::{AtomicU64, Ordering};

/// Custom metrics collector for advanced analytics
pub struct AdvancedMetricsCollector {
    // Existing metrics
    chunks_processed: AtomicU64,
    bytes_processed: AtomicU64,

    // Custom metrics
    compression_ratio: AtomicU64,  // Stored as f64 bits
    deduplication_hits: AtomicU64,
    cache_efficiency: AtomicU64,   // Percentage * 100
}

impl AdvancedMetricsCollector {
    pub fn new() -&gt; Self {
        Self {
            chunks_processed: AtomicU64::new(0),
            bytes_processed: AtomicU64::new(0),
            compression_ratio: AtomicU64::new(0),
            deduplication_hits: AtomicU64::new(0),
            cache_efficiency: AtomicU64::new(0),
        }
    }

    /// Record compression ratio for a chunk
    pub fn record_compression_ratio(&amp;self, original: usize, compressed: usize) {
        let ratio = (compressed as f64 / original as f64) * 100.0;
        self.compression_ratio.store(ratio.to_bits(), Ordering::Relaxed);
    }

    /// Increment deduplication hits counter
    pub fn record_deduplication_hit(&amp;self) {
        self.deduplication_hits.fetch_add(1, Ordering::Relaxed);
    }

    /// Update cache efficiency percentage
    pub fn update_cache_efficiency(&amp;self, hits: u64, total: u64) {
        let efficiency = ((hits as f64 / total as f64) * 10000.0) as u64;
        self.cache_efficiency.store(efficiency, Ordering::Relaxed);
    }

    /// Get compression ratio
    pub fn compression_ratio(&amp;self) -&gt; f64 {
        f64::from_bits(self.compression_ratio.load(Ordering::Relaxed))
    }

    /// Get deduplication hits
    pub fn deduplication_hits(&amp;self) -&gt; u64 {
        self.deduplication_hits.load(Ordering::Relaxed)
    }

    /// Get cache efficiency percentage
    pub fn cache_efficiency_percent(&amp;self) -&gt; f64 {
        self.cache_efficiency.load(Ordering::Relaxed) as f64 / 100.0
    }
}

impl MetricsCollector for AdvancedMetricsCollector {
    fn record_chunk_processed(&amp;self, size: usize) {
        self.chunks_processed.fetch_add(1, Ordering::Relaxed);
        self.bytes_processed.fetch_add(size as u64, Ordering::Relaxed);
    }

    fn chunks_processed(&amp;self) -&gt; u64 {
        self.chunks_processed.load(Ordering::Relaxed)
    }

    fn bytes_processed(&amp;self) -&gt; u64 {
        self.bytes_processed.load(Ordering::Relaxed)
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="architecture-patterns"><a class="header" href="#architecture-patterns">Architecture Patterns</a></h2>
<h3 id="hexagonal-architecture"><a class="header" href="#hexagonal-architecture">Hexagonal Architecture</a></h3>
<p>The pipeline uses hexagonal architecture (Ports and Adapters):</p>
<p><strong>Ports (Interfaces):</strong></p>
<ul>
<li>Domain Services (CompressionService, EncryptionService)</li>
<li>Repositories (StageExecutor)</li>
<li>Value Objects (StageType, CompressionAlgorithm)</li>
</ul>
<p><strong>Adapters (Implementations):</strong></p>
<ul>
<li>Infrastructure Services (BrotliCompressionService, AesEncryptionService)</li>
<li>Infrastructure Adapters (FileIOServiceImpl, SQLitePipelineRepository)</li>
<li>Application Services (PipelineServiceImpl, FileProcessorServiceImpl)</li>
</ul>
<p><strong>Extension Strategy:</strong></p>
<ol>
<li>Define domain trait (port)</li>
<li>Implement infrastructure adapter</li>
<li>Register with dependency injection container</li>
</ol>
<h3 id="dependency-injection"><a class="header" href="#dependency-injection">Dependency Injection</a></h3>
<p><strong>Example: Registering Custom Services</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::sync::Arc;

/// Application dependencies container
pub struct AppDependencies {
    compression_service: Arc&lt;dyn CompressionService&gt;,
    encryption_service: Arc&lt;dyn EncryptionService&gt;,
    file_io_service: Arc&lt;dyn FileIOService&gt;,
}

impl AppDependencies {
    pub fn new_with_custom_compression(
        compression: Arc&lt;dyn CompressionService&gt;,
    ) -&gt; Self {
        Self {
            compression_service: compression,
            encryption_service: Arc::new(DefaultEncryptionService::new()),
            file_io_service: Arc::new(DefaultFileIOService::new()),
        }
    }

    pub fn new_with_cloud_storage(
        cloud_adapter: Arc&lt;CloudStorageAdapter&gt;,
    ) -&gt; Self {
        Self {
            compression_service: Arc::new(DefaultCompressionService::new()),
            encryption_service: Arc::new(DefaultEncryptionService::new()),
            file_io_service: cloud_adapter,
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="best-practices-16"><a class="header" href="#best-practices-16">Best Practices</a></h2>
<h3 id="1-follow-domain-driven-design"><a class="header" href="#1-follow-domain-driven-design">1. Follow Domain-Driven Design</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Domain trait in domain layer
// pipeline-domain/src/services/my_service.rs
pub trait MyService: Send + Sync {
    fn process(&amp;self, data: &amp;[u8]) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt;;
}

// ✅ Good: Infrastructure implementation in infrastructure layer
// pipeline/src/infrastructure/services/my_service_impl.rs
pub struct MyServiceImpl {
    config: MyConfig,
}

impl MyService for MyServiceImpl {
    fn process(&amp;self, data: &amp;[u8]) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        // Implementation details
    }
}

// ❌ Bad: Implementation in domain layer
// pipeline-domain/src/services/my_service.rs
pub struct MyServiceImpl { /* ... */ }  // Wrong layer!
<span class="boring">}</span></code></pre></pre>
<h3 id="2-use-type-safe-configuration"><a class="header" href="#2-use-type-safe-configuration">2. Use Type-Safe Configuration</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Strongly typed configuration
pub struct MyStageConfig {
    algorithm: MyAlgorithm,
    level: u8,
    parallel: bool,
}

impl MyStageConfig {
    pub fn new(algorithm: MyAlgorithm, level: u8) -&gt; Result&lt;Self, PipelineError&gt; {
        if level &gt; 10 {
            return Err(PipelineError::InvalidConfiguration(
                "Level must be 0-10".to_string()
            ));
        }
        Ok(Self { algorithm, level, parallel: true })
    }
}

// ❌ Bad: Stringly-typed configuration
pub struct MyStageConfig {
    algorithm: String,  // Could be anything!
    level: String,      // Not validated!
}
<span class="boring">}</span></code></pre></pre>
<h3 id="3-implement-proper-error-handling"><a class="header" href="#3-implement-proper-error-handling">3. Implement Proper Error Handling</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Specific error types
impl MyService for MyServiceImpl {
    fn process(&amp;self, data: &amp;[u8]) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        self.validate_input(data)
            .map_err(|e| PipelineError::ValidationError(format!("Invalid input: {}", e)))?;

        self.do_processing(data)
            .map_err(|e| PipelineError::ProcessingError(format!("Processing failed: {}", e)))?;

        Ok(result)
    }
}

// ❌ Bad: Generic errors
impl MyService for MyServiceImpl {
    fn process(&amp;self, data: &amp;[u8]) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        // Just returns "something went wrong" - not helpful!
        Ok(self.do_processing(data).unwrap())
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="4-add-comprehensive-tests"><a class="header" href="#4-add-comprehensive-tests">4. Add Comprehensive Tests</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(test)]
mod tests {
    use super::*;

    // ✅ Good: Unit tests for service
    #[test]
    fn test_custom_compression_small_data() {
        let service = SnappyCompressionService::new();
        let chunk = FileChunk::new(0, 0, vec![0u8; 1024]);
        let mut context = ProcessingContext::new();

        let result = service.compress(chunk, &amp;mut context).unwrap();

        assert!(result.data().len() &lt; 1024);
        assert_eq!(context.bytes_processed(), 1024);
    }

    // ✅ Good: Integration test
    #[tokio::test]
    async fn test_custom_cloud_adapter_roundtrip() {
        let adapter = CloudStorageAdapter::new("http://localhost:9000", "test").unwrap();
        let test_data = b"Hello, World!";

        adapter.write_file_data(Path::new("test.txt"), test_data, Default::default())
            .await
            .unwrap();

        let chunks = adapter.read_file_chunks(Path::new("test.txt"), Default::default())
            .await
            .unwrap();

        assert_eq!(chunks[0].data(), test_data);
    }

    // ✅ Good: Error case testing
    #[test]
    fn test_custom_compression_invalid_data() {
        let service = SnappyCompressionService::new();
        let corrupt_chunk = FileChunk::new(0, 0, vec![0xFF; 10]);
        let mut context = ProcessingContext::new();

        let result = service.decompress(corrupt_chunk, &amp;mut context);

        assert!(result.is_err());
        assert!(matches!(result.unwrap_err(), PipelineError::DecompressionError(_)));
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="5-document-extension-points"><a class="header" href="#5-document-extension-points">5. Document Extension Points</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Custom sanitization service for PII removal
///
/// This service implements data sanitization by removing personally
/// identifiable information (PII) from file chunks.
///
/// # Examples
///
/// ```
/// use pipeline::infrastructure::services::SanitizationService;
///
/// let service = SanitizationService::new();
/// let sanitized = service.sanitize(chunk, &amp;mut context)?;
/// ```
///
/// # Performance
///
/// - Throughput: ~200 MB/s (regex-based)
/// - Memory: O(chunk_size)
/// - Thread-safe: Yes
///
/// # Algorithms Supported
///
/// - Email addresses (regex: `\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b`)
/// - Phone numbers (various formats)
/// - Social Security Numbers (US)
/// - Credit card numbers (Luhn validation)
pub struct SanitizationService {
    patterns: Vec&lt;Regex&gt;,
}
<span class="boring">}</span></code></pre></pre>
<h2 id="integration-examples"><a class="header" href="#integration-examples">Integration Examples</a></h2>
<h3 id="example-1-custom-deduplication-stage"><a class="header" href="#example-1-custom-deduplication-stage">Example 1: Custom Deduplication Stage</a></h3>
<p><strong>Complete implementation:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 1. Add to StageType enum
pub enum StageType {
    // ... existing types ...
    Deduplication,
}

// 2. Define domain service
pub trait DeduplicationService: Send + Sync {
    fn deduplicate(
        &amp;self,
        chunk: FileChunk,
        context: &amp;mut ProcessingContext,
    ) -&gt; Result&lt;Option&lt;FileChunk&gt;, PipelineError&gt;;
}

// 3. Implement infrastructure service
pub struct BloomFilterDeduplicationService {
    bloom_filter: Arc&lt;Mutex&lt;BloomFilter&gt;&gt;,
}

impl DeduplicationService for BloomFilterDeduplicationService {
    fn deduplicate(
        &amp;self,
        chunk: FileChunk,
        context: &amp;mut ProcessingContext,
    ) -&gt; Result&lt;Option&lt;FileChunk&gt;, PipelineError&gt; {
        use blake3::hash;

        // Calculate chunk hash
        let hash = hash(chunk.data());

        // Check if chunk seen before
        let mut filter = self.bloom_filter.lock().unwrap();
        if filter.contains(&amp;hash) {
            // Duplicate found
            context.increment_deduplication_hits();
            return Ok(None);
        }

        // New chunk, add to filter
        filter.insert(&amp;hash);
        Ok(Some(chunk))
    }
}

// 4. Register in pipeline configuration
let mut pipeline = Pipeline::new();
pipeline.add_stage(PipelineStage::new(
    StageType::Deduplication,
    "dedup",
    StageConfiguration::new(
        "bloom-filter".to_string(),
        HashMap::new(),
        false,  // Not parallel (shared bloom filter)
    ),
));
<span class="boring">}</span></code></pre></pre>
<h2 id="related-topics-10"><a class="header" href="#related-topics-10">Related Topics</a></h2>
<ul>
<li>See <a href="advanced/custom-stages.html">Custom Stages</a> for detailed stage implementation guide</li>
<li>See <a href="advanced/custom-algorithms.html">Custom Algorithms</a> for algorithm implementation patterns</li>
<li>See <a href="advanced/../architecture/layers.html">Architecture</a> for layered architecture principles</li>
<li>See <a href="advanced/../architecture/ports-adapters.html">Ports and Adapters</a> for hexagonal architecture</li>
</ul>
<h2 id="summary-10"><a class="header" href="#summary-10">Summary</a></h2>
<p>The pipeline provides multiple extension points:</p>
<ol>
<li><strong>Custom Stages</strong>: Extend StageType enum and implement processing logic</li>
<li><strong>Custom Algorithms</strong>: Add new compression, encryption, or hashing algorithms</li>
<li><strong>Custom Services</strong>: Implement domain service traits for specialized operations</li>
<li><strong>Custom Adapters</strong>: Create infrastructure adapters for external systems</li>
<li><strong>Custom Metrics</strong>: Extend observability with custom metrics collectors</li>
</ol>
<p><strong>Key Takeaways:</strong></p>
<ul>
<li>Follow hexagonal architecture (domain → application → infrastructure)</li>
<li>Use dependency injection for loose coupling</li>
<li>Implement domain traits (ports) in infrastructure layer (adapters)</li>
<li>Maintain type safety with strong typing and validation</li>
<li>Add comprehensive tests for all custom implementations</li>
<li>Document extension points and usage examples</li>
</ul>
<p><strong>Extension Checklist:</strong></p>
<ul>
<li><input disabled="" type="checkbox"/>
Define domain trait (if new concept)</li>
<li><input disabled="" type="checkbox"/>
Implement infrastructure adapter</li>
<li><input disabled="" type="checkbox"/>
Add unit tests for adapter</li>
<li><input disabled="" type="checkbox"/>
Add integration tests for end-to-end workflow</li>
<li><input disabled="" type="checkbox"/>
Document configuration options</li>
<li><input disabled="" type="checkbox"/>
Update architectural diagrams (if significant)</li>
<li><input disabled="" type="checkbox"/>
Consider performance impact (benchmark)</li>
<li><input disabled="" type="checkbox"/>
Verify thread safety (Send + Sync)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="custom-stages"><a class="header" href="#custom-stages">Custom Stages</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>This chapter provides a step-by-step guide to creating custom pipeline stages, from defining the stage type through implementation, testing, and integration.</p>
<h2 id="overview-28"><a class="header" href="#overview-28">Overview</a></h2>
<p>Custom stages allow you to extend the pipeline with specialized data processing operations:</p>
<ul>
<li><strong>Data Sanitization</strong>: Remove PII, redact sensitive information</li>
<li><strong>Data Validation</strong>: Enforce schemas, validate formats</li>
<li><strong>Data Transformation</strong>: Convert formats, restructure data</li>
<li><strong>Data Enrichment</strong>: Add metadata, annotations, tags</li>
<li><strong>Custom Business Logic</strong>: Domain-specific operations</li>
</ul>
<p><strong>Key Concepts:</strong></p>
<ul>
<li><strong>StageType</strong>: Enum variant identifying the stage category</li>
<li><strong>StageConfiguration</strong>: Parameters for stage behavior</li>
<li><strong>Service Trait</strong>: Domain interface defining stage operations</li>
<li><strong>Service Implementation</strong>: Infrastructure adapter performing the work</li>
<li><strong>Processing Context</strong>: Shared state for metrics and metadata</li>
</ul>
<h2 id="stage-implementation-steps"><a class="header" href="#stage-implementation-steps">Stage Implementation Steps</a></h2>
<h3 id="step-1-define-stage-type"><a class="header" href="#step-1-define-stage-type">Step 1: Define Stage Type</a></h3>
<p>Add a new variant to the <code>StageType</code> enum:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// pipeline-domain/src/entities/pipeline_stage.rs

#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum StageType {
    Compression,
    Encryption,
    Transform,
    Checksum,
    PassThrough,

    // Custom stage type
    Sanitization,  // Data sanitization
}

impl std::fmt::Display for StageType {
    fn fmt(&amp;self, f: &amp;mut std::fmt::Formatter&lt;'_&gt;) -&gt; std::fmt::Result {
        match self {
            // ... existing types ...
            StageType::Sanitization =&gt; write!(f, "sanitization"),
        }
    }
}

impl std::str::FromStr for StageType {
    type Err = PipelineError;

    fn from_str(s: &amp;str) -&gt; Result&lt;Self, Self::Err&gt; {
        match s.to_lowercase().as_str() {
            // ... existing types ...
            "sanitization" =&gt; Ok(StageType::Sanitization),
            _ =&gt; Err(PipelineError::InvalidConfiguration(format!(
                "Unknown stage type: {}",
                s
            ))),
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="step-2-define-domain-service-trait"><a class="header" href="#step-2-define-domain-service-trait">Step 2: Define Domain Service Trait</a></h3>
<p>Create a trait in the domain layer:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// pipeline-domain/src/services/sanitization_service.rs

use crate::{FileChunk, PipelineError, ProcessingContext};

/// Trait for data sanitization services
///
/// This service removes or redacts sensitive information from file chunks,
/// such as PII (personally identifiable information).
pub trait SanitizationService: Send + Sync {
    /// Sanitize a file chunk by removing sensitive data
    ///
    /// # Arguments
    ///
    /// * `chunk` - File chunk to sanitize
    /// * `context` - Processing context for metrics
    ///
    /// # Returns
    ///
    /// Sanitized chunk with sensitive data removed or redacted
    fn sanitize(
        &amp;self,
        chunk: FileChunk,
        context: &amp;mut ProcessingContext,
    ) -&gt; Result&lt;FileChunk, PipelineError&gt;;

    /// Detect sensitive patterns in chunk
    ///
    /// # Returns
    ///
    /// Count of sensitive patterns found
    fn detect_sensitive_data(
        &amp;self,
        chunk: &amp;FileChunk,
    ) -&gt; Result&lt;usize, PipelineError&gt;;
}
<span class="boring">}</span></code></pre></pre>
<h3 id="step-3-implement-infrastructure-service"><a class="header" href="#step-3-implement-infrastructure-service">Step 3: Implement Infrastructure Service</a></h3>
<p>Create the concrete implementation:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// pipeline/src/infrastructure/services/sanitization_service_impl.rs

use pipeline_domain::services::SanitizationService;
use pipeline_domain::{FileChunk, PipelineError, ProcessingContext};
use regex::Regex;
use std::sync::LazyLock;

/// Regular expressions for detecting sensitive data
static EMAIL_REGEX: LazyLock&lt;Regex&gt; =
    LazyLock::new(|| Regex::new(r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b").unwrap());

static SSN_REGEX: LazyLock&lt;Regex&gt; =
    LazyLock::new(|| Regex::new(r"\b\d{3}-\d{2}-\d{4}\b").unwrap());

static PHONE_REGEX: LazyLock&lt;Regex&gt; =
    LazyLock::new(|| Regex::new(r"\b\d{3}[-.]?\d{3}[-.]?\d{4}\b").unwrap());

static CREDIT_CARD_REGEX: LazyLock&lt;Regex&gt; =
    LazyLock::new(|| Regex::new(r"\b\d{4}[- ]?\d{4}[- ]?\d{4}[- ]?\d{4}\b").unwrap());

/// Sanitization service implementation using regex patterns
pub struct RegexSanitizationService {
    redaction_placeholder: String,
}

impl RegexSanitizationService {
    pub fn new() -&gt; Self {
        Self {
            redaction_placeholder: "[REDACTED]".to_string(),
        }
    }

    pub fn with_placeholder(placeholder: impl Into&lt;String&gt;) -&gt; Self {
        Self {
            redaction_placeholder: placeholder.into(),
        }
    }

    fn redact_emails(&amp;self, text: &amp;str) -&gt; String {
        EMAIL_REGEX.replace_all(text, &amp;self.redaction_placeholder).to_string()
    }

    fn redact_ssns(&amp;self, text: &amp;str) -&gt; String {
        SSN_REGEX.replace_all(text, &amp;self.redaction_placeholder).to_string()
    }

    fn redact_phones(&amp;self, text: &amp;str) -&gt; String {
        PHONE_REGEX.replace_all(text, &amp;self.redaction_placeholder).to_string()
    }

    fn redact_credit_cards(&amp;self, text: &amp;str) -&gt; String {
        CREDIT_CARD_REGEX.replace_all(text, &amp;self.redaction_placeholder).to_string()
    }
}

impl SanitizationService for RegexSanitizationService {
    fn sanitize(
        &amp;self,
        chunk: FileChunk,
        context: &amp;mut ProcessingContext,
    ) -&gt; Result&lt;FileChunk, PipelineError&gt; {
        let start = std::time::Instant::now();

        // Convert chunk data to string
        let text = String::from_utf8_lossy(chunk.data());

        // Apply sanitization
        let sanitized = self.redact_emails(&amp;text);
        let sanitized = self.redact_ssns(&amp;sanitized);
        let sanitized = self.redact_phones(&amp;sanitized);
        let sanitized = self.redact_credit_cards(&amp;sanitized);

        // Update context
        let duration = start.elapsed();
        context.add_bytes_processed(chunk.data().len() as u64);
        context.record_stage_duration(duration);

        // Create sanitized chunk
        let mut result = FileChunk::new(
            chunk.sequence_number(),
            chunk.file_offset(),
            sanitized.into_bytes(),
        );

        result.set_metadata(chunk.metadata().clone());

        Ok(result)
    }

    fn detect_sensitive_data(
        &amp;self,
        chunk: &amp;FileChunk,
    ) -&gt; Result&lt;usize, PipelineError&gt; {
        let text = String::from_utf8_lossy(chunk.data());

        let email_count = EMAIL_REGEX.find_iter(&amp;text).count();
        let ssn_count = SSN_REGEX.find_iter(&amp;text).count();
        let phone_count = PHONE_REGEX.find_iter(&amp;text).count();
        let cc_count = CREDIT_CARD_REGEX.find_iter(&amp;text).count();

        Ok(email_count + ssn_count + phone_count + cc_count)
    }
}

impl Default for RegexSanitizationService {
    fn default() -&gt; Self {
        Self::new()
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="step-4-register-stage-in-pipeline"><a class="header" href="#step-4-register-stage-in-pipeline">Step 4: Register Stage in Pipeline</a></h3>
<p>Add the stage to pipeline configuration:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::entities::{PipelineStage, StageType, StageConfiguration};
use std::collections::HashMap;

// Create sanitization stage
let stage = PipelineStage::new(
    StageType::Sanitization,
    "pii-removal",
    StageConfiguration::new(
        "regex".to_string(),
        HashMap::from([
            ("placeholder".to_string(), "[REDACTED]".to_string()),
        ]),
        true,  // Parallel processing enabled
    ),
);

// Add to pipeline
pipeline.add_stage(stage)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="step-5-integrate-with-stage-executor"><a class="header" href="#step-5-integrate-with-stage-executor">Step 5: Integrate with Stage Executor</a></h3>
<p>Update the stage executor to handle the new stage type:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// pipeline/src/infrastructure/execution/stage_executor_impl.rs

impl StageExecutor for StageExecutorImpl {
    async fn execute(
        &amp;self,
        stage: &amp;PipelineStage,
        chunk: FileChunk,
        context: &amp;mut ProcessingContext,
    ) -&gt; Result&lt;FileChunk, PipelineError&gt; {
        match stage.stage_type() {
            // ... existing types ...

            StageType::Sanitization =&gt; {
                let service = self.sanitization_service
                    .as_ref()
                    .ok_or_else(|| PipelineError::ServiceNotConfigured(
                        "SanitizationService not configured".to_string()
                    ))?;

                service.sanitize(chunk, context)
            }
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="complete-example-data-validation-stage"><a class="header" href="#complete-example-data-validation-stage">Complete Example: Data Validation Stage</a></h2>
<p>Here's a complete example implementing a data validation stage:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 1. Add StageType variant
pub enum StageType {
    // ... existing ...
    Validation,
}

// 2. Define domain trait
pub trait ValidationService: Send + Sync {
    fn validate(
        &amp;self,
        chunk: FileChunk,
        context: &amp;mut ProcessingContext,
    ) -&gt; Result&lt;FileChunk, PipelineError&gt;;
}

// 3. Implement infrastructure service
pub struct JsonSchemaValidationService {
    schema: serde_json::Value,
}

impl JsonSchemaValidationService {
    pub fn new(schema: serde_json::Value) -&gt; Self {
        Self { schema }
    }
}

impl ValidationService for JsonSchemaValidationService {
    fn validate(
        &amp;self,
        chunk: FileChunk,
        context: &amp;mut ProcessingContext,
    ) -&gt; Result&lt;FileChunk, PipelineError&gt; {
        use jsonschema::JSONSchema;

        let start = std::time::Instant::now();

        // Parse chunk data as JSON
        let data: serde_json::Value = serde_json::from_slice(chunk.data())
            .map_err(|e| PipelineError::ValidationError(format!("Invalid JSON: {}", e)))?;

        // Validate against schema
        let compiled_schema = JSONSchema::compile(&amp;self.schema)
            .map_err(|e| PipelineError::ValidationError(format!("Invalid schema: {}", e)))?;

        if let Err(errors) = compiled_schema.validate(&amp;data) {
            let error_messages: Vec&lt;String&gt; = errors
                .map(|e| e.to_string())
                .collect();

            return Err(PipelineError::ValidationError(format!(
                "Validation failed: {}",
                error_messages.join(", ")
            )));
        }

        // Update context
        let duration = start.elapsed();
        context.add_bytes_processed(chunk.data().len() as u64);
        context.record_stage_duration(duration);

        Ok(chunk)  // Return unchanged if valid
    }
}

// 4. Usage in pipeline
let schema = serde_json::json!({
    "type": "object",
    "properties": {
        "name": { "type": "string" },
        "age": { "type": "number", "minimum": 0 }
    },
    "required": ["name", "age"]
});

let validation_service = Arc::new(JsonSchemaValidationService::new(schema));

let stage = PipelineStage::new(
    StageType::Validation,
    "json-schema",
    StageConfiguration::new(
        "jsonschema".to_string(),
        HashMap::new(),
        false,  // Sequential validation
    ),
);

pipeline.add_stage(stage)?;
<span class="boring">}</span></code></pre></pre>
<h2 id="testing-custom-stages"><a class="header" href="#testing-custom-stages">Testing Custom Stages</a></h2>
<h3 id="unit-tests-4"><a class="header" href="#unit-tests-4">Unit Tests</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_sanitization_redacts_emails() {
        let service = RegexSanitizationService::new();
        let test_data = b"Contact: user@example.com for details";

        let chunk = FileChunk::new(0, 0, test_data.to_vec());
        let mut context = ProcessingContext::new();

        let result = service.sanitize(chunk, &amp;mut context).unwrap();

        let sanitized_text = String::from_utf8(result.data().to_vec()).unwrap();
        assert!(sanitized_text.contains("[REDACTED]"));
        assert!(!sanitized_text.contains("user@example.com"));
    }

    #[test]
    fn test_sanitization_detects_multiple_patterns() {
        let service = RegexSanitizationService::new();
        let test_data = b"Email: test@example.com, SSN: 123-45-6789, Phone: 555-123-4567";

        let chunk = FileChunk::new(0, 0, test_data.to_vec());

        let count = service.detect_sensitive_data(&amp;chunk).unwrap();
        assert_eq!(count, 3);  // Email + SSN + Phone
    }

    #[test]
    fn test_validation_rejects_invalid_json() {
        let schema = serde_json::json!({
            "type": "object",
            "properties": {
                "name": { "type": "string" }
            },
            "required": ["name"]
        });

        let service = JsonSchemaValidationService::new(schema);
        let invalid_data = b"{ \"age\": 25 }";  // Missing required "name"

        let chunk = FileChunk::new(0, 0, invalid_data.to_vec());
        let mut context = ProcessingContext::new();

        let result = service.validate(chunk, &amp;mut context);
        assert!(result.is_err());
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="integration-tests-4"><a class="header" href="#integration-tests-4">Integration Tests</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_custom_stage_in_pipeline() {
    // Create pipeline with custom stage
    let mut pipeline = Pipeline::new();

    let sanitization_stage = PipelineStage::new(
        StageType::Sanitization,
        "pii-removal",
        StageConfiguration::default(),
    );

    pipeline.add_stage(sanitization_stage).unwrap();

    // Process test data
    let test_data = b"User: john@example.com, SSN: 123-45-6789";
    let result = pipeline.process(test_data).await.unwrap();

    // Verify sanitization
    let output = String::from_utf8(result).unwrap();
    assert!(!output.contains("john@example.com"));
    assert!(!output.contains("123-45-6789"));
    assert!(output.contains("[REDACTED]"));
}
<span class="boring">}</span></code></pre></pre>
<h2 id="best-practices-17"><a class="header" href="#best-practices-17">Best Practices</a></h2>
<h3 id="1-stateless-services"><a class="header" href="#1-stateless-services">1. Stateless Services</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Stateless service (thread-safe)
pub struct MyService {
    config: MyConfig,  // Immutable configuration
}

impl MyService for MyServiceImpl {
    fn process(&amp;self, chunk: FileChunk, context: &amp;mut ProcessingContext)
        -&gt; Result&lt;FileChunk, PipelineError&gt;
    {
        // No mutable state - safe for concurrent use
    }
}

// ❌ Bad: Stateful service (not thread-safe)
pub struct MyService {
    processed_count: usize,  // Mutable state without synchronization!
}
<span class="boring">}</span></code></pre></pre>
<h3 id="2-proper-error-handling"><a class="header" href="#2-proper-error-handling">2. Proper Error Handling</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Specific error types
fn validate(&amp;self, chunk: FileChunk) -&gt; Result&lt;FileChunk, PipelineError&gt; {
    let data = serde_json::from_slice(chunk.data())
        .map_err(|e| PipelineError::ValidationError(format!("Invalid JSON: {}", e)))?;

    // ...
}

// ❌ Bad: Generic errors
fn validate(&amp;self, chunk: FileChunk) -&gt; Result&lt;FileChunk, PipelineError&gt; {
    let data = serde_json::from_slice(chunk.data()).unwrap();  // Panics!
    // ...
}
<span class="boring">}</span></code></pre></pre>
<h3 id="3-update-processing-context"><a class="header" href="#3-update-processing-context">3. Update Processing Context</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Track metrics
fn process(&amp;self, chunk: FileChunk, context: &amp;mut ProcessingContext)
    -&gt; Result&lt;FileChunk, PipelineError&gt;
{
    let start = std::time::Instant::now();

    // ... do work ...

    context.add_bytes_processed(chunk.data().len() as u64);
    context.record_stage_duration(start.elapsed());

    Ok(result)
}

// ❌ Bad: No metrics
fn process(&amp;self, chunk: FileChunk, context: &amp;mut ProcessingContext)
    -&gt; Result&lt;FileChunk, PipelineError&gt;
{
    // ... do work ...
    Ok(result)  // No metrics recorded!
}
<span class="boring">}</span></code></pre></pre>
<h3 id="4-preserve-chunk-metadata"><a class="header" href="#4-preserve-chunk-metadata">4. Preserve Chunk Metadata</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Preserve metadata
let mut result = FileChunk::new(
    chunk.sequence_number(),
    chunk.file_offset(),
    processed_data,
);
result.set_metadata(chunk.metadata().clone());

// ❌ Bad: Lose metadata
let result = FileChunk::new(0, 0, processed_data);  // Lost sequence info!
<span class="boring">}</span></code></pre></pre>
<h2 id="related-topics-11"><a class="header" href="#related-topics-11">Related Topics</a></h2>
<ul>
<li>See <a href="advanced/extending.html">Extending the Pipeline</a> for overview of extension points</li>
<li>See <a href="advanced/custom-algorithms.html">Custom Algorithms</a> for algorithm implementation</li>
<li>See <a href="advanced/../architecture/layers.html">Architecture</a> for layered architecture principles</li>
</ul>
<h2 id="summary-11"><a class="header" href="#summary-11">Summary</a></h2>
<p>Creating custom stages involves:</p>
<ol>
<li><strong>Define StageType</strong>: Add enum variant for stage category</li>
<li><strong>Define Service Trait</strong>: Create domain interface in <code>pipeline-domain</code></li>
<li><strong>Implement Service</strong>: Build infrastructure adapter in <code>pipeline</code></li>
<li><strong>Register Stage</strong>: Add to pipeline configuration</li>
<li><strong>Integrate Executor</strong>: Update stage executor to handle new type</li>
<li><strong>Test Thoroughly</strong>: Unit and integration tests</li>
</ol>
<p><strong>Key Takeaways:</strong></p>
<ul>
<li>Keep services stateless for thread safety</li>
<li>Use specific error types for better diagnostics</li>
<li>Update processing context with metrics</li>
<li>Preserve chunk metadata through transformations</li>
<li>Add comprehensive tests (unit + integration)</li>
<li>Document configuration options and behavior</li>
</ul>
<p><strong>Stage Development Checklist:</strong></p>
<ul>
<li><input disabled="" type="checkbox"/>
Define StageType enum variant</li>
<li><input disabled="" type="checkbox"/>
Create domain service trait</li>
<li><input disabled="" type="checkbox"/>
Implement infrastructure service</li>
<li><input disabled="" type="checkbox"/>
Add unit tests for service</li>
<li><input disabled="" type="checkbox"/>
Register in pipeline configuration</li>
<li><input disabled="" type="checkbox"/>
Update stage executor</li>
<li><input disabled="" type="checkbox"/>
Add integration tests</li>
<li><input disabled="" type="checkbox"/>
Document usage and configuration</li>
<li><input disabled="" type="checkbox"/>
Benchmark performance (if applicable)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="custom-algorithms"><a class="header" href="#custom-algorithms">Custom Algorithms</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>This chapter demonstrates how to implement custom compression, encryption, and hashing algorithms while integrating them seamlessly with the pipeline's existing infrastructure.</p>
<h2 id="overview-29"><a class="header" href="#overview-29">Overview</a></h2>
<p>The pipeline supports custom algorithm implementations for:</p>
<ul>
<li><strong>Compression</strong>: Add new compression algorithms (e.g., Snappy, LZMA, custom formats)</li>
<li><strong>Encryption</strong>: Implement new ciphers (e.g., alternative AEADs, custom protocols)</li>
<li><strong>Hashing</strong>: Add new checksum algorithms (e.g., BLAKE3, xxHash, custom)</li>
</ul>
<p><strong>Key Concepts:</strong></p>
<ul>
<li><strong>Algorithm Enum</strong>: Type-safe algorithm identifier</li>
<li><strong>Service Trait</strong>: Domain interface defining algorithm operations</li>
<li><strong>Service Implementation</strong>: Concrete algorithm implementation</li>
<li><strong>Configuration</strong>: Algorithm-specific parameters and tuning</li>
</ul>
<h2 id="custom-compression-algorithm"><a class="header" href="#custom-compression-algorithm">Custom Compression Algorithm</a></h2>
<h3 id="step-1-extend-compressionalgorithm-enum"><a class="header" href="#step-1-extend-compressionalgorithm-enum">Step 1: Extend CompressionAlgorithm Enum</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// pipeline-domain/src/value_objects/algorithm.rs

#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum CompressionAlgorithm {
    Brotli,
    Gzip,
    Zstd,
    Lz4,

    // Custom algorithms
    Snappy,         // Google's Snappy
    Lzma,           // LZMA/XZ compression
    Custom(u8),     // Custom algorithm ID
}

impl std::fmt::Display for CompressionAlgorithm {
    fn fmt(&amp;self, f: &amp;mut std::fmt::Formatter&lt;'_&gt;) -&gt; std::fmt::Result {
        match self {
            // ... existing ...
            CompressionAlgorithm::Snappy =&gt; write!(f, "snappy"),
            CompressionAlgorithm::Lzma =&gt; write!(f, "lzma"),
            CompressionAlgorithm::Custom(id) =&gt; write!(f, "custom-{}", id),
        }
    }
}

impl std::str::FromStr for CompressionAlgorithm {
    type Err = PipelineError;

    fn from_str(s: &amp;str) -&gt; Result&lt;Self, Self::Err&gt; {
        match s.to_lowercase().as_str() {
            // ... existing ...
            "snappy" =&gt; Ok(CompressionAlgorithm::Snappy),
            "lzma" =&gt; Ok(CompressionAlgorithm::Lzma),
            s if s.starts_with("custom-") =&gt; {
                let id = s.strip_prefix("custom-")
                    .and_then(|id| id.parse::&lt;u8&gt;().ok())
                    .ok_or_else(|| PipelineError::InvalidConfiguration(
                        format!("Invalid custom ID: {}", s)
                    ))?;
                Ok(CompressionAlgorithm::Custom(id))
            }
            _ =&gt; Err(PipelineError::InvalidConfiguration(format!(
                "Unknown algorithm: {}",
                s
            ))),
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="step-2-implement-compressionservice"><a class="header" href="#step-2-implement-compressionservice">Step 2: Implement CompressionService</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// pipeline/src/infrastructure/services/snappy_compression_service.rs

use pipeline_domain::services::CompressionService;
use pipeline_domain::{FileChunk, PipelineError, ProcessingContext};
use snap::raw::{Encoder, Decoder};

/// Snappy compression service implementation
///
/// Snappy is a fast compression/decompression library developed by Google.
/// It does not aim for maximum compression, or compatibility with any other
/// compression library; instead, it aims for very high speeds and reasonable
/// compression.
///
/// **Performance Characteristics:**
/// - Compression: 250-500 MB/s
/// - Decompression: 500-1000 MB/s
/// - Compression ratio: ~50-70% of original size
pub struct SnappyCompressionService;

impl SnappyCompressionService {
    pub fn new() -&gt; Self {
        Self
    }
}

impl CompressionService for SnappyCompressionService {
    fn compress(
        &amp;self,
        chunk: FileChunk,
        context: &amp;mut ProcessingContext,
    ) -&gt; Result&lt;FileChunk, PipelineError&gt; {
        let start = std::time::Instant::now();

        // Compress using Snappy
        let mut encoder = Encoder::new();
        let compressed = encoder
            .compress_vec(chunk.data())
            .map_err(|e| PipelineError::CompressionError(format!("Snappy: {}", e)))?;

        // Update metrics
        let duration = start.elapsed();
        context.add_bytes_processed(chunk.data().len() as u64);
        context.record_stage_duration(duration);

        // Create compressed chunk
        let mut result = FileChunk::new(
            chunk.sequence_number(),
            chunk.file_offset(),
            compressed,
        );

        result.set_metadata(chunk.metadata().clone());

        Ok(result)
    }

    fn decompress(
        &amp;self,
        chunk: FileChunk,
        context: &amp;mut ProcessingContext,
    ) -&gt; Result&lt;FileChunk, PipelineError&gt; {
        let start = std::time::Instant::now();

        // Decompress using Snappy
        let mut decoder = Decoder::new();
        let decompressed = decoder
            .decompress_vec(chunk.data())
            .map_err(|e| PipelineError::DecompressionError(format!("Snappy: {}", e)))?;

        // Update metrics
        let duration = start.elapsed();
        context.add_bytes_processed(decompressed.len() as u64);
        context.record_stage_duration(duration);

        // Create decompressed chunk
        let mut result = FileChunk::new(
            chunk.sequence_number(),
            chunk.file_offset(),
            decompressed,
        );

        result.set_metadata(chunk.metadata().clone());

        Ok(result)
    }

    fn estimate_compressed_size(&amp;self, chunk: &amp;FileChunk) -&gt; usize {
        // Snappy typically achieves ~50-70% compression
        (chunk.data().len() as f64 * 0.6) as usize
    }
}

impl Default for SnappyCompressionService {
    fn default() -&gt; Self {
        Self::new()
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="step-3-register-algorithm"><a class="header" href="#step-3-register-algorithm">Step 3: Register Algorithm</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// In service factory or dependency injection

use std::sync::Arc;
use pipeline_domain::services::CompressionService;

fn create_compression_service(
    algorithm: CompressionAlgorithm,
) -&gt; Result&lt;Arc&lt;dyn CompressionService&gt;, PipelineError&gt; {
    match algorithm {
        CompressionAlgorithm::Brotli =&gt; Ok(Arc::new(BrotliCompressionService::new())),
        CompressionAlgorithm::Gzip =&gt; Ok(Arc::new(GzipCompressionService::new())),
        CompressionAlgorithm::Zstd =&gt; Ok(Arc::new(ZstdCompressionService::new())),
        CompressionAlgorithm::Lz4 =&gt; Ok(Arc::new(Lz4CompressionService::new())),

        // Custom algorithms
        CompressionAlgorithm::Snappy =&gt; Ok(Arc::new(SnappyCompressionService::new())),
        CompressionAlgorithm::Lzma =&gt; Ok(Arc::new(LzmaCompressionService::new())),

        CompressionAlgorithm::Custom(id) =&gt; {
            Err(PipelineError::InvalidConfiguration(format!(
                "Custom algorithm {} not registered",
                id
            )))
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="custom-encryption-algorithm"><a class="header" href="#custom-encryption-algorithm">Custom Encryption Algorithm</a></h2>
<h3 id="step-1-extend-encryptionalgorithm-enum"><a class="header" href="#step-1-extend-encryptionalgorithm-enum">Step 1: Extend EncryptionAlgorithm Enum</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// pipeline-domain/src/value_objects/algorithm.rs

#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum EncryptionAlgorithm {
    Aes256Gcm,
    ChaCha20Poly1305,
    XChaCha20Poly1305,

    // Custom algorithms
    Aes128Gcm,      // AES-128-GCM (faster, less secure)
    Twofish,        // Twofish cipher
    Custom(u8),     // Custom algorithm ID
}
<span class="boring">}</span></code></pre></pre>
<h3 id="step-2-implement-encryptionservice"><a class="header" href="#step-2-implement-encryptionservice">Step 2: Implement EncryptionService</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// pipeline/src/infrastructure/services/aes128_encryption_service.rs

use pipeline_domain::services::EncryptionService;
use pipeline_domain::{FileChunk, PipelineError, ProcessingContext};
use aes_gcm::{
    aead::{Aead, KeyInit, OsRng},
    Aes128Gcm, Nonce,
};

/// AES-128-GCM encryption service
///
/// This implementation uses AES-128-GCM, which is faster than AES-256-GCM
/// but provides a lower security margin (128-bit vs 256-bit key).
///
/// **Use Cases:**
/// - Performance-critical applications
/// - Scenarios where 128-bit security is sufficient
/// - Systems without AES-NI support (software fallback)
///
/// **Performance:**
/// - Encryption: 800-1200 MB/s (with AES-NI)
/// - Encryption: 150-300 MB/s (software)
pub struct Aes128EncryptionService {
    cipher: Aes128Gcm,
}

impl Aes128EncryptionService {
    pub fn new(key: &amp;[u8; 16]) -&gt; Self {
        let cipher = Aes128Gcm::new(key.into());
        Self { cipher }
    }

    pub fn generate_key() -&gt; [u8; 16] {
        use rand::RngCore;
        let mut key = [0u8; 16];
        OsRng.fill_bytes(&amp;mut key);
        key
    }
}

impl EncryptionService for Aes128EncryptionService {
    fn encrypt(
        &amp;self,
        chunk: FileChunk,
        context: &amp;mut ProcessingContext,
    ) -&gt; Result&lt;FileChunk, PipelineError&gt; {
        let start = std::time::Instant::now();

        // Generate nonce (96-bit for GCM)
        let mut nonce_bytes = [0u8; 12];
        use rand::RngCore;
        OsRng.fill_bytes(&amp;mut nonce_bytes);
        let nonce = Nonce::from_slice(&amp;nonce_bytes);

        // Encrypt data
        let ciphertext = self.cipher
            .encrypt(nonce, chunk.data())
            .map_err(|e| PipelineError::EncryptionError(format!("AES-128-GCM: {}", e)))?;

        // Prepend nonce to ciphertext
        let mut encrypted = nonce_bytes.to_vec();
        encrypted.extend_from_slice(&amp;ciphertext);

        // Update metrics
        let duration = start.elapsed();
        context.add_bytes_processed(chunk.data().len() as u64);
        context.record_stage_duration(duration);

        // Create encrypted chunk
        let mut result = FileChunk::new(
            chunk.sequence_number(),
            chunk.file_offset(),
            encrypted,
        );

        result.set_metadata(chunk.metadata().clone());

        Ok(result)
    }

    fn decrypt(
        &amp;self,
        chunk: FileChunk,
        context: &amp;mut ProcessingContext,
    ) -&gt; Result&lt;FileChunk, PipelineError&gt; {
        let start = std::time::Instant::now();

        // Extract nonce from beginning
        if chunk.data().len() &lt; 12 {
            return Err(PipelineError::DecryptionError(
                "Encrypted data too short".to_string()
            ));
        }

        let (nonce_bytes, ciphertext) = chunk.data().split_at(12);
        let nonce = Nonce::from_slice(nonce_bytes);

        // Decrypt data
        let plaintext = self.cipher
            .decrypt(nonce, ciphertext)
            .map_err(|e| PipelineError::DecryptionError(format!("AES-128-GCM: {}", e)))?;

        // Update metrics
        let duration = start.elapsed();
        context.add_bytes_processed(plaintext.len() as u64);
        context.record_stage_duration(duration);

        // Create decrypted chunk
        let mut result = FileChunk::new(
            chunk.sequence_number(),
            chunk.file_offset(),
            plaintext,
        );

        result.set_metadata(chunk.metadata().clone());

        Ok(result)
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="custom-hashing-algorithm"><a class="header" href="#custom-hashing-algorithm">Custom Hashing Algorithm</a></h2>
<h3 id="step-1-extend-hashalgorithm-enum"><a class="header" href="#step-1-extend-hashalgorithm-enum">Step 1: Extend HashAlgorithm Enum</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// pipeline-domain/src/value_objects/algorithm.rs

#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum HashAlgorithm {
    Sha256,
    Blake3,

    // Custom algorithms
    XxHash,         // xxHash (extremely fast)
    Blake2b,        // BLAKE2b
    Custom(u8),     // Custom algorithm ID
}
<span class="boring">}</span></code></pre></pre>
<h3 id="step-2-implement-checksumservice"><a class="header" href="#step-2-implement-checksumservice">Step 2: Implement ChecksumService</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// pipeline/src/infrastructure/services/xxhash_checksum_service.rs

use pipeline_domain::services::ChecksumService;
use pipeline_domain::{FileChunk, PipelineError, ProcessingContext, Checksum};
use xxhash_rust::xxh3::Xxh3;

/// xxHash checksum service implementation
///
/// xxHash is an extremely fast non-cryptographic hash algorithm,
/// working at RAM speed limits.
///
/// **Performance:**
/// - Hashing: 10-30 GB/s (on modern CPUs)
/// - ~10-20x faster than SHA-256
/// - ~2-3x faster than BLAKE3
///
/// **Use Cases:**
/// - Data integrity verification (non-cryptographic)
/// - Deduplication
/// - Hash tables
/// - NOT for security (use SHA-256 or BLAKE3 instead)
pub struct XxHashChecksumService;

impl XxHashChecksumService {
    pub fn new() -&gt; Self {
        Self
    }
}

impl ChecksumService for XxHashChecksumService {
    fn calculate_checksum(
        &amp;self,
        chunk: &amp;FileChunk,
        context: &amp;mut ProcessingContext,
    ) -&gt; Result&lt;Checksum, PipelineError&gt; {
        let start = std::time::Instant::now();

        // Calculate xxHash64
        let mut hasher = Xxh3::new();
        hasher.update(chunk.data());
        let hash = hasher.digest();

        // Convert to bytes (big-endian)
        let hash_bytes = hash.to_be_bytes();

        // Update metrics
        let duration = start.elapsed();
        context.record_stage_duration(duration);

        Ok(Checksum::new(hash_bytes.to_vec()))
    }

    fn verify_checksum(
        &amp;self,
        chunk: &amp;FileChunk,
        expected: &amp;Checksum,
        context: &amp;mut ProcessingContext,
    ) -&gt; Result&lt;bool, PipelineError&gt; {
        let calculated = self.calculate_checksum(chunk, context)?;
        Ok(calculated == *expected)
    }
}

impl Default for XxHashChecksumService {
    fn default() -&gt; Self {
        Self::new()
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="algorithm-configuration-1"><a class="header" href="#algorithm-configuration-1">Algorithm Configuration</a></h2>
<h3 id="compression-configuration-1"><a class="header" href="#compression-configuration-1">Compression Configuration</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::entities::StageConfiguration;
use std::collections::HashMap;

// Snappy configuration (minimal parameters)
let snappy_config = StageConfiguration::new(
    "snappy".to_string(),
    HashMap::new(),  // No tuning parameters
    true,            // Parallel processing
);

// LZMA configuration (with level)
let lzma_config = StageConfiguration::new(
    "lzma".to_string(),
    HashMap::from([
        ("level".to_string(), "6".to_string()),  // 0-9
    ]),
    true,
);
<span class="boring">}</span></code></pre></pre>
<h3 id="encryption-configuration-2"><a class="header" href="#encryption-configuration-2">Encryption Configuration</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// AES-128-GCM configuration
let aes128_config = StageConfiguration::new(
    "aes128gcm".to_string(),
    HashMap::from([
        ("key".to_string(), base64::encode(&amp;key)),
    ]),
    true,
);
<span class="boring">}</span></code></pre></pre>
<h3 id="hashing-configuration"><a class="header" href="#hashing-configuration">Hashing Configuration</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// xxHash configuration
let xxhash_config = StageConfiguration::new(
    "xxhash".to_string(),
    HashMap::new(),
    true,
);
<span class="boring">}</span></code></pre></pre>
<h2 id="performance-comparison"><a class="header" href="#performance-comparison">Performance Comparison</a></h2>
<h3 id="compression-algorithms"><a class="header" href="#compression-algorithms">Compression Algorithms</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Algorithm</th><th>Compression Speed</th><th>Decompression Speed</th><th>Ratio</th><th>Use Case</th></tr></thead><tbody>
<tr><td><strong>LZ4</strong></td><td>500-700 MB/s</td><td>2000-3000 MB/s</td><td>2-3x</td><td>Real-time, low latency</td></tr>
<tr><td><strong>Snappy</strong></td><td>250-500 MB/s</td><td>500-1000 MB/s</td><td>1.5-2x</td><td>Google services, fast</td></tr>
<tr><td><strong>Zstd</strong></td><td>200-400 MB/s</td><td>600-800 MB/s</td><td>3-5x</td><td>Modern balanced</td></tr>
<tr><td><strong>Brotli</strong></td><td>50-150 MB/s</td><td>300-500 MB/s</td><td>4-8x</td><td>Web, maximum compression</td></tr>
<tr><td><strong>LZMA</strong></td><td>10-30 MB/s</td><td>50-100 MB/s</td><td>5-10x</td><td>Archival, best ratio</td></tr>
</tbody></table>
</div>
<h3 id="encryption-algorithms"><a class="header" href="#encryption-algorithms">Encryption Algorithms</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Algorithm</th><th>Encryption Speed</th><th>Security</th><th>Hardware Support</th></tr></thead><tbody>
<tr><td><strong>AES-128-GCM</strong></td><td>800-1200 MB/s</td><td>Good</td><td>Yes (AES-NI)</td></tr>
<tr><td><strong>AES-256-GCM</strong></td><td>400-800 MB/s</td><td>Excellent</td><td>Yes (AES-NI)</td></tr>
<tr><td><strong>ChaCha20</strong></td><td>200-400 MB/s</td><td>Excellent</td><td>No</td></tr>
</tbody></table>
</div>
<h3 id="hashing-algorithms"><a class="header" href="#hashing-algorithms">Hashing Algorithms</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Algorithm</th><th>Throughput</th><th>Security</th><th>Use Case</th></tr></thead><tbody>
<tr><td><strong>xxHash</strong></td><td>10-30 GB/s</td><td>None</td><td>Integrity, dedup</td></tr>
<tr><td><strong>BLAKE3</strong></td><td>3-10 GB/s</td><td>Cryptographic</td><td>General purpose</td></tr>
<tr><td><strong>SHA-256</strong></td><td>400-800 MB/s</td><td>Cryptographic</td><td>Security, signatures</td></tr>
</tbody></table>
</div>
<h2 id="testing-custom-algorithms"><a class="header" href="#testing-custom-algorithms">Testing Custom Algorithms</a></h2>
<h3 id="unit-tests-5"><a class="header" href="#unit-tests-5">Unit Tests</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_snappy_compression_roundtrip() {
        let service = SnappyCompressionService::new();
        let original_data = b"Hello, World! ".repeat(100);
        let chunk = FileChunk::new(0, 0, original_data.to_vec());
        let mut context = ProcessingContext::new();

        // Compress
        let compressed = service.compress(chunk.clone(), &amp;mut context).unwrap();
        assert!(compressed.data().len() &lt; original_data.len());

        // Decompress
        let decompressed = service.decompress(compressed, &amp;mut context).unwrap();
        assert_eq!(decompressed.data(), &amp;original_data);
    }

    #[test]
    fn test_aes128_encryption_roundtrip() {
        let key = Aes128EncryptionService::generate_key();
        let service = Aes128EncryptionService::new(&amp;key);
        let original_data = b"Secret message";
        let chunk = FileChunk::new(0, 0, original_data.to_vec());
        let mut context = ProcessingContext::new();

        // Encrypt
        let encrypted = service.encrypt(chunk.clone(), &amp;mut context).unwrap();
        assert_ne!(encrypted.data(), original_data);

        // Decrypt
        let decrypted = service.decrypt(encrypted, &amp;mut context).unwrap();
        assert_eq!(decrypted.data(), original_data);
    }

    #[test]
    fn test_xxhash_checksum() {
        let service = XxHashChecksumService::new();
        let data = b"Test data";
        let chunk = FileChunk::new(0, 0, data.to_vec());
        let mut context = ProcessingContext::new();

        let checksum = service.calculate_checksum(&amp;chunk, &amp;mut context).unwrap();
        assert_eq!(checksum.bytes().len(), 8);  // 64-bit hash

        // Verify
        let valid = service.verify_checksum(&amp;chunk, &amp;checksum, &amp;mut context).unwrap();
        assert!(valid);
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="benchmark-custom-algorithm"><a class="header" href="#benchmark-custom-algorithm">Benchmark Custom Algorithm</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use criterion::{black_box, criterion_group, criterion_main, Criterion};

fn benchmark_snappy_vs_lz4(c: &amp;mut Criterion) {
    let snappy = SnappyCompressionService::new();
    let lz4 = Lz4CompressionService::new();

    let test_data = vec![0u8; 1024 * 1024];  // 1 MB
    let chunk = FileChunk::new(0, 0, test_data);
    let mut context = ProcessingContext::new();

    let mut group = c.benchmark_group("compression");

    group.bench_function("snappy", |b| {
        b.iter(|| {
            snappy.compress(black_box(chunk.clone()), &amp;mut context).unwrap()
        });
    });

    group.bench_function("lz4", |b| {
        b.iter(|| {
            lz4.compress(black_box(chunk.clone()), &amp;mut context).unwrap()
        });
    });

    group.finish();
}

criterion_group!(benches, benchmark_snappy_vs_lz4);
criterion_main!(benches);
<span class="boring">}</span></code></pre></pre>
<h2 id="best-practices-18"><a class="header" href="#best-practices-18">Best Practices</a></h2>
<h3 id="1-choose-appropriate-algorithms"><a class="header" href="#1-choose-appropriate-algorithms">1. Choose Appropriate Algorithms</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Match algorithm to use case
let compression = if priority == Speed {
    CompressionAlgorithm::Snappy  // Fastest
} else if priority == Ratio {
    CompressionAlgorithm::Lzma    // Best ratio
} else {
    CompressionAlgorithm::Zstd    // Balanced
};

// ❌ Bad: Always use same algorithm
let compression = CompressionAlgorithm::Brotli;  // Slow!
<span class="boring">}</span></code></pre></pre>
<h3 id="2-handle-errors-gracefully"><a class="header" href="#2-handle-errors-gracefully">2. Handle Errors Gracefully</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Descriptive errors
fn compress(&amp;self, chunk: FileChunk) -&gt; Result&lt;FileChunk, PipelineError&gt; {
    encoder.compress_vec(chunk.data())
        .map_err(|e| PipelineError::CompressionError(
            format!("Snappy compression failed: {}", e)
        ))?
}

// ❌ Bad: Generic errors
fn compress(&amp;self, chunk: FileChunk) -&gt; Result&lt;FileChunk, PipelineError&gt; {
    encoder.compress_vec(chunk.data()).unwrap()  // Panics!
}
<span class="boring">}</span></code></pre></pre>
<h3 id="3-benchmark-performance"><a class="header" href="#3-benchmark-performance">3. Benchmark Performance</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Always benchmark custom algorithms
#[bench]
fn bench_custom_algorithm(b: &amp;mut Bencher) {
    let service = MyCustomService::new();
    let chunk = FileChunk::new(0, 0, vec![0u8; 1024 * 1024]);

    b.iter(|| {
        service.process(black_box(chunk.clone()))
    });
}
<span class="boring">}</span></code></pre></pre>
<h2 id="related-topics-12"><a class="header" href="#related-topics-12">Related Topics</a></h2>
<ul>
<li>See <a href="advanced/extending.html">Extending the Pipeline</a> for extension points overview</li>
<li>See <a href="advanced/custom-stages.html">Custom Stages</a> for stage implementation</li>
<li>See <a href="advanced/performance.html">Performance Optimization</a> for tuning strategies</li>
<li>See <a href="advanced/benchmarking.html">Benchmarking</a> for performance measurement</li>
</ul>
<h2 id="summary-12"><a class="header" href="#summary-12">Summary</a></h2>
<p>Implementing custom algorithms involves:</p>
<ol>
<li><strong>Extend Algorithm Enum</strong>: Add variant for new algorithm</li>
<li><strong>Implement Service Trait</strong>: Create concrete implementation</li>
<li><strong>Register Algorithm</strong>: Add to service factory</li>
<li><strong>Test Thoroughly</strong>: Unit tests, integration tests, benchmarks</li>
<li><strong>Document Performance</strong>: Measure and document characteristics</li>
</ol>
<p><strong>Key Takeaways:</strong></p>
<ul>
<li>Choose algorithms based on workload (speed vs ratio vs security)</li>
<li>Implement complete error handling with specific error types</li>
<li>Benchmark against existing algorithms</li>
<li>Document performance characteristics</li>
<li>Add comprehensive tests (correctness + performance)</li>
<li>Consider hardware acceleration (AES-NI, SIMD)</li>
</ul>
<p><strong>Algorithm Implementation Checklist:</strong></p>
<ul>
<li><input disabled="" type="checkbox"/>
Extend algorithm enum (Display + FromStr)</li>
<li><input disabled="" type="checkbox"/>
Implement service trait</li>
<li><input disabled="" type="checkbox"/>
Add unit tests (correctness)</li>
<li><input disabled="" type="checkbox"/>
Add roundtrip tests (compress/decompress, encrypt/decrypt)</li>
<li><input disabled="" type="checkbox"/>
Benchmark performance</li>
<li><input disabled="" type="checkbox"/>
Compare with existing algorithms</li>
<li><input disabled="" type="checkbox"/>
Document use cases and characteristics</li>
<li><input disabled="" type="checkbox"/>
Register in service factory</li>
<li><input disabled="" type="checkbox"/>
Update configuration documentation</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="software-requirements-specification"><a class="header" href="#software-requirements-specification">Software Requirements Specification</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Comprehensive software requirements specification.</p>
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<p>TODO: Add SRS introduction</p>
<h2 id="functional-requirements"><a class="header" href="#functional-requirements">Functional Requirements</a></h2>
<p>TODO: List functional requirements</p>
<h2 id="non-functional-requirements"><a class="header" href="#non-functional-requirements">Non-Functional Requirements</a></h2>
<p>TODO: List non-functional requirements</p>
<h2 id="system-requirements"><a class="header" href="#system-requirements">System Requirements</a></h2>
<p>TODO: List system requirements</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="software-design-document-sdd"><a class="header" href="#software-design-document-sdd">Software Design Document (SDD)</a></h1>
<p><strong>Version:</strong> 1.0
<strong>Date:</strong> 2025-01-04
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner, Claude Code
<strong>Status:</strong> Active</p>
<hr />
<h2 id="1-introduction"><a class="header" href="#1-introduction">1. Introduction</a></h2>
<h3 id="11-purpose"><a class="header" href="#11-purpose">1.1 Purpose</a></h3>
<p>This Software Design Document (SDD) describes the architectural and detailed design of the Optimized Adaptive Pipeline system. It provides technical specifications for developers implementing, maintaining, and extending the system.</p>
<p><strong>Intended Audience:</strong></p>
<ul>
<li>Software developers implementing features</li>
<li>Technical architects reviewing design decisions</li>
<li>Code reviewers evaluating pull requests</li>
<li>New team members onboarding to the codebase</li>
</ul>
<h3 id="12-scope"><a class="header" href="#12-scope">1.2 Scope</a></h3>
<p>This document covers the design of a high-performance file processing pipeline implemented in Rust, following Domain-Driven Design (DDD), Clean Architecture, and Hexagonal Architecture principles.</p>
<p><strong>Covered Topics:</strong></p>
<ul>
<li>System architecture and layer organization</li>
<li>Component interactions and dependencies</li>
<li>Data structures and algorithms</li>
<li>Interface specifications</li>
<li>Design patterns and rationale</li>
<li>Technology stack and tooling</li>
<li>Cross-cutting concerns</li>
</ul>
<p><strong>Not Covered:</strong></p>
<ul>
<li>Requirements (see SRS)</li>
<li>Testing strategies (see Test Plan)</li>
<li>Deployment procedures</li>
<li>Operational procedures</li>
</ul>
<h3 id="13-design-philosophy"><a class="header" href="#13-design-philosophy">1.3 Design Philosophy</a></h3>
<p><strong>Core Principles:</strong></p>
<ol>
<li><strong>Domain-Driven Design</strong>: Business logic isolated in domain layer</li>
<li><strong>Clean Architecture</strong>: Dependencies point inward toward domain</li>
<li><strong>Hexagonal Architecture</strong>: Ports and adapters isolate external concerns</li>
<li><strong>SOLID Principles</strong>: Single responsibility, dependency inversion</li>
<li><strong>Rust Idioms</strong>: Zero-cost abstractions, ownership, type safety</li>
</ol>
<p><strong>Design Goals:</strong></p>
<ul>
<li><strong>Correctness</strong>: Type-safe, compiler-verified invariants</li>
<li><strong>Performance</strong>: Async I/O, parallel processing, minimal allocations</li>
<li><strong>Maintainability</strong>: Clear separation of concerns, testable components</li>
<li><strong>Extensibility</strong>: Plugin architecture for custom stages</li>
<li><strong>Observability</strong>: Comprehensive metrics and logging</li>
</ul>
<h3 id="14-references"><a class="header" href="#14-references">1.4 References</a></h3>
<ul>
<li><a href="formal/../reference/srs.html">Software Requirements Specification</a></li>
<li><a href="formal/../architecture/overview.html">Architecture Overview</a></li>
<li><a href="formal/../architecture/domain-model.html">Domain Model</a></li>
<li>Clean Architecture: Robert C. Martin, 2017</li>
<li>Domain-Driven Design: Eric Evans, 2003</li>
</ul>
<hr />
<h2 id="2-system-architecture"><a class="header" href="#2-system-architecture">2. System Architecture</a></h2>
<h3 id="21-architectural-overview"><a class="header" href="#21-architectural-overview">2.1 Architectural Overview</a></h3>
<p>The system follows a <strong>layered architecture</strong> with strict dependency rules:</p>
<pre><code>┌─────────────────────────────────────────────┐
│         Presentation Layer                  │ ← CLI, Future APIs
│  (bootstrap, cli, config, logger)           │
├─────────────────────────────────────────────┤
│         Application Layer                    │ ← Use Cases
│  (commands, services, orchestration)         │
├─────────────────────────────────────────────┤
│         Domain Layer (Core)                  │ ← Business Logic
│  (entities, value objects, services)         │
├─────────────────────────────────────────────┤
│         Infrastructure Layer                 │ ← External Concerns
│  (repositories, adapters, file I/O)          │
└─────────────────────────────────────────────┘

Dependency Rule: Inner layers know nothing about outer layers
</code></pre>
<h3 id="22-layered-architecture-details"><a class="header" href="#22-layered-architecture-details">2.2 Layered Architecture Details</a></h3>
<h4 id="221-presentation-layer-bootstrap-crate"><a class="header" href="#221-presentation-layer-bootstrap-crate">2.2.1 Presentation Layer (Bootstrap Crate)</a></h4>
<p><strong>Responsibilities:</strong></p>
<ul>
<li>CLI argument parsing and validation</li>
<li>Application configuration</li>
<li>Logging and console output</li>
<li>Exit code handling</li>
<li>Platform-specific concerns</li>
</ul>
<p><strong>Key Components:</strong></p>
<ul>
<li><code>cli::parser</code> - Command-line argument parsing</li>
<li><code>cli::validator</code> - Input validation and sanitization</li>
<li><code>config::AppConfig</code> - Application configuration</li>
<li><code>logger::Logger</code> - Logging abstraction</li>
<li><code>platform::Platform</code> - Platform detection</li>
</ul>
<p><strong>Design Decisions:</strong></p>
<ul>
<li>Separate crate for clean separation</li>
<li>No domain dependencies</li>
<li>Generic, reusable components</li>
</ul>
<h4 id="222-application-layer"><a class="header" href="#222-application-layer">2.2.2 Application Layer</a></h4>
<p><strong>Responsibilities:</strong></p>
<ul>
<li>Orchestrate use cases</li>
<li>Coordinate domain services</li>
<li>Manage transactions</li>
<li>Handle application-level errors</li>
</ul>
<p><strong>Key Components:</strong></p>
<ul>
<li><code>use_cases::ProcessFileUseCase</code> - File processing orchestration</li>
<li><code>use_cases::RestoreFileUseCase</code> - File restoration orchestration</li>
<li><code>commands::ProcessFileCommand</code> - Command pattern implementation</li>
<li><code>commands::RestoreFileCommand</code> - Restoration command</li>
</ul>
<p><strong>Design Patterns:</strong></p>
<ul>
<li><strong>Command Pattern</strong>: Encapsulate requests as objects</li>
<li><strong>Use Case Pattern</strong>: Application-specific business rules</li>
<li><strong>Dependency Injection</strong>: Services injected via constructors</li>
</ul>
<p><strong>Key Abstractions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[async_trait]
pub trait UseCase&lt;Input, Output&gt; {
    async fn execute(&amp;self, input: Input) -&gt; Result&lt;Output&gt;;
}
<span class="boring">}</span></code></pre></pre>
<h4 id="223-domain-layer"><a class="header" href="#223-domain-layer">2.2.3 Domain Layer</a></h4>
<p><strong>Responsibilities:</strong></p>
<ul>
<li>Core business logic</li>
<li>Domain rules and invariants</li>
<li>Domain events</li>
<li>Pure functions (no I/O)</li>
</ul>
<p><strong>Key Components:</strong></p>
<p><strong>Entities:</strong></p>
<ul>
<li><code>Pipeline</code> - Aggregate root for pipeline configuration</li>
<li><code>PipelineStage</code> - Individual processing stage</li>
<li><code>FileMetadata</code> - File information and state</li>
</ul>
<p><strong>Value Objects:</strong></p>
<ul>
<li><code>ChunkSize</code> - Validated chunk size</li>
<li><code>PipelineId</code> - Unique pipeline identifier</li>
<li><code>StageType</code> - Type-safe stage classification</li>
</ul>
<p><strong>Domain Services:</strong></p>
<ul>
<li><code>CompressionService</code> - Compression/decompression logic</li>
<li><code>EncryptionService</code> - Encryption/decryption logic</li>
<li><code>ChecksumService</code> - Integrity verification logic</li>
<li><code>FileIOService</code> - File operations abstraction</li>
</ul>
<p><strong>Invariants Enforced:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl Pipeline {
    pub fn new(name: String, stages: Vec&lt;PipelineStage&gt;) -&gt; Result&lt;Self&gt; {
        // Invariant: Name must be non-empty
        if name.trim().is_empty() {
            return Err(PipelineError::InvalidName);
        }

        // Invariant: At least one stage required
        if stages.is_empty() {
            return Err(PipelineError::NoStages);
        }

        // Invariant: Stages must have unique, sequential order
        validate_stage_order(&amp;stages)?;

        Ok(Pipeline { name, stages })
    }
}
<span class="boring">}</span></code></pre></pre>
<h4 id="224-infrastructure-layer"><a class="header" href="#224-infrastructure-layer">2.2.4 Infrastructure Layer</a></h4>
<p><strong>Responsibilities:</strong></p>
<ul>
<li>External system integration</li>
<li>Data persistence</li>
<li>File I/O operations</li>
<li>Third-party library adapters</li>
</ul>
<p><strong>Key Components:</strong></p>
<p><strong>Repositories:</strong></p>
<ul>
<li><code>SqlitePipelineRepository</code> - SQLite-based persistence</li>
<li><code>InMemoryPipelineRepository</code> - Testing/development</li>
</ul>
<p><strong>Adapters:</strong></p>
<ul>
<li><code>BrotliCompressionAdapter</code> - Brotli compression</li>
<li><code>ZstdCompressionAdapter</code> - Zstandard compression</li>
<li><code>AesGcmEncryptionAdapter</code> - AES-256-GCM encryption</li>
<li><code>Sha256ChecksumAdapter</code> - SHA-256 hashing</li>
</ul>
<p><strong>File I/O:</strong></p>
<ul>
<li><code>BinaryFileWriter</code> - .adapipe format writer</li>
<li><code>BinaryFileReader</code> - .adapipe format reader</li>
<li><code>ChunkedFileReader</code> - Streaming file reader</li>
<li><code>ChunkedFileWriter</code> - Streaming file writer</li>
</ul>
<h3 id="23-hexagonal-architecture-ports--adapters"><a class="header" href="#23-hexagonal-architecture-ports--adapters">2.3 Hexagonal Architecture (Ports &amp; Adapters)</a></h3>
<p><strong>Primary Ports</strong> (driving adapters - how the world uses us):</p>
<ul>
<li>CLI commands</li>
<li>Future: REST API, gRPC API</li>
</ul>
<p><strong>Secondary Ports</strong> (driven adapters - how we use the world):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Domain defines the interface (port)
#[async_trait]
pub trait CompressionPort: Send + Sync {
    async fn compress(&amp;self, data: &amp;[u8]) -&gt; Result&lt;Vec&lt;u8&gt;&gt;;
    async fn decompress(&amp;self, data: &amp;[u8]) -&gt; Result&lt;Vec&lt;u8&gt;&gt;;
}

// Infrastructure provides implementations (adapters)
pub struct BrotliAdapter {
    quality: u32,
}

#[async_trait]
impl CompressionPort for BrotliAdapter {
    async fn compress(&amp;self, data: &amp;[u8]) -&gt; Result&lt;Vec&lt;u8&gt;&gt; {
        // Brotli-specific implementation
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Benefits:</strong></p>
<ul>
<li>Domain layer testable in isolation</li>
<li>Easy to swap implementations</li>
<li>Third-party dependencies isolated</li>
<li>Technology-agnostic core</li>
</ul>
<hr />
<h2 id="3-component-design"><a class="header" href="#3-component-design">3. Component Design</a></h2>
<h3 id="31-pipeline-processing-engine"><a class="header" href="#31-pipeline-processing-engine">3.1 Pipeline Processing Engine</a></h3>
<p><strong>Architecture:</strong></p>
<pre><code>┌──────────────┐
│   Pipeline   │ (Aggregate Root)
│              │
│  - name      │
│  - stages[]  │
│  - metadata  │
└──────┬───────┘
       │ has many
       ▼
┌──────────────┐
│PipelineStage │
│              │
│  - name      │
│  - type      │
│  - config    │
│  - order     │
└──────────────┘
</code></pre>
<p><strong>Processing Flow:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct PipelineProcessor {
    context: Arc&lt;PipelineContext&gt;,
}

impl PipelineProcessor {
    pub async fn process(&amp;self, pipeline: &amp;Pipeline, input: PathBuf)
        -&gt; Result&lt;PathBuf&gt;
    {
        // 1. Validate input file
        let metadata = self.validate_input(&amp;input).await?;

        // 2. Create output file
        let output = self.create_output_file(&amp;pipeline.name).await?;

        // 3. Process in chunks
        let chunks = self.chunk_file(&amp;input, self.context.chunk_size).await?;

        // 4. Execute stages on each chunk (parallel)
        for chunk in chunks {
            let processed = self.execute_stages(pipeline, chunk).await?;
            self.write_chunk(&amp;output, processed).await?;
        }

        // 5. Write metadata and finalize
        self.finalize(&amp;output, metadata).await?;

        Ok(output)
    }

    async fn execute_stages(&amp;self, pipeline: &amp;Pipeline, chunk: Chunk)
        -&gt; Result&lt;Chunk&gt;
    {
        let mut data = chunk.data;

        for stage in &amp;pipeline.stages {
            data = match stage.stage_type {
                StageType::Compression =&gt; {
                    self.context.compression.compress(&amp;data).await?
                }
                StageType::Encryption =&gt; {
                    self.context.encryption.encrypt(&amp;data).await?
                }
                StageType::Checksum =&gt; {
                    let hash = self.context.checksum.hash(&amp;data).await?;
                    self.context.store_hash(chunk.id, hash);
                    data // Checksum doesn't transform data
                }
                StageType::PassThrough =&gt; data,
            };
        }

        Ok(Chunk { data, ..chunk })
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="32-chunk-processing-strategy"><a class="header" href="#32-chunk-processing-strategy">3.2 Chunk Processing Strategy</a></h3>
<p><strong>Design Rationale:</strong></p>
<ul>
<li>Large files cannot fit in memory</li>
<li>Streaming processing enables arbitrary file sizes</li>
<li>Parallel chunk processing utilizes multiple cores</li>
<li>Fixed chunk size simplifies memory management</li>
</ul>
<p><strong>Implementation:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub const DEFAULT_CHUNK_SIZE: usize = 1_048_576; // 1 MB

pub struct Chunk {
    pub id: usize,
    pub data: Vec&lt;u8&gt;,
    pub is_final: bool,
}

pub struct ChunkedFileReader {
    file: File,
    chunk_size: usize,
    chunk_id: usize,
}

impl ChunkedFileReader {
    pub async fn read_chunk(&amp;mut self) -&gt; Result&lt;Option&lt;Chunk&gt;&gt; {
        let mut buffer = vec![0u8; self.chunk_size];
        let bytes_read = self.file.read(&amp;mut buffer).await?;

        if bytes_read == 0 {
            return Ok(None);
        }

        buffer.truncate(bytes_read);

        let chunk = Chunk {
            id: self.chunk_id,
            data: buffer,
            is_final: bytes_read &lt; self.chunk_size,
        };

        self.chunk_id += 1;
        Ok(Some(chunk))
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Parallel Processing:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tokio::task::JoinSet;

pub async fn process_chunks_parallel(
    chunks: Vec&lt;Chunk&gt;,
    stage: &amp;dyn ProcessingStage,
) -&gt; Result&lt;Vec&lt;Chunk&gt;&gt; {
    let mut join_set = JoinSet::new();

    for chunk in chunks {
        let stage = Arc::clone(&amp;stage);
        join_set.spawn(async move {
            stage.process(chunk).await
        });
    }

    let mut results = Vec::new();
    while let Some(result) = join_set.join_next().await {
        results.push(result??);
    }

    // Sort by chunk ID to maintain order
    results.sort_by_key(|c| c.id);

    Ok(results)
}
<span class="boring">}</span></code></pre></pre>
<h3 id="33-repository-pattern-implementation"><a class="header" href="#33-repository-pattern-implementation">3.3 Repository Pattern Implementation</a></h3>
<p><strong>Interface (Port):</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[async_trait]
pub trait PipelineRepository: Send + Sync {
    async fn save(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;()&gt;;
    async fn find_by_id(&amp;self, id: &amp;str) -&gt; Result&lt;Option&lt;Pipeline&gt;&gt;;
    async fn find_by_name(&amp;self, name: &amp;str) -&gt; Result&lt;Option&lt;Pipeline&gt;&gt;;
    async fn delete(&amp;self, id: &amp;str) -&gt; Result&lt;()&gt;;
    async fn list_all(&amp;self) -&gt; Result&lt;Vec&lt;Pipeline&gt;&gt;;
}
<span class="boring">}</span></code></pre></pre>
<p><strong>SQLite Implementation:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct SqlitePipelineRepository {
    pool: Arc&lt;SqlitePool&gt;,
}

#[async_trait]
impl PipelineRepository for SqlitePipelineRepository {
    async fn save(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;()&gt; {
        let mut tx = self.pool.begin().await?;

        // Insert pipeline
        sqlx::query!(
            "INSERT INTO pipelines (id, name, created_at) VALUES (?, ?, ?)",
            pipeline.id,
            pipeline.name,
            pipeline.created_at
        )
        .execute(&amp;mut *tx)
        .await?;

        // Insert stages
        for stage in &amp;pipeline.stages {
            sqlx::query!(
                "INSERT INTO pipeline_stages
                 (pipeline_id, name, type, order_num, config)
                 VALUES (?, ?, ?, ?, ?)",
                pipeline.id,
                stage.name,
                stage.stage_type.to_string(),
                stage.order,
                serde_json::to_string(&amp;stage.configuration)?
            )
            .execute(&amp;mut *tx)
            .await?;
        }

        tx.commit().await?;
        Ok(())
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="34-adapter-pattern-for-algorithms"><a class="header" href="#34-adapter-pattern-for-algorithms">3.4 Adapter Pattern for Algorithms</a></h3>
<p><strong>Design:</strong> Each algorithm family has a common interface with multiple implementations.</p>
<p><strong>Compression Adapters:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[async_trait]
pub trait CompressionAdapter: Send + Sync {
    fn name(&amp;self) -&gt; &amp;str;
    async fn compress(&amp;self, data: &amp;[u8]) -&gt; Result&lt;Vec&lt;u8&gt;&gt;;
    async fn decompress(&amp;self, data: &amp;[u8]) -&gt; Result&lt;Vec&lt;u8&gt;&gt;;
}

pub struct BrotliAdapter {
    quality: u32,
}

#[async_trait]
impl CompressionAdapter for BrotliAdapter {
    fn name(&amp;self) -&gt; &amp;str { "brotli" }

    async fn compress(&amp;self, data: &amp;[u8]) -&gt; Result&lt;Vec&lt;u8&gt;&gt; {
        let mut output = Vec::new();
        let mut compressor = brotli::CompressorReader::new(
            data,
            4096,
            self.quality,
            22
        );
        compressor.read_to_end(&amp;mut output)?;
        Ok(output)
    }

    async fn decompress(&amp;self, data: &amp;[u8]) -&gt; Result&lt;Vec&lt;u8&gt;&gt; {
        let mut output = Vec::new();
        let mut decompressor = brotli::Decompressor::new(data, 4096);
        decompressor.read_to_end(&amp;mut output)?;
        Ok(output)
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Factory Pattern:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct AdapterFactory;

impl AdapterFactory {
    pub fn create_compression(
        algorithm: &amp;str
    ) -&gt; Result&lt;Box&lt;dyn CompressionAdapter&gt;&gt; {
        match algorithm.to_lowercase().as_str() {
            "brotli" =&gt; Ok(Box::new(BrotliAdapter::new(11))),
            "zstd" =&gt; Ok(Box::new(ZstdAdapter::new(3))),
            "gzip" =&gt; Ok(Box::new(GzipAdapter::new(6))),
            "lz4" =&gt; Ok(Box::new(Lz4Adapter::new())),
            _ =&gt; Err(PipelineError::UnsupportedAlgorithm(
                algorithm.to_string()
            )),
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="4-data-design"><a class="header" href="#4-data-design">4. Data Design</a></h2>
<h3 id="41-domain-entities"><a class="header" href="#41-domain-entities">4.1 Domain Entities</a></h3>
<p><strong>Pipeline Entity:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct Pipeline {
    id: PipelineId,
    name: String,
    stages: Vec&lt;PipelineStage&gt;,
    created_at: DateTime&lt;Utc&gt;,
    updated_at: DateTime&lt;Utc&gt;,
}

impl Pipeline {
    // Factory method enforces invariants
    pub fn new(name: String, stages: Vec&lt;PipelineStage&gt;) -&gt; Result&lt;Self&gt; {
        // Validation logic
    }

    // Domain behavior
    pub fn add_stage(&amp;mut self, stage: PipelineStage) -&gt; Result&lt;()&gt; {
        // Ensure stage order is valid
        // Ensure no duplicate stage names
    }

    pub fn execute_order(&amp;self) -&gt; &amp;[PipelineStage] {
        &amp;self.stages
    }

    pub fn restore_order(&amp;self) -&gt; Vec&lt;PipelineStage&gt; {
        self.stages.iter().rev().cloned().collect()
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>PipelineStage Entity:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct PipelineStage {
    name: String,
    stage_type: StageType,
    configuration: StageConfiguration,
    order: usize,
}

pub enum StageType {
    Compression,
    Encryption,
    Checksum,
    PassThrough,
}

pub struct StageConfiguration {
    pub algorithm: String,
    pub parameters: HashMap&lt;String, String&gt;,
    pub parallel_processing: bool,
    pub chunk_size: Option&lt;usize&gt;,
}
<span class="boring">}</span></code></pre></pre>
<h3 id="42-value-objects"><a class="header" href="#42-value-objects">4.2 Value Objects</a></h3>
<p><strong>ChunkSize:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub struct ChunkSize(usize);

impl ChunkSize {
    pub const MIN: usize = 1024;  // 1 KB
    pub const MAX: usize = 100 * 1024 * 1024;  // 100 MB
    pub const DEFAULT: usize = 1_048_576;  // 1 MB

    pub fn new(size: usize) -&gt; Result&lt;Self&gt; {
        if size &lt; Self::MIN || size &gt; Self::MAX {
            return Err(PipelineError::InvalidChunkSize {
                size,
                min: Self::MIN,
                max: Self::MAX
            });
        }
        Ok(ChunkSize(size))
    }

    pub fn value(&amp;self) -&gt; usize {
        self.0
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="43-database-schema"><a class="header" href="#43-database-schema">4.3 Database Schema</a></h3>
<p><strong>Schema Management:</strong> Using <code>sqlx</code> with migrations</p>
<pre><code class="language-sql">-- migrations/20250101000000_initial_schema.sql

-- Pipelines table: stores pipeline configurations
CREATE TABLE IF NOT EXISTS pipelines (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL UNIQUE,
    archived BOOLEAN NOT NULL DEFAULT false,
    created_at TEXT NOT NULL,
    updated_at TEXT NOT NULL
);

-- Pipeline configuration: key-value pairs for pipeline settings
CREATE TABLE IF NOT EXISTS pipeline_configuration (
    pipeline_id TEXT NOT NULL,
    key TEXT NOT NULL,
    value TEXT NOT NULL,
    archived BOOLEAN NOT NULL DEFAULT FALSE,
    created_at TEXT NOT NULL,
    updated_at TEXT NOT NULL,
    PRIMARY KEY (pipeline_id, key),
    FOREIGN KEY (pipeline_id) REFERENCES pipelines(id) ON DELETE CASCADE
);

-- Pipeline stages: defines processing stages in a pipeline
CREATE TABLE IF NOT EXISTS pipeline_stages (
    id TEXT PRIMARY KEY,
    pipeline_id TEXT NOT NULL,
    name TEXT NOT NULL,
    stage_type TEXT NOT NULL,
    enabled BOOLEAN NOT NULL DEFAULT TRUE,
    stage_order INTEGER NOT NULL,
    algorithm TEXT NOT NULL,
    parallel_processing BOOLEAN NOT NULL DEFAULT FALSE,
    chunk_size INTEGER,
    archived BOOLEAN NOT NULL DEFAULT FALSE,
    created_at TEXT NOT NULL,
    updated_at TEXT NOT NULL,
    FOREIGN KEY (pipeline_id) REFERENCES pipelines(id) ON DELETE CASCADE
);

-- Stage parameters: configuration for individual stages
CREATE TABLE IF NOT EXISTS stage_parameters (
    stage_id TEXT NOT NULL,
    key TEXT NOT NULL,
    value TEXT NOT NULL,
    archived BOOLEAN NOT NULL DEFAULT FALSE,
    created_at TEXT NOT NULL,
    updated_at TEXT NOT NULL,
    PRIMARY KEY (stage_id, key),
    FOREIGN KEY (stage_id) REFERENCES pipeline_stages(id) ON DELETE CASCADE
);

-- Processing metrics: tracks pipeline execution metrics
CREATE TABLE IF NOT EXISTS processing_metrics (
    pipeline_id TEXT PRIMARY KEY,
    bytes_processed INTEGER NOT NULL DEFAULT 0,
    bytes_total INTEGER NOT NULL DEFAULT 0,
    chunks_processed INTEGER NOT NULL DEFAULT 0,
    chunks_total INTEGER NOT NULL DEFAULT 0,
    start_time_rfc3339 TEXT,
    end_time_rfc3339 TEXT,
    processing_duration_ms INTEGER,
    throughput_bytes_per_second REAL NOT NULL DEFAULT 0.0,
    compression_ratio REAL,
    error_count INTEGER NOT NULL DEFAULT 0,
    warning_count INTEGER NOT NULL DEFAULT 0,
    input_file_size_bytes INTEGER NOT NULL DEFAULT 0,
    output_file_size_bytes INTEGER NOT NULL DEFAULT 0,
    input_file_checksum TEXT,
    output_file_checksum TEXT,
    FOREIGN KEY (pipeline_id) REFERENCES pipelines(id) ON DELETE CASCADE
);

-- Indexes for performance
CREATE INDEX IF NOT EXISTS idx_pipeline_stages_pipeline_id ON pipeline_stages(pipeline_id);
CREATE INDEX IF NOT EXISTS idx_pipeline_stages_order ON pipeline_stages(pipeline_id, stage_order);
CREATE INDEX IF NOT EXISTS idx_pipeline_configuration_pipeline_id ON pipeline_configuration(pipeline_id);
CREATE INDEX IF NOT EXISTS idx_stage_parameters_stage_id ON stage_parameters(stage_id);
CREATE INDEX IF NOT EXISTS idx_pipelines_name ON pipelines(name) WHERE archived = false;
</code></pre>
<h3 id="44-binary-file-format-adapipe"><a class="header" href="#44-binary-file-format-adapipe">4.4 Binary File Format (.adapipe)</a></h3>
<p><strong>Format Specification:</strong></p>
<pre><code>.adapipe File Structure:
┌────────────────────────────────────┐
│ Magic Number (4 bytes): "ADPI"    │
│ Version (2 bytes): Major.Minor     │
│ Header Length (4 bytes)            │
├────────────────────────────────────┤
│ Header (JSON):                     │
│  - pipeline_name                   │
│  - stage_configs[]                 │
│  - original_filename               │
│  - original_size                   │
│  - chunk_count                     │
│  - checksum_algorithm              │
├────────────────────────────────────┤
│ Chunk 0:                           │
│  - Length (4 bytes)                │
│  - Checksum (32 bytes)             │
│  - Data (variable)                 │
├────────────────────────────────────┤
│ Chunk 1: ...                       │
├────────────────────────────────────┤
│ ...                                │
├────────────────────────────────────┤
│ Footer (JSON):                     │
│  - total_checksum                  │
│  - created_at                      │
└────────────────────────────────────┘
</code></pre>
<p><strong>Implementation:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct AdapipeHeader {
    pub pipeline_name: String,
    pub stages: Vec&lt;StageConfig&gt;,
    pub original_filename: String,
    pub original_size: u64,
    pub chunk_count: usize,
    pub checksum_algorithm: String,
}

pub struct BinaryFileWriter {
    file: File,
}

impl BinaryFileWriter {
    pub async fn write_header(&amp;mut self, header: &amp;AdapipeHeader)
        -&gt; Result&lt;()&gt;
    {
        // Magic number
        self.file.write_all(b"ADPI").await?;

        // Version
        self.file.write_u8(1).await?;  // Major
        self.file.write_u8(0).await?;  // Minor

        // Header JSON
        let header_json = serde_json::to_vec(header)?;
        self.file.write_u32(header_json.len() as u32).await?;
        self.file.write_all(&amp;header_json).await?;

        Ok(())
    }

    pub async fn write_chunk(&amp;mut self, chunk: &amp;ProcessedChunk)
        -&gt; Result&lt;()&gt;
    {
        self.file.write_u32(chunk.data.len() as u32).await?;
        self.file.write_all(&amp;chunk.checksum).await?;
        self.file.write_all(&amp;chunk.data).await?;
        Ok(())
    }
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="5-interface-design"><a class="header" href="#5-interface-design">5. Interface Design</a></h2>
<h3 id="51-public-api"><a class="header" href="#51-public-api">5.1 Public API</a></h3>
<p><strong>Command-Line Interface:</strong></p>
<pre><code class="language-bash"># Process a file
pipeline process \
    --input ./document.pdf \
    --output ./document.adapipe \
    --pipeline secure-archive \
    --compress zstd \
    --encrypt aes256gcm \
    --key-file ./key.txt

# Restore a file
pipeline restore \
    --input ./document.adapipe \
    --output ./document.pdf \
    --key-file ./key.txt

# List pipelines
pipeline list

# Create pipeline configuration
pipeline create \
    --name my-pipeline \
    --stages compress:zstd,encrypt:aes256gcm
</code></pre>
<p><strong>Future: Library API:</strong></p>
<pre><pre class="playground"><code class="language-rust">use pipeline_domain::entities::{Pipeline, PipelineStage, StageType, StageConfiguration};
use std::collections::HashMap;

#[tokio::main]
async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    // Create pipeline stages
    let compression_stage = PipelineStage::new(
        "compress".to_string(),
        StageType::Compression,
        StageConfiguration::new(
            "zstd".to_string(),
            HashMap::new(),
            false,
        ),
        0,
    )?;

    let encryption_stage = PipelineStage::new(
        "encrypt".to_string(),
        StageType::Encryption,
        StageConfiguration::new(
            "aes256gcm".to_string(),
            HashMap::new(),
            false,
        ),
        1,
    )?;

    // Create pipeline with stages
    let pipeline = Pipeline::new(
        "my-pipeline".to_string(),
        vec![compression_stage, encryption_stage],
    )?;

    // Process file (future implementation)
    // let processor = PipelineProcessor::new()?;
    // let output = processor.process(&amp;pipeline, "input.txt").await?;

    println!("Pipeline created: {}", pipeline.name());
    Ok(())
}</code></pre></pre>
<h3 id="52-internal-apis"><a class="header" href="#52-internal-apis">5.2 Internal APIs</a></h3>
<p><strong>Domain Service Interfaces:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[async_trait]
pub trait CompressionService: Send + Sync {
    async fn compress(
        &amp;self,
        algorithm: &amp;str,
        data: &amp;[u8]
    ) -&gt; Result&lt;Vec&lt;u8&gt;&gt;;

    async fn decompress(
        &amp;self,
        algorithm: &amp;str,
        data: &amp;[u8]
    ) -&gt; Result&lt;Vec&lt;u8&gt;&gt;;
}

#[async_trait]
pub trait EncryptionService: Send + Sync {
    async fn encrypt(
        &amp;self,
        algorithm: &amp;str,
        data: &amp;[u8],
        key: &amp;[u8]
    ) -&gt; Result&lt;Vec&lt;u8&gt;&gt;;

    async fn decrypt(
        &amp;self,
        algorithm: &amp;str,
        data: &amp;[u8],
        key: &amp;[u8]
    ) -&gt; Result&lt;Vec&lt;u8&gt;&gt;;
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="6-technology-stack"><a class="header" href="#6-technology-stack">6. Technology Stack</a></h2>
<h3 id="61-core-technologies"><a class="header" href="#61-core-technologies">6.1 Core Technologies</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Technology</th><th>Purpose</th><th>Version</th></tr></thead><tbody>
<tr><td>Rust</td><td>Primary language</td><td>1.75+</td></tr>
<tr><td>Tokio</td><td>Async runtime</td><td>1.35+</td></tr>
<tr><td>SQLx</td><td>Database access</td><td>0.7+</td></tr>
<tr><td>Serde</td><td>Serialization</td><td>1.0+</td></tr>
<tr><td>Anyhow</td><td>Error handling</td><td>1.0+</td></tr>
<tr><td>Clap</td><td>CLI parsing</td><td>4.5+</td></tr>
</tbody></table>
</div>
<h3 id="62-algorithms--libraries"><a class="header" href="#62-algorithms--libraries">6.2 Algorithms &amp; Libraries</a></h3>
<p><strong>Compression:</strong></p>
<ul>
<li>Brotli (<code>brotli</code> crate)</li>
<li>Zstandard (<code>zstd</code> crate)</li>
<li>Gzip (<code>flate2</code> crate)</li>
<li>LZ4 (<code>lz4</code> crate)</li>
</ul>
<p><strong>Encryption:</strong></p>
<ul>
<li>AES-GCM (<code>aes-gcm</code> crate)</li>
<li>ChaCha20-Poly1305 (<code>chacha20poly1305</code> crate)</li>
</ul>
<p><strong>Hashing:</strong></p>
<ul>
<li>SHA-256/SHA-512 (<code>sha2</code> crate)</li>
<li>BLAKE3 (<code>blake3</code> crate)</li>
</ul>
<p><strong>Testing:</strong></p>
<ul>
<li>Criterion (benchmarking)</li>
<li>Proptest (property testing)</li>
<li>Mockall (mocking)</li>
</ul>
<h3 id="63-development-tools"><a class="header" href="#63-development-tools">6.3 Development Tools</a></h3>
<ul>
<li><strong>Cargo</strong>: Build system and package manager</li>
<li><strong>Clippy</strong>: Linter</li>
<li><strong>Rustfmt</strong>: Code formatter</li>
<li><strong>Cargo-audit</strong>: Security auditing</li>
<li><strong>Cargo-deny</strong>: Dependency validation</li>
<li><strong>mdBook</strong>: Documentation generation</li>
</ul>
<hr />
<h2 id="7-design-patterns"><a class="header" href="#7-design-patterns">7. Design Patterns</a></h2>
<h3 id="71-repository-pattern"><a class="header" href="#71-repository-pattern">7.1 Repository Pattern</a></h3>
<p><strong>Purpose:</strong> Abstract data persistence logic from domain logic.</p>
<p><strong>Implementation:</strong> See Section 3.3</p>
<p><strong>Benefits:</strong></p>
<ul>
<li>Domain layer independent of storage mechanism</li>
<li>Easy to swap SQLite for PostgreSQL or other storage</li>
<li>Testable with in-memory implementation</li>
</ul>
<h3 id="72-adapter-pattern"><a class="header" href="#72-adapter-pattern">7.2 Adapter Pattern</a></h3>
<p><strong>Purpose:</strong> Integrate third-party algorithms without coupling.</p>
<p><strong>Implementation:</strong> See Section 3.4</p>
<p><strong>Benefits:</strong></p>
<ul>
<li>Easy to add new algorithms</li>
<li>Algorithm selection at runtime</li>
<li>Consistent interface across all adapters</li>
</ul>
<h3 id="73-strategy-pattern"><a class="header" href="#73-strategy-pattern">7.3 Strategy Pattern</a></h3>
<p><strong>Purpose:</strong> Select algorithm implementation dynamically.</p>
<p><strong>Example:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct PipelineProcessor {
    compression_strategy: Box&lt;dyn CompressionAdapter&gt;,
    encryption_strategy: Box&lt;dyn EncryptionAdapter&gt;,
}

impl PipelineProcessor {
    pub fn new(
        compression: &amp;str,
        encryption: &amp;str
    ) -&gt; Result&lt;Self&gt; {
        Ok(Self {
            compression_strategy: AdapterFactory::create_compression(
                compression
            )?,
            encryption_strategy: AdapterFactory::create_encryption(
                encryption
            )?,
        })
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="74-builder-pattern"><a class="header" href="#74-builder-pattern">7.4 Builder Pattern</a></h3>
<p><strong>Purpose:</strong> Construct complex objects step by step.</p>
<p><strong>Note:</strong> Builder pattern is a potential future enhancement. Current API uses direct construction.</p>
<p><strong>Current API:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let pipeline = Pipeline::new(
    "my-pipeline".to_string(),
    vec![compression_stage, encryption_stage],
)?;
<span class="boring">}</span></code></pre></pre>
<p><strong>Potential Future Builder API:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Not yet implemented - potential future enhancement
let pipeline = Pipeline::builder()
    .name("my-pipeline")
    .add_stage(compression_stage)
    .add_stage(encryption_stage)
    .build()?;
<span class="boring">}</span></code></pre></pre>
<h3 id="75-command-pattern"><a class="header" href="#75-command-pattern">7.5 Command Pattern</a></h3>
<p><strong>Purpose:</strong> Encapsulate requests as objects for undo/redo, queueing.</p>
<p><strong>Example:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[async_trait]
pub trait Command: Send + Sync {
    async fn execute(&amp;self) -&gt; Result&lt;()&gt;;
}

pub struct ProcessFileCommand {
    pipeline: Pipeline,
    input: PathBuf,
    output: PathBuf,
}

#[async_trait]
impl Command for ProcessFileCommand {
    async fn execute(&amp;self) -&gt; Result&lt;()&gt; {
        // Processing logic
    }
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="8-cross-cutting-concerns"><a class="header" href="#8-cross-cutting-concerns">8. Cross-Cutting Concerns</a></h2>
<h3 id="81-error-handling"><a class="header" href="#81-error-handling">8.1 Error Handling</a></h3>
<p><strong>Strategy:</strong> Use <code>anyhow::Result</code> for application errors, custom error types for domain errors.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Debug, thiserror::Error)]
pub enum PipelineError {
    #[error("Invalid pipeline name: {0}")]
    InvalidName(String),

    #[error("No stages defined")]
    NoStages,

    #[error("Invalid chunk size: {size} (must be {min}-{max})")]
    InvalidChunkSize { size: usize, min: usize, max: usize },

    #[error("Stage order conflict at position {0}")]
    StageOrderConflict(usize),

    #[error("Unsupported algorithm: {0}")]
    UnsupportedAlgorithm(String),

    #[error("I/O error: {0}")]
    Io(#[from] std::io::Error),
}
<span class="boring">}</span></code></pre></pre>
<h3 id="82-logging"><a class="header" href="#82-logging">8.2 Logging</a></h3>
<p><strong>Strategy:</strong> Structured logging with tracing.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tracing::{info, warn, error, debug, instrument};

#[instrument(skip(self, data))]
async fn process_chunk(&amp;self, chunk_id: usize, data: &amp;[u8])
    -&gt; Result&lt;Vec&lt;u8&gt;&gt;
{
    debug!(chunk_id, size = data.len(), "Processing chunk");

    let start = Instant::now();
    let result = self.compress(data).await?;

    info!(
        chunk_id,
        original_size = data.len(),
        compressed_size = result.len(),
        duration_ms = start.elapsed().as_millis(),
        "Chunk compressed"
    );

    Ok(result)
}
<span class="boring">}</span></code></pre></pre>
<h3 id="83-metrics-collection"><a class="header" href="#83-metrics-collection">8.3 Metrics Collection</a></h3>
<p><strong>Strategy:</strong> Prometheus-style metrics.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct PipelineMetrics {
    pub chunks_processed: AtomicUsize,
    pub bytes_processed: AtomicU64,
    pub compression_ratio: AtomicU64,
    pub processing_duration: Duration,
}

impl PipelineMetrics {
    pub fn record_chunk(&amp;self, original_size: usize, compressed_size: usize) {
        self.chunks_processed.fetch_add(1, Ordering::Relaxed);
        self.bytes_processed.fetch_add(
            original_size as u64,
            Ordering::Relaxed
        );

        let ratio = (compressed_size as f64 / original_size as f64 * 100.0)
            as u64;
        self.compression_ratio.store(ratio, Ordering::Relaxed);
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="84-security"><a class="header" href="#84-security">8.4 Security</a></h3>
<p><strong>Key Management:</strong></p>
<ul>
<li>Keys never logged or persisted unencrypted</li>
<li>Argon2 key derivation from passwords</li>
<li>Secure memory wiping for key material</li>
</ul>
<p><strong>Input Validation:</strong></p>
<ul>
<li>CLI inputs sanitized against injection</li>
<li>File paths validated</li>
<li>Chunk sizes bounded</li>
</ul>
<p><strong>Algorithm Selection:</strong></p>
<ul>
<li>Only vetted, well-known algorithms</li>
<li>Default to secure settings</li>
<li>AEAD ciphers for authenticated encryption</li>
</ul>
<hr />
<h2 id="9-performance-considerations"><a class="header" href="#9-performance-considerations">9. Performance Considerations</a></h2>
<h3 id="91-asynchronous-io"><a class="header" href="#91-asynchronous-io">9.1 Asynchronous I/O</a></h3>
<p><strong>Rationale:</strong> File I/O is the primary bottleneck. Async I/O allows overlap of CPU and I/O operations.</p>
<p><strong>Implementation:</strong></p>
<ul>
<li>Tokio async runtime</li>
<li><code>tokio::fs</code> for file operations</li>
<li><code>tokio::io::AsyncRead</code> and <code>AsyncWrite</code> traits</li>
</ul>
<h3 id="92-parallel-processing"><a class="header" href="#92-parallel-processing">9.2 Parallel Processing</a></h3>
<p><strong>Chunk-level Parallelism:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use rayon::prelude::*;

pub fn process_chunks_parallel(
    chunks: Vec&lt;Chunk&gt;,
    processor: &amp;ChunkProcessor,
) -&gt; Result&lt;Vec&lt;ProcessedChunk&gt;&gt; {
    chunks
        .par_iter()
        .map(|chunk| processor.process(chunk))
        .collect()
}
<span class="boring">}</span></code></pre></pre>
<h3 id="93-memory-management"><a class="header" href="#93-memory-management">9.3 Memory Management</a></h3>
<p><strong>Strategies:</strong></p>
<ul>
<li>Fixed chunk size limits peak memory</li>
<li>Streaming processing avoids loading entire files</li>
<li>Buffer pooling for frequently allocated buffers</li>
<li>Zero-copy where possible</li>
</ul>
<h3 id="94-algorithm-selection"><a class="header" href="#94-algorithm-selection">9.4 Algorithm Selection</a></h3>
<p><strong>Performance Profiles:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>Algorithm</th><th>Speed</th><th>Ratio</th><th>Memory</th></tr></thead><tbody>
<tr><td>LZ4</td><td>★★★★★</td><td>★★☆☆☆</td><td>★★★★★</td></tr>
<tr><td>Zstd</td><td>★★★★☆</td><td>★★★★☆</td><td>★★★★☆</td></tr>
<tr><td>Gzip</td><td>★★★☆☆</td><td>★★★☆☆</td><td>★★★★☆</td></tr>
<tr><td>Brotli</td><td>★★☆☆☆</td><td>★★★★★</td><td>★★★☆☆</td></tr>
</tbody></table>
</div>
<hr />
<h2 id="10-testing-strategy-overview"><a class="header" href="#10-testing-strategy-overview">10. Testing Strategy (Overview)</a></h2>
<h3 id="101-test-organization"><a class="header" href="#101-test-organization">10.1 Test Organization</a></h3>
<ul>
<li><strong>Unit Tests</strong>: <code>#[cfg(test)]</code> modules in source files</li>
<li><strong>Integration Tests</strong>: <code>tests/integration/</code></li>
<li><strong>E2E Tests</strong>: <code>tests/e2e/</code></li>
<li><strong>Benchmarks</strong>: <code>benches/</code></li>
</ul>
<h3 id="102-test-coverage-goals"><a class="header" href="#102-test-coverage-goals">10.2 Test Coverage Goals</a></h3>
<ul>
<li>Domain layer: 90%+ coverage</li>
<li>Application layer: 80%+ coverage</li>
<li>Infrastructure: 70%+ coverage (mocked external deps)</li>
</ul>
<h3 id="103-testing-tools"><a class="header" href="#103-testing-tools">10.3 Testing Tools</a></h3>
<ul>
<li>Mockall for mocking</li>
<li>Proptest for property-based testing</li>
<li>Criterion for benchmarking</li>
<li>Cargo-tarpaulin for coverage</li>
</ul>
<hr />
<h2 id="11-future-enhancements"><a class="header" href="#11-future-enhancements">11. Future Enhancements</a></h2>
<h3 id="111-planned-features"><a class="header" href="#111-planned-features">11.1 Planned Features</a></h3>
<ol>
<li><strong>Distributed Processing</strong>: Process files across multiple machines</li>
<li><strong>Cloud Integration</strong>: S3, Azure Blob, GCS support</li>
<li><strong>REST API</strong>: HTTP API for remote processing</li>
<li><strong>Plugin System</strong>: Dynamic loading of custom stages</li>
<li><strong>Web UI</strong>: Browser-based configuration and monitoring</li>
</ol>
<h3 id="112-architectural-evolution"><a class="header" href="#112-architectural-evolution">11.2 Architectural Evolution</a></h3>
<p><strong>Phase 1 (Current):</strong> Single-machine CLI application</p>
<p><strong>Phase 2:</strong> Library + CLI + REST API</p>
<p><strong>Phase 3:</strong> Distributed processing with coordinator nodes</p>
<p><strong>Phase 4:</strong> Cloud-native deployment with Kubernetes</p>
<hr />
<h2 id="12-conclusion"><a class="header" href="#12-conclusion">12. Conclusion</a></h2>
<p>This Software Design Document describes a robust, extensible file processing pipeline built on solid architectural principles. The design prioritizes:</p>
<ul>
<li><strong>Correctness</strong> through type safety and domain-driven design</li>
<li><strong>Performance</strong> through async I/O and parallel processing</li>
<li><strong>Maintainability</strong> through clean architecture and separation of concerns</li>
<li><strong>Extensibility</strong> through ports &amp; adapters and plugin architecture</li>
</ul>
<p>The implementation follows Rust best practices and leverages the language's strengths in safety, concurrency, and zero-cost abstractions.</p>
<hr />
<h2 id="appendix-a-glossary"><a class="header" href="#appendix-a-glossary">Appendix A: Glossary</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Term</th><th>Definition</th></tr></thead><tbody>
<tr><td>Aggregate</td><td>Domain entity that serves as consistency boundary</td></tr>
<tr><td>AEAD</td><td>Authenticated Encryption with Associated Data</td></tr>
<tr><td>Chunk</td><td>Fixed-size portion of file for streaming processing</td></tr>
<tr><td>DDD</td><td>Domain-Driven Design architectural approach</td></tr>
<tr><td>Port</td><td>Interface defined by application core</td></tr>
<tr><td>Adapter</td><td>Implementation of port for external system</td></tr>
<tr><td>Use Case</td><td>Application-specific business rule</td></tr>
<tr><td>Value Object</td><td>Immutable object defined by its attributes</td></tr>
</tbody></table>
</div>
<hr />
<h2 id="appendix-b-diagrams"><a class="header" href="#appendix-b-diagrams">Appendix B: Diagrams</a></h2>
<p>See the following chapters for detailed diagrams:</p>
<ul>
<li><a href="formal/../architecture/layers.html">Layered Architecture</a></li>
<li><a href="formal/../architecture/domain-model.html">Domain Model</a></li>
<li><a href="formal/../diagrams/pipeline-flow.svg">Pipeline Flow</a></li>
<li><a href="formal/../architecture/overview.html">Hexagonal Architecture</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="software-test-plan-stp"><a class="header" href="#software-test-plan-stp">Software Test Plan (STP)</a></h1>
<p><strong>Version:</strong> 1.0
<strong>Date:</strong> 2025-01-04
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner, Claude Code
<strong>Status:</strong> Active</p>
<hr />
<h2 id="1-introduction-1"><a class="header" href="#1-introduction-1">1. Introduction</a></h2>
<h3 id="11-purpose-1"><a class="header" href="#11-purpose-1">1.1 Purpose</a></h3>
<p>This Software Test Plan (STP) defines the testing strategy, approach, and organization for the Optimized Adaptive Pipeline system. It ensures the system meets all functional and non-functional requirements specified in the SRS through comprehensive, systematic testing.</p>
<p><strong>Intended Audience:</strong></p>
<ul>
<li>QA engineers implementing tests</li>
<li>Developers writing testable code</li>
<li>Project managers tracking test progress</li>
<li>Stakeholders evaluating quality assurance</li>
</ul>
<h3 id="12-scope-1"><a class="header" href="#12-scope-1">1.2 Scope</a></h3>
<p><strong>Testing Coverage:</strong></p>
<ul>
<li>Unit testing of all domain, application, and infrastructure components</li>
<li>Integration testing of component interactions</li>
<li>End-to-end testing of complete workflows</li>
<li>Architecture compliance testing</li>
<li>Performance and benchmark testing</li>
<li>Security testing</li>
</ul>
<p><strong>Out of Scope:</strong></p>
<ul>
<li>User acceptance testing (no end users yet)</li>
<li>Load testing (single-machine application)</li>
<li>Penetration testing (no network exposure)</li>
<li>GUI testing (CLI only)</li>
</ul>
<h3 id="13-test-objectives"><a class="header" href="#13-test-objectives">1.3 Test Objectives</a></h3>
<ol>
<li><strong>Verify Correctness</strong>: Ensure all requirements are met</li>
<li><strong>Validate Design</strong>: Confirm architectural compliance</li>
<li><strong>Ensure Quality</strong>: Maintain high code quality standards</li>
<li><strong>Prevent Regression</strong>: Catch bugs before they reach production</li>
<li><strong>Document Behavior</strong>: Tests serve as living documentation</li>
<li><strong>Enable Refactoring</strong>: Safe code changes through comprehensive tests</li>
</ol>
<h3 id="14-references-1"><a class="header" href="#14-references-1">1.4 References</a></h3>
<ul>
<li><a href="formal/../reference/srs.html">Software Requirements Specification (SRS)</a></li>
<li><a href="formal/design.html">Software Design Document (SDD)</a></li>
<li><a href="formal/../../../../docs/TEST_ORGANIZATION.html">Test Organization Documentation</a></li>
<li>Rust Testing Documentation: https://doc.rust-lang.org/book/ch11-00-testing.html</li>
</ul>
<hr />
<h2 id="2-test-strategy"><a class="header" href="#2-test-strategy">2. Test Strategy</a></h2>
<h3 id="21-testing-approach"><a class="header" href="#21-testing-approach">2.1 Testing Approach</a></h3>
<p><strong>Test-Driven Development (TDD):</strong></p>
<ul>
<li>Write tests before implementation when feasible</li>
<li>Red-Green-Refactor cycle</li>
<li>Tests as specification</li>
</ul>
<p><strong>Behavior-Driven Development (BDD):</strong></p>
<ul>
<li>Given-When-Then structure for integration tests</li>
<li>Readable test names describing behavior</li>
<li>Focus on outcomes, not implementation</li>
</ul>
<p><strong>Property-Based Testing:</strong></p>
<ul>
<li>Use Proptest for algorithmic correctness</li>
<li>Generate random inputs to find edge cases</li>
<li>Verify invariants hold for all inputs</li>
</ul>
<h3 id="22-test-pyramid"><a class="header" href="#22-test-pyramid">2.2 Test Pyramid</a></h3>
<pre><code>        ┌─────────────┐
        │   E2E (11)  │  ← Few, slow, high-level
        ├─────────────┤
        │Integration  │  ← Medium count, medium speed
        │    (35)     │
        ├─────────────┤
        │   Unit      │  ← Many, fast, focused
        │   (314)     │
        └─────────────┘
</code></pre>
<p><strong>Rationale:</strong></p>
<ul>
<li>Most tests are fast unit tests for quick feedback</li>
<li>Integration tests verify component collaboration</li>
<li>E2E tests validate complete user workflows</li>
<li>Architecture tests ensure design compliance</li>
</ul>
<h3 id="23-test-organization-post-reorganization"><a class="header" href="#23-test-organization-post-reorganization">2.3 Test Organization (Post-Reorganization)</a></h3>
<p>Following Rust best practices:</p>
<p><strong>Unit Tests:</strong></p>
<ul>
<li>Location: <code>#[cfg(test)]</code> modules within source files</li>
<li>Scope: Single function/struct in isolation</li>
<li>Run with: <code>cargo test --lib</code></li>
<li>Count: 314 tests (68 bootstrap + 90 pipeline + 156 pipeline-domain)</li>
<li>Example: <code>pipeline-domain/src/entities/pipeline_stage.rs:590-747</code></li>
</ul>
<p><strong>Integration Tests:</strong></p>
<ul>
<li>Location: <code>pipeline/tests/integration/</code></li>
<li>Entry: <code>pipeline/tests/integration.rs</code></li>
<li>Scope: Multiple components working together</li>
<li>Run with: <code>cargo test --test integration</code></li>
<li>Count: 35 tests (3 ignored pending work)</li>
</ul>
<p><strong>End-to-End Tests:</strong></p>
<ul>
<li>Location: <code>pipeline/tests/e2e/</code></li>
<li>Entry: <code>pipeline/tests/e2e.rs</code></li>
<li>Scope: Complete workflows from input to output</li>
<li>Run with: <code>cargo test --test e2e</code></li>
<li>Count: 11 tests</li>
</ul>
<p><strong>Architecture Compliance Tests:</strong></p>
<ul>
<li>Location: <code>pipeline/tests/architecture_compliance_test.rs</code></li>
<li>Scope: Validate DDD, Clean Architecture, Hexagonal Architecture</li>
<li>Run with: <code>cargo test --test architecture_compliance_test</code></li>
<li>Count: 2 tests</li>
</ul>
<p><strong>Documentation Tests:</strong></p>
<ul>
<li>Location: Doc comments with <code>```</code> code blocks</li>
<li>Run with: <code>cargo test --doc</code></li>
<li>Count: 27 tests (12 ignored)</li>
</ul>
<p><strong>Total Test Count: 389 tests</strong> (15 ignored)</p>
<hr />
<h2 id="3-test-levels"><a class="header" href="#3-test-levels">3. Test Levels</a></h2>
<h3 id="31-unit-testing"><a class="header" href="#31-unit-testing">3.1 Unit Testing</a></h3>
<p><strong>Objectives:</strong></p>
<ul>
<li>Test individual functions and methods in isolation</li>
<li>Verify domain logic correctness</li>
<li>Ensure edge cases are handled</li>
<li>Validate error conditions</li>
</ul>
<p><strong>Coverage Goals:</strong></p>
<ul>
<li>Domain layer: 90%+ line coverage</li>
<li>Application layer: 85%+ line coverage</li>
<li>Infrastructure layer: 75%+ line coverage</li>
</ul>
<p><strong>Example Test Structure:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_pipeline_creation_validates_name() {
        // Given: Empty name
        let name = "";
        let stages = vec![create_test_stage()];

        // When: Creating pipeline
        let result = Pipeline::new(name.to_string(), stages);

        // Then: Should fail validation
        assert!(result.is_err());
        assert!(matches!(result, Err(PipelineError::InvalidName(_))));
    }

    #[test]
    fn test_pipeline_requires_at_least_one_stage() {
        // Given: Valid name but no stages
        let name = "test-pipeline";
        let stages = vec![];

        // When: Creating pipeline
        let result = Pipeline::new(name.to_string(), stages);

        // Then: Should fail validation
        assert!(result.is_err());
        assert!(matches!(result, Err(PipelineError::NoStages)));
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Key Test Categories:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>Category</th><th>Description</th><th>Example</th></tr></thead><tbody>
<tr><td>Happy Path</td><td>Normal, expected inputs</td><td>Valid pipeline creation</td></tr>
<tr><td>Edge Cases</td><td>Boundary conditions</td><td>Empty strings, max values</td></tr>
<tr><td>Error Handling</td><td>Invalid inputs</td><td>Malformed data, constraints violated</td></tr>
<tr><td>Invariants</td><td>Domain rules always hold</td><td>Stage order uniqueness</td></tr>
</tbody></table>
</div>
<h3 id="32-integration-testing"><a class="header" href="#32-integration-testing">3.2 Integration Testing</a></h3>
<p><strong>Objectives:</strong></p>
<ul>
<li>Test component interactions</li>
<li>Verify interfaces between layers</li>
<li>Validate repository operations</li>
<li>Test service collaboration</li>
</ul>
<p><strong>Test Structure:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_pipeline_repository_save_and_retrieve() {
    // Given: In-memory repository and pipeline
    let repo = Arc::new(InMemoryPipelineRepository::new());
    let pipeline = create_test_pipeline();

    // When: Saving and retrieving pipeline
    repo.save(&amp;pipeline).await.unwrap();
    let retrieved = repo.find_by_name(&amp;pipeline.name)
        .await
        .unwrap()
        .unwrap();

    // Then: Retrieved pipeline matches original
    assert_eq!(retrieved.name, pipeline.name);
    assert_eq!(retrieved.stages.len(), pipeline.stages.len());
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Integration Test Suites:</strong></p>
<ol>
<li>
<p><strong>Application Layer Integration</strong> (<code>application_integration_test.rs</code>)</p>
<ul>
<li>Command execution</li>
<li>Use case orchestration</li>
<li>Service coordination</li>
</ul>
</li>
<li>
<p><strong>Application Services</strong> (<code>application_services_integration_test.rs</code>)</p>
<ul>
<li>Service interactions</li>
<li>Transaction handling</li>
<li>Error propagation</li>
</ul>
</li>
<li>
<p><strong>Domain Services</strong> (<code>domain_services_test.rs</code>)</p>
<ul>
<li>Compression service integration</li>
<li>Encryption service integration</li>
<li>Checksum service integration</li>
<li>File I/O service integration</li>
</ul>
</li>
<li>
<p><strong>Schema Integration</strong> (<code>schema_integration_test.rs</code>)</p>
<ul>
<li>Database creation</li>
<li>Schema migrations</li>
<li>Idempotent initialization</li>
</ul>
</li>
<li>
<p><strong>Pipeline Name Validation</strong> (<code>pipeline_name_validation_tests.rs</code>)</p>
<ul>
<li>Name normalization</li>
<li>Validation rules</li>
<li>Reserved names</li>
</ul>
</li>
</ol>
<h3 id="33-end-to-end-testing"><a class="header" href="#33-end-to-end-testing">3.3 End-to-End Testing</a></h3>
<p><strong>Objectives:</strong></p>
<ul>
<li>Validate complete user workflows</li>
<li>Test real file processing scenarios</li>
<li>Verify .adapipe format correctness</li>
<li>Ensure restoration matches original</li>
</ul>
<p><strong>Test Structure:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_e2e_complete_pipeline_workflow() {
    // Given: Test input file and pipeline configuration
    let input_path = create_test_file_with_content("Hello, World!");
    let pipeline = create_secure_pipeline(); // compress + encrypt + checksum

    // When: Processing the file
    let processor = PipelineProcessor::new()?;
    let output_path = processor.process(&amp;pipeline, &amp;input_path).await?;

    // Then: Output file should exist and be valid .adapipe format
    assert!(output_path.exists());
    assert!(output_path.extension().unwrap() == "adapipe");

    // And: Can restore original file
    let restored_path = processor.restore(&amp;output_path).await?;
    let restored_content = fs::read_to_string(&amp;restored_path).await?;
    assert_eq!(restored_content, "Hello, World!");

    // Cleanup
    cleanup_test_files(&amp;[input_path, output_path, restored_path])?;
}
<span class="boring">}</span></code></pre></pre>
<p><strong>E2E Test Scenarios:</strong></p>
<ol>
<li>
<p><strong>Binary Format Complete Roundtrip</strong> (<code>e2e_binary_format_test.rs</code>)</p>
<ul>
<li>Process file through all stages</li>
<li>Verify .adapipe format structure</li>
<li>Restore and compare with original</li>
<li>Test large files (memory mapping)</li>
<li>Test corruption detection</li>
<li>Test version compatibility</li>
</ul>
</li>
<li>
<p><strong>Restoration Pipeline</strong> (<code>e2e_restore_pipeline_test.rs</code>)</p>
<ul>
<li>Multi-stage restoration</li>
<li>Stage ordering (reverse of processing)</li>
<li>Real-world document restoration</li>
<li>File header roundtrip</li>
<li>Chunk processing verification</li>
</ul>
</li>
</ol>
<h3 id="34-architecture-compliance-testing"><a class="header" href="#34-architecture-compliance-testing">3.4 Architecture Compliance Testing</a></h3>
<p><strong>Objectives:</strong></p>
<ul>
<li>Enforce architectural boundaries</li>
<li>Validate design patterns</li>
<li>Ensure dependency rules</li>
<li>Verify SOLID principles</li>
</ul>
<p><strong>Test Structure:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_ddd_compliance() {
    println!("Testing DDD Compliance");

    // Test domain entities in isolation
    test_domain_entity_isolation().await;

    // Test value objects for immutability
    test_value_object_patterns().await;

    // Test domain services through interfaces
    test_domain_service_interfaces().await;

    println!("✅ DDD Compliance: PASSED");
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Compliance Checks:</strong></p>
<ol>
<li>
<p><strong>Domain-Driven Design (DDD)</strong></p>
<ul>
<li>Entities tested in isolation</li>
<li>Value objects immutable</li>
<li>Domain services use interfaces</li>
<li>Aggregates maintain consistency</li>
</ul>
</li>
<li>
<p><strong>Clean Architecture</strong></p>
<ul>
<li>Dependency flow is inward only</li>
<li>Use cases independent</li>
<li>Infrastructure tested through abstractions</li>
</ul>
</li>
<li>
<p><strong>Hexagonal Architecture</strong></p>
<ul>
<li>Primary ports (driving adapters) tested</li>
<li>Secondary ports (driven adapters) mocked</li>
<li>Application core isolated</li>
</ul>
</li>
<li>
<p><strong>Dependency Inversion Principle</strong></p>
<ul>
<li>High-level modules depend on abstractions</li>
<li>Low-level modules implement abstractions</li>
<li>Abstractions remain stable</li>
</ul>
</li>
</ol>
<hr />
<h2 id="4-test-automation"><a class="header" href="#4-test-automation">4. Test Automation</a></h2>
<h3 id="41-continuous-integration"><a class="header" href="#41-continuous-integration">4.1 Continuous Integration</a></h3>
<p><strong>CI Pipeline:</strong></p>
<pre><code class="language-yaml"># .github/workflows/ci.yml (example)
name: CI

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
      - name: Run unit tests
        run: cargo test --lib
      - name: Run integration tests
        run: cargo test --test integration
      - name: Run E2E tests
        run: cargo test --test e2e
      - name: Run architecture tests
        run: cargo test --test architecture_compliance_test
      - name: Run doc tests
        run: cargo test --doc

  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run clippy
        run: cargo clippy -- -D warnings
      - name: Check formatting
        run: cargo fmt -- --check

  coverage:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Install tarpaulin
        run: cargo install cargo-tarpaulin
      - name: Generate coverage
        run: cargo tarpaulin --out Xml
      - name: Upload to codecov
        uses: codecov/codecov-action@v3
</code></pre>
<h3 id="42-pre-commit-hooks"><a class="header" href="#42-pre-commit-hooks">4.2 Pre-commit Hooks</a></h3>
<p><strong>Git Hooks:</strong></p>
<pre><code class="language-bash">#!/bin/sh
# .git/hooks/pre-commit

# Run tests
cargo test --lib || exit 1

# Check formatting
cargo fmt -- --check || exit 1

# Run clippy
cargo clippy -- -D warnings || exit 1

echo "✅ All pre-commit checks passed"
</code></pre>
<h3 id="43-test-commands"><a class="header" href="#43-test-commands">4.3 Test Commands</a></h3>
<p><strong>Quick Feedback (fast):</strong></p>
<pre><code class="language-bash">cargo test --lib  # Unit tests only (~1 second)
</code></pre>
<p><strong>Full Test Suite:</strong></p>
<pre><code class="language-bash">cargo test  # All tests (~15 seconds)
</code></pre>
<p><strong>Specific Test Suites:</strong></p>
<pre><code class="language-bash">cargo test --test integration          # Integration tests
cargo test --test e2e                  # E2E tests
cargo test --test architecture_compliance_test  # Architecture tests
cargo test --doc                       # Documentation tests
</code></pre>
<p><strong>With Coverage:</strong></p>
<pre><code class="language-bash">cargo tarpaulin --out Html --output-dir coverage/
</code></pre>
<hr />
<h2 id="5-testing-tools-and-frameworks"><a class="header" href="#5-testing-tools-and-frameworks">5. Testing Tools and Frameworks</a></h2>
<h3 id="51-core-testing-framework"><a class="header" href="#51-core-testing-framework">5.1 Core Testing Framework</a></h3>
<p><strong>Built-in Rust Testing:</strong></p>
<ul>
<li><code>#[test]</code> attribute for unit tests</li>
<li><code>#[tokio::test]</code> for async tests</li>
<li><code>assert!</code>, <code>assert_eq!</code>, <code>assert_ne!</code> macros</li>
<li><code>#[should_panic]</code> for error testing</li>
</ul>
<p><strong>Async Testing:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_async_operation() {
    let result = async_function().await;
    assert!(result.is_ok());
}
<span class="boring">}</span></code></pre></pre>
<h3 id="52-mocking-and-test-doubles"><a class="header" href="#52-mocking-and-test-doubles">5.2 Mocking and Test Doubles</a></h3>
<p><strong>Mockall:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[automock]
#[async_trait]
pub trait PipelineRepository {
    async fn save(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;()&gt;;
    async fn find_by_id(&amp;self, id: &amp;str) -&gt; Result&lt;Option&lt;Pipeline&gt;&gt;;
}

#[tokio::test]
async fn test_with_mock_repository() {
    let mut mock_repo = MockPipelineRepository::new();
    mock_repo
        .expect_save()
        .times(1)
        .returning(|_| Ok(()));

    let service = PipelineService::new(Arc::new(mock_repo));
    let result = service.create_pipeline("test").await;

    assert!(result.is_ok());
}
<span class="boring">}</span></code></pre></pre>
<h3 id="53-property-based-testing"><a class="header" href="#53-property-based-testing">5.3 Property-Based Testing</a></h3>
<p><strong>Proptest:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use proptest::prelude::*;

proptest! {
    #[test]
    fn test_chunk_size_always_valid(size in 1024usize..=100_000_000) {
        // Given: Any size within valid range
        let chunk_size = ChunkSize::new(size);

        // Then: Should always succeed
        prop_assert!(chunk_size.is_ok());
        prop_assert_eq!(chunk_size.unwrap().value(), size);
    }

    #[test]
    fn test_compression_roundtrip(
        data in prop::collection::vec(any::&lt;u8&gt;(), 0..10000)
    ) {
        // Given: Random byte array
        let compressed = compress(&amp;data)?;
        let decompressed = decompress(&amp;compressed)?;

        // Then: Should match original
        prop_assert_eq!(data, decompressed);
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="54-benchmarking"><a class="header" href="#54-benchmarking">5.4 Benchmarking</a></h3>
<p><strong>Criterion:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use criterion::{black_box, criterion_group, criterion_main, Criterion};

fn bench_compression(c: &amp;mut Criterion) {
    let data = vec![0u8; 1_000_000]; // 1 MB

    c.bench_function("brotli_compression", |b| {
        b.iter(|| {
            let adapter = BrotliAdapter::new(6);
            adapter.compress(black_box(&amp;data))
        })
    });

    c.bench_function("zstd_compression", |b| {
        b.iter(|| {
            let adapter = ZstdAdapter::new(3);
            adapter.compress(black_box(&amp;data))
        })
    });
}

criterion_group!(benches, bench_compression);
criterion_main!(benches);
<span class="boring">}</span></code></pre></pre>
<p><strong>Run Benchmarks:</strong></p>
<pre><code class="language-bash">cargo bench
</code></pre>
<h3 id="55-code-coverage"><a class="header" href="#55-code-coverage">5.5 Code Coverage</a></h3>
<p><strong>Cargo-tarpaulin:</strong></p>
<pre><code class="language-bash"># Install
cargo install cargo-tarpaulin

# Generate coverage report
cargo tarpaulin --out Html --output-dir coverage/

# View report
open coverage/index.html
</code></pre>
<p><strong>Coverage Goals:</strong></p>
<ul>
<li>Overall: 80%+ line coverage</li>
<li>Domain layer: 90%+ coverage</li>
<li>Critical paths: 95%+ coverage</li>
</ul>
<hr />
<h2 id="6-test-data-management"><a class="header" href="#6-test-data-management">6. Test Data Management</a></h2>
<h3 id="61-test-fixtures"><a class="header" href="#61-test-fixtures">6.1 Test Fixtures</a></h3>
<p><strong>Fixture Organization:</strong></p>
<pre><code>testdata/
├── input/
│   ├── sample.txt
│   ├── large_file.bin (10 MB)
│   └── document.pdf
├── expected/
│   ├── sample_compressed.bin
│   └── sample_encrypted.bin
└── schemas/
    └── v1_pipeline.json
</code></pre>
<p><strong>Fixture Helpers:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn create_test_file(content: &amp;str) -&gt; PathBuf {
    let temp_dir = tempfile::tempdir().unwrap();
    let file_path = temp_dir.path().join("test_file.txt");
    std::fs::write(&amp;file_path, content).unwrap();
    file_path
}

pub fn create_test_pipeline() -&gt; Pipeline {
    Pipeline::builder()
        .name("test-pipeline")
        .add_stage(compression_stage("compress", "zstd", 1))
        .add_stage(encryption_stage("encrypt", "aes256gcm", 2))
        .build()
        .unwrap()
}
<span class="boring">}</span></code></pre></pre>
<h3 id="62-test-database"><a class="header" href="#62-test-database">6.2 Test Database</a></h3>
<p><strong>In-Memory SQLite:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn create_test_db() -&gt; SqlitePool {
    let pool = SqlitePoolOptions::new()
        .connect(":memory:")
        .await
        .unwrap();

    // Run migrations
    sqlx::migrate!("./migrations")
        .run(&amp;pool)
        .await
        .unwrap();

    pool
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Test Isolation:</strong></p>
<ul>
<li>Each test gets fresh database</li>
<li>Transactions rolled back after test</li>
<li>No test interdependencies</li>
</ul>
<hr />
<h2 id="7-performance-testing"><a class="header" href="#7-performance-testing">7. Performance Testing</a></h2>
<h3 id="71-benchmark-suites"><a class="header" href="#71-benchmark-suites">7.1 Benchmark Suites</a></h3>
<p><strong>Algorithm Benchmarks:</strong></p>
<ul>
<li>Compression algorithms (Brotli, Zstd, Gzip, LZ4)</li>
<li>Encryption algorithms (AES-256-GCM, ChaCha20-Poly1305)</li>
<li>Hashing algorithms (SHA-256, SHA-512, BLAKE3)</li>
</ul>
<p><strong>File Size Benchmarks:</strong></p>
<ul>
<li>Small files (&lt; 1 MB)</li>
<li>Medium files (1-100 MB)</li>
<li>Large files (&gt; 100 MB)</li>
</ul>
<p><strong>Concurrency Benchmarks:</strong></p>
<ul>
<li>Single-threaded vs multi-threaded</li>
<li>Async vs sync I/O</li>
<li>Chunk size variations</li>
</ul>
<h3 id="72-performance-regression-testing"><a class="header" href="#72-performance-regression-testing">7.2 Performance Regression Testing</a></h3>
<p><strong>Baseline Establishment:</strong></p>
<pre><code class="language-bash"># Run benchmarks and save baseline
cargo bench -- --save-baseline main
</code></pre>
<p><strong>Regression Detection:</strong></p>
<pre><code class="language-bash"># Compare against baseline
cargo bench -- --baseline main
</code></pre>
<p><strong>CI Integration:</strong></p>
<ul>
<li>Fail PR if &gt;10% performance degradation</li>
<li>Alert on &gt;5% degradation</li>
<li>Celebrate &gt;10% improvement</li>
</ul>
<hr />
<h2 id="8-security-testing"><a class="header" href="#8-security-testing">8. Security Testing</a></h2>
<h3 id="81-input-validation-testing"><a class="header" href="#81-input-validation-testing">8.1 Input Validation Testing</a></h3>
<p><strong>Test Cases:</strong></p>
<ul>
<li>Path traversal attempts (<code>../../etc/passwd</code>)</li>
<li>Command injection attempts</li>
<li>SQL injection (SQLx prevents, but verify)</li>
<li>Buffer overflow attempts</li>
<li>Invalid UTF-8 sequences</li>
</ul>
<p><strong>Example:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[test]
fn test_rejects_path_traversal() {
    let malicious_path = "../../etc/passwd";
    let result = validate_input_path(malicious_path);
    assert!(result.is_err());
}
<span class="boring">}</span></code></pre></pre>
<h3 id="82-cryptographic-testing"><a class="header" href="#82-cryptographic-testing">8.2 Cryptographic Testing</a></h3>
<p><strong>Test Cases:</strong></p>
<ul>
<li>Key derivation reproducibility</li>
<li>Encryption/decryption roundtrips</li>
<li>Authentication tag verification</li>
<li>Nonce uniqueness</li>
<li>Secure memory wiping</li>
</ul>
<p><strong>Example:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_encryption_with_wrong_key_fails() {
    let data = b"secret data";
    let correct_key = generate_key();
    let wrong_key = generate_key();

    let encrypted = encrypt(data, &amp;correct_key).await?;
    let result = decrypt(&amp;encrypted, &amp;wrong_key).await;

    assert!(result.is_err());
}
<span class="boring">}</span></code></pre></pre>
<h3 id="83-dependency-security"><a class="header" href="#83-dependency-security">8.3 Dependency Security</a></h3>
<p><strong>Cargo-audit:</strong></p>
<pre><code class="language-bash"># Install
cargo install cargo-audit

# Check for vulnerabilities
cargo audit

# CI integration
cargo audit --deny warnings
</code></pre>
<p><strong>Cargo-deny:</strong></p>
<pre><code class="language-bash"># Check licenses and security
cargo deny check
</code></pre>
<hr />
<h2 id="9-test-metrics-and-reporting"><a class="header" href="#9-test-metrics-and-reporting">9. Test Metrics and Reporting</a></h2>
<h3 id="91-test-metrics"><a class="header" href="#91-test-metrics">9.1 Test Metrics</a></h3>
<p><strong>Key Metrics:</strong></p>
<ul>
<li>Test count: 389 tests (15 ignored)</li>
<li>Test pass rate: Target 100%</li>
<li>Code coverage: Target 80%+</li>
<li>Test execution time: &lt; 20 seconds for full suite</li>
<li>Benchmark performance: Track trends</li>
</ul>
<p><strong>Tracking:</strong></p>
<pre><code class="language-bash"># Test count
cargo test -- --list | wc -l

# Coverage
cargo tarpaulin --out Json | jq '.coverage'

# Execution time
time cargo test
</code></pre>
<h3 id="92-test-reporting"><a class="header" href="#92-test-reporting">9.2 Test Reporting</a></h3>
<p><strong>Console Output:</strong></p>
<pre><code>running 389 tests
test unit::test_pipeline_creation ... ok
test integration::test_repository_save ... ok
test e2e::test_complete_workflow ... ok

test result: ok. 374 passed; 0 failed; 15 ignored; 0 measured; 0 filtered out; finished in 15.67s
</code></pre>
<p><strong>Coverage Report:</strong></p>
<pre><code>|| Tested/Total Lines:
|| src/domain/entities/pipeline.rs: 95/100 (95%)
|| src/domain/services/compression.rs: 87/95 (91.6%)
|| Overall: 2847/3421 (83.2%)
</code></pre>
<p><strong>Benchmark Report:</strong></p>
<pre><code>brotli_compression      time:   [45.2 ms 46.1 ms 47.0 ms]
                        change: [-2.3% +0.1% +2.5%] (p = 0.91 &gt; 0.05)
                        No change in performance detected.

zstd_compression        time:   [12.5 ms 12.7 ms 12.9 ms]
                        change: [-8.2% -6.5% -4.8%] (p = 0.00 &lt; 0.05)
                        Performance has improved.
</code></pre>
<hr />
<h2 id="10-test-maintenance"><a class="header" href="#10-test-maintenance">10. Test Maintenance</a></h2>
<h3 id="101-test-review-process"><a class="header" href="#101-test-review-process">10.1 Test Review Process</a></h3>
<p><strong>Code Review Checklist:</strong></p>
<ul>
<li><input disabled="" type="checkbox"/>
Tests cover new functionality</li>
<li><input disabled="" type="checkbox"/>
Tests follow naming conventions</li>
<li><input disabled="" type="checkbox"/>
Tests are independent and isolated</li>
<li><input disabled="" type="checkbox"/>
Test data is appropriate</li>
<li><input disabled="" type="checkbox"/>
Assertions are clear and specific</li>
<li><input disabled="" type="checkbox"/>
Edge cases are tested</li>
<li><input disabled="" type="checkbox"/>
Error conditions are tested</li>
</ul>
<h3 id="102-test-refactoring"><a class="header" href="#102-test-refactoring">10.2 Test Refactoring</a></h3>
<p><strong>When to Refactor Tests:</strong></p>
<ul>
<li>Duplicate test logic (extract helpers)</li>
<li>Brittle tests (too coupled to implementation)</li>
<li>Slow tests (optimize or move to integration)</li>
<li>Unclear test names (rename for clarity)</li>
</ul>
<p><strong>Test Helpers:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Instead of duplicating this in every test:
#[test]
fn test_something() {
    let stage = PipelineStage::new(
        "compress".to_string(),
        StageType::Compression,
        StageConfiguration {
            algorithm: "zstd".to_string(),
            parameters: HashMap::new(),
            parallel_processing: false,
            chunk_size: None,
        },
        1
    ).unwrap();
    // ...
}

// Extract helper:
fn create_compression_stage(name: &amp;str, order: usize) -&gt; PipelineStage {
    PipelineStage::compression(name, "zstd", order).unwrap()
}

#[test]
fn test_something() {
    let stage = create_compression_stage("compress", 1);
    // ...
}
<span class="boring">}</span></code></pre></pre>
<h3 id="103-flaky-test-prevention"><a class="header" href="#103-flaky-test-prevention">10.3 Flaky Test Prevention</a></h3>
<p><strong>Common Causes:</strong></p>
<ul>
<li>Time-dependent tests</li>
<li>Filesystem race conditions</li>
<li>Nondeterministic ordering</li>
<li>Shared state between tests</li>
</ul>
<p><strong>Solutions:</strong></p>
<ul>
<li>Mock time with <code>mockall</code></li>
<li>Use unique temp directories</li>
<li>Sort collections before assertions</li>
<li>Ensure test isolation</li>
</ul>
<hr />
<h2 id="11-test-schedule"><a class="header" href="#11-test-schedule">11. Test Schedule</a></h2>
<h3 id="111-development-workflow"><a class="header" href="#111-development-workflow">11.1 Development Workflow</a></h3>
<p><strong>During Development:</strong></p>
<pre><code class="language-bash"># Quick check (unit tests only)
cargo test --lib

# Before commit
cargo test &amp;&amp; cargo clippy
</code></pre>
<p><strong>Before Push:</strong></p>
<pre><code class="language-bash"># Full test suite
cargo test

# Check formatting
cargo fmt -- --check

# Lint
cargo clippy -- -D warnings
</code></pre>
<p><strong>Before Release:</strong></p>
<pre><code class="language-bash"># All tests
cargo test

# Benchmarks
cargo bench

# Coverage
cargo tarpaulin

# Security audit
cargo audit
</code></pre>
<h3 id="112-cicd-integration"><a class="header" href="#112-cicd-integration">11.2 CI/CD Integration</a></h3>
<p><strong>On Every Commit:</strong></p>
<ul>
<li>Unit tests</li>
<li>Integration tests</li>
<li>Clippy linting</li>
<li>Format checking</li>
</ul>
<p><strong>On Pull Request:</strong></p>
<ul>
<li>Full test suite</li>
<li>Coverage report</li>
<li>Benchmark comparison</li>
<li>Security audit</li>
</ul>
<p><strong>On Release:</strong></p>
<ul>
<li>Full test suite</li>
<li>Performance benchmarks</li>
<li>Security scan</li>
<li>Documentation build</li>
</ul>
<hr />
<h2 id="12-test-deliverables"><a class="header" href="#12-test-deliverables">12. Test Deliverables</a></h2>
<h3 id="121-test-documentation"><a class="header" href="#121-test-documentation">12.1 Test Documentation</a></h3>
<ul>
<li><input disabled="" type="checkbox" checked=""/>
Test Organization Guide (<code>docs/TEST_ORGANIZATION.md</code>)</li>
<li><input disabled="" type="checkbox" checked=""/>
Architecture Compliance Tests (<code>tests/architecture_compliance_test.rs</code>)</li>
<li><input disabled="" type="checkbox" checked=""/>
This Software Test Plan</li>
</ul>
<h3 id="122-test-code"><a class="header" href="#122-test-code">12.2 Test Code</a></h3>
<ul>
<li><input disabled="" type="checkbox" checked=""/>
314 unit tests in source files (68 bootstrap + 90 pipeline + 156 pipeline-domain)</li>
<li><input disabled="" type="checkbox" checked=""/>
35 integration tests in <code>tests/integration/</code> (3 ignored)</li>
<li><input disabled="" type="checkbox" checked=""/>
11 E2E tests in <code>tests/e2e/</code></li>
<li><input disabled="" type="checkbox" checked=""/>
2 architecture compliance tests</li>
<li><input disabled="" type="checkbox" checked=""/>
27 documentation tests (12 ignored)</li>
<li><input disabled="" type="checkbox"/>
Benchmark suite (TODO)</li>
<li><input disabled="" type="checkbox"/>
Property-based tests (TODO)</li>
</ul>
<h3 id="123-test-reports"><a class="header" href="#123-test-reports">12.3 Test Reports</a></h3>
<p><strong>Generated Artifacts:</strong></p>
<ul>
<li>Test execution report (console output)</li>
<li>Coverage report (HTML, XML)</li>
<li>Benchmark report (HTML, JSON)</li>
<li>Security audit report</li>
</ul>
<hr />
<h2 id="13-risks-and-mitigation"><a class="header" href="#13-risks-and-mitigation">13. Risks and Mitigation</a></h2>
<h3 id="131-testing-risks"><a class="header" href="#131-testing-risks">13.1 Testing Risks</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Risk</th><th>Impact</th><th>Probability</th><th>Mitigation</th></tr></thead><tbody>
<tr><td>Flaky tests</td><td>Medium</td><td>Low</td><td>Test isolation, deterministic behavior</td></tr>
<tr><td>Slow tests</td><td>Medium</td><td>Medium</td><td>Optimize, parallelize, tiered testing</td></tr>
<tr><td>Low coverage</td><td>High</td><td>Low</td><td>Coverage tracking, CI enforcement</td></tr>
<tr><td>Missing edge cases</td><td>High</td><td>Medium</td><td>Property-based testing, code review</td></tr>
<tr><td>Test maintenance burden</td><td>Medium</td><td>High</td><td>Helper functions, clear conventions</td></tr>
</tbody></table>
</div>
<h3 id="132-mitigation-strategies"><a class="header" href="#132-mitigation-strategies">13.2 Mitigation Strategies</a></h3>
<p><strong>Flaky Tests:</strong></p>
<ul>
<li>Run tests multiple times in CI</li>
<li>Investigate and fix immediately</li>
<li>Use deterministic test data</li>
</ul>
<p><strong>Slow Tests:</strong></p>
<ul>
<li>Profile test execution</li>
<li>Optimize slow tests</li>
<li>Move to higher test level if appropriate</li>
</ul>
<p><strong>Low Coverage:</strong></p>
<ul>
<li>Track coverage in CI</li>
<li>Require minimum coverage for PRs</li>
<li>Review uncovered code paths</li>
</ul>
<hr />
<h2 id="14-conclusion"><a class="header" href="#14-conclusion">14. Conclusion</a></h2>
<p>This Software Test Plan establishes a comprehensive testing strategy for the Optimized Adaptive Pipeline system. Key highlights:</p>
<ul>
<li><strong>389 tests</strong> across all levels (314 unit, 35 integration, 11 E2E, 2 architecture, 27 doc)</li>
<li><strong>Organized structure</strong> following Rust best practices</li>
<li><strong>Automated CI/CD</strong> integration for continuous quality</li>
<li><strong>High coverage goals</strong> (80%+ overall, 90%+ domain layer)</li>
<li><strong>Multiple testing approaches</strong> (TDD, BDD, property-based)</li>
<li><strong>Performance monitoring</strong> through benchmarks</li>
<li><strong>Security validation</strong> through audits and crypto testing</li>
</ul>
<p>The testing strategy ensures the system meets all requirements, maintains high quality, and remains maintainable as it evolves.</p>
<hr />
<h2 id="appendix-a-test-command-reference"><a class="header" href="#appendix-a-test-command-reference">Appendix A: Test Command Reference</a></h2>
<pre><code class="language-bash"># Run all tests
cargo test

# Run specific test levels
cargo test --lib                    # Unit tests only
cargo test --test integration       # Integration tests
cargo test --test e2e              # E2E tests
cargo test --test architecture_compliance_test  # Architecture tests
cargo test --doc                    # Doc tests

# Run specific test
cargo test test_pipeline_creation

# Run tests matching pattern
cargo test pipeline

# Show test output
cargo test -- --nocapture

# Run tests in parallel (default)
cargo test

# Run tests serially
cargo test -- --test-threads=1

# Generate coverage
cargo tarpaulin --out Html

# Run benchmarks
cargo bench

# Security audit
cargo audit
</code></pre>
<hr />
<h2 id="appendix-b-test-naming-conventions"><a class="header" href="#appendix-b-test-naming-conventions">Appendix B: Test Naming Conventions</a></h2>
<p><strong>Unit Tests:</strong></p>
<ul>
<li><code>test_&lt;function&gt;_&lt;scenario&gt;_&lt;expected_result&gt;</code></li>
<li>Example: <code>test_pipeline_creation_with_empty_name_fails</code></li>
</ul>
<p><strong>Integration Tests:</strong></p>
<ul>
<li><code>test_&lt;component&gt;_&lt;interaction&gt;_&lt;expected_result&gt;</code></li>
<li>Example: <code>test_repository_save_and_retrieve_pipeline</code></li>
</ul>
<p><strong>E2E Tests:</strong></p>
<ul>
<li><code>test_e2e_&lt;workflow&gt;_&lt;scenario&gt;</code></li>
<li>Example: <code>test_e2e_complete_pipeline_roundtrip</code></li>
</ul>
<p><strong>Property Tests:</strong></p>
<ul>
<li><code>test_&lt;property&gt;_holds_for_all_&lt;inputs&gt;</code></li>
<li>Example: <code>test_compression_roundtrip_succeeds_for_all_data</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="public-api-reference"><a class="header" href="#public-api-reference">Public API Reference</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Public API documentation and examples.</p>
<h2 id="api-overview"><a class="header" href="#api-overview">API Overview</a></h2>
<p>TODO: Add API overview</p>
<h2 id="core-types"><a class="header" href="#core-types">Core Types</a></h2>
<p>TODO: List core public types</p>
<h2 id="examples-1"><a class="header" href="#examples-1">Examples</a></h2>
<p>TODO: Add usage examples</p>
<h2 id="generated-documentation"><a class="header" href="#generated-documentation">Generated Documentation</a></h2>
<p>See <a href="api/../../../target/doc/pipeline/index.html">rustdoc API documentation</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="internal-apis"><a class="header" href="#internal-apis">Internal APIs</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Internal API documentation for contributors.</p>
<h2 id="internal-architecture"><a class="header" href="#internal-architecture">Internal Architecture</a></h2>
<p>TODO: Add internal API overview</p>
<h2 id="module-organization"><a class="header" href="#module-organization">Module Organization</a></h2>
<p>TODO: Explain module structure</p>
<h2 id="extension-points-1"><a class="header" href="#extension-points-1">Extension Points</a></h2>
<p>TODO: List internal extension points</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
