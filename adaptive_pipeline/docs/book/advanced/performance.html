<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Performance Optimization - Pipeline Developer Guide</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Pipeline Developer Guide</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="performance-optimization"><a class="header" href="#performance-optimization">Performance Optimization</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> 2025-01-04
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>This chapter explores performance optimization strategies for the adaptive pipeline, including benchmarking methodologies, tuning parameters, and common performance bottlenecks with their solutions.</p>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>The pipeline is designed for high-performance file processing with several optimization strategies:</p>
<ol>
<li><strong>Adaptive Configuration</strong>: Automatically selects optimal settings based on file characteristics</li>
<li><strong>Parallel Processing</strong>: Leverages multi-core systems with Tokio and Rayon</li>
<li><strong>Resource Management</strong>: Prevents oversubscription with CPU/I/O token governance</li>
<li><strong>Memory Efficiency</strong>: Streaming processing with bounded memory usage</li>
<li><strong>I/O Optimization</strong>: Memory mapping, chunked I/O, and device-specific tuning</li>
</ol>
<p><strong>Performance Goals:</strong></p>
<ul>
<li><strong>Throughput</strong>: 100-500 MB/s for compression/encryption pipelines</li>
<li><strong>Latency</strong>: &lt; 100 ms overhead for small files (&lt; 10 MB)</li>
<li><strong>Memory</strong>: Bounded memory usage regardless of file size</li>
<li><strong>Scalability</strong>: Linear scaling up to available CPU cores</li>
</ul>
<h2 id="performance-metrics"><a class="header" href="#performance-metrics">Performance Metrics</a></h2>
<h3 id="throughput"><a class="header" href="#throughput">Throughput</a></h3>
<p><strong>Definition</strong>: Bytes processed per second</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use adaptive_pipeline_domain::entities::ProcessingMetrics;

let metrics = ProcessingMetrics::new();
metrics.start();

// ... process data ...
metrics.add_bytes_processed(file_size);

metrics.end();

println!("Throughput: {:.2} MB/s", metrics.throughput_mb_per_second());
<span class="boring">}</span></code></pre></pre>
<p><strong>Typical Values:</strong></p>
<ul>
<li><strong>Uncompressed I/O</strong>: 500-2000 MB/s (limited by storage device)</li>
<li><strong>LZ4 compression</strong>: 300-600 MB/s (fast, low compression)</li>
<li><strong>Brotli compression</strong>: 50-150 MB/s (slow, high compression)</li>
<li><strong>AES-256-GCM encryption</strong>: 400-800 MB/s (hardware-accelerated)</li>
<li><strong>ChaCha20-Poly1305</strong>: 200-400 MB/s (software)</li>
</ul>
<h3 id="latency"><a class="header" href="#latency">Latency</a></h3>
<p><strong>Definition</strong>: Time from start to completion</p>
<p><strong>Components:</strong></p>
<ul>
<li><strong>Setup overhead</strong>: File opening, thread pool initialization (1-5 ms)</li>
<li><strong>I/O time</strong>: Reading/writing chunks (varies by device and size)</li>
<li><strong>Processing time</strong>: Compression, encryption, hashing (varies by algorithm)</li>
<li><strong>Coordination overhead</strong>: Task spawning, semaphore acquisition (&lt; 1 ms)</li>
</ul>
<p><strong>Optimization Strategies:</strong></p>
<ul>
<li>Minimize setup overhead by reusing resources</li>
<li>Use memory mapping for large files to reduce I/O time</li>
<li>Choose faster algorithms (LZ4 vs Brotli, ChaCha20 vs AES)</li>
<li>Batch small operations to amortize coordination overhead</li>
</ul>
<h3 id="memory-usage"><a class="header" href="#memory-usage">Memory Usage</a></h3>
<p><strong>Formula:</strong></p>
<pre><code class="language-text">Peak Memory ≈ chunk_size × active_workers × files_concurrent
</code></pre>
<p><strong>Example:</strong></p>
<pre><code class="language-text">chunk_size = 64 MB
active_workers = 7
files_concurrent = 1
Peak Memory ≈ 64 MB × 7 × 1 = 448 MB
</code></pre>
<p><strong>Monitoring:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use adaptive_pipeline::infrastructure::metrics::CONCURRENCY_METRICS;

let mem_mb = CONCURRENCY_METRICS.memory_used_mb();
let mem_pct = CONCURRENCY_METRICS.memory_utilization_percent();

println!("Memory: {:.2} MB ({:.1}%)", mem_mb, mem_pct);
<span class="boring">}</span></code></pre></pre>
<h2 id="optimization-strategies"><a class="header" href="#optimization-strategies">Optimization Strategies</a></h2>
<h3 id="1-chunk-size-optimization"><a class="header" href="#1-chunk-size-optimization">1. Chunk Size Optimization</a></h3>
<p><strong>Impact</strong>: Chunk size affects memory usage, I/O efficiency, and parallelism.</p>
<p><strong>Adaptive Chunk Sizing:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use adaptive_pipeline_domain::value_objects::ChunkSize;

// Automatically selects optimal chunk size based on file size
let chunk_size = ChunkSize::optimal_for_file_size(file_size);

println!("Optimal chunk size: {}", chunk_size);  // e.g., "4.0MB"
<span class="boring">}</span></code></pre></pre>
<p><strong>Guidelines:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>File Size</th><th>Chunk Size</th><th>Rationale</th></tr></thead><tbody>
<tr><td>&lt; 10 MB (small)</td><td>64-256 KB</td><td>Minimize memory, enable fine-grained parallelism</td></tr>
<tr><td>10-100 MB (medium)</td><td>256 KB-1 MB</td><td>Balance memory and I/O efficiency</td></tr>
<tr><td>100 MB-1 GB (large)</td><td>1-4 MB</td><td>Reduce I/O overhead, acceptable memory usage</td></tr>
<tr><td>&gt; 1 GB (huge)</td><td>4-16 MB</td><td>Maximize I/O throughput, still bounded memory</td></tr>
</tbody></table>
</div>
<p><strong>Trade-offs:</strong></p>
<ul>
<li><strong>Small chunks</strong>: ✅ Lower memory, better parallelism ❌ Higher I/O overhead</li>
<li><strong>Large chunks</strong>: ✅ Lower I/O overhead ❌ Higher memory, less parallelism</li>
</ul>
<h3 id="2-worker-count-optimization"><a class="header" href="#2-worker-count-optimization">2. Worker Count Optimization</a></h3>
<p><strong>Impact</strong>: Worker count affects CPU utilization and resource contention.</p>
<p><strong>Adaptive Worker Count:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use adaptive_pipeline_domain::value_objects::WorkerCount;

// File size + system resources + processing type
let workers = WorkerCount::optimal_for_processing_type(
    file_size,
    available_cores,
    is_cpu_intensive,  // true for compression/encryption
);

println!("Optimal workers: {}", workers);  // e.g., "8 workers"
<span class="boring">}</span></code></pre></pre>
<p><strong>Empirically Validated Strategies:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>File Size</th><th>Worker Count</th><th>Strategy</th><th>Benchmark Result</th></tr></thead><tbody>
<tr><td>5 MB (small)</td><td>9</td><td>Aggressive parallelism</td><td>+102% speedup</td></tr>
<tr><td>50 MB (medium)</td><td>5</td><td>Balanced approach</td><td>+70% speedup</td></tr>
<tr><td>2 GB (huge)</td><td>3</td><td>Conservative (avoid overhead)</td><td>+76% speedup</td></tr>
</tbody></table>
</div>
<p><strong>Why these strategies work:</strong></p>
<ul>
<li><strong>Small files</strong>: Task overhead is amortized quickly with many workers</li>
<li><strong>Medium files</strong>: Balanced to avoid both under-utilization and over-subscription</li>
<li><strong>Huge files</strong>: Fewer workers prevent memory pressure and coordination overhead</li>
</ul>
<h3 id="3-memory-mapping-vs-regular-io"><a class="header" href="#3-memory-mapping-vs-regular-io">3. Memory Mapping vs Regular I/O</a></h3>
<p><strong>When to use memory mapping:</strong></p>
<ul>
<li>✅ Files &gt; 100 MB (amortizes setup cost)</li>
<li>✅ Random access patterns (page cache efficiency)</li>
<li>✅ Read-heavy workloads (no write overhead)</li>
</ul>
<p><strong>When to use regular I/O:</strong></p>
<ul>
<li>✅ Files &lt; 10 MB (lower setup cost)</li>
<li>✅ Sequential access patterns (streaming)</li>
<li>✅ Write-heavy workloads (buffered writes)</li>
</ul>
<p><strong>Configuration:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use adaptive_pipeline::infrastructure::adapters::file_io::TokioFileIO;
use adaptive_pipeline_domain::services::file_io_service::FileIOConfig;

let config = FileIOConfig {
    enable_memory_mapping: true,
    max_mmap_size: 1024 * 1024 * 1024,  // 1 GB threshold
    default_chunk_size: 64 * 1024,       // 64 KB chunks
    ..Default::default()
};

let service = TokioFileIO::new(config);
<span class="boring">}</span></code></pre></pre>
<p><strong>Benchmark Results</strong> (from <code>pipeline/benches/file_io_benchmark.rs</code>):</p>
<div class="table-wrapper"><table><thead><tr><th>File Size</th><th>Regular I/O</th><th>Memory Mapping</th><th>Winner</th></tr></thead><tbody>
<tr><td>1 MB</td><td>2000 MB/s</td><td>1500 MB/s</td><td>Regular I/O</td></tr>
<tr><td>10 MB</td><td>1800 MB/s</td><td>1900 MB/s</td><td>Comparable</td></tr>
<tr><td>50 MB</td><td>1500 MB/s</td><td>2200 MB/s</td><td>Memory Mapping</td></tr>
<tr><td>100 MB</td><td>1400 MB/s</td><td>2500 MB/s</td><td>Memory Mapping</td></tr>
</tbody></table>
</div>
<h3 id="4-compression-algorithm-selection"><a class="header" href="#4-compression-algorithm-selection">4. Compression Algorithm Selection</a></h3>
<p><strong>Performance vs Compression Ratio:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>Algorithm</th><th>Compression Speed</th><th>Decompression Speed</th><th>Ratio</th><th>Use Case</th></tr></thead><tbody>
<tr><td><strong>LZ4</strong></td><td>500-700 MB/s</td><td>2000-3000 MB/s</td><td>2-3x</td><td>Real-time, low latency</td></tr>
<tr><td><strong>Zstd</strong></td><td>200-400 MB/s</td><td>600-800 MB/s</td><td>3-5x</td><td>Balanced, general use</td></tr>
<tr><td><strong>Brotli</strong></td><td>50-150 MB/s</td><td>300-500 MB/s</td><td>4-8x</td><td>Storage, high compression</td></tr>
</tbody></table>
</div>
<p><strong>Adaptive Selection:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use adaptive_pipeline_domain::services::CompressionPriority;

// Automatic algorithm selection
let config = service.get_optimal_config(
    "data.bin",
    &amp;sample_data,
    CompressionPriority::Speed,  // or CompressionPriority::Ratio
)?;

println!("Selected: {:?}", config.algorithm);
<span class="boring">}</span></code></pre></pre>
<p><strong>Guidelines:</strong></p>
<ul>
<li><strong>Speed priority</strong>: LZ4 for streaming, real-time processing</li>
<li><strong>Balanced</strong>: Zstandard for general-purpose use</li>
<li><strong>Ratio priority</strong>: Brotli for archival, storage optimization</li>
</ul>
<h3 id="5-encryption-algorithm-selection"><a class="header" href="#5-encryption-algorithm-selection">5. Encryption Algorithm Selection</a></h3>
<p><strong>Performance Characteristics:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>Algorithm</th><th>Throughput</th><th>Security</th><th>Hardware Support</th></tr></thead><tbody>
<tr><td><strong>AES-256-GCM</strong></td><td>400-800 MB/s</td><td>Excellent</td><td>Yes (AES-NI)</td></tr>
<tr><td><strong>ChaCha20-Poly1305</strong></td><td>200-400 MB/s</td><td>Excellent</td><td>No</td></tr>
<tr><td><strong>XChaCha20-Poly1305</strong></td><td>180-350 MB/s</td><td>Excellent</td><td>No</td></tr>
</tbody></table>
</div>
<p><strong>Configuration:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use adaptive_pipeline_domain::services::EncryptionAlgorithm;

// Use AES-256-GCM if hardware support available
let algorithm = if has_aes_ni() {
    EncryptionAlgorithm::Aes256Gcm  // 2-4x faster with AES-NI
} else {
    EncryptionAlgorithm::ChaCha20Poly1305  // Software fallback
};
<span class="boring">}</span></code></pre></pre>
<h2 id="common-bottlenecks"><a class="header" href="#common-bottlenecks">Common Bottlenecks</a></h2>
<h3 id="1-cpu-bottleneck"><a class="header" href="#1-cpu-bottleneck">1. CPU Bottleneck</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>CPU saturation &gt; 80%</li>
<li>High CPU wait times (P95 &gt; 50 ms)</li>
<li>Low I/O utilization</li>
</ul>
<p><strong>Causes:</strong></p>
<ul>
<li>Too many CPU-intensive operations (compression, encryption)</li>
<li>Insufficient worker count for CPU-bound work</li>
<li>Slow algorithms (Brotli on large files)</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Increase CPU tokens to match cores
let config = ResourceConfig {
    cpu_tokens: Some(available_cores),  // Use all cores
    ..Default::default()
};

// Use faster algorithms
let compression = CompressionAlgorithm::Lz4;  // Instead of Brotli
let encryption = EncryptionAlgorithm::Aes256Gcm;  // With AES-NI

// Optimize worker count
let workers = WorkerCount::optimal_for_processing_type(
    file_size,
    available_cores,
    true,  // CPU-intensive = true
);
<span class="boring">}</span></code></pre></pre>
<h3 id="2-io-bottleneck"><a class="header" href="#2-io-bottleneck">2. I/O Bottleneck</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>I/O saturation &gt; 80%</li>
<li>High I/O wait times (P95 &gt; 100 ms)</li>
<li>Low CPU utilization</li>
</ul>
<p><strong>Causes:</strong></p>
<ul>
<li>Too many concurrent I/O operations</li>
<li>Small chunk sizes causing excessive syscalls</li>
<li>Storage device queue depth exceeded</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Increase chunk size to reduce I/O overhead
let chunk_size = ChunkSize::from_mb(4)?;  // 4 MB chunks

// Reduce I/O concurrency for HDD
let config = ResourceConfig {
    storage_type: StorageType::HDD,  // 4 I/O tokens
    ..Default::default()
};

// Use memory mapping for large files
let use_mmap = file_size &gt; 100 * 1024 * 1024;  // &gt; 100 MB
<span class="boring">}</span></code></pre></pre>
<p><strong>I/O Optimization by Device:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>Device Type</th><th>Optimal Chunk Size</th><th>I/O Tokens</th><th>Strategy</th></tr></thead><tbody>
<tr><td><strong>HDD</strong></td><td>1-4 MB</td><td>4</td><td>Sequential, large chunks</td></tr>
<tr><td><strong>SSD</strong></td><td>256 KB-1 MB</td><td>12</td><td>Balanced</td></tr>
<tr><td><strong>NVMe</strong></td><td>64 KB-256 KB</td><td>24</td><td>Parallel, small chunks</td></tr>
</tbody></table>
</div>
<h3 id="3-memory-bottleneck"><a class="header" href="#3-memory-bottleneck">3. Memory Bottleneck</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>Memory utilization &gt; 80%</li>
<li>Swapping (check <code>vmstat</code>)</li>
<li>OOM errors</li>
</ul>
<p><strong>Causes:</strong></p>
<ul>
<li>Too many concurrent chunks allocated</li>
<li>Large chunk size × high worker count</li>
<li>Memory leaks or unbounded buffers</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Reduce chunk size
let chunk_size = ChunkSize::from_mb(16)?;  // Smaller chunks

// Limit concurrent workers
let config = ResourceConfig {
    cpu_tokens: Some(3),  // Fewer workers = less memory
    ..Default::default()
};

// Monitor memory closely
if CONCURRENCY_METRICS.memory_utilization_percent() &gt; 80.0 {
    warn!("High memory usage, reducing chunk size");
    chunk_size = ChunkSize::from_mb(8)?;
}
<span class="boring">}</span></code></pre></pre>
<h3 id="4-coordination-overhead"><a class="header" href="#4-coordination-overhead">4. Coordination Overhead</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>High task spawn latency</li>
<li>Context switching &gt; 10k/sec</li>
<li>Low overall throughput despite low resource usage</li>
</ul>
<p><strong>Causes:</strong></p>
<ul>
<li>Too many small tasks (excessive spawn_blocking calls)</li>
<li>High semaphore contention</li>
<li>Channel backpressure</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Batch small operations
if chunks.len() &lt; 10 {
    // Sequential for small batches (avoid spawn overhead)
    for chunk in chunks {
        process_chunk_sync(chunk)?;
    }
} else {
    // Parallel for large batches
    tokio::task::spawn_blocking(move || {
        RAYON_POOLS.cpu_bound_pool().install(|| {
            chunks.into_par_iter().map(process_chunk_sync).collect()
        })
    }).await??
}

// Reduce worker count to lower contention
let workers = WorkerCount::new(available_cores / 2);
<span class="boring">}</span></code></pre></pre>
<h2 id="tuning-parameters"><a class="header" href="#tuning-parameters">Tuning Parameters</a></h2>
<h3 id="chunk-size-tuning"><a class="header" href="#chunk-size-tuning">Chunk Size Tuning</a></h3>
<p><strong>Parameters:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct ChunkSize {
    pub const MIN_SIZE: usize = 1;              // 1 byte
    pub const MAX_SIZE: usize = 512 * 1024 * 1024;  // 512 MB
    pub const DEFAULT_SIZE: usize = 1024 * 1024;    // 1 MB
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Configuration:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Via ChunkSize value object
let chunk_size = ChunkSize::from_mb(4)?;

// Via CLI/config file
let chunk_size_mb = 4;
let chunk_size = ChunkSize::from_mb(chunk_size_mb)?;

// Adaptive (recommended)
let chunk_size = ChunkSize::optimal_for_file_size(file_size);
<span class="boring">}</span></code></pre></pre>
<p><strong>Impact:</strong></p>
<ul>
<li><strong>Memory</strong>: Directly proportional (2x chunk = 2x memory per worker)</li>
<li><strong>I/O overhead</strong>: Inversely proportional (2x chunk = 0.5x syscalls)</li>
<li><strong>Parallelism</strong>: Inversely proportional (2x chunk = 0.5x parallel units)</li>
</ul>
<h3 id="worker-count-tuning"><a class="header" href="#worker-count-tuning">Worker Count Tuning</a></h3>
<p><strong>Parameters:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct WorkerCount {
    pub const MIN_WORKERS: usize = 1;
    pub const MAX_WORKERS: usize = 32;
    pub const DEFAULT_WORKERS: usize = 4;
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Configuration:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Manual
let workers = WorkerCount::new(8);

// Adaptive (recommended)
let workers = WorkerCount::optimal_for_file_size(file_size);

// With system resources
let workers = WorkerCount::optimal_for_file_and_system(
    file_size,
    available_cores,
);

// With processing type
let workers = WorkerCount::optimal_for_processing_type(
    file_size,
    available_cores,
    is_cpu_intensive,
);
<span class="boring">}</span></code></pre></pre>
<p><strong>Impact:</strong></p>
<ul>
<li><strong>Throughput</strong>: Generally increases with workers (up to cores)</li>
<li><strong>Memory</strong>: Directly proportional (2x workers = 2x memory)</li>
<li><strong>Context switching</strong>: Increases with workers (diminishing returns &gt; 2x cores)</li>
</ul>
<h3 id="resource-token-tuning"><a class="header" href="#resource-token-tuning">Resource Token Tuning</a></h3>
<p><strong>CPU Tokens:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let config = ResourceConfig {
    cpu_tokens: Some(7),  // cores - 1 (default)
    ..Default::default()
};
<span class="boring">}</span></code></pre></pre>
<p><strong>I/O Tokens:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let config = ResourceConfig {
    io_tokens: Some(24),          // Device-specific
    storage_type: StorageType::NVMe,
    ..Default::default()
};
<span class="boring">}</span></code></pre></pre>
<p><strong>Impact:</strong></p>
<ul>
<li><strong>CPU tokens</strong>: Limits total CPU-bound parallelism across all files</li>
<li><strong>I/O tokens</strong>: Limits total I/O concurrency across all files</li>
<li><strong>Both</strong>: Prevent system oversubscription</li>
</ul>
<h2 id="performance-monitoring"><a class="header" href="#performance-monitoring">Performance Monitoring</a></h2>
<h3 id="real-time-metrics"><a class="header" href="#real-time-metrics">Real-Time Metrics</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use adaptive_pipeline::infrastructure::metrics::CONCURRENCY_METRICS;
use std::time::Duration;

// Spawn monitoring task
tokio::spawn(async {
    let mut interval = tokio::time::interval(Duration::from_secs(5));
    loop {
        interval.tick().await;

        // Resource saturation
        let cpu_sat = CONCURRENCY_METRICS.cpu_saturation_percent();
        let io_sat = CONCURRENCY_METRICS.io_saturation_percent();
        let mem_util = CONCURRENCY_METRICS.memory_utilization_percent();

        // Wait time percentiles
        let cpu_p95 = CONCURRENCY_METRICS.cpu_wait_p95();
        let io_p95 = CONCURRENCY_METRICS.io_wait_p95();

        info!(
            "Resources: CPU={:.1}%, I/O={:.1}%, Mem={:.1}% | Wait: CPU={}ms, I/O={}ms",
            cpu_sat, io_sat, mem_util, cpu_p95, io_p95
        );

        // Alert on issues
        if cpu_sat &gt; 90.0 {
            warn!("CPU saturated - consider increasing workers or faster algorithms");
        }
        if mem_util &gt; 80.0 {
            warn!("High memory - consider reducing chunk size or workers");
        }
    }
});
<span class="boring">}</span></code></pre></pre>
<h3 id="processing-metrics"><a class="header" href="#processing-metrics">Processing Metrics</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use adaptive_pipeline_domain::entities::ProcessingMetrics;

let metrics = ProcessingMetrics::new();
metrics.start();

// Process file...
for chunk in chunks {
    metrics.add_bytes_processed(chunk.data.len() as u64);
}

metrics.end();

// Report performance
println!("Throughput: {:.2} MB/s", metrics.throughput_mb_per_second());
println!("Duration: {:.2}s", metrics.duration().as_secs_f64());
println!("Processed: {} MB", metrics.bytes_processed() / (1024 * 1024));

// Stage-specific metrics
for stage_metrics in metrics.stage_metrics() {
    println!("  {}: {:.2} MB/s", stage_metrics.stage_name, stage_metrics.throughput);
}
<span class="boring">}</span></code></pre></pre>
<h2 id="performance-best-practices"><a class="header" href="#performance-best-practices">Performance Best Practices</a></h2>
<h3 id="1-use-adaptive-configuration"><a class="header" href="#1-use-adaptive-configuration">1. Use Adaptive Configuration</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Let the system optimize
let chunk_size = ChunkSize::optimal_for_file_size(file_size);
let workers = WorkerCount::optimal_for_processing_type(
    file_size,
    available_cores,
    is_cpu_intensive,
);

// ❌ Bad: Fixed values
let chunk_size = ChunkSize::from_mb(1)?;
let workers = WorkerCount::new(8);
<span class="boring">}</span></code></pre></pre>
<h3 id="2-choose-appropriate-algorithms"><a class="header" href="#2-choose-appropriate-algorithms">2. Choose Appropriate Algorithms</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Algorithm selection based on priority
let compression_config = service.get_optimal_config(
    file_extension,
    &amp;sample_data,
    CompressionPriority::Speed,  // or Ratio
)?;

// ❌ Bad: Always use same algorithm
let compression_config = CompressionConfig {
    algorithm: CompressionAlgorithm::Brotli,  // Slow!
    ..Default::default()
};
<span class="boring">}</span></code></pre></pre>
<h3 id="3-monitor-and-measure"><a class="header" href="#3-monitor-and-measure">3. Monitor and Measure</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Measure actual performance
let start = Instant::now();
let result = process_file(path).await?;
let duration = start.elapsed();

let throughput_mb_s = (file_size as f64 / duration.as_secs_f64()) / (1024.0 * 1024.0);
info!("Throughput: {:.2} MB/s", throughput_mb_s);

// ❌ Bad: Assume performance without measurement
let result = process_file(path).await?;
<span class="boring">}</span></code></pre></pre>
<h3 id="4-batch-small-operations"><a class="header" href="#4-batch-small-operations">4. Batch Small Operations</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Batch to amortize overhead
tokio::task::spawn_blocking(move || {
    RAYON_POOLS.cpu_bound_pool().install(|| {
        chunks.into_par_iter()
            .map(|chunk| process_chunk(chunk))
            .collect::&lt;Result&lt;Vec&lt;_&gt;, _&gt;&gt;()
    })
}).await??

// ❌ Bad: Spawn for each small operation
for chunk in chunks {
    tokio::task::spawn_blocking(move || {
        process_chunk(chunk)  // Excessive spawn overhead!
    }).await??
}
<span class="boring">}</span></code></pre></pre>
<h3 id="5-use-device-specific-settings"><a class="header" href="#5-use-device-specific-settings">5. Use Device-Specific Settings</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Configure for storage type
let config = ResourceConfig {
    storage_type: StorageType::NVMe,  // 24 I/O tokens
    io_tokens: Some(24),
    ..Default::default()
};

// ❌ Bad: One size fits all
let config = ResourceConfig {
    io_tokens: Some(12),  // May be suboptimal
    ..Default::default()
};
<span class="boring">}</span></code></pre></pre>
<h2 id="troubleshooting-performance-issues"><a class="header" href="#troubleshooting-performance-issues">Troubleshooting Performance Issues</a></h2>
<h3 id="issue-1-low-throughput-despite-low-resource-usage"><a class="header" href="#issue-1-low-throughput-despite-low-resource-usage">Issue 1: Low Throughput Despite Low Resource Usage</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>Throughput &lt; 100 MB/s</li>
<li>CPU usage &lt; 50%</li>
<li>I/O usage &lt; 50%</li>
</ul>
<p><strong>Diagnosis:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Check coordination overhead
let queue_depth = CONCURRENCY_METRICS.cpu_queue_depth();
let active_workers = CONCURRENCY_METRICS.active_workers();

println!("Queue: {}, Active: {}", queue_depth, active_workers);
<span class="boring">}</span></code></pre></pre>
<p><strong>Causes:</strong></p>
<ul>
<li>Too few workers (underutilization)</li>
<li>Small batch sizes (high spawn overhead)</li>
<li>Synchronous bottlenecks</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Increase workers
let workers = WorkerCount::new(available_cores);

// Batch operations
let batch_size = 100;
for batch in chunks.chunks(batch_size) {
    process_batch(batch).await?;
}
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-2-inconsistent-performance"><a class="header" href="#issue-2-inconsistent-performance">Issue 2: Inconsistent Performance</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>Performance varies widely between runs</li>
<li>High P99 latencies (&gt; 10x P50)</li>
</ul>
<p><strong>Diagnosis:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Check wait time distribution
let p50 = CONCURRENCY_METRICS.cpu_wait_p50();
let p95 = CONCURRENCY_METRICS.cpu_wait_p95();
let p99 = CONCURRENCY_METRICS.cpu_wait_p99();

println!("Wait times: P50={}ms, P95={}ms, P99={}ms", p50, p95, p99);
<span class="boring">}</span></code></pre></pre>
<p><strong>Causes:</strong></p>
<ul>
<li>Resource contention (high wait times)</li>
<li>GC pauses or memory pressure</li>
<li>External system interference</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Reduce contention
let config = ResourceConfig {
    cpu_tokens: Some(available_cores - 2),  // Leave headroom
    ..Default::default()
};

// Monitor memory
if mem_util &gt; 70.0 {
    chunk_size = ChunkSize::from_mb(chunk_size_mb / 2)?;
}
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-3-memory-growth"><a class="header" href="#issue-3-memory-growth">Issue 3: Memory Growth</a></h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>Memory usage grows over time</li>
<li>Eventually triggers OOM or swapping</li>
</ul>
<p><strong>Diagnosis:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Track memory trends
let mem_start = CONCURRENCY_METRICS.memory_used_mb();
// ... process files ...
let mem_end = CONCURRENCY_METRICS.memory_used_mb();

if mem_end &gt; mem_start * 1.5 {
    warn!("Memory grew {:.1}%", ((mem_end - mem_start) / mem_start) * 100.0);
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Causes:</strong></p>
<ul>
<li>Memory leaks (improper cleanup)</li>
<li>Unbounded queues or buffers</li>
<li>Large chunk size with many workers</li>
</ul>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Use RAII guards for cleanup
struct ChunkBuffer {
    data: Vec&lt;u8&gt;,
    _guard: MemoryGuard,
}

// Limit queue depth
let (tx, rx) = tokio::sync::mpsc::channel(100);  // Bounded channel

// Reduce chunk size
let chunk_size = ChunkSize::from_mb(16)?;  // Smaller
<span class="boring">}</span></code></pre></pre>
<h2 id="related-topics"><a class="header" href="#related-topics">Related Topics</a></h2>
<ul>
<li>See <a href="benchmarking.html">Benchmarking</a> for detailed benchmark methodology</li>
<li>See <a href="profiling.html">Profiling</a> for CPU and memory profiling techniques</li>
<li>See <a href="thread-pooling.html">Thread Pooling</a> for worker configuration</li>
<li>See <a href="resources.html">Resource Management</a> for token governance</li>
</ul>
<h2 id="summary"><a class="header" href="#summary">Summary</a></h2>
<p>The pipeline's performance optimization system provides:</p>
<ol>
<li><strong>Adaptive Configuration</strong>: Automatic chunk size and worker count optimization</li>
<li><strong>Algorithm Selection</strong>: Choose algorithms based on speed/ratio priority</li>
<li><strong>Resource Governance</strong>: Prevent oversubscription with token limits</li>
<li><strong>Memory Efficiency</strong>: Bounded memory usage with streaming processing</li>
<li><strong>Comprehensive Monitoring</strong>: Real-time metrics and performance tracking</li>
</ol>
<p><strong>Key Takeaways:</strong></p>
<ul>
<li>Use adaptive configuration (ChunkSize::optimal_for_file_size, WorkerCount::optimal_for_processing_type)</li>
<li>Choose algorithms based on workload (LZ4 for speed, Brotli for ratio)</li>
<li>Monitor metrics regularly (CPU/I/O saturation, wait times, throughput)</li>
<li>Tune based on bottleneck (CPU: increase workers/faster algorithms, I/O: increase chunk size, Memory: reduce chunk/workers)</li>
<li>Benchmark and measure actual performance (don't assume)</li>
</ul>
<p><strong>Performance Goals Achieved:</strong></p>
<ul>
<li>✅ Throughput: 100-500 MB/s (algorithm-dependent)</li>
<li>✅ Latency: &lt; 100 ms overhead for small files</li>
<li>✅ Memory: Bounded usage (chunk_size × workers × files)</li>
<li>✅ Scalability: Linear scaling up to available cores</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../advanced/resources.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../advanced/benchmarking.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../advanced/resources.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../advanced/benchmarking.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
