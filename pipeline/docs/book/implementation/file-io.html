<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>File I/O - Pipeline Developer Guide</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Pipeline Developer Guide</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="file-io"><a class="header" href="#file-io">File I/O</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>This chapter provides a comprehensive overview of the file input/output architecture in the adaptive pipeline system. Learn how file chunks, type-safe paths, and streaming I/O work together to enable efficient, memory-safe file processing.</p>
<hr />
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#file-io-architecture">File I/O Architecture</a></li>
<li><a href="#filechunk-value-object">FileChunk Value Object</a></li>
<li><a href="#filepath-value-object">FilePath Value Object</a></li>
<li><a href="#chunksize-value-object">ChunkSize Value Object</a></li>
<li><a href="#fileioservice-interface">FileIOService Interface</a></li>
<li><a href="#file-reading">File Reading</a></li>
<li><a href="#file-writing">File Writing</a></li>
<li><a href="#memory-management">Memory Management</a></li>
<li><a href="#error-handling">Error Handling</a></li>
<li><a href="#performance-optimization">Performance Optimization</a></li>
<li><a href="#usage-examples">Usage Examples</a></li>
<li><a href="#best-practices">Best Practices</a></li>
<li><a href="#troubleshooting">Troubleshooting</a></li>
<li><a href="#testing-strategies">Testing Strategies</a></li>
<li><a href="#next-steps">Next Steps</a></li>
</ul>
<hr />
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p><strong>File I/O</strong> in the adaptive pipeline system is designed for efficient, memory-safe processing of files of any size through chunking, streaming, and intelligent memory management. The system uses immutable value objects and async I/O for optimal performance.</p>
<h3 id="key-features"><a class="header" href="#key-features">Key Features</a></h3>
<ul>
<li><strong>Chunked Processing</strong>: Files split into manageable chunks for parallel processing</li>
<li><strong>Streaming I/O</strong>: Process files without loading entirely into memory</li>
<li><strong>Type-Safe Paths</strong>: Compile-time path category enforcement</li>
<li><strong>Immutable Chunks</strong>: Thread-safe, corruption-proof file chunks</li>
<li><strong>Validated Sizes</strong>: Chunk sizes validated at creation</li>
<li><strong>Async Operations</strong>: Non-blocking I/O for better concurrency</li>
</ul>
<h3 id="file-io-stack"><a class="header" href="#file-io-stack">File I/O Stack</a></h3>
<pre><code class="language-text">┌──────────────────────────────────────────────────────────┐
│                  Application Layer                        │
│  ┌────────────────────────────────────────────────┐      │
│  │   File Processor Service                       │      │
│  │   - Orchestrates chunking and processing       │      │
│  └────────────────────────────────────────────────┘      │
└──────────────────────────────────────────────────────────┘
                         ↓ uses
┌──────────────────────────────────────────────────────────┐
│                    Domain Layer                           │
│  ┌────────────────────────────────────────────────┐      │
│  │   FileIOService (Trait)                        │      │
│  │   - read_file_chunks(), write_file_chunks()    │      │
│  └────────────────────────────────────────────────┘      │
│  ┌────────────┬───────────┬──────────────┐              │
│  │ FileChunk  │ FilePath  │  ChunkSize   │              │
│  │ (immutable)│(type-safe)│(validated)   │              │
│  └────────────┴───────────┴──────────────┘              │
└──────────────────────────────────────────────────────────┘
                         ↓ implements
┌──────────────────────────────────────────────────────────┐
│              Infrastructure Layer                         │
│  ┌────────────────────────────────────────────────┐      │
│  │   Async File I/O Implementation                │      │
│  │   - tokio::fs for async operations             │      │
│  │   - Streaming, chunking, buffering             │      │
│  └────────────────────────────────────────────────┘      │
└──────────────────────────────────────────────────────────┘
                         ↓ reads/writes
┌──────────────────────────────────────────────────────────┐
│                  File System                              │
│  - Input files, output files, temporary files            │
└──────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="design-principles"><a class="header" href="#design-principles">Design Principles</a></h3>
<ol>
<li><strong>Immutability</strong>: File chunks cannot be modified after creation</li>
<li><strong>Streaming</strong>: Process files without loading entirely into memory</li>
<li><strong>Type Safety</strong>: Compile-time path category enforcement</li>
<li><strong>Async-First</strong>: Non-blocking I/O for better concurrency</li>
<li><strong>Memory Efficiency</strong>: Bounded memory usage regardless of file size</li>
</ol>
<hr />
<h2 id="file-io-architecture"><a class="header" href="#file-io-architecture">File I/O Architecture</a></h2>
<p>The file I/O layer uses value objects and async services to provide efficient, safe file processing.</p>
<h3 id="architectural-components"><a class="header" href="#architectural-components">Architectural Components</a></h3>
<pre><code class="language-text">┌─────────────────────────────────────────────────────────────┐
│ Value Objects (Domain)                                      │
│  ┌────────────────┬────────────────┬─────────────────┐     │
│  │  FileChunk     │  FilePath&lt;T&gt;   │   ChunkSize     │     │
│  │  - Immutable   │  - Type-safe   │   - Validated   │     │
│  │  - UUID ID     │  - Category    │   - 1B-512MB    │     │
│  │  - Sequence #  │  - Validated   │   - Default 1MB │     │
│  └────────────────┴────────────────┴─────────────────┘     │
└─────────────────────────────────────────────────────────────┘
                         ↓
┌─────────────────────────────────────────────────────────────┐
│ Service Interface (Domain)                                  │
│  ┌──────────────────────────────────────────────────┐      │
│  │  FileIOService (async trait)                     │      │
│  │  - read_file_chunks()                            │      │
│  │  - write_file_chunks()                           │      │
│  │  - stream_chunks()                               │      │
│  └──────────────────────────────────────────────────┘      │
└─────────────────────────────────────────────────────────────┘
                         ↓
┌─────────────────────────────────────────────────────────────┐
│ Implementation (Infrastructure)                             │
│  ┌──────────────────────────────────────────────────┐      │
│  │  Async File I/O                                  │      │
│  │  - tokio::fs::File                               │      │
│  │  - Buffering, streaming                          │      │
│  │  - Memory mapping (large files)                  │      │
│  └──────────────────────────────────────────────────┘      │
└─────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="processing-flow"><a class="header" href="#processing-flow">Processing Flow</a></h3>
<p><strong>File → Chunks → Processing → Chunks → File:</strong></p>
<pre><code class="language-text">Input File (e.g., 100MB)
        ↓
Split into chunks (1MB each)
        ↓
┌─────────┬─────────┬─────────┬─────────┐
│ Chunk 0 │ Chunk 1 │ Chunk 2 │   ...   │
│ (1MB)   │ (1MB)   │ (1MB)   │ (1MB)   │
└─────────┴─────────┴─────────┴─────────┘
        ↓ parallel processing
┌─────────┬─────────┬─────────┬─────────┐
│Processed│Processed│Processed│   ...   │
│ Chunk 0 │ Chunk 1 │ Chunk 2 │ (1MB)   │
└─────────┴─────────┴─────────┴─────────┘
        ↓
Reassemble chunks
        ↓
Output File (100MB)
</code></pre>
<hr />
<h2 id="filechunk-value-object"><a class="header" href="#filechunk-value-object">FileChunk Value Object</a></h2>
<p><code>FileChunk</code> is an immutable value object representing a portion of a file for processing.</p>
<h3 id="filechunk-structure"><a class="header" href="#filechunk-structure">FileChunk Structure</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]
pub struct FileChunk {
    id: Uuid,                           // Unique identifier
    sequence_number: u64,               // Order in file (0-based)
    offset: u64,                        // Byte offset in original file
    size: ChunkSize,                    // Validated chunk size
    data: Vec&lt;u8&gt;,                      // Actual chunk data
    checksum: Option&lt;String&gt;,           // Optional SHA-256 checksum
    is_final: bool,                     // Last chunk flag
    created_at: DateTime&lt;Utc&gt;,          // Creation timestamp
}
<span class="boring">}</span></code></pre></pre>
<h3 id="key-characteristics"><a class="header" href="#key-characteristics">Key Characteristics</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Feature</th><th>Description</th></tr></thead><tbody>
<tr><td><strong>Immutability</strong></td><td>Once created, chunks cannot be modified</td></tr>
<tr><td><strong>Unique Identity</strong></td><td>Each chunk has a UUID for tracking</td></tr>
<tr><td><strong>Sequence Ordering</strong></td><td>Maintains position for reassembly</td></tr>
<tr><td><strong>Integrity</strong></td><td>Optional checksums for validation</td></tr>
<tr><td><strong>Thread Safety</strong></td><td>Fully thread-safe due to immutability</td></tr>
</tbody></table>
</div>
<h3 id="creating-chunks"><a class="header" href="#creating-chunks">Creating Chunks</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::FileChunk;

// Basic chunk creation
let data = vec![1, 2, 3, 4, 5];
let chunk = FileChunk::new(
    0,        // sequence_number
    0,        // offset
    data,     // data
    false,    // is_final
)?;

println!("Chunk ID: {}", chunk.id());
println!("Size: {} bytes", chunk.size().bytes());
<span class="boring">}</span></code></pre></pre>
<h3 id="chunk-with-checksum"><a class="header" href="#chunk-with-checksum">Chunk with Checksum</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Create chunk with checksum
let data = vec![1, 2, 3, 4, 5];
let chunk = FileChunk::new(0, 0, data, false)?
    .with_calculated_checksum();

// Verify checksum
if let Some(checksum) = chunk.checksum() {
    println!("Checksum: {}", checksum);
}

// Verify data integrity
assert!(chunk.verify_checksum());
<span class="boring">}</span></code></pre></pre>
<h3 id="chunk-properties"><a class="header" href="#chunk-properties">Chunk Properties</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Access chunk properties
println!("ID: {}", chunk.id());
println!("Sequence: {}", chunk.sequence_number());
println!("Offset: {}", chunk.offset());
println!("Size: {} bytes", chunk.size().bytes());
println!("Is final: {}", chunk.is_final());
println!("Created: {}", chunk.created_at());

// Access data
let data: &amp;[u8] = chunk.data();
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="filepath-value-object"><a class="header" href="#filepath-value-object">FilePath Value Object</a></h2>
<p><code>FilePath&lt;T&gt;</code> is a type-safe, validated file path with compile-time category enforcement.</p>
<h3 id="path-categories"><a class="header" href="#path-categories">Path Categories</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Type-safe path categories
pub struct InputPath;      // For input files
pub struct OutputPath;     // For output files
pub struct TempPath;       // For temporary files
pub struct LogPath;        // For log files

// Usage with phantom types
let input: FilePath&lt;InputPath&gt; = FilePath::new("./input.dat")?;
let output: FilePath&lt;OutputPath&gt; = FilePath::new("./output.dat")?;

// ✅ Type system prevents mixing categories
// ❌ Cannot assign input path to output variable
// let wrong: FilePath&lt;OutputPath&gt; = input;  // Compile error!
<span class="boring">}</span></code></pre></pre>
<h3 id="path-validation"><a class="header" href="#path-validation">Path Validation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::value_objects::{FilePath, InputPath};

// Create and validate path
let path = FilePath::&lt;InputPath&gt;::new("/path/to/file.dat")?;

// Path properties
println!("Path: {}", path.as_str());
println!("Exists: {}", path.exists());
println!("Is file: {}", path.is_file());
println!("Is dir: {}", path.is_dir());
println!("Is absolute: {}", path.is_absolute());

// Convert to std::path::Path
let std_path: &amp;Path = path.as_path();
<span class="boring">}</span></code></pre></pre>
<h3 id="path-operations"><a class="header" href="#path-operations">Path Operations</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Get file name
let file_name = path.file_name();

// Get parent directory
let parent = path.parent();

// Get file extension
let extension = path.extension();

// Convert to string
let path_str = path.to_string();
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="chunksize-value-object"><a class="header" href="#chunksize-value-object">ChunkSize Value Object</a></h2>
<p><code>ChunkSize</code> represents a validated chunk size within system bounds.</p>
<h3 id="size-constraints"><a class="header" href="#size-constraints">Size Constraints</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Chunk size constants
ChunkSize::MIN_SIZE  // 1 byte
ChunkSize::MAX_SIZE  // 512 MB
ChunkSize::DEFAULT   // 1 MB
<span class="boring">}</span></code></pre></pre>
<h3 id="creating-chunk-sizes"><a class="header" href="#creating-chunk-sizes">Creating Chunk Sizes</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::ChunkSize;

// From bytes
let size = ChunkSize::new(1024 * 1024)?;  // 1 MB
assert_eq!(size.bytes(), 1_048_576);

// From kilobytes
let size_kb = ChunkSize::from_kb(512)?;  // 512 KB
assert_eq!(size_kb.kilobytes(), 512.0);

// From megabytes
let size_mb = ChunkSize::from_mb(16)?;  // 16 MB
assert_eq!(size_mb.megabytes(), 16.0);

// Default (1 MB)
let default_size = ChunkSize::default();
assert_eq!(default_size.megabytes(), 1.0);
<span class="boring">}</span></code></pre></pre>
<h3 id="size-validation"><a class="header" href="#size-validation">Size Validation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Valid sizes
let valid = ChunkSize::new(64 * 1024)?;  // 64 KB

// ❌ Invalid: too small
let too_small = ChunkSize::new(0);  // Error: must be ≥ 1 byte
assert!(too_small.is_err());

// ❌ Invalid: too large
let too_large = ChunkSize::new(600 * 1024 * 1024);  // Error: must be ≤ 512 MB
assert!(too_large.is_err());
<span class="boring">}</span></code></pre></pre>
<h3 id="optimal-sizing"><a class="header" href="#optimal-sizing">Optimal Sizing</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Calculate optimal chunk size for file
let file_size = 100 * 1024 * 1024;  // 100 MB
let optimal = ChunkSize::optimal_for_file_size(file_size);

println!("Optimal chunk size: {} MB", optimal.megabytes());

// Size conversions
let size = ChunkSize::from_mb(4)?;
println!("Bytes: {}", size.bytes());
println!("Kilobytes: {}", size.kilobytes());
println!("Megabytes: {}", size.megabytes());
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="fileioservice-interface"><a class="header" href="#fileioservice-interface">FileIOService Interface</a></h2>
<p><code>FileIOService</code> is an async trait defining file I/O operations.</p>
<h3 id="service-interface"><a class="header" href="#service-interface">Service Interface</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[async_trait]
pub trait FileIOService: Send + Sync {
    /// Read file into chunks
    async fn read_file_chunks(
        &amp;self,
        path: &amp;Path,
        chunk_size: ChunkSize,
    ) -&gt; Result&lt;Vec&lt;FileChunk&gt;, PipelineError&gt;;

    /// Write chunks to file
    async fn write_file_chunks(
        &amp;self,
        path: &amp;Path,
        chunks: Vec&lt;FileChunk&gt;,
    ) -&gt; Result&lt;(), PipelineError&gt;;

    /// Stream chunks for processing
    async fn stream_chunks(
        &amp;self,
        path: &amp;Path,
        chunk_size: ChunkSize,
    ) -&gt; Result&lt;impl Stream&lt;Item = Result&lt;FileChunk, PipelineError&gt;&gt;, PipelineError&gt;;
}
<span class="boring">}</span></code></pre></pre>
<h3 id="why-async"><a class="header" href="#why-async">Why Async?</a></h3>
<p>The service is async because file I/O is <strong>I/O-bound</strong>, not CPU-bound:</p>
<p><strong>Benefits:</strong></p>
<ul>
<li><strong>Non-Blocking</strong>: Doesn't block the async runtime</li>
<li><strong>Concurrent</strong>: Multiple files can be processed concurrently</li>
<li><strong>tokio Integration</strong>: Natural integration with tokio::fs</li>
<li><strong>Performance</strong>: Better throughput for I/O operations</li>
</ul>
<p><strong>Classification:</strong></p>
<ul>
<li>This is an <strong>infrastructure port</strong>, not a domain service</li>
<li>Domain services (compression, encryption) are CPU-bound and sync</li>
<li>Infrastructure ports (file I/O, network, database) are I/O-bound and async</li>
</ul>
<hr />
<h2 id="file-reading"><a class="header" href="#file-reading">File Reading</a></h2>
<p>File reading uses streaming and chunking for memory-efficient processing.</p>
<h3 id="reading-small-files"><a class="header" href="#reading-small-files">Reading Small Files</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::FileIOService;

// Read entire file into chunks
let service: Arc&lt;dyn FileIOService&gt; = /* ... */;
let chunks = service.read_file_chunks(
    Path::new("./input.dat"),
    ChunkSize::from_mb(1)?,
).await?;

println!("Read {} chunks", chunks.len());
for chunk in chunks {
    println!("Chunk {}: {} bytes", chunk.sequence_number(), chunk.size().bytes());
}
<span class="boring">}</span></code></pre></pre>
<h3 id="streaming-large-files"><a class="header" href="#streaming-large-files">Streaming Large Files</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tokio_stream::StreamExt;

// Stream chunks for memory efficiency
let mut stream = service.stream_chunks(
    Path::new("./large_file.dat"),
    ChunkSize::from_mb(4)?,
).await?;

while let Some(chunk_result) = stream.next().await {
    let chunk = chunk_result?;

    // Process chunk without loading entire file
    process_chunk(chunk).await?;
}
<span class="boring">}</span></code></pre></pre>
<h3 id="reading-with-metadata"><a class="header" href="#reading-with-metadata">Reading with Metadata</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::fs::metadata;

// Get file metadata first
let metadata = metadata("./input.dat")?;
let file_size = metadata.len();

// Choose optimal chunk size
let chunk_size = ChunkSize::optimal_for_file_size(file_size);

// Read with optimal settings
let chunks = service.read_file_chunks(
    Path::new("./input.dat"),
    chunk_size,
).await?;
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="file-writing"><a class="header" href="#file-writing">File Writing</a></h2>
<p>File writing assembles processed chunks back into complete files.</p>
<h3 id="writing-chunks-to-file"><a class="header" href="#writing-chunks-to-file">Writing Chunks to File</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Write chunks to output file
let processed_chunks: Vec&lt;FileChunk&gt; = /* ... */;

service.write_file_chunks(
    Path::new("./output.dat"),
    processed_chunks,
).await?;

println!("File written successfully");
<span class="boring">}</span></code></pre></pre>
<h3 id="streaming-write"><a class="header" href="#streaming-write">Streaming Write</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tokio::fs::File;
use tokio::io::AsyncWriteExt;

// Stream chunks directly to file
let mut file = File::create("./output.dat").await?;

for chunk in processed_chunks {
    file.write_all(chunk.data()).await?;
}

file.flush().await?;
println!("File written: {} bytes", file.metadata().await?.len());
<span class="boring">}</span></code></pre></pre>
<h3 id="atomic-writes"><a class="header" href="#atomic-writes">Atomic Writes</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tempfile::NamedTempFile;

// Write to temporary file first
let temp_file = NamedTempFile::new()?;
service.write_file_chunks(
    temp_file.path(),
    chunks,
).await?;

// Atomically move to final location
temp_file.persist("./output.dat")?;
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="memory-management"><a class="header" href="#memory-management">Memory Management</a></h2>
<p>The system uses several strategies to manage memory efficiently.</p>
<h3 id="bounded-memory-usage"><a class="header" href="#bounded-memory-usage">Bounded Memory Usage</a></h3>
<pre><code class="language-text">File Size: 1 GB
Chunk Size: 4 MB
Memory Usage: ~4 MB (single chunk in memory at a time)

Without chunking: 1 GB in memory
With chunking: 4 MB in memory (250x reduction!)
</code></pre>
<h3 id="memory-mapping-for-large-files"><a class="header" href="#memory-mapping-for-large-files">Memory Mapping for Large Files</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Automatically uses memory mapping for files &gt; threshold
let config = FileIOConfig {
    chunk_size: ChunkSize::from_mb(4)?,
    use_memory_mapping: true,
    memory_mapping_threshold: 100 * 1024 * 1024,  // 100 MB
    ..Default::default()
};

// Files &gt; 100 MB use memory mapping
let chunks = service.read_file_chunks_with_config(
    Path::new("./large_file.dat"),
    &amp;config,
).await?;
<span class="boring">}</span></code></pre></pre>
<h3 id="streaming-vs-buffering"><a class="header" href="#streaming-vs-buffering">Streaming vs. Buffering</a></h3>
<p><strong>Streaming (Memory-Efficient):</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Process one chunk at a time
let mut stream = service.stream_chunks(path, chunk_size).await?;
while let Some(chunk) = stream.next().await {
    process_chunk(chunk?).await?;
}
// Peak memory: 1 chunk size
<span class="boring">}</span></code></pre></pre>
<p><strong>Buffering (Performance):</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Load all chunks into memory
let chunks = service.read_file_chunks(path, chunk_size).await?;
process_all_chunks(chunks).await?;
// Peak memory: all chunks
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="error-handling"><a class="header" href="#error-handling">Error Handling</a></h2>
<p>The file I/O system handles various error conditions.</p>
<h3 id="common-errors"><a class="header" href="#common-errors">Common Errors</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::PipelineError;

match service.read_file_chunks(path, chunk_size).await {
    Ok(chunks) =&gt; { /* success */ },
    Err(PipelineError::FileNotFound(path)) =&gt; {
        eprintln!("File not found: {}", path);
    },
    Err(PipelineError::PermissionDenied(path)) =&gt; {
        eprintln!("Permission denied: {}", path);
    },
    Err(PipelineError::InsufficientDiskSpace) =&gt; {
        eprintln!("Not enough disk space");
    },
    Err(PipelineError::InvalidChunk(msg)) =&gt; {
        eprintln!("Invalid chunk: {}", msg);
    },
    Err(e) =&gt; {
        eprintln!("I/O error: {}", e);
    },
}
<span class="boring">}</span></code></pre></pre>
<h3 id="retry-logic"><a class="header" href="#retry-logic">Retry Logic</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tokio::time::{sleep, Duration};

async fn read_with_retry(
    service: &amp;dyn FileIOService,
    path: &amp;Path,
    chunk_size: ChunkSize,
    max_retries: u32,
) -&gt; Result&lt;Vec&lt;FileChunk&gt;, PipelineError&gt; {
    let mut retries = 0;

    loop {
        match service.read_file_chunks(path, chunk_size).await {
            Ok(chunks) =&gt; return Ok(chunks),
            Err(e) if retries &lt; max_retries =&gt; {
                retries += 1;
                warn!("Read failed (attempt {}/{}): {}", retries, max_retries, e);
                sleep(Duration::from_secs(1 &lt;&lt; retries)).await;  // Exponential backoff
            },
            Err(e) =&gt; return Err(e),
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="partial-reads"><a class="header" href="#partial-reads">Partial Reads</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Handle partial reads gracefully
async fn read_available_chunks(
    service: &amp;dyn FileIOService,
    path: &amp;Path,
    chunk_size: ChunkSize,
) -&gt; Result&lt;Vec&lt;FileChunk&gt;, PipelineError&gt; {
    let mut chunks = Vec::new();
    let mut stream = service.stream_chunks(path, chunk_size).await?;

    while let Some(chunk_result) = stream.next().await {
        match chunk_result {
            Ok(chunk) =&gt; chunks.push(chunk),
            Err(e) =&gt; {
                warn!("Chunk read error: {}, stopping", e);
                break;  // Return partial results
            },
        }
    }

    Ok(chunks)
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="performance-optimization"><a class="header" href="#performance-optimization">Performance Optimization</a></h2>
<p>Several strategies optimize file I/O performance.</p>
<h3 id="optimal-chunk-size-selection"><a class="header" href="#optimal-chunk-size-selection">Optimal Chunk Size Selection</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Chunk size recommendations
fn optimal_chunk_size(file_size: u64) -&gt; ChunkSize {
    match file_size {
        0..=10_485_760 =&gt; ChunkSize::from_mb(1).unwrap(),          // &lt; 10 MB: 1 MB chunks
        10_485_761..=104_857_600 =&gt; ChunkSize::from_mb(4).unwrap(), // 10-100 MB: 4 MB chunks
        104_857_601..=1_073_741_824 =&gt; ChunkSize::from_mb(8).unwrap(), // 100 MB - 1 GB: 8 MB chunks
        _ =&gt; ChunkSize::from_mb(16).unwrap(),                       // &gt; 1 GB: 16 MB chunks
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="parallel-processing"><a class="header" href="#parallel-processing">Parallel Processing</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use futures::future::try_join_all;

// Process chunks in parallel
let futures: Vec&lt;_&gt; = chunks.into_iter()
    .map(|chunk| async move {
        process_chunk(chunk).await
    })
    .collect();

let results = try_join_all(futures).await?;
<span class="boring">}</span></code></pre></pre>
<h3 id="buffered-io"><a class="header" href="#buffered-io">Buffered I/O</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tokio::io::BufReader;

// Use buffered reading for better performance
let file = File::open(path).await?;
let mut reader = BufReader::with_capacity(8 * 1024 * 1024, file);  // 8 MB buffer

// Read chunks with buffering
while let Some(chunk) = read_chunk(&amp;mut reader).await? {
    process_chunk(chunk).await?;
}
<span class="boring">}</span></code></pre></pre>
<h3 id="performance-benchmarks"><a class="header" href="#performance-benchmarks">Performance Benchmarks</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Small Files (&lt; 10 MB)</th><th>Medium Files (100 MB)</th><th>Large Files (&gt; 1 GB)</th></tr></thead><tbody>
<tr><td><strong>Read</strong></td><td>~50 MB/s</td><td>~200 MB/s</td><td>~300 MB/s</td></tr>
<tr><td><strong>Write</strong></td><td>~40 MB/s</td><td>~180 MB/s</td><td>~280 MB/s</td></tr>
<tr><td><strong>Stream</strong></td><td>~45 MB/s</td><td>~190 MB/s</td><td>~290 MB/s</td></tr>
</tbody></table>
</div>
<p><em>Benchmarks on SSD with 4 MB chunks</em></p>
<hr />
<h2 id="usage-examples"><a class="header" href="#usage-examples">Usage Examples</a></h2>
<h3 id="example-1-basic-file-processing"><a class="header" href="#example-1-basic-file-processing">Example 1: Basic File Processing</a></h3>
<pre><pre class="playground"><code class="language-rust">use pipeline_domain::{FileIOService, ChunkSize};
use std::path::Path;

#[tokio::main]
async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    let service: Arc&lt;dyn FileIOService&gt; = /* ... */;

    // Read file
    let chunks = service.read_file_chunks(
        Path::new("./input.dat"),
        ChunkSize::from_mb(1)?,
    ).await?;

    println!("Read {} chunks", chunks.len());

    // Process chunks
    let processed: Vec&lt;_&gt; = chunks.into_iter()
        .map(|chunk| process_chunk(chunk))
        .collect();

    // Write output
    service.write_file_chunks(
        Path::new("./output.dat"),
        processed,
    ).await?;

    println!("Processing complete!");
    Ok(())
}

fn process_chunk(chunk: FileChunk) -&gt; FileChunk {
    // Transform chunk data
    let transformed_data = chunk.data().to_vec();
    FileChunk::new(
        chunk.sequence_number(),
        chunk.offset(),
        transformed_data,
        chunk.is_final(),
    ).unwrap()
}</code></pre></pre>
<h3 id="example-2-streaming-large-files"><a class="header" href="#example-2-streaming-large-files">Example 2: Streaming Large Files</a></h3>
<pre><pre class="playground"><code class="language-rust">use tokio_stream::StreamExt;

#[tokio::main]
async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    let service: Arc&lt;dyn FileIOService&gt; = /* ... */;

    // Stream large file
    let mut stream = service.stream_chunks(
        Path::new("./large_file.dat"),
        ChunkSize::from_mb(8)?,
    ).await?;

    let mut processed_chunks = Vec::new();

    while let Some(chunk_result) = stream.next().await {
        let chunk = chunk_result?;

        // Process chunk in streaming fashion
        let processed = process_chunk(chunk);
        processed_chunks.push(processed);

        // Optional: write chunks as they're processed
        // write_chunk_to_file(&amp;processed).await?;
    }

    println!("Processed {} chunks", processed_chunks.len());
    Ok(())
}</code></pre></pre>
<h3 id="example-3-parallel-chunk-processing"><a class="header" href="#example-3-parallel-chunk-processing">Example 3: Parallel Chunk Processing</a></h3>
<pre><pre class="playground"><code class="language-rust">use futures::future::try_join_all;

#[tokio::main]
async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    let service: Arc&lt;dyn FileIOService&gt; = /* ... */;

    // Read all chunks
    let chunks = service.read_file_chunks(
        Path::new("./input.dat"),
        ChunkSize::from_mb(4)?,
    ).await?;

    // Process chunks in parallel
    let futures = chunks.into_iter().map(|chunk| {
        tokio::spawn(async move {
            process_chunk_async(chunk).await
        })
    });

    let results = try_join_all(futures).await?;
    let processed: Vec&lt;_&gt; = results.into_iter()
        .collect::&lt;Result&lt;Vec&lt;_&gt;, _&gt;&gt;()?;

    // Write results
    service.write_file_chunks(
        Path::new("./output.dat"),
        processed,
    ).await?;

    Ok(())
}

async fn process_chunk_async(chunk: FileChunk) -&gt; Result&lt;FileChunk, PipelineError&gt; {
    // Async processing
    tokio::time::sleep(Duration::from_millis(10)).await;
    Ok(process_chunk(chunk))
}</code></pre></pre>
<h3 id="example-4-error-handling-and-retry"><a class="header" href="#example-4-error-handling-and-retry">Example 4: Error Handling and Retry</a></h3>
<pre><pre class="playground"><code class="language-rust">#[tokio::main]
async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    let service: Arc&lt;dyn FileIOService&gt; = /* ... */;
    let path = Path::new("./input.dat");
    let chunk_size = ChunkSize::from_mb(1)?;

    // Retry on failure
    let chunks = read_with_retry(&amp;*service, path, chunk_size, 3).await?;

    println!("Successfully read {} chunks", chunks.len());
    Ok(())
}

async fn read_with_retry(
    service: &amp;dyn FileIOService,
    path: &amp;Path,
    chunk_size: ChunkSize,
    max_retries: u32,
) -&gt; Result&lt;Vec&lt;FileChunk&gt;, PipelineError&gt; {
    for attempt in 1..=max_retries {
        match service.read_file_chunks(path, chunk_size).await {
            Ok(chunks) =&gt; return Ok(chunks),
            Err(e) if attempt &lt; max_retries =&gt; {
                eprintln!("Attempt {}/{} failed: {}", attempt, max_retries, e);
                tokio::time::sleep(Duration::from_secs(2_u64.pow(attempt))).await;
            },
            Err(e) =&gt; return Err(e),
        }
    }
    unreachable!()
}</code></pre></pre>
<hr />
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<h3 id="1-choose-appropriate-chunk-sizes"><a class="header" href="#1-choose-appropriate-chunk-sizes">1. Choose Appropriate Chunk Sizes</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Optimize chunk size for file
let file_size = metadata(path)?.len();
let chunk_size = ChunkSize::optimal_for_file_size(file_size);

// ❌ Bad: Fixed chunk size for all files
let chunk_size = ChunkSize::from_mb(1)?;  // May be suboptimal
<span class="boring">}</span></code></pre></pre>
<h3 id="2-use-streaming-for-large-files"><a class="header" href="#2-use-streaming-for-large-files">2. Use Streaming for Large Files</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Stream large files
let mut stream = service.stream_chunks(path, chunk_size).await?;
while let Some(chunk) = stream.next().await {
    process_chunk(chunk?).await?;
}

// ❌ Bad: Load entire large file into memory
let chunks = service.read_file_chunks(path, chunk_size).await?;
// Entire file in memory!
<span class="boring">}</span></code></pre></pre>
<h3 id="3-validate-chunk-integrity"><a class="header" href="#3-validate-chunk-integrity">3. Validate Chunk Integrity</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Verify checksums
for chunk in chunks {
    if !chunk.verify_checksum() {
        return Err(PipelineError::ChecK sumMismatch);
    }
    process_chunk(chunk)?;
}
<span class="boring">}</span></code></pre></pre>
<h3 id="4-handle-errors-gracefully"><a class="header" href="#4-handle-errors-gracefully">4. Handle Errors Gracefully</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Specific error handling
match service.read_file_chunks(path, chunk_size).await {
    Ok(chunks) =&gt; process(chunks),
    Err(PipelineError::FileNotFound(_)) =&gt; handle_missing_file(),
    Err(PipelineError::PermissionDenied(_)) =&gt; handle_permissions(),
    Err(e) =&gt; handle_generic_error(e),
}
<span class="boring">}</span></code></pre></pre>
<h3 id="5-use-type-safe-paths"><a class="header" href="#5-use-type-safe-paths">5. Use Type-Safe Paths</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Type-safe paths
let input: FilePath&lt;InputPath&gt; = FilePath::new("./input.dat")?;
let output: FilePath&lt;OutputPath&gt; = FilePath::new("./output.dat")?;

// ❌ Bad: Raw strings
let input = "./input.dat";
let output = "./output.dat";
<span class="boring">}</span></code></pre></pre>
<h3 id="6-clean-up-temporary-files"><a class="header" href="#6-clean-up-temporary-files">6. Clean Up Temporary Files</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Automatic cleanup with tempfile
let temp = NamedTempFile::new()?;
write_chunks(temp.path(), chunks).await?;
// Automatically deleted when dropped

// Or explicit cleanup
temp.close()?;
<span class="boring">}</span></code></pre></pre>
<h3 id="7-monitor-memory-usage"><a class="header" href="#7-monitor-memory-usage">7. Monitor Memory Usage</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Track memory usage
let chunks_in_memory = chunks.len();
let memory_used = chunks_in_memory * chunk_size.bytes();
if memory_used &gt; max_memory {
    // Switch to streaming
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h2>
<h3 id="issue-1-out-of-memory"><a class="header" href="#issue-1-out-of-memory">Issue 1: Out of Memory</a></h3>
<p><strong>Symptom:</strong></p>
<pre><code class="language-text">Error: Out of memory
</code></pre>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 1. Reduce chunk size
let chunk_size = ChunkSize::from_mb(1)?;  // Smaller chunks

// 2. Use streaming instead of buffering
let mut stream = service.stream_chunks(path, chunk_size).await?;

// 3. Process chunks one at a time
while let Some(chunk) = stream.next().await {
    process_chunk(chunk?).await?;
    // Chunk dropped, memory freed
}
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-2-slow-file-io"><a class="header" href="#issue-2-slow-file-io">Issue 2: Slow File I/O</a></h3>
<p><strong>Diagnosis:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let start = Instant::now();
let chunks = service.read_file_chunks(path, chunk_size).await?;
let duration = start.elapsed();
println!("Read took: {:?}", duration);
<span class="boring">}</span></code></pre></pre>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 1. Increase chunk size
let chunk_size = ChunkSize::from_mb(8)?;  // Larger chunks = fewer I/O ops

// 2. Use memory mapping for large files
let config = FileIOConfig {
    use_memory_mapping: true,
    memory_mapping_threshold: 50 * 1024 * 1024,  // 50 MB
    ..Default::default()
};

// 3. Use buffered I/O
let reader = BufReader::with_capacity(8 * 1024 * 1024, file);
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-3-checksum-mismatch"><a class="header" href="#issue-3-checksum-mismatch">Issue 3: Checksum Mismatch</a></h3>
<p><strong>Symptom:</strong></p>
<pre><code class="language-text">Error: Checksum mismatch for chunk 42
</code></pre>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 1. Verify during read
let chunk = chunk.with_calculated_checksum();
if !chunk.verify_checksum() {
    // Re-read chunk
}

// 2. Log and skip corrupted chunks
match chunk.verify_checksum() {
    true =&gt; process_chunk(chunk),
    false =&gt; {
        warn!("Chunk {} corrupted, skipping", chunk.sequence_number());
        continue;
    },
}
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-4-file-permission-errors"><a class="header" href="#issue-4-file-permission-errors">Issue 4: File Permission Errors</a></h3>
<p><strong>Symptom:</strong></p>
<pre><code class="language-text">Error: Permission denied: ./output.dat
</code></pre>
<p><strong>Solutions:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 1. Check permissions before writing
use std::fs;
let metadata = fs::metadata(parent_dir)?;
if metadata.permissions().readonly() {
    return Err("Output directory is read-only".into());
}

// 2. Use appropriate path categories
let output: FilePath&lt;OutputPath&gt; = FilePath::new("./output.dat")?;
output.ensure_writable()?;
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="testing-strategies"><a class="header" href="#testing-strategies">Testing Strategies</a></h2>
<h3 id="unit-testing-with-mock-chunks"><a class="header" href="#unit-testing-with-mock-chunks">Unit Testing with Mock Chunks</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_chunk_creation() {
        let data = vec![1, 2, 3, 4, 5];
        let chunk = FileChunk::new(0, 0, data.clone(), false).unwrap();

        assert_eq!(chunk.sequence_number(), 0);
        assert_eq!(chunk.data(), &amp;data);
        assert!(!chunk.is_final());
    }

    #[test]
    fn test_chunk_checksum() {
        let data = vec![1, 2, 3, 4, 5];
        let chunk = FileChunk::new(0, 0, data, false)
            .unwrap()
            .with_calculated_checksum();

        assert!(chunk.checksum().is_some());
        assert!(chunk.verify_checksum());
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="integration-testing-with-files"><a class="header" href="#integration-testing-with-files">Integration Testing with Files</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_file_round_trip() {
    let service: Arc&lt;dyn FileIOService&gt; = create_test_service();

    // Create test data
    let test_data = vec![0u8; 10 * 1024 * 1024];  // 10 MB
    let input_path = temp_dir().join("test_input.dat");
    std::fs::write(&amp;input_path, &amp;test_data).unwrap();

    // Read chunks
    let chunks = service.read_file_chunks(
        &amp;input_path,
        ChunkSize::from_mb(1).unwrap(),
    ).await.unwrap();

    assert_eq!(chunks.len(), 10);  // 10 x 1MB chunks

    // Write chunks
    let output_path = temp_dir().join("test_output.dat");
    service.write_file_chunks(&amp;output_path, chunks).await.unwrap();

    // Verify
    let output_data = std::fs::read(&amp;output_path).unwrap();
    assert_eq!(test_data, output_data);
}
<span class="boring">}</span></code></pre></pre>
<h3 id="streaming-tests"><a class="header" href="#streaming-tests">Streaming Tests</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_streaming() {
    use tokio_stream::StreamExt;

    let service: Arc&lt;dyn FileIOService&gt; = create_test_service();
    let path = create_test_file(100 * 1024 * 1024);  // 100 MB

    let mut stream = service.stream_chunks(
        &amp;path,
        ChunkSize::from_mb(4).unwrap(),
    ).await.unwrap();

    let mut chunk_count = 0;
    while let Some(chunk_result) = stream.next().await {
        let chunk = chunk_result.unwrap();
        assert!(chunk.size().bytes() &lt;= 4 * 1024 * 1024);
        chunk_count += 1;
    }

    assert_eq!(chunk_count, 25);  // 100 MB / 4 MB = 25 chunks
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="next-steps"><a class="header" href="#next-steps">Next Steps</a></h2>
<p>After understanding file I/O fundamentals, explore specific implementations:</p>
<h3 id="detailed-file-io-topics"><a class="header" href="#detailed-file-io-topics">Detailed File I/O Topics</a></h3>
<ol>
<li><strong><a href="chunking.html">Chunking Strategy</a></strong>: Deep dive into chunking algorithms and optimization</li>
<li><strong><a href="binary-format.html">Binary Format</a></strong>: File format specification and serialization</li>
</ol>
<h3 id="related-topics"><a class="header" href="#related-topics">Related Topics</a></h3>
<ul>
<li><strong><a href="stages.html">Stage Processing</a></strong>: How stages process file chunks</li>
<li><strong><a href="integrity.html">Integrity Checking</a></strong>: Checksums and verification</li>
<li><strong><a href="../advanced/performance.html">Performance Optimization</a></strong>: I/O performance tuning</li>
</ul>
<h3 id="advanced-topics"><a class="header" href="#advanced-topics">Advanced Topics</a></h3>
<ul>
<li><strong><a href="../advanced/concurrency.html">Concurrency Model</a></strong>: Parallel file processing</li>
<li><strong><a href="../advanced/extending.html">Extending the Pipeline</a></strong>: Custom file formats and I/O</li>
</ul>
<hr />
<h2 id="summary"><a class="header" href="#summary">Summary</a></h2>
<p><strong>Key Takeaways:</strong></p>
<ol>
<li><strong>FileChunk</strong> is an immutable value object representing file data portions</li>
<li><strong>FilePath<T></strong> provides type-safe, validated file paths with phantom types</li>
<li><strong>ChunkSize</strong> validates chunk sizes within 1 byte to 512 MB bounds</li>
<li><strong>FileIOService</strong> defines async I/O operations for streaming and chunking</li>
<li><strong>Streaming</strong> enables memory-efficient processing of files of any size</li>
<li><strong>Memory Management</strong> uses bounded buffers and optional memory mapping</li>
<li><strong>Error Handling</strong> provides retry logic and graceful degradation</li>
</ol>
<p><strong>Architecture File References:</strong></p>
<ul>
<li><strong>FileChunk:</strong> <code>pipeline-domain/src/value_objects/file_chunk.rs:176</code></li>
<li><strong>FilePath:</strong> <code>pipeline-domain/src/value_objects/file_path.rs:1</code></li>
<li><strong>ChunkSize:</strong> <code>pipeline-domain/src/value_objects/chunk_size.rs:1</code></li>
<li><strong>FileIOService:</strong> <code>pipeline-domain/src/services/file_io_service.rs:185</code></li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../implementation/schema.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../implementation/chunking.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../implementation/schema.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../implementation/chunking.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
