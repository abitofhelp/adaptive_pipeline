<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Pipeline Developer Guide</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Pipeline Developer Guide</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="pipeline-developer-guide"><a class="header" href="#pipeline-developer-guide">Pipeline Developer Guide</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<h2 id="welcome"><a class="header" href="#welcome">Welcome</a></h2>
<p>This is the comprehensive technical guide for the Optimized Adaptive Pipeline. Whether you're learning advanced Rust patterns, contributing to the project, or using the pipeline in production, this guide provides the depth you need.</p>
<h2 id="how-to-use-this-guide"><a class="header" href="#how-to-use-this-guide">How to Use This Guide</a></h2>
<p>This guide follows a <strong>progressive disclosure</strong> approach - each section builds on previous ones:</p>
<h3 id="start-here-fundamentals"><a class="header" href="#start-here-fundamentals">Start Here: Fundamentals</a></h3>
<p>If you're new to the pipeline, start with <strong>Fundamentals</strong>. This section introduces core concepts in an accessible way:</p>
<ul>
<li>What pipelines do and why they're useful</li>
<li>Key terminology and concepts</li>
<li>How stages work together</li>
<li>Basic configuration</li>
<li>Running your first pipeline</li>
</ul>
<p><strong>Time commitment:</strong> 30-45 minutes</p>
<h3 id="building-understanding-architecture"><a class="header" href="#building-understanding-architecture">Building Understanding: Architecture</a></h3>
<p>Once you understand the basics, explore the <strong>Architecture</strong> section. This explains <em>how</em> the pipeline is designed:</p>
<ul>
<li>Layered architecture (Domain, Application, Infrastructure)</li>
<li>Domain-Driven Design concepts</li>
<li>Design patterns in use (Repository, Service, Adapter, Observer)</li>
<li>Dependency management</li>
</ul>
<p>This section bridges the gap between basic usage and implementation details.</p>
<p><strong>Time commitment:</strong> 1-2 hours</p>
<h3 id="going-deeper-implementation"><a class="header" href="#going-deeper-implementation">Going Deeper: Implementation</a></h3>
<p>The <strong>Implementation</strong> section covers how specific features work:</p>
<ul>
<li>Stage processing details</li>
<li>Compression and encryption</li>
<li>Data persistence and schema management</li>
<li>File I/O and chunking</li>
<li>Metrics and observability</li>
</ul>
<p>Perfect for contributors or those adapting the pipeline for specific needs.</p>
<p><strong>Time commitment:</strong> 2-3 hours</p>
<h3 id="expert-level-advanced-topics"><a class="header" href="#expert-level-advanced-topics">Expert Level: Advanced Topics</a></h3>
<p>For optimization and extension, the <strong>Advanced Topics</strong> section covers:</p>
<ul>
<li>Concurrency model and thread pooling</li>
<li>Performance optimization techniques</li>
<li>Creating custom stages and algorithms</li>
</ul>
<p><strong>Time commitment:</strong> 2-4 hours depending on depth</p>
<h3 id="reference-formal-documentation"><a class="header" href="#reference-formal-documentation">Reference: Formal Documentation</a></h3>
<p>The <strong>Formal Documentation</strong> section contains:</p>
<ul>
<li>Software Requirements Specification (SRS)</li>
<li>Software Design Document (SDD)</li>
<li>Test Strategy (STP)</li>
</ul>
<p>These are comprehensive reference documents.</p>
<h2 id="documentation-scope"><a class="header" href="#documentation-scope">Documentation Scope</a></h2>
<p>Following our <strong>"reasonable" principle</strong>, this guide focuses on:</p>
<p>✅ <strong>What you need to know</strong> to use, contribute to, or extend the pipeline
✅ <strong>Why decisions were made</strong> with just enough context
✅ <strong>How to accomplish tasks</strong> with practical examples
✅ <strong>Advanced Rust patterns</strong> demonstrated in real code</p>
<p>We intentionally <strong>do not</strong> include:</p>
<p>❌ Rust language tutorials (see <a href="https://doc.rust-lang.org/book/">The Rust Book</a>)
❌ General programming concepts
❌ Third-party library documentation (links provided instead)
❌ Exhaustive algorithm details (high-level explanations with references)</p>
<h2 id="learning-path-recommendations"><a class="header" href="#learning-path-recommendations">Learning Path Recommendations</a></h2>
<h3 id="i-want-to-use-the-pipeline"><a class="header" href="#i-want-to-use-the-pipeline">I want to use the pipeline</a></h3>
<p>→ Read <a href="fundamentals/what-is-a-pipeline.html">Fundamentals</a>
→ Skip to <a href="implementation/stages.html">Implementation</a> for specific features</p>
<h3 id="i-want-to-contribute"><a class="header" href="#i-want-to-contribute">I want to contribute</a></h3>
<p>→ Read Fundamentals + Architecture (full sections)
→ Review relevant Implementation chapters
→ Check <a href="../../docs/book/contributing/guidelines.html">Contributing Guide</a></p>
<h3 id="i-want-to-learn-advanced-rust-patterns"><a class="header" href="#i-want-to-learn-advanced-rust-patterns">I want to learn advanced Rust patterns</a></h3>
<p>→ Focus on Architecture section (patterns)
→ Review Implementation for real-world examples
→ Study Advanced Topics for concurrency/performance</p>
<h3 id="im-building-something-similar"><a class="header" href="#im-building-something-similar">I'm building something similar</a></h3>
<p>→ Read Architecture + Implementation
→ Study formal documentation (SRS/SDD)
→ Review source code with this guide as reference</p>
<h2 id="conventions-used"><a class="header" href="#conventions-used">Conventions Used</a></h2>
<p>Throughout this guide:</p>
<ul>
<li><strong>Code examples</strong> are complete and runnable unless marked otherwise</li>
<li><strong>File paths</strong> use format <code>module/file.rs:line</code> for source references</li>
<li><strong>Diagrams</strong> are in PlantUML (SVG rendered in book)</li>
<li><strong>Callouts</strong> highlight important information:</li>
</ul>
<blockquote>
<p><strong>Note:</strong> Additional helpful information</p>
</blockquote>
<blockquote>
<p><strong>Warning:</strong> Important caveats or gotchas</p>
</blockquote>
<blockquote>
<p><strong>Example:</strong> Practical code demonstration</p>
</blockquote>
<h2 id="ready-to-start"><a class="header" href="#ready-to-start">Ready to Start?</a></h2>
<p>Choose your path:</p>
<ul>
<li><strong>New users:</strong> <a href="fundamentals/what-is-a-pipeline.html">What is a Pipeline?</a></li>
<li><strong>Contributors:</strong> <a href="architecture/overview.html">Architecture Overview</a></li>
<li><strong>Specific feature:</strong> Use search (press 's') or browse <a href="introduction.html#">table of contents</a></li>
</ul>
<p>Let's dive in!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="what-is-a-pipeline"><a class="header" href="#what-is-a-pipeline">What is a Pipeline?</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Introduction to pipelines and their purpose.</p>
<h2 id="what-is-a-pipeline-1"><a class="header" href="#what-is-a-pipeline-1">What is a Pipeline?</a></h2>
<p>A <strong>pipeline</strong> is a series of connected processing stages that transform data from input to output. Each stage performs a specific operation, and data flows through the stages sequentially or in parallel.</p>
<p>Think of it like a factory assembly line:</p>
<ul>
<li>Raw materials (input file) enter at one end</li>
<li>Each station (stage) performs a specific task</li>
<li>The finished product (processed file) exits at the other end</li>
</ul>
<h2 id="real-world-analogy"><a class="header" href="#real-world-analogy">Real-World Analogy</a></h2>
<h3 id="assembly-line"><a class="header" href="#assembly-line">Assembly Line</a></h3>
<p>Imagine an automobile assembly line:</p>
<pre><code>Raw Materials → Welding → Painting → Assembly → Quality Check → Finished Car
</code></pre>
<p>In our pipeline system:</p>
<pre><code>Input File → Compression → Encryption → Validation → Output File
</code></pre>
<p>Each stage:</p>
<ul>
<li>Receives data from the previous stage</li>
<li>Performs its specific transformation</li>
<li>Passes the result to the next stage</li>
</ul>
<h2 id="why-use-a-pipeline"><a class="header" href="#why-use-a-pipeline">Why Use a Pipeline?</a></h2>
<h3 id="modularity"><a class="header" href="#modularity">Modularity</a></h3>
<p>Each stage does one thing well. You can:</p>
<ul>
<li>Add new stages easily</li>
<li>Remove stages you don't need</li>
<li>Reorder stages as needed</li>
</ul>
<p><strong>Example</strong>: Need encryption? Add an encryption stage. Don't need compression? Remove the compression stage.</p>
<h3 id="reusability"><a class="header" href="#reusability">Reusability</a></h3>
<p>Stages can be used in multiple pipelines:</p>
<ul>
<li>Use the same compression stage in different workflows</li>
<li>Share validation logic across projects</li>
<li>Build libraries of reusable components</li>
</ul>
<h3 id="testability"><a class="header" href="#testability">Testability</a></h3>
<p>Each stage can be tested independently:</p>
<ul>
<li>Unit test individual stages</li>
<li>Mock stage inputs/outputs</li>
<li>Verify stage behavior in isolation</li>
</ul>
<h3 id="scalability"><a class="header" href="#scalability">Scalability</a></h3>
<p>Pipelines can process data efficiently:</p>
<ul>
<li>Process file chunks in parallel</li>
<li>Distribute work across CPU cores</li>
<li>Handle files of any size</li>
</ul>
<h2 id="our-pipeline-system"><a class="header" href="#our-pipeline-system">Our Pipeline System</a></h2>
<p>The Optimized Adaptive Pipeline provides:</p>
<p><strong>File Processing</strong>: Transform files through configurable stages</p>
<ul>
<li>Input: Any file type</li>
<li>Stages: Compression, encryption, validation</li>
<li>Output: Processed <code>.adapipe</code> file</li>
</ul>
<p><strong>Flexibility</strong>: Configure stages for your needs</p>
<ul>
<li>Enable/disable stages</li>
<li>Choose algorithms (Brotli, LZ4, Zstandard for compression)</li>
<li>Set security levels (Public → Top Secret)</li>
</ul>
<p><strong>Performance</strong>: Handle large files efficiently</p>
<ul>
<li>Stream processing (low memory usage)</li>
<li>Parallel chunk processing</li>
<li>Optimized algorithms</li>
</ul>
<p><strong>Security</strong>: Protect sensitive data</p>
<ul>
<li>AES-256-GCM encryption</li>
<li>Argon2 key derivation</li>
<li>Integrity verification with checksums</li>
</ul>
<h2 id="pipeline-flow"><a class="header" href="#pipeline-flow">Pipeline Flow</a></h2>
<p>Here's how data flows through the pipeline:</p>
<p><img src="fundamentals/../diagrams/pipeline-flow.svg" alt="Pipeline Flow" /></p>
<ol>
<li><strong>Input</strong>: Read file from disk</li>
<li><strong>Chunk</strong>: Split into manageable pieces (default 1MB)</li>
<li><strong>Process</strong>: Apply stages to each chunk
<ul>
<li>Compress (optional)</li>
<li>Encrypt (optional)</li>
<li>Calculate checksum (always)</li>
</ul>
</li>
<li><strong>Store</strong>: Write processed data and metadata</li>
<li><strong>Verify</strong>: Confirm integrity of output</li>
</ol>
<h2 id="what-you-can-do"><a class="header" href="#what-you-can-do">What You Can Do</a></h2>
<p>With this pipeline, you can:</p>
<p>✅ <strong>Compress files</strong> to save storage space
✅ <strong>Encrypt files</strong> to protect sensitive data
✅ <strong>Validate integrity</strong> to detect corruption
✅ <strong>Process large files</strong> without running out of memory
✅ <strong>Customize workflows</strong> with configurable stages
✅ <strong>Track metrics</strong> to monitor performance</p>
<h2 id="next-steps"><a class="header" href="#next-steps">Next Steps</a></h2>
<p>Continue to:</p>
<ul>
<li><a href="fundamentals/core-concepts.html">Core Concepts</a> - Key terminology and ideas</li>
<li><a href="fundamentals/stages.html">Pipeline Stages</a> - Understanding stage types</li>
<li><a href="fundamentals/configuration.html">Configuration Basics</a> - How to configure pipelines</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="core-concepts"><a class="header" href="#core-concepts">Core Concepts</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Essential concepts for understanding the pipeline.</p>
<h2 id="key-terminology"><a class="header" href="#key-terminology">Key Terminology</a></h2>
<h3 id="pipeline"><a class="header" href="#pipeline">Pipeline</a></h3>
<p>A complete file processing workflow with:</p>
<ul>
<li><strong>Unique ID</strong>: Every pipeline has a ULID identifier</li>
<li><strong>Input path</strong>: Source file to process</li>
<li><strong>Output path</strong>: Destination for processed data</li>
<li><strong>Stages</strong>: Ordered list of processing steps</li>
<li><strong>Status</strong>: Created → Running → Completed (or Failed)</li>
</ul>
<h3 id="stage"><a class="header" href="#stage">Stage</a></h3>
<p>An individual processing operation within a pipeline:</p>
<ul>
<li><strong>Type</strong>: Compression, Encryption, or Integrity Check</li>
<li><strong>Algorithm</strong>: Specific implementation (e.g., Brotli, AES-256-GCM)</li>
<li><strong>Sequence</strong>: Order in the pipeline (1, 2, 3, ...)</li>
<li><strong>Configuration</strong>: Stage-specific settings</li>
</ul>
<h3 id="file-chunk"><a class="header" href="#file-chunk">File Chunk</a></h3>
<p>A portion of a file processed independently:</p>
<ul>
<li><strong>Size</strong>: Configurable (default 1MB)</li>
<li><strong>Sequence</strong>: Chunk number (0, 1, 2, ...)</li>
<li><strong>Checksum</strong>: Integrity verification value</li>
<li><strong>Offset</strong>: Position in original file</li>
</ul>
<h2 id="core-components"><a class="header" href="#core-components">Core Components</a></h2>
<h3 id="entities"><a class="header" href="#entities">Entities</a></h3>
<p><strong>Pipeline Entity</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Pipeline {
    id: PipelineId,
    input_file_path: FilePath,
    output_file_path: FilePath,
    stages: Vec&lt;PipelineStage&gt;,
    status: PipelineStatus,
    created_at: DateTime,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>PipelineStage Entity</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>PipelineStage {
    id: StageId,
    pipeline_id: PipelineId,
    stage_type: StageType,
    algorithm: Algorithm,
    sequence_number: u32,
}
<span class="boring">}</span></code></pre></pre>
<h3 id="value-objects"><a class="header" href="#value-objects">Value Objects</a></h3>
<p><strong>FilePath</strong> - Validated file system path</p>
<ul>
<li>Must exist (for input) or be writable (for output)</li>
<li>Supports absolute and relative paths</li>
<li>Cross-platform compatibility</li>
</ul>
<p><strong>FileSize</strong> - File size in bytes</p>
<ul>
<li>Human-readable display (KB, MB, GB)</li>
<li>Validation for reasonable limits</li>
<li>Efficient storage representation</li>
</ul>
<p><strong>Algorithm</strong> - Processing algorithm specification</p>
<ul>
<li>Compression: Brotli, LZ4, Zstandard</li>
<li>Encryption: AES-256-GCM, ChaCha20-Poly1305</li>
<li>Checksum: Blake3, SHA-256</li>
</ul>
<h2 id="data-flow"><a class="header" href="#data-flow">Data Flow</a></h2>
<h3 id="sequential-processing"><a class="header" href="#sequential-processing">Sequential Processing</a></h3>
<p>Stages execute in order:</p>
<pre><code>Input → Stage 1 → Stage 2 → Stage 3 → Output
</code></pre>
<h3 id="parallel-chunk-processing"><a class="header" href="#parallel-chunk-processing">Parallel Chunk Processing</a></h3>
<p>Chunks process independently:</p>
<pre><code>Chunk 0 ──┐
Chunk 1 ──┼→ All go through stages → Reassemble
Chunk 2 ──┘
</code></pre>
<p>This enables:</p>
<ul>
<li><strong>Concurrency</strong>: Multiple chunks processed simultaneously</li>
<li><strong>Memory efficiency</strong>: Only active chunks in memory</li>
<li><strong>Scalability</strong>: Leverage multiple CPU cores</li>
</ul>
<h3 id="pipeline-execution-sequence"><a class="header" href="#pipeline-execution-sequence">Pipeline Execution Sequence</a></h3>
<p><img src="fundamentals/../diagrams/stage-execution.svg" alt="Stage Execution" /></p>
<ol>
<li><strong>CLI</strong> receives command</li>
<li><strong>Pipeline Service</strong> creates pipeline</li>
<li><strong>File Processor</strong> reads input file</li>
<li>For each chunk:
<ul>
<li>Apply compression (if enabled)</li>
<li>Apply encryption (if enabled)</li>
<li>Calculate checksum (always)</li>
<li>Store chunk metadata</li>
<li>Write processed chunk</li>
</ul>
</li>
<li>Update pipeline status</li>
<li>Return result to user</li>
</ol>
<h2 id="domain-model"><a class="header" href="#domain-model">Domain Model</a></h2>
<p>Our domain model follows Domain-Driven Design principles:</p>
<p><img src="fundamentals/../diagrams/domain-model.svg" alt="Domain Model" /></p>
<h3 id="aggregates"><a class="header" href="#aggregates">Aggregates</a></h3>
<p><strong>Pipeline Aggregate</strong> - The root entity</p>
<ul>
<li>Contains Pipeline entity</li>
<li>Manages associated FileChunks</li>
<li>Enforces business rules</li>
<li>Ensures consistency</li>
</ul>
<h3 id="relationships"><a class="header" href="#relationships">Relationships</a></h3>
<ul>
<li>Pipeline <strong>has many</strong> PipelineStages (1:N)</li>
<li>Pipeline <strong>processes</strong> FileChunks (1:N)</li>
<li>FileChunk <strong>belongs to</strong> Pipeline (N:1)</li>
<li>PipelineStage <strong>uses</strong> Algorithm (N:1)</li>
</ul>
<h2 id="processing-guarantees"><a class="header" href="#processing-guarantees">Processing Guarantees</a></h2>
<h3 id="integrity"><a class="header" href="#integrity">Integrity</a></h3>
<p>Every chunk has a checksum:</p>
<ul>
<li>Calculated after processing</li>
<li>Verified on read/restore</li>
<li>Detects any corruption</li>
</ul>
<h3 id="atomicity"><a class="header" href="#atomicity">Atomicity</a></h3>
<p>Pipeline operations are transactional:</p>
<ul>
<li>All stages complete, or none do</li>
<li>Metadata stored consistently</li>
<li>No partial outputs on failure</li>
</ul>
<h3 id="durability"><a class="header" href="#durability">Durability</a></h3>
<p>Processed data is persisted:</p>
<ul>
<li>SQLite database for metadata</li>
<li>File system for binary data</li>
<li>Recoverable after crashes</li>
</ul>
<h2 id="next-steps-1"><a class="header" href="#next-steps-1">Next Steps</a></h2>
<p>Continue to:</p>
<ul>
<li><a href="fundamentals/stages.html">Pipeline Stages</a> - Types of stages available</li>
<li><a href="fundamentals/configuration.html">Configuration Basics</a> - How to configure pipelines</li>
<li><a href="fundamentals/first-run.html">Running Your First Pipeline</a> - Hands-on tutorial</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pipeline-stages"><a class="header" href="#pipeline-stages">Pipeline Stages</a></h1>
<p><strong>Version:</strong> 1.0
<strong>Date:</strong> 2025-01-04
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner, Claude Code
<strong>Status:</strong> Active</p>
<h2 id="what-is-a-stage"><a class="header" href="#what-is-a-stage">What is a Stage?</a></h2>
<p>A <strong>pipeline stage</strong> is a single processing operation that transforms data in a specific way. Each stage performs one well-defined task, like compressing data, encrypting it, or verifying its integrity.</p>
<p>Think of stages like workstations on an assembly line. Each workstation has specialized tools and performs one specific operation. The product moves from one workstation to the next until it's complete.</p>
<h2 id="stage-types"><a class="header" href="#stage-types">Stage Types</a></h2>
<p>Our pipeline supports three main categories of stages:</p>
<h3 id="1-compression-stages"><a class="header" href="#1-compression-stages">1. Compression Stages</a></h3>
<p>Compression stages reduce the size of your data. This is useful for:</p>
<ul>
<li>Saving disk space</li>
<li>Reducing network bandwidth</li>
<li>Faster file transfers</li>
<li>Lower storage costs</li>
</ul>
<p><strong>Available Compression Algorithms:</strong></p>
<ul>
<li>
<p><strong>Brotli</strong> - Best compression ratio, slower speed</p>
<ul>
<li>Best for: Text files, web content, logs</li>
<li>Performance: Excellent compression, moderate speed</li>
<li>Memory: Higher memory usage</li>
</ul>
</li>
<li>
<p><strong>Gzip</strong> - General-purpose compression</p>
<ul>
<li>Best for: General files, wide compatibility</li>
<li>Performance: Good balance of speed and ratio</li>
<li>Memory: Moderate memory usage</li>
</ul>
</li>
<li>
<p><strong>Zstandard (zstd)</strong> - Modern, fast compression</p>
<ul>
<li>Best for: Large files, real-time compression</li>
<li>Performance: Excellent speed and ratio</li>
<li>Memory: Efficient memory usage</li>
</ul>
</li>
<li>
<p><strong>LZ4</strong> - Extremely fast compression</p>
<ul>
<li>Best for: Real-time applications, live data streams</li>
<li>Performance: Fastest compression, moderate ratio</li>
<li>Memory: Low memory usage</li>
</ul>
</li>
</ul>
<h3 id="2-encryption-stages"><a class="header" href="#2-encryption-stages">2. Encryption Stages</a></h3>
<p>Encryption stages protect your data by making it unreadable without the correct key. This is essential for:</p>
<ul>
<li>Protecting sensitive information</li>
<li>Compliance with security regulations</li>
<li>Secure data transmission</li>
<li>Privacy protection</li>
</ul>
<p><strong>Available Encryption Algorithms:</strong></p>
<ul>
<li>
<p><strong>AES-256-GCM</strong> - Industry standard encryption</p>
<ul>
<li>Key Size: 256 bits (32 bytes)</li>
<li>Security: FIPS approved, very strong</li>
<li>Performance: Excellent with AES-NI hardware support</li>
<li>Authentication: Built-in integrity verification</li>
</ul>
</li>
<li>
<p><strong>ChaCha20-Poly1305</strong> - Modern stream cipher</p>
<ul>
<li>Key Size: 256 bits (32 bytes)</li>
<li>Security: Strong, constant-time implementation</li>
<li>Performance: Consistent across all platforms</li>
<li>Authentication: Built-in integrity verification</li>
</ul>
</li>
<li>
<p><strong>AES-128-GCM</strong> - Faster AES variant</p>
<ul>
<li>Key Size: 128 bits (16 bytes)</li>
<li>Security: Still very secure, slightly faster</li>
<li>Performance: Faster than AES-256</li>
<li>Authentication: Built-in integrity verification</li>
</ul>
</li>
</ul>
<h3 id="3-integrity-verification-stages"><a class="header" href="#3-integrity-verification-stages">3. Integrity Verification Stages</a></h3>
<p>Integrity stages ensure your data hasn't been corrupted or tampered with. They create a unique "fingerprint" of your data called a checksum or hash.</p>
<p><strong>Available Hashing Algorithms:</strong></p>
<ul>
<li>
<p><strong>SHA-256</strong> - Industry standard hashing</p>
<ul>
<li>Output: 256 bits (32 bytes)</li>
<li>Security: Cryptographically secure</li>
<li>Performance: Good balance</li>
<li>Use Case: General integrity verification</li>
</ul>
</li>
<li>
<p><strong>SHA-512</strong> - Stronger SHA variant</p>
<ul>
<li>Output: 512 bits (64 bytes)</li>
<li>Security: Stronger than SHA-256</li>
<li>Performance: Good on 64-bit systems</li>
<li>Use Case: High-security applications</li>
</ul>
</li>
<li>
<p><strong>BLAKE3</strong> - Modern, high-performance hashing</p>
<ul>
<li>Output: 256 bits (32 bytes)</li>
<li>Security: Strong security properties</li>
<li>Performance: Very fast</li>
<li>Use Case: High-performance applications</li>
</ul>
</li>
</ul>
<h2 id="stage-configuration"><a class="header" href="#stage-configuration">Stage Configuration</a></h2>
<p>Each stage has a configuration that specifies how it should process data:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::{PipelineStage, Algorithm};

// Example: Compression stage
let compression_stage = PipelineStage::new(
    "compress".to_string(),
    Algorithm::zstd(),
    1, // stage order
)?;

// Example: Encryption stage
let encryption_stage = PipelineStage::new(
    "encrypt".to_string(),
    Algorithm::aes_256_gcm(),
    2, // stage order
)?;

// Example: Integrity verification stage
let integrity_stage = PipelineStage::new(
    "verify".to_string(),
    Algorithm::sha256(),
    3, // stage order
)?;
<span class="boring">}</span></code></pre></pre>
<h2 id="stage-execution-order"><a class="header" href="#stage-execution-order">Stage Execution Order</a></h2>
<p>Stages execute in the order you define them. The output of one stage becomes the input to the next stage.</p>
<p><strong>Recommended Order for Processing:</strong></p>
<ol>
<li>Compress (reduce size first)</li>
<li>Encrypt (protect compressed data)</li>
<li>Verify integrity (create checksum of encrypted data)</li>
</ol>
<p><strong>For Restoration (reverse order):</strong></p>
<ol>
<li>Verify integrity (check encrypted data)</li>
<li>Decrypt (recover compressed data)</li>
<li>Decompress (restore original file)</li>
</ol>
<pre><code class="language-text">Processing Pipeline:
Input File → Compress → Encrypt → Verify → Output File

Restoration Pipeline:
Input File → Verify → Decrypt → Decompress → Output File
</code></pre>
<h2 id="combining-stages"><a class="header" href="#combining-stages">Combining Stages</a></h2>
<p>You can combine stages in different ways depending on your needs:</p>
<h3 id="maximum-security"><a class="header" href="#maximum-security">Maximum Security</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>vec![
    PipelineStage::new("compress", Algorithm::brotli(), 1)?,
    PipelineStage::new("encrypt", Algorithm::aes_256_gcm(), 2)?,
    PipelineStage::new("verify", Algorithm::blake3(), 3)?,
]
<span class="boring">}</span></code></pre></pre>
<h3 id="maximum-speed"><a class="header" href="#maximum-speed">Maximum Speed</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>vec![
    PipelineStage::new("compress", Algorithm::lz4(), 1)?,
    PipelineStage::new("encrypt", Algorithm::chacha20_poly1305(), 2)?,
]
<span class="boring">}</span></code></pre></pre>
<h3 id="balanced-approach"><a class="header" href="#balanced-approach">Balanced Approach</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>vec![
    PipelineStage::new("compress", Algorithm::zstd(), 1)?,
    PipelineStage::new("encrypt", Algorithm::aes_256_gcm(), 2)?,
    PipelineStage::new("verify", Algorithm::sha256(), 3)?,
]
<span class="boring">}</span></code></pre></pre>
<h2 id="parallel-processing"><a class="header" href="#parallel-processing">Parallel Processing</a></h2>
<p>Stages process file chunks in parallel for better performance:</p>
<pre><code class="language-text">File Split into Chunks:
┌──────┬──────┬──────┬──────┐
│Chunk1│Chunk2│Chunk3│Chunk4│
└──┬───┴──┬───┴──┬───┴──┬───┘
   │      │      │      │
   ▼      ▼      ▼      ▼
   ┌──────┬──────┬──────┬──────┐
   │Stage1│Stage1│Stage1│Stage1│ (Parallel)
   └──┬───┴──┬───┴──┬───┴──┬───┘
      ▼      ▼      ▼      ▼
   ┌──────┬──────┬──────┬──────┐
   │Stage2│Stage2│Stage2│Stage2│ (Parallel)
   └──┬───┴──┬───┴──┬───┴──┬───┘
      │      │      │      │
      ▼      ▼      ▼      ▼
   Combined Output File
</code></pre>
<p>This parallel processing allows the pipeline to utilize multiple CPU cores for faster throughput.</p>
<h2 id="stage-validation"><a class="header" href="#stage-validation">Stage Validation</a></h2>
<p>The pipeline validates stages at creation time:</p>
<ul>
<li><strong>Algorithm compatibility</strong>: Ensures compression algorithms are only used in compression stages</li>
<li><strong>Stage order</strong>: Verifies stages have unique, sequential order numbers</li>
<li><strong>Configuration validity</strong>: Checks all stage parameters are valid</li>
<li><strong>Dependency checks</strong>: Ensures restoration pipelines match processing pipelines</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// This will fail - wrong algorithm for stage type
PipelineStage::new(
    "compress",
    Algorithm::aes_256_gcm(), // Encryption algorithm!
    1
) // ❌ Error: Algorithm not compatible with stage type
<span class="boring">}</span></code></pre></pre>
<h2 id="next-steps-2"><a class="header" href="#next-steps-2">Next Steps</a></h2>
<p>Now that you understand pipeline stages, you can learn about:</p>
<ul>
<li><a href="fundamentals/configuration.html">Configuration</a> - How to configure pipelines and stages</li>
<li><a href="fundamentals/first-run.html">Your First Pipeline</a> - Run your first pipeline</li>
<li><a href="fundamentals/../architecture/overview.html">Architecture Overview</a> - Deeper dive into the architecture</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="configuration-basics"><a class="header" href="#configuration-basics">Configuration Basics</a></h1>
<p><strong>Version:</strong> 1.0
<strong>Date:</strong> 2025-01-04
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner, Claude Code
<strong>Status:</strong> Active</p>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>The pipeline system provides flexible configuration through command-line options, environment variables, and configuration files. This chapter covers the basics of configuring your pipelines.</p>
<h2 id="command-line-interface"><a class="header" href="#command-line-interface">Command-Line Interface</a></h2>
<p>The pipeline CLI provides several commands for managing and running pipelines.</p>
<h3 id="basic-commands"><a class="header" href="#basic-commands">Basic Commands</a></h3>
<h4 id="process-a-file"><a class="header" href="#process-a-file">Process a File</a></h4>
<pre><code class="language-bash">pipeline process \
  --input /path/to/input.txt \
  --output /path/to/output.bin \
  --pipeline my-pipeline
</code></pre>
<h4 id="create-a-pipeline"><a class="header" href="#create-a-pipeline">Create a Pipeline</a></h4>
<pre><code class="language-bash">pipeline create \
  --name my-pipeline \
  --stages compression,encryption,integrity
</code></pre>
<h4 id="list-pipelines"><a class="header" href="#list-pipelines">List Pipelines</a></h4>
<pre><code class="language-bash">pipeline list
</code></pre>
<h4 id="show-pipeline-details"><a class="header" href="#show-pipeline-details">Show Pipeline Details</a></h4>
<pre><code class="language-bash">pipeline show my-pipeline
</code></pre>
<h4 id="delete-a-pipeline"><a class="header" href="#delete-a-pipeline">Delete a Pipeline</a></h4>
<pre><code class="language-bash">pipeline delete my-pipeline --force
</code></pre>
<h3 id="performance-options"><a class="header" href="#performance-options">Performance Options</a></h3>
<h4 id="cpu-threads"><a class="header" href="#cpu-threads">CPU Threads</a></h4>
<p>Control the number of worker threads for CPU-bound operations (compression, encryption):</p>
<pre><code class="language-bash">pipeline process \
  --input file.txt \
  --output file.bin \
  --pipeline my-pipeline \
  --cpu-threads 8
</code></pre>
<p><strong>Default:</strong> Number of CPU cores - 1 (reserves one core for I/O)</p>
<p><strong>Tips:</strong></p>
<ul>
<li>Too high: CPU thrashing, context switching overhead</li>
<li>Too low: Underutilized cores, slower processing</li>
<li>Monitor CPU saturation metrics to tune</li>
</ul>
<h4 id="io-threads"><a class="header" href="#io-threads">I/O Threads</a></h4>
<p>Control the number of concurrent I/O operations:</p>
<pre><code class="language-bash">pipeline process \
  --input file.txt \
  --output file.bin \
  --pipeline my-pipeline \
  --io-threads 24
</code></pre>
<p><strong>Default:</strong> Device-specific (NVMe: 24, SSD: 12, HDD: 4)</p>
<p><strong>Storage Type Detection:</strong></p>
<pre><code class="language-bash">pipeline process \
  --input file.txt \
  --output file.bin \
  --pipeline my-pipeline \
  --storage-type nvme  # or ssd, hdd
</code></pre>
<h4 id="channel-depth"><a class="header" href="#channel-depth">Channel Depth</a></h4>
<p>Control backpressure in the pipeline stages:</p>
<pre><code class="language-bash">pipeline process \
  --input file.txt \
  --output file.bin \
  --pipeline my-pipeline \
  --channel-depth 8
</code></pre>
<p><strong>Default:</strong> 4</p>
<p><strong>Tips:</strong></p>
<ul>
<li>Lower values: Less memory, may cause pipeline stalls</li>
<li>Higher values: More buffering, higher memory usage</li>
<li>Optimal value depends on chunk processing time and I/O latency</li>
</ul>
<h4 id="chunk-size"><a class="header" href="#chunk-size">Chunk Size</a></h4>
<p>Configure the size of file chunks for parallel processing:</p>
<pre><code class="language-bash">pipeline process \
  --input file.txt \
  --output file.bin \
  --pipeline my-pipeline \
  --chunk-size-mb 10
</code></pre>
<p><strong>Default:</strong> Automatically determined based on file size and available resources</p>
<h3 id="global-options"><a class="header" href="#global-options">Global Options</a></h3>
<h4 id="verbose-logging"><a class="header" href="#verbose-logging">Verbose Logging</a></h4>
<p>Enable detailed logging output:</p>
<pre><code class="language-bash">pipeline --verbose process \
  --input file.txt \
  --output file.bin \
  --pipeline my-pipeline
</code></pre>
<h4 id="configuration-file"><a class="header" href="#configuration-file">Configuration File</a></h4>
<p>Use a custom configuration file:</p>
<pre><code class="language-bash">pipeline --config /path/to/config.toml process \
  --input file.txt \
  --output file.bin \
  --pipeline my-pipeline
</code></pre>
<h2 id="configuration-files"><a class="header" href="#configuration-files">Configuration Files</a></h2>
<p>Configuration files use TOML format and allow you to save pipeline settings for reuse.</p>
<h3 id="basic-configuration"><a class="header" href="#basic-configuration">Basic Configuration</a></h3>
<pre><code class="language-toml">[pipeline]
name = "my-pipeline"
stages = ["compression", "encryption", "integrity"]

[performance]
cpu_threads = 8
io_threads = 24
channel_depth = 4

[processing]
chunk_size_mb = 10
</code></pre>
<h3 id="algorithm-configuration"><a class="header" href="#algorithm-configuration">Algorithm Configuration</a></h3>
<pre><code class="language-toml">[stages.compression]
algorithm = "zstd"

[stages.encryption]
algorithm = "aes-256-gcm"
key_file = "/path/to/keyfile"

[stages.integrity]
algorithm = "sha256"
</code></pre>
<h3 id="complete-example"><a class="header" href="#complete-example">Complete Example</a></h3>
<pre><code class="language-toml"># Pipeline configuration example
[pipeline]
name = "secure-archival"
description = "High compression with encryption for archival"

[stages.compression]
algorithm = "brotli"
level = 11  # Maximum compression

[stages.encryption]
algorithm = "aes-256-gcm"
key_derivation = "argon2"

[stages.integrity]
algorithm = "blake3"

[performance]
cpu_threads = 16
io_threads = 24
channel_depth = 8
storage_type = "nvme"

[processing]
chunk_size_mb = 64
parallel_workers = 16
</code></pre>
<h3 id="using-configuration-files"><a class="header" href="#using-configuration-files">Using Configuration Files</a></h3>
<pre><code class="language-bash"># Use a configuration file
pipeline --config secure-archival.toml process \
  --input large-dataset.tar \
  --output large-dataset.bin

# Override configuration file settings
pipeline --config secure-archival.toml \
  --cpu-threads 8 \
  process --input file.txt --output file.bin
</code></pre>
<h2 id="environment-variables"><a class="header" href="#environment-variables">Environment Variables</a></h2>
<p>Environment variables provide another way to configure the pipeline:</p>
<pre><code class="language-bash"># Set performance defaults
export PIPELINE_CPU_THREADS=8
export PIPELINE_IO_THREADS=24
export PIPELINE_CHANNEL_DEPTH=8

# Set default chunk size
export PIPELINE_CHUNK_SIZE_MB=10

# Enable verbose logging
export PIPELINE_VERBOSE=true

# Run pipeline
pipeline process --input file.txt --output file.bin --pipeline my-pipeline
</code></pre>
<h2 id="configuration-priority"><a class="header" href="#configuration-priority">Configuration Priority</a></h2>
<p>When the same setting is configured in multiple places, the following priority applies (highest to lowest):</p>
<ol>
<li><strong>Command-line arguments</strong> - Explicit flags like <code>--cpu-threads</code></li>
<li><strong>Environment variables</strong> - <code>PIPELINE_*</code> variables</li>
<li><strong>Configuration file</strong> - Settings from <code>--config</code> file</li>
<li><strong>Default values</strong> - Built-in intelligent defaults</li>
</ol>
<p>Example:</p>
<pre><code class="language-bash"># Config file says cpu_threads = 8
# Environment says PIPELINE_CPU_THREADS=12
# Command line says --cpu-threads=16

# Result: Uses 16 (command-line wins)
</code></pre>
<h2 id="performance-tuning-guidelines"><a class="header" href="#performance-tuning-guidelines">Performance Tuning Guidelines</a></h2>
<h3 id="for-maximum-speed"><a class="header" href="#for-maximum-speed">For Maximum Speed</a></h3>
<ul>
<li>Use LZ4 compression</li>
<li>Use ChaCha20-Poly1305 encryption</li>
<li>Increase CPU threads to match cores</li>
<li>Use large chunks (32-64 MB)</li>
<li>Higher channel depth (8-16)</li>
</ul>
<pre><code class="language-bash">pipeline process \
  --input file.txt \
  --output file.bin \
  --pipeline speed-pipeline \
  --cpu-threads 16 \
  --chunk-size-mb 64 \
  --channel-depth 16
</code></pre>
<h3 id="for-maximum-compression"><a class="header" href="#for-maximum-compression">For Maximum Compression</a></h3>
<ul>
<li>Use Brotli compression</li>
<li>Smaller chunks for better compression ratio</li>
<li>More CPU threads for parallel compression</li>
</ul>
<pre><code class="language-bash">pipeline process \
  --input file.txt \
  --output file.bin \
  --pipeline compression-pipeline \
  --cpu-threads 16 \
  --chunk-size-mb 4
</code></pre>
<h3 id="for-resource-constrained-systems"><a class="header" href="#for-resource-constrained-systems">For Resource-Constrained Systems</a></h3>
<ul>
<li>Reduce CPU and I/O threads</li>
<li>Smaller chunks</li>
<li>Lower channel depth</li>
</ul>
<pre><code class="language-bash">pipeline process \
  --input file.txt \
  --output file.bin \
  --pipeline minimal-pipeline \
  --cpu-threads 2 \
  --io-threads 4 \
  --chunk-size-mb 2 \
  --channel-depth 2
</code></pre>
<h2 id="next-steps-3"><a class="header" href="#next-steps-3">Next Steps</a></h2>
<p>Now that you understand configuration, you're ready to:</p>
<ul>
<li><a href="fundamentals/first-run.html">Run Your First Pipeline</a> - Step-by-step tutorial</li>
<li><a href="fundamentals/stages.html">Learn About Stages</a> - Deep dive into pipeline stages</li>
<li><a href="fundamentals/../architecture/overview.html">Explore Architecture</a> - Understand the system design</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="running-your-first-pipeline"><a class="header" href="#running-your-first-pipeline">Running Your First Pipeline</a></h1>
<p><strong>Version:</strong> 1.0
<strong>Date:</strong> 2025-01-04
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner, Claude Code
<strong>Status:</strong> Active</p>
<h2 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h2>
<p>Before running your first pipeline, ensure you have:</p>
<ul>
<li>
<p><strong>Pipeline binary</strong> - Built and available in your PATH</p>
<pre><code class="language-bash">cargo build --release
cp target/release/pipeline /usr/local/bin/  # or add to PATH
</code></pre>
</li>
<li>
<p><strong>Test file</strong> - A sample file to process</p>
<pre><code class="language-bash">echo "Hello, Pipeline World!" &gt; test.txt
</code></pre>
</li>
<li>
<p><strong>Permissions</strong> - Read/write access to input and output directories</p>
</li>
</ul>
<h2 id="quick-start-5-minutes"><a class="header" href="#quick-start-5-minutes">Quick Start (5 minutes)</a></h2>
<p>Let's run a simple compression and encryption pipeline in 3 steps:</p>
<h3 id="step-1-create-a-pipeline"><a class="header" href="#step-1-create-a-pipeline">Step 1: Create a Pipeline</a></h3>
<pre><code class="language-bash">pipeline create \
  --name my-first-pipeline \
  --stages compression,encryption
</code></pre>
<p>You should see output like:</p>
<pre><code>✓ Created pipeline: my-first-pipeline
  Stages: compression (zstd), encryption (aes-256-gcm)
</code></pre>
<h3 id="step-2-process-a-file"><a class="header" href="#step-2-process-a-file">Step 2: Process a File</a></h3>
<pre><code class="language-bash">pipeline process \
  --input test.txt \
  --output test.bin \
  --pipeline my-first-pipeline
</code></pre>
<p>You should see progress output:</p>
<pre><code>Processing: test.txt
Pipeline: my-first-pipeline
  Stage 1/2: Compression (zstd)... ✓
  Stage 2/2: Encryption (aes-256-gcm)... ✓
Output: test.bin (24 bytes)
Time: 0.05s
</code></pre>
<h3 id="step-3-restore-the-file"><a class="header" href="#step-3-restore-the-file">Step 3: Restore the File</a></h3>
<pre><code class="language-bash">pipeline restore \
  --input test.bin \
  --output restored.txt
</code></pre>
<p>Verify the restoration:</p>
<pre><code class="language-bash">diff test.txt restored.txt
# No output = files are identical ✓
</code></pre>
<h2 id="detailed-walkthrough"><a class="header" href="#detailed-walkthrough">Detailed Walkthrough</a></h2>
<p>Let's explore each step in more detail.</p>
<h3 id="creating-pipelines"><a class="header" href="#creating-pipelines">Creating Pipelines</a></h3>
<h4 id="basic-pipeline"><a class="header" href="#basic-pipeline">Basic Pipeline</a></h4>
<pre><code class="language-bash">pipeline create \
  --name basic \
  --stages compression
</code></pre>
<p>This creates a simple compression-only pipeline using default settings (zstd compression).</p>
<h4 id="secure-pipeline"><a class="header" href="#secure-pipeline">Secure Pipeline</a></h4>
<pre><code class="language-bash">pipeline create \
  --name secure \
  --stages compression,encryption,integrity
</code></pre>
<p>This creates a complete security pipeline with:</p>
<ul>
<li>Compression (reduces size)</li>
<li>Encryption (protects data)</li>
<li>Integrity verification (detects tampering)</li>
</ul>
<h4 id="save-pipeline-configuration"><a class="header" href="#save-pipeline-configuration">Save Pipeline Configuration</a></h4>
<pre><code class="language-bash">pipeline create \
  --name archival \
  --stages compression,encryption \
  --output archival-pipeline.toml
</code></pre>
<p>This saves the pipeline configuration to a file for reuse.</p>
<h3 id="processing-files"><a class="header" href="#processing-files">Processing Files</a></h3>
<h4 id="basic-processing"><a class="header" href="#basic-processing">Basic Processing</a></h4>
<pre><code class="language-bash"># Process a file
pipeline process \
  --input large-file.log \
  --output large-file.bin \
  --pipeline secure
</code></pre>
<h4 id="with-performance-options"><a class="header" href="#with-performance-options">With Performance Options</a></h4>
<pre><code class="language-bash"># Process with custom settings
pipeline process \
  --input large-file.log \
  --output large-file.bin \
  --pipeline secure \
  --cpu-threads 8 \
  --chunk-size-mb 32
</code></pre>
<h4 id="with-verbose-logging"><a class="header" href="#with-verbose-logging">With Verbose Logging</a></h4>
<pre><code class="language-bash"># See detailed progress
pipeline --verbose process \
  --input large-file.log \
  --output large-file.bin \
  --pipeline secure
</code></pre>
<h3 id="restoring-files"><a class="header" href="#restoring-files">Restoring Files</a></h3>
<p>The pipeline automatically detects the processing stages from the output file's metadata:</p>
<pre><code class="language-bash"># Restore automatically reverses all stages
pipeline restore \
  --input large-file.bin \
  --output restored-file.log
</code></pre>
<p>The system will:</p>
<ol>
<li>Read metadata from the file header</li>
<li>Apply stages in reverse order</li>
<li>Verify integrity if available</li>
<li>Restore original file</li>
</ol>
<h3 id="managing-pipelines"><a class="header" href="#managing-pipelines">Managing Pipelines</a></h3>
<h4 id="list-all-pipelines"><a class="header" href="#list-all-pipelines">List All Pipelines</a></h4>
<pre><code class="language-bash">pipeline list
</code></pre>
<p>Output:</p>
<pre><code>Available Pipelines:
  - my-first-pipeline (compression, encryption)
  - secure (compression, encryption, integrity)
  - archival (compression, encryption)
</code></pre>
<h4 id="show-pipeline-details-1"><a class="header" href="#show-pipeline-details-1">Show Pipeline Details</a></h4>
<pre><code class="language-bash">pipeline show secure
</code></pre>
<p>Output:</p>
<pre><code>Pipeline: secure
  Stage 1: Compression (zstd)
  Stage 2: Encryption (aes-256-gcm)
  Stage 3: Integrity (sha256)
Created: 2025-01-04 10:30:00
</code></pre>
<h4 id="delete-a-pipeline-1"><a class="header" href="#delete-a-pipeline-1">Delete a Pipeline</a></h4>
<pre><code class="language-bash">pipeline delete my-first-pipeline --force
</code></pre>
<h2 id="understanding-output"><a class="header" href="#understanding-output">Understanding Output</a></h2>
<h3 id="successful-processing"><a class="header" href="#successful-processing">Successful Processing</a></h3>
<p>When processing completes successfully:</p>
<pre><code>Processing: test.txt
Pipeline: my-first-pipeline
  Stage 1/2: Compression (zstd)... ✓
  Stage 2/2: Encryption (aes-256-gcm)... ✓

Statistics:
  Input size:  1,024 KB
  Output size: 512 KB
  Compression ratio: 50%
  Processing time: 0.15s
  Throughput: 6.8 MB/s

Output: test.bin
</code></pre>
<h3 id="performance-metrics"><a class="header" href="#performance-metrics">Performance Metrics</a></h3>
<p>With <code>--verbose</code> flag, you'll see detailed metrics:</p>
<pre><code>Pipeline Execution Metrics:
  Chunks processed: 64
  Parallel workers: 8
  Average chunk time: 2.3ms
  CPU utilization: 87%
  I/O wait: 3%

Stage Breakdown:
  Compression: 0.08s (53%)
  Encryption: 0.05s (33%)
  I/O: 0.02s (14%)
</code></pre>
<h3 id="error-messages"><a class="header" href="#error-messages">Error Messages</a></h3>
<h4 id="file-not-found"><a class="header" href="#file-not-found">File Not Found</a></h4>
<pre><code>Error: Input file not found: test.txt
  Check the file path and try again
</code></pre>
<h4 id="permission-denied"><a class="header" href="#permission-denied">Permission Denied</a></h4>
<pre><code>Error: Permission denied: /protected/output.bin
  Ensure you have write access to the output directory
</code></pre>
<h4 id="invalid-pipeline"><a class="header" href="#invalid-pipeline">Invalid Pipeline</a></h4>
<pre><code>Error: Pipeline not found: nonexistent
  Use 'pipeline list' to see available pipelines
</code></pre>
<h2 id="common-scenarios"><a class="header" href="#common-scenarios">Common Scenarios</a></h2>
<h3 id="scenario-1-compress-large-log-files"><a class="header" href="#scenario-1-compress-large-log-files">Scenario 1: Compress Large Log Files</a></h3>
<pre><code class="language-bash"># Create compression pipeline
pipeline create --name logs --stages compression

# Process log files
pipeline process \
  --input app.log \
  --output app.log.bin \
  --pipeline logs \
  --chunk-size-mb 64

# Compression ratio is typically 70-90% for text logs
</code></pre>
<h3 id="scenario-2-secure-sensitive-files"><a class="header" href="#scenario-2-secure-sensitive-files">Scenario 2: Secure Sensitive Files</a></h3>
<pre><code class="language-bash"># Create secure pipeline with all protections
pipeline create --name sensitive --stages compression,encryption,integrity

# Process sensitive file
pipeline process \
  --input customer-data.csv \
  --output customer-data.bin \
  --pipeline sensitive

# File is now compressed, encrypted, and tamper-evident
</code></pre>
<h3 id="scenario-3-high-performance-batch-processing"><a class="header" href="#scenario-3-high-performance-batch-processing">Scenario 3: High-Performance Batch Processing</a></h3>
<pre><code class="language-bash"># Process multiple files with optimized settings
for file in data/*.csv; do
  pipeline process \
    --input "$file" \
    --output "processed/$(basename $file).bin" \
    --pipeline fast \
    --cpu-threads 16 \
    --chunk-size-mb 128 \
    --channel-depth 16
done
</code></pre>
<h3 id="scenario-4-restore-and-verify"><a class="header" href="#scenario-4-restore-and-verify">Scenario 4: Restore and Verify</a></h3>
<pre><code class="language-bash"># Restore file
pipeline restore \
  --input customer-data.bin \
  --output customer-data-restored.csv

# Verify restoration
sha256sum customer-data.csv customer-data-restored.csv
# Both checksums should match
</code></pre>
<h2 id="testing-your-pipeline"><a class="header" href="#testing-your-pipeline">Testing Your Pipeline</a></h2>
<h3 id="create-test-data"><a class="header" href="#create-test-data">Create Test Data</a></h3>
<pre><code class="language-bash"># Create a test file
dd if=/dev/urandom of=test-10mb.bin bs=1M count=10

# Calculate original checksum
sha256sum test-10mb.bin &gt; original.sha256
</code></pre>
<h3 id="process-and-restore"><a class="header" href="#process-and-restore">Process and Restore</a></h3>
<pre><code class="language-bash"># Process the file
pipeline process \
  --input test-10mb.bin \
  --output test-10mb.processed \
  --pipeline my-first-pipeline

# Restore the file
pipeline restore \
  --input test-10mb.processed \
  --output test-10mb.restored
</code></pre>
<h3 id="verify-integrity"><a class="header" href="#verify-integrity">Verify Integrity</a></h3>
<pre><code class="language-bash"># Verify restored file matches original
sha256sum -c original.sha256
# Should output: test-10mb.bin: OK
</code></pre>
<h2 id="next-steps-4"><a class="header" href="#next-steps-4">Next Steps</a></h2>
<p>Congratulations! You've run your first pipeline. Now you can:</p>
<ul>
<li>
<p><strong>Explore Advanced Features</strong></p>
<ul>
<li><a href="fundamentals/../architecture/overview.html">Architecture Overview</a> - Understand the system design</li>
<li><a href="fundamentals/../implementation/compression.html">Implementation Details</a> - Learn about algorithms</li>
<li><a href="fundamentals/../advanced/performance.html">Performance Tuning</a> - Optimize for your use case</li>
</ul>
</li>
<li>
<p><strong>Learn More About Configuration</strong></p>
<ul>
<li><a href="fundamentals/configuration.html">Configuration Guide</a> - Detailed configuration options</li>
<li><a href="fundamentals/stages.html">Stage Types</a> - Available processing stages</li>
</ul>
</li>
<li>
<p><strong>Build Custom Pipelines</strong></p>
<ul>
<li>Experiment with different stage combinations</li>
<li>Test different algorithms for your workload</li>
<li>Benchmark performance with your data</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="architecture-overview"><a class="header" href="#architecture-overview">Architecture Overview</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>High-level architectural overview of the pipeline system.</p>
<h2 id="design-philosophy"><a class="header" href="#design-philosophy">Design Philosophy</a></h2>
<p>The Optimized Adaptive Pipeline is built on three foundational architectural patterns:</p>
<ol>
<li><strong>Clean Architecture</strong> - Organizing code by dependency direction</li>
<li><strong>Domain-Driven Design (DDD)</strong> - Modeling the business domain</li>
<li><strong>Hexagonal Architecture</strong> - Isolating business logic from infrastructure</li>
</ol>
<p>These patterns work together to create a maintainable, testable, and flexible system.</p>
<h2 id="layered-architecture"><a class="header" href="#layered-architecture">Layered Architecture</a></h2>
<p>The pipeline follows a strict layered architecture where dependencies flow inward:</p>
<p><img src="architecture/../diagrams/layered-architecture.svg" alt="Layered Architecture" /></p>
<h3 id="layer-overview"><a class="header" href="#layer-overview">Layer Overview</a></h3>
<p><strong>Presentation Layer</strong> (Outermost)</p>
<ul>
<li>CLI interface for user interaction</li>
<li>Configuration management</li>
<li>Request/response handling</li>
</ul>
<p><strong>Application Layer</strong></p>
<ul>
<li>Use cases and application services</li>
<li>Pipeline orchestration</li>
<li>File processing coordination</li>
</ul>
<p><strong>Domain Layer</strong> (Core)</p>
<ul>
<li>Business logic and rules</li>
<li>Entities (Pipeline, PipelineStage)</li>
<li>Value objects (FilePath, FileSize, Algorithm)</li>
<li>Domain services</li>
</ul>
<p><strong>Infrastructure Layer</strong> (Outermost)</p>
<ul>
<li>Database implementations (SQLite)</li>
<li>File system operations</li>
<li>External system adapters</li>
<li>Metrics collection</li>
</ul>
<h2 id="clean-architecture"><a class="header" href="#clean-architecture">Clean Architecture</a></h2>
<p>Clean Architecture ensures that business logic doesn't depend on implementation details:</p>
<p><img src="architecture/../diagrams/dependency-flow.svg" alt="Dependency Flow" /></p>
<h3 id="key-principles"><a class="header" href="#key-principles">Key Principles</a></h3>
<p><strong>Dependency Rule</strong>: Source code dependencies point only inward, toward higher-level policies.</p>
<ul>
<li><strong>High-level policy</strong> (Application layer) defines what the system does</li>
<li><strong>Abstractions</strong> (Traits) define how components interact</li>
<li><strong>Low-level details</strong> (Infrastructure) implements the abstractions</li>
</ul>
<p>This means:</p>
<ul>
<li>Domain layer has <strong>zero external dependencies</strong></li>
<li>Application layer depends only on domain traits</li>
<li>Infrastructure implements domain interfaces</li>
</ul>
<h3 id="benefits"><a class="header" href="#benefits">Benefits</a></h3>
<p>✅ <strong>Testability</strong>: Business logic can be tested without database or file system
✅ <strong>Flexibility</strong>: Swap implementations (SQLite → PostgreSQL) without changing business logic
✅ <strong>Independence</strong>: Domain logic doesn't know about HTTP, databases, or file formats</p>
<h2 id="hexagonal-architecture-ports-and-adapters"><a class="header" href="#hexagonal-architecture-ports-and-adapters">Hexagonal Architecture (Ports and Adapters)</a></h2>
<p>The pipeline uses Hexagonal Architecture to isolate the core business logic:</p>
<p><img src="architecture/../diagrams/hexagonal-architecture.svg" alt="Hexagonal Architecture" /></p>
<h3 id="core-components-1"><a class="header" href="#core-components-1">Core Components</a></h3>
<p><strong>Application Core</strong></p>
<ul>
<li>Domain model (entities, value objects)</li>
<li>Business logic (pipeline orchestration)</li>
<li>Ports (trait definitions)</li>
</ul>
<p><strong>Primary Adapters</strong> (Driving)</p>
<ul>
<li>CLI adapter - drives the application</li>
<li>HTTP adapter - future API endpoints</li>
</ul>
<p><strong>Secondary Adapters</strong> (Driven)</p>
<ul>
<li>SQLite repository adapter - driven by the application</li>
<li>File system adapter - driven by the application</li>
<li>Prometheus metrics adapter - driven by the application</li>
</ul>
<h3 id="how-it-works"><a class="header" href="#how-it-works">How It Works</a></h3>
<ol>
<li><strong>User</strong> interacts with <strong>Primary Adapter</strong> (CLI)</li>
<li><strong>Primary Adapter</strong> calls <strong>Application Core</strong> through defined ports</li>
<li><strong>Application Core</strong> uses <strong>Ports</strong> (traits) to interact with infrastructure</li>
<li><strong>Secondary Adapters</strong> implement these ports</li>
<li><strong>Adapters</strong> connect to external systems (database, files)</li>
</ol>
<p><strong>Example Flow</strong>:</p>
<pre><code>CLI → Pipeline Service → Repository Port → SQLite Adapter → Database
</code></pre>
<p>The application core never knows it's using SQLite - it only knows the <code>Repository</code> trait.</p>
<h2 id="architecture-integration"><a class="header" href="#architecture-integration">Architecture Integration</a></h2>
<p>These three patterns work together:</p>
<pre><code class="language-text">Clean Architecture:    Layers with dependency direction
Domain-Driven Design:  Business modeling within layers
Hexagonal Architecture: Ports/Adapters at layer boundaries
</code></pre>
<p><strong>In Practice</strong>:</p>
<ul>
<li><strong>Domain layer</strong> contains pure business logic (DDD entities)</li>
<li><strong>Application layer</strong> orchestrates use cases (Clean Architecture)</li>
<li><strong>Infrastructure</strong> implements ports (Hexagonal Architecture)</li>
</ul>
<p>This combination provides:</p>
<ul>
<li>Clear separation of concerns</li>
<li>Testable business logic</li>
<li>Flexible infrastructure</li>
<li>Maintainable codebase</li>
</ul>
<h2 id="next-steps-5"><a class="header" href="#next-steps-5">Next Steps</a></h2>
<p>Continue to:</p>
<ul>
<li><a href="architecture/layers.html">Layered Architecture Details</a> - Deep dive into each layer</li>
<li><a href="architecture/domain-model.html">Domain Model</a> - Understanding entities and value objects</li>
<li><a href="architecture/patterns.html">Design Patterns</a> - Patterns used throughout the codebase</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="layered-architecture-1"><a class="header" href="#layered-architecture-1">Layered Architecture</a></h1>
<p><strong>Version:</strong> 1.0
<strong>Date:</strong> 2025-01-04
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner, Claude Code
<strong>Status:</strong> Active</p>
<h2 id="overview-1"><a class="header" href="#overview-1">Overview</a></h2>
<p>The pipeline system is organized into four distinct layers, each with specific responsibilities and clear boundaries. This layered architecture provides separation of concerns, testability, and maintainability.</p>
<p><img src="architecture/../diagrams/layered-architecture.svg" alt="Layered Architecture" /></p>
<h2 id="the-four-layers"><a class="header" href="#the-four-layers">The Four Layers</a></h2>
<pre><code class="language-text">┌─────────────────────────────────────────┐
│         Presentation Layer              │  ← User interface (CLI)
│  - CLI commands                         │
│  - User interaction                     │
└─────────────────┬───────────────────────┘
                  │ depends on
┌─────────────────▼───────────────────────┐
│         Application Layer               │  ← Use cases, orchestration
│  - Use cases                            │
│  - Application services                 │
│  - Commands/Queries                     │
└─────────────────┬───────────────────────┘
                  │ depends on
┌─────────────────▼───────────────────────┐
│           Domain Layer                  │  ← Core business logic
│  - Entities                             │
│  - Value objects                        │
│  - Domain services (interfaces)         │
│  - Business rules                       │
└─────────────────△───────────────────────┘
                  │ implements interfaces
┌─────────────────┴───────────────────────┐
│        Infrastructure Layer             │  ← External dependencies
│  - Database repositories                │
│  - File I/O                             │
│  - External services                    │
│  - Encryption/Compression               │
└─────────────────────────────────────────┘
</code></pre>
<h2 id="dependency-rule"><a class="header" href="#dependency-rule">Dependency Rule</a></h2>
<p>The <strong>dependency rule</strong> is the most important principle in layered architecture:</p>
<blockquote>
<p><strong>Dependencies flow inward toward the domain layer.</strong></p>
</blockquote>
<ul>
<li>Presentation depends on Application</li>
<li>Application depends on Domain</li>
<li>Infrastructure depends on Domain (via interfaces)</li>
<li><strong>Domain depends on nothing</strong> (pure business logic)</li>
</ul>
<p>This means:</p>
<ul>
<li>✅ Application can use Domain types</li>
<li>✅ Infrastructure implements Domain interfaces</li>
<li>❌ Domain cannot use Application types</li>
<li>❌ Domain cannot use Infrastructure types</li>
</ul>
<h2 id="domain-layer"><a class="header" href="#domain-layer">Domain Layer</a></h2>
<h3 id="purpose"><a class="header" href="#purpose">Purpose</a></h3>
<p>The domain layer contains the <strong>core business logic</strong> and is the heart of the application. It's completely independent of external concerns like databases, user interfaces, or frameworks.</p>
<h3 id="responsibilities"><a class="header" href="#responsibilities">Responsibilities</a></h3>
<ul>
<li>Define business entities and value objects</li>
<li>Enforce business rules and invariants</li>
<li>Provide domain service interfaces</li>
<li>Emit domain events</li>
<li>Define repository interfaces</li>
</ul>
<h3 id="structure"><a class="header" href="#structure">Structure</a></h3>
<pre><code>pipeline-domain/
├── entities/
│   ├── pipeline.rs           # Pipeline entity
│   ├── pipeline_stage.rs     # Stage entity
│   ├── processing_context.rs # Processing state
│   └── security_context.rs   # Security management
├── value_objects/
│   ├── algorithm.rs          # Algorithm value object
│   ├── chunk_size.rs         # Chunk size validation
│   ├── file_path.rs          # Type-safe paths
│   └── pipeline_id.rs        # Type-safe IDs
├── services/
│   ├── compression_service.rs    # Compression interface
│   ├── encryption_service.rs     # Encryption interface
│   └── checksum_service.rs       # Checksum interface
├── repositories/
│   └── pipeline_repository.rs    # Repository interface
├── events/
│   └── domain_events.rs      # Business events
└── error/
    └── pipeline_error.rs     # Domain errors
</code></pre>
<h3 id="example"><a class="header" href="#example">Example</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Domain layer - pure business logic
pub struct Pipeline {
    id: PipelineId,
    name: String,
    stages: Vec&lt;PipelineStage&gt;,
    // ... no database or UI dependencies
}

impl Pipeline {
    pub fn new(name: String, stages: Vec&lt;PipelineStage&gt;) -&gt; Result&lt;Self, PipelineError&gt; {
        // Business rule: must have at least one stage
        if stages.is_empty() {
            return Err(PipelineError::InvalidConfiguration(
                "Pipeline must have at least one stage".to_string()
            ));
        }

        // Create pipeline with validated business rules
        Ok(Self {
            id: PipelineId::new(),
            name,
            stages,
            // ...
        })
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="key-characteristics"><a class="header" href="#key-characteristics">Key Characteristics</a></h3>
<ul>
<li><strong>No external dependencies</strong> - Only standard library and domain types</li>
<li><strong>Highly testable</strong> - Can test without databases or files</li>
<li><strong>Portable</strong> - Can be used in any context (web, CLI, embedded)</li>
<li><strong>Stable</strong> - Rarely changes except for business requirement changes</li>
</ul>
<h2 id="application-layer"><a class="header" href="#application-layer">Application Layer</a></h2>
<h3 id="purpose-1"><a class="header" href="#purpose-1">Purpose</a></h3>
<p>The application layer orchestrates the execution of business use cases. It coordinates domain objects and delegates to domain services to accomplish specific tasks.</p>
<h3 id="responsibilities-1"><a class="header" href="#responsibilities-1">Responsibilities</a></h3>
<ul>
<li>Implement use cases (user actions)</li>
<li>Coordinate domain objects</li>
<li>Manage transactions</li>
<li>Handle application-specific workflows</li>
<li>Emit application events</li>
</ul>
<h3 id="structure-1"><a class="header" href="#structure-1">Structure</a></h3>
<pre><code>pipeline/src/application/
├── use_cases/
│   ├── process_file.rs       # File processing use case
│   ├── restore_file.rs       # File restoration use case
│   └── create_pipeline.rs    # Pipeline creation
├── services/
│   ├── pipeline_service.rs   # Pipeline orchestration
│   ├── file_processor_service.rs  # File processing
│   └── transactional_chunk_writer.rs  # Chunk writing
├── commands/
│   └── commands.rs           # CQRS commands
└── utilities/
    └── generic_service_base.rs  # Service helpers
</code></pre>
<h3 id="example-1"><a class="header" href="#example-1">Example</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Application layer - orchestrates domain objects
pub struct FileProcessorService {
    pipeline_repo: Arc&lt;dyn PipelineRepository&gt;,
    compression: Arc&lt;dyn CompressionService&gt;,
    encryption: Arc&lt;dyn EncryptionService&gt;,
}

impl FileProcessorService {
    pub async fn process_file(
        &amp;self,
        pipeline_id: &amp;PipelineId,
        input_path: &amp;FilePath,
        output_path: &amp;FilePath,
    ) -&gt; Result&lt;ProcessingMetrics, PipelineError&gt; {
        // 1. Fetch pipeline from repository
        let pipeline = self.pipeline_repo
            .find_by_id(pipeline_id)
            .await?
            .ok_or(PipelineError::NotFound)?;

        // 2. Create processing context
        let context = ProcessingContext::new(
            pipeline.id().clone(),
            input_path.clone(),
            output_path.clone(),
        );

        // 3. Process each stage
        for stage in pipeline.stages() {
            match stage.stage_type() {
                StageType::Compression =&gt; {
                    self.compression.compress(/* ... */).await?;
                }
                StageType::Encryption =&gt; {
                    self.encryption.encrypt(/* ... */).await?;
                }
                // ... more stages
            }
        }

        // 4. Return metrics
        Ok(context.metrics().clone())
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="key-characteristics-1"><a class="header" href="#key-characteristics-1">Key Characteristics</a></h3>
<ul>
<li><strong>Thin layer</strong> - Delegates to domain for business logic</li>
<li><strong>Workflow coordination</strong> - Orchestrates multiple domain operations</li>
<li><strong>Transaction management</strong> - Ensures atomic operations</li>
<li><strong>No business logic</strong> - Business rules belong in domain layer</li>
</ul>
<h2 id="infrastructure-layer"><a class="header" href="#infrastructure-layer">Infrastructure Layer</a></h2>
<h3 id="purpose-2"><a class="header" href="#purpose-2">Purpose</a></h3>
<p>The infrastructure layer provides concrete implementations of interfaces defined in the domain layer. It handles all external concerns like databases, file systems, and third-party services.</p>
<h3 id="responsibilities-2"><a class="header" href="#responsibilities-2">Responsibilities</a></h3>
<ul>
<li>Implement repository interfaces</li>
<li>Provide database access</li>
<li>Handle file I/O operations</li>
<li>Implement compression/encryption services</li>
<li>Integrate with external systems</li>
<li>Provide logging and metrics</li>
</ul>
<h3 id="structure-2"><a class="header" href="#structure-2">Structure</a></h3>
<pre><code>pipeline/src/infrastructure/
├── repositories/
│   ├── sqlite_pipeline_repository.rs  # SQLite implementation
│   └── stage_executor.rs              # Stage execution
├── adapters/
│   ├── compression_service_adapter.rs # Compression implementation
│   ├── encryption_service_adapter.rs  # Encryption implementation
│   └── repositories/
│       └── sqlite_repository_adapter.rs  # Repository adapter
├── services/
│   └── binary_format_service.rs       # File format handling
├── metrics/
│   ├── metrics_service.rs             # Prometheus metrics
│   └── metrics_observer.rs            # Metrics collection
├── logging/
│   └── observability_service.rs       # Logging setup
└── config/
    └── config_service.rs              # Configuration management
</code></pre>
<h3 id="example-2"><a class="header" href="#example-2">Example</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Infrastructure layer - implements domain interfaces
pub struct SQLitePipelineRepository {
    pool: SqlitePool,
}

#[async_trait]
impl PipelineRepository for SQLitePipelineRepository {
    async fn find_by_id(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;Option&lt;Pipeline&gt;, PipelineError&gt; {
        // Database-specific code
        let row = sqlx::query_as::&lt;_, PipelineRow&gt;(
            "SELECT * FROM pipelines WHERE id = ?"
        )
        .bind(id.to_string())
        .fetch_optional(&amp;self.pool)
        .await
        .map_err(|e| PipelineError::RepositoryError(e.to_string()))?;

        // Map database row to domain entity
        row.map(|r| self.to_domain_entity(r)).transpose()
    }

    async fn create(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt; {
        // Convert domain entity to database row
        let row = self.to_persistence_model(pipeline);

        // Insert into database
        sqlx::query(
            "INSERT INTO pipelines (id, name, ...) VALUES (?, ?, ...)"
        )
        .bind(&amp;row.id)
        .bind(&amp;row.name)
        .execute(&amp;self.pool)
        .await
        .map_err(|e| PipelineError::RepositoryError(e.to_string()))?;

        Ok(())
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="key-characteristics-2"><a class="header" href="#key-characteristics-2">Key Characteristics</a></h3>
<ul>
<li><strong>Implements domain interfaces</strong> - Provides concrete implementations</li>
<li><strong>Database access</strong> - Handles all persistence operations</li>
<li><strong>External integrations</strong> - Communicates with external systems</li>
<li><strong>Technology-specific</strong> - Uses specific libraries and frameworks</li>
<li><strong>Replaceable</strong> - Can swap implementations without changing domain</li>
</ul>
<h2 id="presentation-layer"><a class="header" href="#presentation-layer">Presentation Layer</a></h2>
<h3 id="purpose-3"><a class="header" href="#purpose-3">Purpose</a></h3>
<p>The presentation layer handles user interaction and input/output. It translates user commands into application use cases and presents results back to the user.</p>
<h3 id="responsibilities-3"><a class="header" href="#responsibilities-3">Responsibilities</a></h3>
<ul>
<li>Parse and validate user input</li>
<li>Execute application use cases</li>
<li>Format and display output</li>
<li>Handle user interaction</li>
<li>Map errors to user-friendly messages</li>
</ul>
<h3 id="structure-3"><a class="header" href="#structure-3">Structure</a></h3>
<pre><code>pipeline/src/presentation/
├── mod.rs                    # Presentation module
└── (CLI is in main.rs)
</code></pre>
<h3 id="example-3"><a class="header" href="#example-3">Example</a></h3>
<pre><pre class="playground"><code class="language-rust">// Presentation layer - CLI interaction
#[tokio::main]
async fn main() -&gt; std::process::ExitCode {
    // 1. Parse CLI arguments
    let cli = bootstrap::bootstrap_cli()
        .unwrap_or_else(|e| {
            eprintln!("Error: {}", e);
            std::process::exit(65);
        });

    // 2. Set up dependencies (infrastructure)
    let db_pool = create_database_pool().await?;
    let pipeline_repo = Arc::new(SQLitePipelineRepository::new(db_pool));
    let compression = Arc::new(CompressionServiceAdapter::new());
    let file_processor = FileProcessorService::new(pipeline_repo, compression);

    // 3. Execute use case based on command
    let result = match cli.command {
        Commands::Process { input, output, pipeline } =&gt; {
            // Call application service
            file_processor.process_file(&amp;pipeline, &amp;input, &amp;output).await
        }
        Commands::Create { name, stages } =&gt; {
            // Call application service
            create_pipeline_service.create(&amp;name, stages).await
        }
        // ... more commands
    };

    // 4. Handle result and display to user
    match result {
        Ok(_) =&gt; {
            println!("✓ Processing completed successfully");
            ExitCode::SUCCESS
        }
        Err(e) =&gt; {
            eprintln!("✗ Error: {}", e);
            ExitCode::FAILURE
        }
    }
}</code></pre></pre>
<h3 id="key-characteristics-3"><a class="header" href="#key-characteristics-3">Key Characteristics</a></h3>
<ul>
<li><strong>Thin layer</strong> - Minimal logic, delegates to application</li>
<li><strong>User-facing</strong> - Handles all user interaction</li>
<li><strong>Input validation</strong> - Validates user input before processing</li>
<li><strong>Error formatting</strong> - Converts technical errors to user-friendly messages</li>
</ul>
<h2 id="layer-interactions"><a class="header" href="#layer-interactions">Layer Interactions</a></h2>
<h3 id="example-processing-a-file"><a class="header" href="#example-processing-a-file">Example: Processing a File</a></h3>
<p>Here's how the layers work together to process a file:</p>
<pre><code class="language-text">1. Presentation Layer (CLI)
   ↓ User runs: pipeline process --input file.txt --output file.bin --pipeline my-pipeline
   ├─ Parse command-line arguments
   ├─ Validate input parameters
   └─ Call Application Service

2. Application Layer (FileProcessorService)
   ↓ process_file(pipeline_id, input_path, output_path)
   ├─ Fetch Pipeline from Repository (Infrastructure)
   ├─ Create ProcessingContext (Domain)
   ├─ For each stage:
   │  ├─ Call CompressionService (Infrastructure)
   │  ├─ Call EncryptionService (Infrastructure)
   │  └─ Update metrics (Domain)
   └─ Return ProcessingMetrics (Domain)

3. Domain Layer (Pipeline, ProcessingContext)
   ↓ Enforce business rules
   ├─ Validate stage compatibility
   ├─ Enforce chunk sequencing
   └─ Calculate metrics

4. Infrastructure Layer (Repositories, Services)
   ↓ Handle external operations
   ├─ Query SQLite database
   ├─ Read/write files
   ├─ Compress data (brotli, zstd, etc.)
   └─ Encrypt data (AES, ChaCha20)
</code></pre>
<h2 id="benefits-of-layered-architecture"><a class="header" href="#benefits-of-layered-architecture">Benefits of Layered Architecture</a></h2>
<h3 id="separation-of-concerns"><a class="header" href="#separation-of-concerns">Separation of Concerns</a></h3>
<p>Each layer has a single, well-defined responsibility. This makes the code easier to understand and maintain.</p>
<h3 id="testability-1"><a class="header" href="#testability-1">Testability</a></h3>
<ul>
<li><strong>Domain Layer</strong>: Test business logic without any infrastructure</li>
<li><strong>Application Layer</strong>: Test workflows with mock repositories</li>
<li><strong>Infrastructure Layer</strong>: Test database operations independently</li>
<li><strong>Presentation Layer</strong>: Test user interaction separately</li>
</ul>
<h3 id="flexibility"><a class="header" href="#flexibility">Flexibility</a></h3>
<p>You can change infrastructure (e.g., swap SQLite for PostgreSQL) without touching domain or application layers.</p>
<h3 id="maintainability"><a class="header" href="#maintainability">Maintainability</a></h3>
<p>Changes in one layer typically don't affect other layers, reducing the risk of breaking existing functionality.</p>
<h3 id="parallel-development"><a class="header" href="#parallel-development">Parallel Development</a></h3>
<p>Teams can work on different layers simultaneously without conflicts.</p>
<h2 id="common-pitfalls"><a class="header" href="#common-pitfalls">Common Pitfalls</a></h2>
<h3 id="-breaking-the-dependency-rule"><a class="header" href="#-breaking-the-dependency-rule">❌ Breaking the Dependency Rule</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// WRONG: Domain depending on infrastructure
pub struct Pipeline {
    id: PipelineId,
    db_connection: SqlitePool,  // ❌ Database dependency in domain!
}
<span class="boring">}</span></code></pre></pre>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// CORRECT: Domain independent of infrastructure
pub struct Pipeline {
    id: PipelineId,
    name: String,
    stages: Vec&lt;PipelineStage&gt;,  // ✅ Pure domain types
}
<span class="boring">}</span></code></pre></pre>
<h3 id="-business-logic-in-application-layer"><a class="header" href="#-business-logic-in-application-layer">❌ Business Logic in Application Layer</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// WRONG: Business logic in application
impl FileProcessorService {
    pub async fn process_file(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), Error&gt; {
        // ❌ Business rule in application layer!
        if pipeline.stages().is_empty() {
            return Err(Error::InvalidPipeline);
        }
        // ...
    }
}
<span class="boring">}</span></code></pre></pre>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// CORRECT: Business logic in domain
impl Pipeline {
    pub fn new(name: String, stages: Vec&lt;PipelineStage&gt;) -&gt; Result&lt;Self, PipelineError&gt; {
        // ✅ Business rule in domain layer
        if stages.is_empty() {
            return Err(PipelineError::InvalidConfiguration(
                "Pipeline must have at least one stage".to_string()
            ));
        }
        Ok(Self { /* ... */ })
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="-direct-infrastructure-access-from-presentation"><a class="header" href="#-direct-infrastructure-access-from-presentation">❌ Direct Infrastructure Access from Presentation</a></h3>
<pre><pre class="playground"><code class="language-rust">// WRONG: Presentation accessing infrastructure directly
async fn main() {
    let db_pool = create_database_pool().await?;
    // ❌ CLI directly using repository!
    let pipeline = db_pool.query("SELECT * FROM pipelines").await?;
}</code></pre></pre>
<pre><pre class="playground"><code class="language-rust">// CORRECT: Presentation using application services
async fn main() {
    let file_processor = create_file_processor().await?;
    // ✅ CLI using application service
    let result = file_processor.process_file(&amp;pipeline_id, &amp;input, &amp;output).await?;
}</code></pre></pre>
<h2 id="next-steps-6"><a class="header" href="#next-steps-6">Next Steps</a></h2>
<p>Now that you understand the layered architecture:</p>
<ul>
<li><a href="architecture/adapter-pattern.html">Hexagonal Architecture</a> - Ports and adapters pattern</li>
<li><a href="architecture/dependencies.html">Dependency Inversion</a> - Managing dependencies</li>
<li><a href="architecture/domain-model.html">Domain Model</a> - Deep dive into the domain layer</li>
<li><a href="architecture/repository-pattern.html">Repository Pattern</a> - Data persistence abstraction</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dependency-flow"><a class="header" href="#dependency-flow">Dependency Flow</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Understanding dependency direction and inversion.</p>
<h2 id="dependency-rule-1"><a class="header" href="#dependency-rule-1">Dependency Rule</a></h2>
<p>TODO: Explain dependency direction</p>
<h2 id="dependency-inversion"><a class="header" href="#dependency-inversion">Dependency Inversion</a></h2>
<p>TODO: Explain DIP application</p>
<h2 id="trait-abstractions"><a class="header" href="#trait-abstractions">Trait Abstractions</a></h2>
<p>TODO: Explain trait usage</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="domain-model-1"><a class="header" href="#domain-model-1">Domain Model</a></h1>
<p><strong>Version:</strong> 1.0
<strong>Date:</strong> 2025-01-04
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner, Claude Code
<strong>Status:</strong> Active</p>
<h2 id="overview-2"><a class="header" href="#overview-2">Overview</a></h2>
<p>The domain model is the heart of the pipeline system. It captures the core business concepts, rules, and behaviors using Domain-Driven Design (DDD) principles. This chapter explains how the domain model is structured and why it's designed this way.</p>
<p><img src="architecture/../diagrams/domain-model.svg" alt="Domain Model" /></p>
<h2 id="domain-driven-design-principles"><a class="header" href="#domain-driven-design-principles">Domain-Driven Design Principles</a></h2>
<p>Domain-Driven Design (DDD) is a software development approach that emphasizes:</p>
<ol>
<li><strong>Focus on the core domain</strong> - The business logic is the most important part</li>
<li><strong>Model-driven design</strong> - The domain model drives the software design</li>
<li><strong>Ubiquitous language</strong> - Shared vocabulary between developers and domain experts</li>
<li><strong>Bounded contexts</strong> - Clear boundaries between different parts of the system</li>
</ol>
<h3 id="why-ddd"><a class="header" href="#why-ddd">Why DDD?</a></h3>
<p>For a pipeline processing system, DDD provides:</p>
<ul>
<li><strong>Clear separation</strong> between business logic and infrastructure</li>
<li><strong>Testable code</strong> - Domain logic can be tested without databases or files</li>
<li><strong>Flexibility</strong> - Easy to change infrastructure without touching business rules</li>
<li><strong>Maintainability</strong> - Business rules are explicit and well-organized</li>
</ul>
<h2 id="core-domain-concepts"><a class="header" href="#core-domain-concepts">Core Domain Concepts</a></h2>
<h3 id="entities-1"><a class="header" href="#entities-1">Entities</a></h3>
<p><strong>Entities</strong> are objects with a unique identity that persists through time. Two entities are equal if they have the same ID, even if all their other attributes differ.</p>
<h4 id="pipeline-entity"><a class="header" href="#pipeline-entity">Pipeline Entity</a></h4>
<p>The central entity representing a file processing workflow.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct Pipeline {
    id: PipelineId,                    // Unique identity
    name: String,                      // Human-readable name
    stages: Vec&lt;PipelineStage&gt;,        // Ordered processing stages
    configuration: HashMap&lt;String, String&gt;,  // Custom settings
    metrics: ProcessingMetrics,        // Performance data
    archived: bool,                    // Lifecycle state
    created_at: DateTime&lt;Utc&gt;,         // Creation timestamp
    updated_at: DateTime&lt;Utc&gt;,         // Last modification
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Key characteristics:</strong></p>
<ul>
<li>Has unique <code>PipelineId</code></li>
<li>Can be modified while maintaining identity</li>
<li>Enforces business rules (e.g., must have at least one stage)</li>
<li>Automatically adds integrity verification stages</li>
</ul>
<p><strong>Example:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::Pipeline;

// Two pipelines with same ID are equal, even if names differ
let pipeline1 = Pipeline::new("Original Name", stages.clone())?;
let pipeline2 = pipeline1.clone();
pipeline2.set_name("Different Name");

assert_eq!(pipeline1.id(), pipeline2.id());  // Same identity
<span class="boring">}</span></code></pre></pre>
<h4 id="pipelinestage-entity"><a class="header" href="#pipelinestage-entity">PipelineStage Entity</a></h4>
<p>Represents a single processing operation within a pipeline.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct PipelineStage {
    id: StageId,                       // Unique identity
    name: String,                      // Stage name
    stage_type: StageType,             // Compression, Encryption, etc.
    configuration: StageConfiguration, // Algorithm and parameters
    order: usize,                      // Execution order
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Stage Types:</strong></p>
<ul>
<li><code>Compression</code> - Data compression</li>
<li><code>Encryption</code> - Data encryption</li>
<li><code>Integrity</code> - Checksum verification</li>
<li><code>Custom</code> - User-defined operations</li>
</ul>
<h4 id="processingcontext-entity"><a class="header" href="#processingcontext-entity">ProcessingContext Entity</a></h4>
<p>Manages the runtime execution state of a pipeline.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct ProcessingContext {
    id: ProcessingContextId,           // Unique identity
    pipeline_id: PipelineId,           // Associated pipeline
    input_path: FilePath,              // Input file
    output_path: FilePath,             // Output file
    current_stage: usize,              // Current stage index
    status: ProcessingStatus,          // Running, Completed, Failed
    metrics: ProcessingMetrics,        // Runtime metrics
}
<span class="boring">}</span></code></pre></pre>
<h4 id="securitycontext-entity"><a class="header" href="#securitycontext-entity">SecurityContext Entity</a></h4>
<p>Manages security and permissions for pipeline operations.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct SecurityContext {
    id: SecurityContextId,             // Unique identity
    user_id: UserId,                   // User performing operation
    security_level: SecurityLevel,     // Required security level
    permissions: Vec&lt;Permission&gt;,      // Granted permissions
    encryption_key_id: Option&lt;EncryptionKeyId&gt;,  // Key for encryption
}
<span class="boring">}</span></code></pre></pre>
<h3 id="value-objects-1"><a class="header" href="#value-objects-1">Value Objects</a></h3>
<p><strong>Value Objects</strong> are immutable objects defined by their attributes. Two value objects with the same attributes are considered equal.</p>
<h4 id="algorithm-value-object"><a class="header" href="#algorithm-value-object">Algorithm Value Object</a></h4>
<p>Type-safe representation of processing algorithms.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct Algorithm(String);

impl Algorithm {
    // Predefined compression algorithms
    pub fn brotli() -&gt; Self { /* ... */ }
    pub fn gzip() -&gt; Self { /* ... */ }
    pub fn zstd() -&gt; Self { /* ... */ }
    pub fn lz4() -&gt; Self { /* ... */ }

    // Predefined encryption algorithms
    pub fn aes_256_gcm() -&gt; Self { /* ... */ }
    pub fn chacha20_poly1305() -&gt; Self { /* ... */ }

    // Predefined hashing algorithms
    pub fn sha256() -&gt; Self { /* ... */ }
    pub fn blake3() -&gt; Self { /* ... */ }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Key characteristics:</strong></p>
<ul>
<li>Immutable after creation</li>
<li>Self-validating (enforces format rules)</li>
<li>Category detection (is_compression(), is_encryption())</li>
<li>Type-safe (can't accidentally use wrong algorithm)</li>
</ul>
<h4 id="chunksize-value-object"><a class="header" href="#chunksize-value-object">ChunkSize Value Object</a></h4>
<p>Represents validated chunk sizes for file processing.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct ChunkSize(usize);

impl ChunkSize {
    pub fn new(bytes: usize) -&gt; Result&lt;Self, PipelineError&gt; {
        // Validates size is within acceptable range
        if bytes &lt; MIN_CHUNK_SIZE || bytes &gt; MAX_CHUNK_SIZE {
            return Err(PipelineError::InvalidConfiguration(/* ... */));
        }
        Ok(Self(bytes))
    }

    pub fn from_megabytes(mb: usize) -&gt; Result&lt;Self, PipelineError&gt; {
        Self::new(mb * 1024 * 1024)
    }
}
<span class="boring">}</span></code></pre></pre>
<h4 id="filechunk-value-object"><a class="header" href="#filechunk-value-object">FileChunk Value Object</a></h4>
<p>Immutable representation of a piece of file data.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct FileChunk {
    id: FileChunkId,                   // Unique chunk identifier
    sequence: usize,                   // Position in file
    data: Vec&lt;u8&gt;,                     // Chunk data
    is_final: bool,                    // Last chunk flag
    checksum: Option&lt;String&gt;,          // Integrity verification
}
<span class="boring">}</span></code></pre></pre>
<h4 id="filepath-value-object"><a class="header" href="#filepath-value-object">FilePath Value Object</a></h4>
<p>Type-safe, validated file paths.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct FilePath(PathBuf);

impl FilePath {
    pub fn new(path: impl Into&lt;PathBuf&gt;) -&gt; Result&lt;Self, PipelineError&gt; {
        let path = path.into();
        // Validation:
        // - Path traversal prevention
        // - Null byte checks
        // - Length limits
        // - Encoding validation
        Ok(Self(path))
    }
}
<span class="boring">}</span></code></pre></pre>
<h4 id="pipelineid-stageid-userid-type-safe-ids"><a class="header" href="#pipelineid-stageid-userid-type-safe-ids">PipelineId, StageId, UserId (Type-Safe IDs)</a></h4>
<p>All identifiers are wrapped in newtype value objects for type safety:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct PipelineId(Ulid);  // Can't accidentally use StageId as PipelineId
pub struct StageId(Ulid);
pub struct UserId(Ulid);
pub struct ProcessingContextId(Ulid);
pub struct SecurityContextId(Ulid);
<span class="boring">}</span></code></pre></pre>
<p>This prevents common bugs like passing the wrong ID to a function.</p>
<h3 id="domain-services"><a class="header" href="#domain-services">Domain Services</a></h3>
<p><strong>Domain Services</strong> contain business logic that doesn't naturally fit in an entity or value object. They are stateless and operate on domain objects.</p>
<p>Domain services in our system are defined as traits (interfaces) in the domain layer and implemented in the infrastructure layer.</p>
<h4 id="compressionservice"><a class="header" href="#compressionservice">CompressionService</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[async_trait]
pub trait CompressionService: Send + Sync {
    async fn compress(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt;;

    async fn decompress(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt;;
}
<span class="boring">}</span></code></pre></pre>
<h4 id="encryptionservice"><a class="header" href="#encryptionservice">EncryptionService</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[async_trait]
pub trait EncryptionService: Send + Sync {
    async fn encrypt(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
        key: &amp;EncryptionKey,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt;;

    async fn decrypt(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
        key: &amp;EncryptionKey,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt;;
}
<span class="boring">}</span></code></pre></pre>
<h4 id="checksumservice"><a class="header" href="#checksumservice">ChecksumService</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub trait ChecksumService: Send + Sync {
    fn calculate(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
    ) -&gt; Result&lt;String, PipelineError&gt;;

    fn verify(
        &amp;self,
        data: &amp;[u8],
        expected: &amp;str,
        algorithm: &amp;Algorithm,
    ) -&gt; Result&lt;bool, PipelineError&gt;;
}
<span class="boring">}</span></code></pre></pre>
<h3 id="repositories"><a class="header" href="#repositories">Repositories</a></h3>
<p><strong>Repositories</strong> abstract data persistence, allowing the domain to work with collections without knowing about storage details.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[async_trait]
pub trait PipelineRepository: Send + Sync {
    async fn create(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt;;
    async fn find_by_id(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;Option&lt;Pipeline&gt;, PipelineError&gt;;
    async fn find_by_name(&amp;self, name: &amp;str) -&gt; Result&lt;Option&lt;Pipeline&gt;, PipelineError&gt;;
    async fn update(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt;;
    async fn delete(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;(), PipelineError&gt;;
    async fn list_all(&amp;self) -&gt; Result&lt;Vec&lt;Pipeline&gt;, PipelineError&gt;;
}
<span class="boring">}</span></code></pre></pre>
<p>The repository interface is defined in the domain layer, but implementations live in the infrastructure layer. This follows the Dependency Inversion Principle.</p>
<h3 id="domain-events"><a class="header" href="#domain-events">Domain Events</a></h3>
<p><strong>Domain Events</strong> represent significant business occurrences that other parts of the system might care about.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub enum DomainEvent {
    PipelineCreated {
        pipeline_id: PipelineId,
        name: String,
        created_at: DateTime&lt;Utc&gt;,
    },
    ProcessingStarted {
        pipeline_id: PipelineId,
        context_id: ProcessingContextId,
        input_path: FilePath,
    },
    ProcessingCompleted {
        pipeline_id: PipelineId,
        context_id: ProcessingContextId,
        metrics: ProcessingMetrics,
    },
    ProcessingFailed {
        pipeline_id: PipelineId,
        context_id: ProcessingContextId,
        error: String,
    },
}
<span class="boring">}</span></code></pre></pre>
<p>Events enable:</p>
<ul>
<li><strong>Loose coupling</strong> - Components don't need direct references</li>
<li><strong>Audit trails</strong> - Track all significant operations</li>
<li><strong>Integration</strong> - External systems can react to events</li>
<li><strong>Event sourcing</strong> - Reconstruct state from event history</li>
</ul>
<h2 id="business-rules-and-invariants"><a class="header" href="#business-rules-and-invariants">Business Rules and Invariants</a></h2>
<p>The domain model enforces critical business rules:</p>
<h3 id="pipeline-rules"><a class="header" href="#pipeline-rules">Pipeline Rules</a></h3>
<ol>
<li>
<p><strong>Pipelines must have at least one user-defined stage</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>if user_stages.is_empty() {
    return Err(PipelineError::InvalidConfiguration(
        "Pipeline must have at least one stage".to_string()
    ));
}
<span class="boring">}</span></code></pre></pre>
</li>
<li>
<p><strong>Stage order must be sequential and valid</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Stages are automatically reordered: 0, 1, 2, 3...
// Input checksum = 0
// User stages = 1, 2, 3...
// Output checksum = final
<span class="boring">}</span></code></pre></pre>
</li>
<li>
<p><strong>Pipeline names must be unique</strong> (enforced by repository)</p>
</li>
</ol>
<h3 id="chunk-processing-rules"><a class="header" href="#chunk-processing-rules">Chunk Processing Rules</a></h3>
<ol>
<li>
<p><strong>Chunks must have non-zero size</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>if size == 0 {
    return Err(PipelineError::InvalidChunkSize);
}
<span class="boring">}</span></code></pre></pre>
</li>
<li>
<p><strong>Chunk sequence numbers must be sequential</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Chunks are numbered 0, 1, 2, 3...
// Missing sequences cause processing to fail
<span class="boring">}</span></code></pre></pre>
</li>
<li>
<p><strong>Final chunks must be properly marked</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>if chunk.is_final() {
    // No more chunks should follow
}
<span class="boring">}</span></code></pre></pre>
</li>
</ol>
<h3 id="security-rules"><a class="header" href="#security-rules">Security Rules</a></h3>
<ol>
<li>
<p><strong>Security contexts must be validated</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>security_context.validate()?;
<span class="boring">}</span></code></pre></pre>
</li>
<li>
<p><strong>Encryption keys must meet strength requirements</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>if key.len() &lt; MIN_KEY_LENGTH {
    return Err(PipelineError::WeakEncryptionKey);
}
<span class="boring">}</span></code></pre></pre>
</li>
<li>
<p><strong>Access permissions must be checked</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>if !security_context.has_permission(Permission::ProcessFile) {
    return Err(PipelineError::PermissionDenied);
}
<span class="boring">}</span></code></pre></pre>
</li>
</ol>
<h2 id="ubiquitous-language"><a class="header" href="#ubiquitous-language">Ubiquitous Language</a></h2>
<p>The domain model uses consistent terminology shared between developers and domain experts:</p>
<div class="table-wrapper"><table><thead><tr><th>Term</th><th>Meaning</th></tr></thead><tbody>
<tr><td><strong>Pipeline</strong></td><td>An ordered sequence of processing stages</td></tr>
<tr><td><strong>Stage</strong></td><td>A single processing operation (compress, encrypt, etc.)</td></tr>
<tr><td><strong>Chunk</strong></td><td>A piece of a file processed in parallel</td></tr>
<tr><td><strong>Algorithm</strong></td><td>A specific processing method (zstd, aes-256-gcm, etc.)</td></tr>
<tr><td><strong>Repository</strong></td><td>Storage abstraction for domain objects</td></tr>
<tr><td><strong>Context</strong></td><td>Runtime execution state</td></tr>
<tr><td><strong>Metrics</strong></td><td>Performance and operational measurements</td></tr>
<tr><td><strong>Integrity</strong></td><td>Data verification through checksums</td></tr>
<tr><td><strong>Security Level</strong></td><td>Required protection level (Public, Confidential, Secret)</td></tr>
</tbody></table>
</div>
<h2 id="testing-domain-logic"><a class="header" href="#testing-domain-logic">Testing Domain Logic</a></h2>
<p>Domain objects are designed for easy testing:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn pipeline_enforces_minimum_stages() {
        // Domain logic can be tested without any infrastructure
        let result = Pipeline::new("test".to_string(), vec![]);
        assert!(result.is_err());
    }

    #[test]
    fn algorithm_validates_format() {
        // Value objects self-validate
        let result = Algorithm::new("INVALID-NAME".to_string());
        assert!(result.is_err());

        let result = Algorithm::new("valid-name".to_string());
        assert!(result.is_ok());
    }

    #[test]
    fn chunk_size_enforces_limits() {
        // Business rules are explicit and testable
        let too_small = ChunkSize::new(1);
        assert!(too_small.is_err());

        let valid = ChunkSize::from_megabytes(10);
        assert!(valid.is_ok());
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="benefits-of-this-domain-model"><a class="header" href="#benefits-of-this-domain-model">Benefits of This Domain Model</a></h2>
<ol>
<li><strong>Pure Business Logic</strong> - No infrastructure dependencies</li>
<li><strong>Highly Testable</strong> - Can test without databases, files, or networks</li>
<li><strong>Type Safety</strong> - Strong typing prevents many bugs at compile time</li>
<li><strong>Self-Documenting</strong> - Code structure reflects business concepts</li>
<li><strong>Flexible</strong> - Easy to change infrastructure without touching domain</li>
<li><strong>Maintainable</strong> - Business rules are explicit and centralized</li>
</ol>
<h2 id="next-steps-7"><a class="header" href="#next-steps-7">Next Steps</a></h2>
<p>Now that you understand the domain model:</p>
<ul>
<li><a href="architecture/layered-architecture.html">Layered Architecture</a> - How the domain fits into the overall architecture</li>
<li><a href="architecture/hexagonal-architecture.html">Hexagonal Architecture</a> - Ports and adapters pattern</li>
<li><a href="architecture/repository-pattern.html">Repository Pattern</a> - Data persistence abstraction</li>
<li><a href="architecture/event-driven.html">Domain Events</a> - Event-driven communication</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="entities-2"><a class="header" href="#entities-2">Entities</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Understanding entities in the domain model.</p>
<h2 id="what-are-entities"><a class="header" href="#what-are-entities">What are Entities?</a></h2>
<p>TODO: Define entities</p>
<h2 id="pipeline-entity-1"><a class="header" href="#pipeline-entity-1">Pipeline Entity</a></h2>
<p>TODO: Extract from entities/pipeline.rs</p>
<h2 id="stage-entity"><a class="header" href="#stage-entity">Stage Entity</a></h2>
<p>TODO: Extract from entities/pipeline_stage.rs</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="value-objects-2"><a class="header" href="#value-objects-2">Value Objects</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Understanding value objects in the domain model.</p>
<h2 id="what-are-value-objects"><a class="header" href="#what-are-value-objects">What are Value Objects?</a></h2>
<p>TODO: Define value objects</p>
<h2 id="immutability"><a class="header" href="#immutability">Immutability</a></h2>
<p>TODO: Explain immutability</p>
<h2 id="examples"><a class="header" href="#examples">Examples</a></h2>
<p>TODO: List key value objects</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="aggregates-1"><a class="header" href="#aggregates-1">Aggregates</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Understanding aggregates and aggregate roots.</p>
<h2 id="what-are-aggregates"><a class="header" href="#what-are-aggregates">What are Aggregates?</a></h2>
<p>TODO: Define aggregates</p>
<h2 id="aggregate-boundaries"><a class="header" href="#aggregate-boundaries">Aggregate Boundaries</a></h2>
<p>TODO: Explain boundaries</p>
<h2 id="pipeline-aggregate"><a class="header" href="#pipeline-aggregate">Pipeline Aggregate</a></h2>
<p>TODO: Explain pipeline aggregate</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="design-patterns"><a class="header" href="#design-patterns">Design Patterns</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Design patterns used throughout the pipeline.</p>
<h2 id="pattern-overview"><a class="header" href="#pattern-overview">Pattern Overview</a></h2>
<p>TODO: List patterns</p>
<h2 id="when-to-use-each-pattern"><a class="header" href="#when-to-use-each-pattern">When to Use Each Pattern</a></h2>
<p>TODO: Add guidance</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="repository-pattern"><a class="header" href="#repository-pattern">Repository Pattern</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>The Repository pattern for data persistence.</p>
<h2 id="pattern-overview-1"><a class="header" href="#pattern-overview-1">Pattern Overview</a></h2>
<p>The Repository pattern provides an abstraction layer between the domain and data mapping layers. It acts like an in-memory collection of domain objects, hiding the complexities of database operations.</p>
<p><strong>Key Idea</strong>: Your business logic shouldn't know whether data comes from SQLite, PostgreSQL, or a file. It just uses a <code>Repository</code> trait.</p>
<h2 id="architecture"><a class="header" href="#architecture">Architecture</a></h2>
<p><img src="architecture/../diagrams/repository-pattern.svg" alt="Repository Pattern" /></p>
<h3 id="components"><a class="header" href="#components">Components</a></h3>
<p><strong>Repository Trait</strong> (Domain Layer)</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>trait PipelineRepository {
    fn create(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;()&gt;;
    fn find_by_id(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;Option&lt;Pipeline&gt;&gt;;
    fn update(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;()&gt;;
    fn delete(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;()&gt;;
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Repository Adapter</strong> (Infrastructure Layer)</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct PipelineRepositoryAdapter {
    repository: SQLitePipelineRepository,
}

impl PipelineRepository for PipelineRepositoryAdapter {
    // Implements trait methods
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Concrete Repository</strong> (Infrastructure Layer)</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct SQLitePipelineRepository {
    pool: SqlitePool,
    mapper: PipelineMapper,
}
<span class="boring">}</span></code></pre></pre>
<h2 id="layer-responsibilities"><a class="header" href="#layer-responsibilities">Layer Responsibilities</a></h2>
<h3 id="domain-layer-1"><a class="header" href="#domain-layer-1">Domain Layer</a></h3>
<p>Defines <strong>what</strong> operations are needed:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Domain defines the interface
pub trait PipelineRepository: Send + Sync {
    async fn create(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;()&gt;;
    async fn find_by_id(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;Option&lt;Pipeline&gt;&gt;;
    // ... more methods
}
<span class="boring">}</span></code></pre></pre>
<p>Domain knows:</p>
<ul>
<li>What operations it needs</li>
<li>What domain entities look like</li>
<li>Business rules and validations</li>
</ul>
<p>Domain <strong>doesn't know</strong>:</p>
<ul>
<li>SQL syntax</li>
<li>Database technology</li>
<li>Connection pooling</li>
</ul>
<h3 id="infrastructure-layer-1"><a class="header" href="#infrastructure-layer-1">Infrastructure Layer</a></h3>
<p>Implements <strong>how</strong> to persist data:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl PipelineRepository for PipelineRepositoryAdapter {
    async fn create(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;()&gt; {
        // Convert domain entity to database row
        let row = self.mapper.to_persistence(pipeline);

        // Execute SQL
        sqlx::query("INSERT INTO pipelines ...")
            .execute(&amp;self.pool)
            .await?;

        Ok(())
    }
}
<span class="boring">}</span></code></pre></pre>
<p>Infrastructure knows:</p>
<ul>
<li>SQL syntax and queries</li>
<li>Database schema</li>
<li>Connection management</li>
<li>Error handling</li>
</ul>
<h2 id="data-mapping"><a class="header" href="#data-mapping">Data Mapping</a></h2>
<p>The <strong>Mapper</strong> separates domain models from database schema:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct PipelineMapper;

impl PipelineMapper {
    // Domain → Database
    fn to_persistence(&amp;self, pipeline: &amp;Pipeline) -&gt; PipelineRow {
        PipelineRow {
            id: pipeline.id().to_string(),
            input_path: pipeline.input_path().to_string(),
            // ... map all fields
        }
    }

    // Database → Domain
    fn to_domain(&amp;self, row: SqliteRow) -&gt; Result&lt;Pipeline&gt; {
        Pipeline::new(
            PipelineId::from_string(&amp;row.id)?,
            FilePath::new(&amp;row.input_path)?,
            FilePath::new(&amp;row.output_path)?,
        )
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Why mapping?</strong></p>
<ul>
<li>Domain entities stay pure (no database annotations)</li>
<li>Database schema can change independently</li>
<li>Different databases can use different schemas</li>
<li>Validation happens in domain layer</li>
</ul>
<h2 id="benefits-1"><a class="header" href="#benefits-1">Benefits</a></h2>
<h3 id="1-testability"><a class="header" href="#1-testability">1. Testability</a></h3>
<p>Business logic can be tested without a database:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(test)]
mod tests {
    use mockall::mock;

    mock! {
        PipelineRepo {}

        impl PipelineRepository for PipelineRepo {
            async fn create(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;()&gt;;
            // ... mock other methods
        }
    }

    #[tokio::test]
    async fn test_pipeline_service() {
        let mut mock_repo = MockPipelineRepo::new();
        mock_repo.expect_create()
            .returning(|_| Ok(()));

        let service = PipelineService::new(Arc::new(mock_repo));
        // Test business logic without database
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="2-flexibility"><a class="header" href="#2-flexibility">2. Flexibility</a></h3>
<p>Swap implementations without changing business logic:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Start with SQLite
let repo = SQLitePipelineRepositoryAdapter::new(pool);
let service = PipelineService::new(Arc::new(repo));

// Later, switch to PostgreSQL
let repo = PostgresPipelineRepositoryAdapter::new(pool);
let service = PipelineService::new(Arc::new(repo));
// Business logic unchanged!
<span class="boring">}</span></code></pre></pre>
<h3 id="3-centralized-data-access"><a class="header" href="#3-centralized-data-access">3. Centralized Data Access</a></h3>
<p>All database queries in one place:</p>
<ul>
<li>Easier to optimize</li>
<li>Easier to audit</li>
<li>Easier to cache</li>
<li>Easier to add logging</li>
</ul>
<h3 id="4-domain-purity"><a class="header" href="#4-domain-purity">4. Domain Purity</a></h3>
<p>Domain layer stays technology-agnostic:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Domain doesn't import sqlx, postgres, etc.
// Only depends on standard Rust types
pub struct Pipeline {
    id: PipelineId,           // Not i64 or UUID from database
    input_path: FilePath,     // Not String from database
    status: PipelineStatus,   // Not database enum
}
<span class="boring">}</span></code></pre></pre>
<h2 id="usage-example"><a class="header" href="#usage-example">Usage Example</a></h2>
<h3 id="application-layer-1"><a class="header" href="#application-layer-1">Application Layer</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct PipelineService {
    repository: Arc&lt;dyn PipelineRepository&gt;,
}

impl PipelineService {
    pub async fn create_pipeline(
        &amp;self,
        input: FilePath,
        output: FilePath,
    ) -&gt; Result&lt;Pipeline&gt; {
        // Create domain entity
        let pipeline = Pipeline::new(
            PipelineId::new(),
            input,
            output,
        )?;

        // Persist using repository
        self.repository.create(&amp;pipeline).await?;

        Ok(pipeline)
    }

    pub async fn get_pipeline(
        &amp;self,
        id: PipelineId,
    ) -&gt; Result&lt;Option&lt;Pipeline&gt;&gt; {
        self.repository.find_by_id(&amp;id).await
    }
}
<span class="boring">}</span></code></pre></pre>
<p>The service doesn't know or care:</p>
<ul>
<li>Which database is used</li>
<li>How data is stored</li>
<li>What the SQL looks like</li>
</ul>
<p>It just uses the <code>Repository</code> trait!</p>
<h2 id="implementation-in-pipeline"><a class="header" href="#implementation-in-pipeline">Implementation in Pipeline</a></h2>
<p>Our pipeline uses this pattern for:</p>
<p><strong>PipelineRepository</strong> - Stores pipeline metadata</p>
<ul>
<li><code>pipeline/domain/src/repositories/pipeline_repository.rs</code> (trait)</li>
<li><code>pipeline/src/infrastructure/repositories/sqlite_pipeline_repository.rs</code> (impl)</li>
</ul>
<p><strong>FileChunkRepository</strong> - Stores chunk metadata</p>
<ul>
<li><code>pipeline/domain/src/repositories/file_chunk_repository.rs</code> (trait)</li>
<li><code>pipeline/src/infrastructure/repositories/sqlite_file_chunk_repository.rs</code> (impl)</li>
</ul>
<h2 id="next-steps-8"><a class="header" href="#next-steps-8">Next Steps</a></h2>
<p>Continue to:</p>
<ul>
<li><a href="architecture/service-pattern.html">Service Pattern</a> - Business logic organization</li>
<li><a href="architecture/adapter-pattern.html">Adapter Pattern</a> - Infrastructure integration</li>
<li><a href="architecture/../implementation/repositories.html">Implementation: Repositories</a> - Concrete implementations</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="service-pattern"><a class="header" href="#service-pattern">Service Pattern</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>The Service pattern for domain and application logic.</p>
<h2 id="pattern-overview-2"><a class="header" href="#pattern-overview-2">Pattern Overview</a></h2>
<p>TODO: Extract from service files</p>
<h2 id="domain-vs-application-services"><a class="header" href="#domain-vs-application-services">Domain vs Application Services</a></h2>
<p>TODO: Explain distinction</p>
<h2 id="implementation"><a class="header" href="#implementation">Implementation</a></h2>
<p>TODO: Show examples</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hexagonal-architecture-ports-and-adapters-1"><a class="header" href="#hexagonal-architecture-ports-and-adapters-1">Hexagonal Architecture (Ports and Adapters)</a></h1>
<p><strong>Version:</strong> 1.0
<strong>Date:</strong> 2025-01-04
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner, Claude Code
<strong>Status:</strong> Active</p>
<h2 id="overview-3"><a class="header" href="#overview-3">Overview</a></h2>
<p>Hexagonal Architecture, also known as <strong>Ports and Adapters</strong>, is a pattern that isolates the core business logic (domain) from external concerns. The pipeline system uses this pattern to keep the domain pure and infrastructure replaceable.</p>
<p><img src="architecture/../diagrams/hexagonal-architecture.svg" alt="Hexagonal Architecture" /></p>
<h2 id="the-hexagon-metaphor"><a class="header" href="#the-hexagon-metaphor">The Hexagon Metaphor</a></h2>
<p>Think of your application as a hexagon:</p>
<pre><code class="language-text">                     ┌─────────────────┐
                     │   Primary       │
                     │   Adapters      │
                     │  (Drivers)      │
                     └────────┬────────┘
                              │
        ┌─────────────────────┼─────────────────────┐
        │                     │                     │
        │              ┌──────▼──────┐              │
        │              │             │              │
        │              │   Domain    │              │
        │              │    (Core)   │              │
        │              │             │              │
        │              └──────┬──────┘              │
        │                     │                     │
        └─────────────────────┼─────────────────────┘
                              │
                     ┌────────▼────────┐
                     │   Secondary     │
                     │   Adapters      │
                     │  (Driven)       │
                     └─────────────────┘
</code></pre>
<ul>
<li><strong>The Hexagon (Core)</strong>: Your domain logic - completely independent</li>
<li><strong>Ports</strong>: Interfaces that define how to interact with the core</li>
<li><strong>Adapters</strong>: Implementations that connect the core to the outside world</li>
</ul>
<h2 id="ports-the-interfaces"><a class="header" href="#ports-the-interfaces">Ports: The Interfaces</a></h2>
<p><strong>Ports</strong> are interfaces defined by the domain layer. They specify what the domain needs without caring about implementation details.</p>
<h3 id="primary-ports-driving"><a class="header" href="#primary-ports-driving">Primary Ports (Driving)</a></h3>
<p>Primary ports define <strong>use cases</strong> - what the application can do. External systems drive the application through these ports.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Domain layer defines the interface (port)
#[async_trait]
pub trait FileProcessorService: Send + Sync {
    async fn process_file(
        &amp;self,
        pipeline_id: &amp;PipelineId,
        input_path: &amp;FilePath,
        output_path: &amp;FilePath,
    ) -&gt; Result&lt;ProcessingMetrics, PipelineError&gt;;
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Examples in our system:</strong></p>
<ul>
<li><code>FileProcessorService</code> - File processing operations</li>
<li><code>PipelineService</code> - Pipeline management operations</li>
</ul>
<h3 id="secondary-ports-driven"><a class="header" href="#secondary-ports-driven">Secondary Ports (Driven)</a></h3>
<p>Secondary ports define <strong>dependencies</strong> - what the domain needs from the outside world. The application drives these external systems.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Domain layer defines what it needs (port)
#[async_trait]
pub trait PipelineRepository: Send + Sync {
    async fn create(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt;;
    async fn find_by_id(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;Option&lt;Pipeline&gt;, PipelineError&gt;;
    async fn update(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt;;
    async fn delete(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;(), PipelineError&gt;;
}

#[async_trait]
pub trait CompressionService: Send + Sync {
    async fn compress(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt;;

    async fn decompress(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt;;
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Examples in our system:</strong></p>
<ul>
<li><code>PipelineRepository</code> - Data persistence</li>
<li><code>CompressionService</code> - Data compression</li>
<li><code>EncryptionService</code> - Data encryption</li>
<li><code>ChecksumService</code> - Integrity verification</li>
</ul>
<h2 id="adapters-the-implementations"><a class="header" href="#adapters-the-implementations">Adapters: The Implementations</a></h2>
<p><strong>Adapters</strong> are concrete implementations of ports. They translate between the domain and external systems.</p>
<h3 id="primary-adapters-driving"><a class="header" href="#primary-adapters-driving">Primary Adapters (Driving)</a></h3>
<p>Primary adapters <strong>drive</strong> the application. They take input from the outside world and call the domain.</p>
<h4 id="cli-adapter-mainrs"><a class="header" href="#cli-adapter-mainrs">CLI Adapter (main.rs)</a></h4>
<pre><pre class="playground"><code class="language-rust">// Primary adapter - drives the application
#[tokio::main]
async fn main() -&gt; std::process::ExitCode {
    // 1. Parse user input
    let cli = bootstrap::bootstrap_cli()?;

    // 2. Set up infrastructure (dependency injection)
    let services = setup_services().await?;

    // 3. Drive the domain through primary port
    match cli.command {
        Commands::Process { input, output, pipeline } =&gt; {
            // Call domain through FileProcessorService port
            services.file_processor
                .process_file(&amp;pipeline, &amp;input, &amp;output)
                .await?
        }
        Commands::Create { name, stages } =&gt; {
            // Call domain through PipelineService port
            services.pipeline_service
                .create_pipeline(&amp;name, stages)
                .await?
        }
        // ... more commands
    }
}</code></pre></pre>
<p><strong>Key characteristics:</strong></p>
<ul>
<li>Translates user input to domain operations</li>
<li>Handles presentation concerns (formatting, errors)</li>
<li>Drives the application core</li>
</ul>
<h4 id="http-api-adapter-future"><a class="header" href="#http-api-adapter-future">HTTP API Adapter (future)</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Another primary adapter for HTTP API
async fn handle_process_request(
    req: HttpRequest,
    services: Arc&lt;Services&gt;,
) -&gt; HttpResponse {
    // Parse HTTP request
    let body: ProcessFileRequest = req.json().await?;

    // Drive domain through the same port
    let result = services.file_processor
        .process_file(&amp;body.pipeline_id, &amp;body.input, &amp;body.output)
        .await;

    // Convert result to HTTP response
    match result {
        Ok(metrics) =&gt; HttpResponse::Ok().json(metrics),
        Err(e) =&gt; HttpResponse::BadRequest().json(e),
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Notice:</strong> Both CLI and HTTP adapters use the <strong>same domain ports</strong>. The domain doesn't know or care which adapter is calling it.</p>
<h3 id="secondary-adapters-driven"><a class="header" href="#secondary-adapters-driven">Secondary Adapters (Driven)</a></h3>
<p>Secondary adapters are <strong>driven by</strong> the application. They implement the interfaces the domain needs.</p>
<h4 id="sqlite-repository-adapter"><a class="header" href="#sqlite-repository-adapter">SQLite Repository Adapter</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Infrastructure layer - implements domain port
pub struct SQLitePipelineRepository {
    pool: SqlitePool,
}

#[async_trait]
impl PipelineRepository for SQLitePipelineRepository {
    async fn create(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt; {
        // Convert domain entity to database row
        let row = PipelineRow::from_domain(pipeline);

        // Persist to SQLite
        sqlx::query(
            "INSERT INTO pipelines (id, name, archived, created_at, updated_at)
             VALUES (?, ?, ?, ?, ?)"
        )
        .bind(&amp;row.id)
        .bind(&amp;row.name)
        .bind(row.archived)
        .bind(&amp;row.created_at)
        .bind(&amp;row.updated_at)
        .execute(&amp;self.pool)
        .await
        .map_err(|e| PipelineError::RepositoryError(e.to_string()))?;

        Ok(())
    }

    async fn find_by_id(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;Option&lt;Pipeline&gt;, PipelineError&gt; {
        // Query SQLite
        let row = sqlx::query_as::&lt;_, PipelineRow&gt;(
            "SELECT * FROM pipelines WHERE id = ?"
        )
        .bind(id.to_string())
        .fetch_optional(&amp;self.pool)
        .await
        .map_err(|e| PipelineError::RepositoryError(e.to_string()))?;

        // Convert database row to domain entity
        row.map(|r| Pipeline::from_database(r)).transpose()
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Key characteristics:</strong></p>
<ul>
<li>Implements domain-defined interface</li>
<li>Handles database-specific operations</li>
<li>Translates between domain models and database rows</li>
<li>Can be swapped without changing domain</li>
</ul>
<h4 id="compression-service-adapter"><a class="header" href="#compression-service-adapter">Compression Service Adapter</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Infrastructure layer - implements domain port
pub struct CompressionServiceAdapter {
    // Internal state for compression libraries
}

#[async_trait]
impl CompressionService for CompressionServiceAdapter {
    async fn compress(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        // Route to appropriate compression library
        match algorithm.name() {
            "brotli" =&gt; {
                let mut compressed = Vec::new();
                brotli::BrotliCompress(
                    &amp;mut Cursor::new(data),
                    &amp;mut compressed,
                    &amp;Default::default(),
                )?;
                Ok(compressed)
            }
            "zstd" =&gt; {
                let compressed = zstd::encode_all(data, 3)?;
                Ok(compressed)
            }
            "lz4" =&gt; {
                let compressed = lz4::block::compress(data, None, false)?;
                Ok(compressed)
            }
            _ =&gt; Err(PipelineError::UnsupportedAlgorithm(
                algorithm.name().to_string()
            )),
        }
    }

    async fn decompress(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        // Similar implementation for decompression
        // ...
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Key characteristics:</strong></p>
<ul>
<li>Wraps external libraries (brotli, zstd, lz4)</li>
<li>Implements domain interface</li>
<li>Handles library-specific details</li>
<li>Can be swapped for different implementations</li>
</ul>
<h2 id="benefits-of-hexagonal-architecture"><a class="header" href="#benefits-of-hexagonal-architecture">Benefits of Hexagonal Architecture</a></h2>
<h3 id="1-testability-1"><a class="header" href="#1-testability-1">1. Testability</a></h3>
<p>You can test the domain in isolation using mock adapters:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Mock adapter for testing
struct MockPipelineRepository {
    pipelines: Mutex&lt;HashMap&lt;PipelineId, Pipeline&gt;&gt;,
}

#[async_trait]
impl PipelineRepository for MockPipelineRepository {
    async fn create(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt; {
        self.pipelines.lock().unwrap()
            .insert(pipeline.id().clone(), pipeline.clone());
        Ok(())
    }

    async fn find_by_id(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;Option&lt;Pipeline&gt;, PipelineError&gt; {
        Ok(self.pipelines.lock().unwrap().get(id).cloned())
    }
}

#[tokio::test]
async fn test_file_processor_service() {
    // Use mock adapter instead of real database
    let repo = Arc::new(MockPipelineRepository::new());
    let service = FileProcessorService::new(repo);

    // Test domain logic without database
    let result = service.process_file(/* ... */).await;
    assert!(result.is_ok());
}
<span class="boring">}</span></code></pre></pre>
<h3 id="2-flexibility-1"><a class="header" href="#2-flexibility-1">2. Flexibility</a></h3>
<p>Swap implementations without changing the domain:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Start with SQLite
let repo: Arc&lt;dyn PipelineRepository&gt; =
    Arc::new(SQLitePipelineRepository::new(pool));

// Later, switch to PostgreSQL
let repo: Arc&lt;dyn PipelineRepository&gt; =
    Arc::new(PostgresPipelineRepository::new(pool));

// Domain doesn't change - same interface!
let service = FileProcessorService::new(repo);
<span class="boring">}</span></code></pre></pre>
<h3 id="3-multiple-interfaces"><a class="header" href="#3-multiple-interfaces">3. Multiple Interfaces</a></h3>
<p>Support multiple input sources using the same domain:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// CLI adapter
async fn cli_handler(cli: Cli, services: Arc&lt;Services&gt;) {
    services.file_processor.process_file(/* ... */).await?;
}

// HTTP adapter
async fn http_handler(req: HttpRequest, services: Arc&lt;Services&gt;) {
    services.file_processor.process_file(/* ... */).await?;
}

// gRPC adapter
async fn grpc_handler(req: GrpcRequest, services: Arc&lt;Services&gt;) {
    services.file_processor.process_file(/* ... */).await?;
}
<span class="boring">}</span></code></pre></pre>
<p>All three adapters use the <strong>same domain logic</strong> through the <strong>same port</strong>.</p>
<h3 id="4-technology-independence"><a class="header" href="#4-technology-independence">4. Technology Independence</a></h3>
<p>The domain doesn't depend on specific technologies:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Domain doesn't know about:
// - SQLite, PostgreSQL, or MongoDB
// - HTTP, gRPC, or CLI
// - Brotli, Zstd, or LZ4
// - Any specific framework or library

// It only knows about:
// - Business concepts (Pipeline, Stage, Chunk)
// - Business rules (validation, ordering)
// - Interfaces it needs (Repository, CompressionService)
<span class="boring">}</span></code></pre></pre>
<h2 id="dependency-inversion-1"><a class="header" href="#dependency-inversion-1">Dependency Inversion</a></h2>
<p>Hexagonal Architecture relies on <strong>Dependency Inversion Principle</strong>:</p>
<pre><code class="language-text">Traditional:                    Hexagonal:

┌──────────┐                   ┌──────────┐
│   CLI    │                   │   CLI    │
└────┬─────┘                   └────┬─────┘
     │ depends on                   │ depends on
     ▼                              ▼
┌──────────┐                   ┌──────────┐
│ Domain   │                   │  Port    │ ← Interface
└────┬─────┘                   │ (trait)  │
     │ depends on               └────△─────┘
     ▼                               │ implements
┌──────────┐                   ┌────┴─────┐
│ Database │                   │  Domain  │
└──────────┘                   └──────────┘
                                     △
                                     │ implements
                               ┌─────┴─────┐
                               │ Database  │
                               │ Adapter   │
                               └───────────┘
</code></pre>
<p><strong>Traditional:</strong> Domain depends on Database (tight coupling)
<strong>Hexagonal:</strong> Database depends on Domain interface (loose coupling)</p>
<h2 id="our-adapter-structure"><a class="header" href="#our-adapter-structure">Our Adapter Structure</a></h2>
<pre><code class="language-text">pipeline/src/
├── infrastructure/
│   └── adapters/
│       ├── compression_service_adapter.rs    # Implements CompressionService
│       ├── encryption_service_adapter.rs     # Implements EncryptionService
│       ├── async_compression_adapter.rs      # Async wrapper
│       ├── async_encryption_adapter.rs       # Async wrapper
│       └── repositories/
│           ├── sqlite_repository_adapter.rs  # Implements PipelineRepository
│           └── sqlite_base_repository.rs     # Base repository utilities
</code></pre>
<h2 id="adapter-responsibilities"><a class="header" href="#adapter-responsibilities">Adapter Responsibilities</a></h2>
<h3 id="what-adapters-should-do"><a class="header" href="#what-adapters-should-do">What Adapters Should Do</a></h3>
<p>✅ <strong>Translate</strong> between domain and external systems
✅ <strong>Handle</strong> technology-specific details
✅ <strong>Implement</strong> domain-defined interfaces
✅ <strong>Convert</strong> data formats (domain ↔ database, domain ↔ API)
✅ <strong>Manage</strong> external resources (connections, files, etc.)</p>
<h3 id="what-adapters-should-not-do"><a class="header" href="#what-adapters-should-not-do">What Adapters Should NOT Do</a></h3>
<p>❌ <strong>Contain business logic</strong> - belongs in domain
❌ <strong>Make business decisions</strong> - belongs in domain
❌ <strong>Validate business rules</strong> - belongs in domain
❌ <strong>Know about other adapters</strong> - should be independent
❌ <strong>Expose infrastructure details</strong> to domain</p>
<h2 id="example-complete-flow"><a class="header" href="#example-complete-flow">Example: Complete Flow</a></h2>
<p>Let's trace a complete request through the hexagonal architecture:</p>
<pre><code class="language-text">1. Primary Adapter (CLI)
   ↓ User types: pipeline process --input file.txt --output file.bin

2. Parse and validate input
   ↓ Create FilePath("/path/to/file.txt")

3. Call Primary Port (FileProcessorService)
   ↓ process_file(pipeline_id, input_path, output_path)

4. Domain Logic
   ├─ Fetch Pipeline (via PipelineRepository port)
   │  └─ Secondary Adapter queries SQLite
   ├─ Process each stage
   │  ├─ Compress (via CompressionService port)
   │  │  └─ Secondary Adapter uses brotli library
   │  ├─ Encrypt (via EncryptionService port)
   │  │  └─ Secondary Adapter uses aes-gcm library
   │  └─ Calculate checksum (via ChecksumService port)
   │     └─ Secondary Adapter uses sha2 library
   └─ Return ProcessingMetrics

5. Primary Adapter formats output
   ↓ Display metrics to user
</code></pre>
<h2 id="common-adapter-patterns"><a class="header" href="#common-adapter-patterns">Common Adapter Patterns</a></h2>
<h3 id="repository-adapter-pattern"><a class="header" href="#repository-adapter-pattern">Repository Adapter Pattern</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 1. Domain defines interface (port)
pub trait PipelineRepository: Send + Sync {
    async fn find_by_id(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;Option&lt;Pipeline&gt;&gt;;
}

// 2. Infrastructure implements adapter
pub struct SQLitePipelineRepository { /* ... */ }

impl PipelineRepository for SQLitePipelineRepository {
    async fn find_by_id(&amp;self, id: &amp;PipelineId) -&gt; Result&lt;Option&lt;Pipeline&gt;&gt; {
        // Database-specific implementation
    }
}

// 3. Application uses through interface
pub struct FileProcessorService {
    repository: Arc&lt;dyn PipelineRepository&gt;,  // Uses interface, not concrete type
}
<span class="boring">}</span></code></pre></pre>
<h3 id="service-adapter-pattern"><a class="header" href="#service-adapter-pattern">Service Adapter Pattern</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 1. Domain defines interface
pub trait CompressionService: Send + Sync {
    async fn compress(&amp;self, data: &amp;[u8], algo: &amp;Algorithm) -&gt; Result&lt;Vec&lt;u8&gt;&gt;;
}

// 2. Infrastructure implements adapter
pub struct CompressionServiceAdapter { /* ... */ }

impl CompressionService for CompressionServiceAdapter {
    async fn compress(&amp;self, data: &amp;[u8], algo: &amp;Algorithm) -&gt; Result&lt;Vec&lt;u8&gt;&gt; {
        // Library-specific implementation
    }
}

// 3. Application uses through interface
pub struct StageExecutor {
    compression: Arc&lt;dyn CompressionService&gt;,  // Uses interface
}
<span class="boring">}</span></code></pre></pre>
<h2 id="testing-with-adapters"><a class="header" href="#testing-with-adapters">Testing with Adapters</a></h2>
<h3 id="unit-tests-domain-layer"><a class="header" href="#unit-tests-domain-layer">Unit Tests (Domain Layer)</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Test domain logic without any adapters
#[test]
fn test_pipeline_validation() {
    // Pure domain logic - no infrastructure needed
    let result = Pipeline::new("test", vec![]);
    assert!(result.is_err());
}
<span class="boring">}</span></code></pre></pre>
<h3 id="integration-tests-with-mock-adapters"><a class="header" href="#integration-tests-with-mock-adapters">Integration Tests (With Mock Adapters)</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_file_processing() {
    // Use mock adapters
    let mock_repo = Arc::new(MockPipelineRepository::new());
    let mock_compression = Arc::new(MockCompressionService::new());

    let service = FileProcessorService::new(mock_repo, mock_compression);

    // Test without real database or compression
    let result = service.process_file(/* ... */).await;
    assert!(result.is_ok());
}
<span class="boring">}</span></code></pre></pre>
<h3 id="end-to-end-tests-with-real-adapters"><a class="header" href="#end-to-end-tests-with-real-adapters">End-to-End Tests (With Real Adapters)</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_real_file_processing() {
    // Use real adapters
    let db_pool = create_test_database().await;
    let real_repo = Arc::new(SQLitePipelineRepository::new(db_pool));
    let real_compression = Arc::new(CompressionServiceAdapter::new());

    let service = FileProcessorService::new(real_repo, real_compression);

    // Test with real infrastructure
    let result = service.process_file(/* ... */).await;
    assert!(result.is_ok());
}
<span class="boring">}</span></code></pre></pre>
<h2 id="next-steps-9"><a class="header" href="#next-steps-9">Next Steps</a></h2>
<p>Now that you understand hexagonal architecture:</p>
<ul>
<li><a href="architecture/dependencies.html">Dependency Inversion</a> - Managing dependencies properly</li>
<li><a href="architecture/layers.html">Layered Architecture</a> - How layers relate to ports/adapters</li>
<li><a href="architecture/repository-pattern.html">Repository Pattern</a> - Detailed repository implementation</li>
<li><a href="architecture/domain-model.html">Domain Model</a> - Understanding the core domain</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="observer-pattern"><a class="header" href="#observer-pattern">Observer Pattern</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>The Observer pattern for metrics and events.</p>
<h2 id="pattern-overview-3"><a class="header" href="#pattern-overview-3">Pattern Overview</a></h2>
<p>TODO: Extract from metrics observer</p>
<h2 id="implementation-1"><a class="header" href="#implementation-1">Implementation</a></h2>
<p>TODO: Show implementation</p>
<h2 id="use-cases"><a class="header" href="#use-cases">Use Cases</a></h2>
<p>TODO: List use cases</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="stage-processing"><a class="header" href="#stage-processing">Stage Processing</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>How stages process data in the pipeline.</p>
<h2 id="stage-lifecycle"><a class="header" href="#stage-lifecycle">Stage Lifecycle</a></h2>
<p>TODO: Explain stage lifecycle</p>
<h2 id="stage-interface"><a class="header" href="#stage-interface">Stage Interface</a></h2>
<p>TODO: Show stage trait</p>
<h2 id="stage-execution"><a class="header" href="#stage-execution">Stage Execution</a></h2>
<p>TODO: Explain execution model</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="compression-implementation"><a class="header" href="#compression-implementation">Compression Implementation</a></h1>
<p><strong>Version:</strong> 1.0
<strong>Date:</strong> 2025-01-04
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner, Claude Code
<strong>Status:</strong> Active</p>
<h2 id="overview-4"><a class="header" href="#overview-4">Overview</a></h2>
<p>The compression service provides multiple compression algorithms optimized for different use cases. It's implemented as an infrastructure adapter that implements the domain's <code>CompressionService</code> trait.</p>
<p><strong>File:</strong> <code>pipeline/src/infrastructure/adapters/compression_service_adapter.rs</code></p>
<h2 id="supported-algorithms"><a class="header" href="#supported-algorithms">Supported Algorithms</a></h2>
<h3 id="brotli"><a class="header" href="#brotli">Brotli</a></h3>
<ul>
<li><strong>Best for:</strong> Web content, text files, logs</li>
<li><strong>Compression ratio:</strong> Excellent (typically 15-25% better than gzip)</li>
<li><strong>Speed:</strong> Slower compression, fast decompression</li>
<li><strong>Memory:</strong> Higher memory usage (~10-20 MB)</li>
<li><strong>Library:</strong> <code>brotli</code> crate</li>
</ul>
<p><strong>Use cases:</strong></p>
<ul>
<li>Archival storage where size is critical</li>
<li>Web assets (HTML, CSS, JavaScript)</li>
<li>Log files with repetitive patterns</li>
</ul>
<p><strong>Performance characteristics:</strong></p>
<pre><code class="language-text">File Type    | Compression Ratio | Speed      | Memory
-------------|-------------------|------------|--------
Text logs    | 85-90%           | Slow       | High
HTML/CSS     | 80-85%           | Slow       | High
Binary data  | 60-70%           | Moderate   | High
</code></pre>
<h3 id="gzip"><a class="header" href="#gzip">Gzip</a></h3>
<ul>
<li><strong>Best for:</strong> General-purpose compression</li>
<li><strong>Compression ratio:</strong> Good (industry standard)</li>
<li><strong>Speed:</strong> Moderate compression and decompression</li>
<li><strong>Memory:</strong> Moderate usage (~5-10 MB)</li>
<li><strong>Library:</strong> <code>flate2</code> crate</li>
</ul>
<p><strong>Use cases:</strong></p>
<ul>
<li>General file compression</li>
<li>Compatibility with other systems</li>
<li>Balanced performance needs</li>
</ul>
<p><strong>Performance characteristics:</strong></p>
<pre><code class="language-text">File Type    | Compression Ratio | Speed      | Memory
-------------|-------------------|------------|--------
Text logs    | 75-80%           | Moderate   | Moderate
HTML/CSS     | 70-75%           | Moderate   | Moderate
Binary data  | 50-60%           | Moderate   | Moderate
</code></pre>
<h3 id="zstandard-zstd"><a class="header" href="#zstandard-zstd">Zstandard (Zstd)</a></h3>
<ul>
<li><strong>Best for:</strong> Modern systems, real-time compression</li>
<li><strong>Compression ratio:</strong> Very good (better than gzip)</li>
<li><strong>Speed:</strong> Very fast compression and decompression</li>
<li><strong>Memory:</strong> Efficient (~5-15 MB depending on level)</li>
<li><strong>Library:</strong> <code>zstd</code> crate</li>
</ul>
<p><strong>Use cases:</strong></p>
<ul>
<li>Real-time data processing</li>
<li>Large file compression</li>
<li>Network transmission</li>
<li>Modern backup systems</li>
</ul>
<p><strong>Performance characteristics:</strong></p>
<pre><code class="language-text">File Type    | Compression Ratio | Speed      | Memory
-------------|-------------------|------------|--------
Text logs    | 80-85%           | Fast       | Low
HTML/CSS     | 75-80%           | Fast       | Low
Binary data  | 55-65%           | Fast       | Low
</code></pre>
<h3 id="lz4"><a class="header" href="#lz4">LZ4</a></h3>
<ul>
<li><strong>Best for:</strong> Real-time applications, live streams</li>
<li><strong>Compression ratio:</strong> Moderate</li>
<li><strong>Speed:</strong> Extremely fast (fastest available)</li>
<li><strong>Memory:</strong> Very low usage (~1-5 MB)</li>
<li><strong>Library:</strong> <code>lz4</code> crate</li>
</ul>
<p><strong>Use cases:</strong></p>
<ul>
<li>Real-time data streams</li>
<li>Low-latency requirements</li>
<li>Systems with limited memory</li>
<li>Network protocols</li>
</ul>
<p><strong>Performance characteristics:</strong></p>
<pre><code class="language-text">File Type    | Compression Ratio | Speed         | Memory
-------------|-------------------|---------------|--------
Text logs    | 60-70%           | Very Fast     | Very Low
HTML/CSS     | 55-65%           | Very Fast     | Very Low
Binary data  | 40-50%           | Very Fast     | Very Low
</code></pre>
<h2 id="architecture-1"><a class="header" href="#architecture-1">Architecture</a></h2>
<h3 id="service-interface-domain-layer"><a class="header" href="#service-interface-domain-layer">Service Interface (Domain Layer)</a></h3>
<p>The domain layer defines what compression operations are needed:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// pipeline-domain/src/services/compression_service.rs
use async_trait::async_trait;
use crate::value_objects::Algorithm;
use crate::error::PipelineError;

#[async_trait]
pub trait CompressionService: Send + Sync {
    /// Compress data using the specified algorithm
    async fn compress(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt;;

    /// Decompress data using the specified algorithm
    async fn decompress(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt;;
}
<span class="boring">}</span></code></pre></pre>
<h3 id="service-implementation-infrastructure-layer"><a class="header" href="#service-implementation-infrastructure-layer">Service Implementation (Infrastructure Layer)</a></h3>
<p>The infrastructure layer provides the concrete implementation:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// pipeline/src/infrastructure/adapters/compression_service_adapter.rs
pub struct CompressionServiceAdapter {
    // Configuration and state
}

#[async_trait]
impl CompressionService for CompressionServiceAdapter {
    async fn compress(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        // Route to appropriate algorithm
        match algorithm.name() {
            "brotli" =&gt; self.compress_brotli(data),
            "gzip" =&gt; self.compress_gzip(data),
            "zstd" =&gt; self.compress_zstd(data),
            "lz4" =&gt; self.compress_lz4(data),
            _ =&gt; Err(PipelineError::UnsupportedAlgorithm(
                algorithm.name().to_string()
            )),
        }
    }

    async fn decompress(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        // Route to appropriate algorithm
        match algorithm.name() {
            "brotli" =&gt; self.decompress_brotli(data),
            "gzip" =&gt; self.decompress_gzip(data),
            "zstd" =&gt; self.decompress_zstd(data),
            "lz4" =&gt; self.decompress_lz4(data),
            _ =&gt; Err(PipelineError::UnsupportedAlgorithm(
                algorithm.name().to_string()
            )),
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="algorithm-implementations"><a class="header" href="#algorithm-implementations">Algorithm Implementations</a></h2>
<h3 id="brotli-implementation"><a class="header" href="#brotli-implementation">Brotli Implementation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl CompressionServiceAdapter {
    fn compress_brotli(&amp;self, data: &amp;[u8]) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        use brotli::enc::BrotliEncoderParams;
        use std::io::Cursor;

        let mut compressed = Vec::new();
        let mut params = BrotliEncoderParams::default();

        // Quality level 11 = maximum compression
        params.quality = 11;

        brotli::BrotliCompress(
            &amp;mut Cursor::new(data),
            &amp;mut compressed,
            &amp;params,
        ).map_err(|e| PipelineError::CompressionError(e.to_string()))?;

        Ok(compressed)
    }

    fn decompress_brotli(&amp;self, data: &amp;[u8]) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        use brotli::Decompressor;
        use std::io::Read;

        let mut decompressed = Vec::new();
        let mut decompressor = Decompressor::new(data, 4096);

        decompressor.read_to_end(&amp;mut decompressed)
            .map_err(|e| PipelineError::DecompressionError(e.to_string()))?;

        Ok(decompressed)
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="gzip-implementation"><a class="header" href="#gzip-implementation">Gzip Implementation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl CompressionServiceAdapter {
    fn compress_gzip(&amp;self, data: &amp;[u8]) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        use flate2::write::GzEncoder;
        use flate2::Compression;
        use std::io::Write;

        let mut encoder = GzEncoder::new(Vec::new(), Compression::default());
        encoder.write_all(data)
            .map_err(|e| PipelineError::CompressionError(e.to_string()))?;

        encoder.finish()
            .map_err(|e| PipelineError::CompressionError(e.to_string()))
    }

    fn decompress_gzip(&amp;self, data: &amp;[u8]) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        use flate2::read::GzDecoder;
        use std::io::Read;

        let mut decoder = GzDecoder::new(data);
        let mut decompressed = Vec::new();

        decoder.read_to_end(&amp;mut decompressed)
            .map_err(|e| PipelineError::DecompressionError(e.to_string()))?;

        Ok(decompressed)
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="zstandard-implementation"><a class="header" href="#zstandard-implementation">Zstandard Implementation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl CompressionServiceAdapter {
    fn compress_zstd(&amp;self, data: &amp;[u8]) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        // Level 3 provides good balance of speed and compression
        zstd::encode_all(data, 3)
            .map_err(|e| PipelineError::CompressionError(e.to_string()))
    }

    fn decompress_zstd(&amp;self, data: &amp;[u8]) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        zstd::decode_all(data)
            .map_err(|e| PipelineError::DecompressionError(e.to_string()))
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="lz4-implementation"><a class="header" href="#lz4-implementation">LZ4 Implementation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl CompressionServiceAdapter {
    fn compress_lz4(&amp;self, data: &amp;[u8]) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        lz4::block::compress(data, None, false)
            .map_err(|e| PipelineError::CompressionError(e.to_string()))
    }

    fn decompress_lz4(&amp;self, data: &amp;[u8]) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        // Need to know original size for LZ4
        // This is stored in the file metadata
        lz4::block::decompress(data, None)
            .map_err(|e| PipelineError::DecompressionError(e.to_string()))
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="performance-optimizations"><a class="header" href="#performance-optimizations">Performance Optimizations</a></h2>
<h3 id="parallel-chunk-processing-1"><a class="header" href="#parallel-chunk-processing-1">Parallel Chunk Processing</a></h3>
<p>The compression service processes file chunks in parallel using Rayon:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use rayon::prelude::*;

pub async fn compress_chunks(
    chunks: Vec&lt;FileChunk&gt;,
    algorithm: &amp;Algorithm,
    compression_service: &amp;Arc&lt;dyn CompressionService&gt;,
) -&gt; Result&lt;Vec&lt;CompressedChunk&gt;, PipelineError&gt; {
    // Process chunks in parallel
    chunks.par_iter()
        .map(|chunk| {
            // Compress each chunk independently
            let compressed_data = compression_service
                .compress(&amp;chunk.data, algorithm)?;

            Ok(CompressedChunk {
                sequence: chunk.sequence,
                data: compressed_data,
                original_size: chunk.data.len(),
            })
        })
        .collect()
}
<span class="boring">}</span></code></pre></pre>
<h3 id="memory-management"><a class="header" href="#memory-management">Memory Management</a></h3>
<p>Efficient buffer management reduces allocations:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct CompressionBuffer {
    input_buffer: Vec&lt;u8&gt;,
    output_buffer: Vec&lt;u8&gt;,
}

impl CompressionBuffer {
    pub fn new(chunk_size: usize) -&gt; Self {
        Self {
            // Pre-allocate buffers
            input_buffer: Vec::with_capacity(chunk_size),
            output_buffer: Vec::with_capacity(chunk_size * 2), // Assume 2x for safety
        }
    }

    pub fn compress(&amp;mut self, data: &amp;[u8], algorithm: &amp;Algorithm) -&gt; Result&lt;&amp;[u8]&gt; {
        // Reuse buffers instead of allocating new ones
        self.input_buffer.clear();
        self.output_buffer.clear();

        self.input_buffer.extend_from_slice(data);
        // Compress from input_buffer to output_buffer
        // ...

        Ok(&amp;self.output_buffer)
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="adaptive-compression-levels"><a class="header" href="#adaptive-compression-levels">Adaptive Compression Levels</a></h3>
<p>Adjust compression levels based on data characteristics:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn select_compression_level(data: &amp;[u8]) -&gt; u32 {
    // Analyze data entropy
    let entropy = calculate_entropy(data);

    if entropy &lt; 0.5 {
        // Low entropy (highly repetitive) - use maximum compression
        11
    } else if entropy &lt; 0.7 {
        // Medium entropy - balanced compression
        6
    } else {
        // High entropy (random-like) - fast compression
        3
    }
}

fn calculate_entropy(data: &amp;[u8]) -&gt; f64 {
    // Calculate Shannon entropy
    let mut freq = [0u32; 256];
    for &amp;byte in data {
        freq[byte as usize] += 1;
    }

    let len = data.len() as f64;
    freq.iter()
        .filter(|&amp;&amp;f| f &gt; 0)
        .map(|&amp;f| {
            let p = f as f64 / len;
            -p * p.log2()
        })
        .sum()
}
<span class="boring">}</span></code></pre></pre>
<h2 id="configuration"><a class="header" href="#configuration">Configuration</a></h2>
<h3 id="compression-levels"><a class="header" href="#compression-levels">Compression Levels</a></h3>
<p>Different algorithms support different compression levels:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct CompressionConfig {
    pub algorithm: Algorithm,
    pub level: CompressionLevel,
    pub chunk_size: usize,
    pub parallel_chunks: usize,
}

pub enum CompressionLevel {
    Fastest,      // LZ4, Zstd level 1
    Fast,         // Zstd level 3, Gzip level 1
    Balanced,     // Zstd level 6, Gzip level 6
    Best,         // Brotli level 11, Gzip level 9
    BestSize,     // Brotli level 11 with maximum window
}

impl CompressionConfig {
    pub fn for_speed() -&gt; Self {
        Self {
            algorithm: Algorithm::lz4(),
            level: CompressionLevel::Fastest,
            chunk_size: 64 * 1024 * 1024, // 64 MB chunks
            parallel_chunks: num_cpus::get(),
        }
    }

    pub fn for_size() -&gt; Self {
        Self {
            algorithm: Algorithm::brotli(),
            level: CompressionLevel::BestSize,
            chunk_size: 4 * 1024 * 1024, // 4 MB chunks for better compression
            parallel_chunks: num_cpus::get(),
        }
    }

    pub fn balanced() -&gt; Self {
        Self {
            algorithm: Algorithm::zstd(),
            level: CompressionLevel::Balanced,
            chunk_size: 16 * 1024 * 1024, // 16 MB chunks
            parallel_chunks: num_cpus::get(),
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="error-handling"><a class="header" href="#error-handling">Error Handling</a></h2>
<p>Comprehensive error handling for compression failures:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Debug, thiserror::Error)]
pub enum CompressionError {
    #[error("Compression failed: {0}")]
    CompressionFailed(String),

    #[error("Decompression failed: {0}")]
    DecompressionFailed(String),

    #[error("Unsupported algorithm: {0}")]
    UnsupportedAlgorithm(String),

    #[error("Invalid compression level: {0}")]
    InvalidLevel(u32),

    #[error("Buffer overflow during compression")]
    BufferOverflow,

    #[error("Corrupted compressed data")]
    CorruptedData,
}

impl From&lt;CompressionError&gt; for PipelineError {
    fn from(err: CompressionError) -&gt; Self {
        match err {
            CompressionError::CompressionFailed(msg) =&gt;
                PipelineError::CompressionError(msg),
            CompressionError::DecompressionFailed(msg) =&gt;
                PipelineError::DecompressionError(msg),
            CompressionError::UnsupportedAlgorithm(algo) =&gt;
                PipelineError::UnsupportedAlgorithm(algo),
            _ =&gt; PipelineError::CompressionError(err.to_string()),
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="usage-examples"><a class="header" href="#usage-examples">Usage Examples</a></h2>
<h3 id="basic-compression"><a class="header" href="#basic-compression">Basic Compression</a></h3>
<pre><pre class="playground"><code class="language-rust">use pipeline::infrastructure::adapters::CompressionServiceAdapter;
use pipeline_domain::services::CompressionService;
use pipeline_domain::value_objects::Algorithm;

#[tokio::main]
async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    // Create compression service
    let compression = CompressionServiceAdapter::new();

    // Compress data
    let data = b"Hello, World!".to_vec();
    let compressed = compression.compress(&amp;data, &amp;Algorithm::zstd()).await?;

    println!("Original size: {} bytes", data.len());
    println!("Compressed size: {} bytes", compressed.len());
    println!("Compression ratio: {:.2}%",
        (1.0 - compressed.len() as f64 / data.len() as f64) * 100.0);

    // Decompress data
    let decompressed = compression.decompress(&amp;compressed, &amp;Algorithm::zstd()).await?;
    assert_eq!(data, decompressed);

    Ok(())
}</code></pre></pre>
<h3 id="comparing-algorithms"><a class="header" href="#comparing-algorithms">Comparing Algorithms</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn compare_algorithms(data: &amp;[u8]) -&gt; Result&lt;(), PipelineError&gt; {
    let compression = CompressionServiceAdapter::new();
    let algorithms = vec![
        Algorithm::brotli(),
        Algorithm::gzip(),
        Algorithm::zstd(),
        Algorithm::lz4(),
    ];

    println!("Original size: {} bytes\n", data.len());

    for algo in algorithms {
        let start = Instant::now();
        let compressed = compression.compress(data, &amp;algo).await?;
        let compress_time = start.elapsed();

        let start = Instant::now();
        let _decompressed = compression.decompress(&amp;compressed, &amp;algo).await?;
        let decompress_time = start.elapsed();

        println!("Algorithm: {}", algo.name());
        println!("  Compressed size: {} bytes ({:.2}% reduction)",
            compressed.len(),
            (1.0 - compressed.len() as f64 / data.len() as f64) * 100.0
        );
        println!("  Compression time: {:?}", compress_time);
        println!("  Decompression time: {:?}\n", decompress_time);
    }

    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h2 id="benchmarks"><a class="header" href="#benchmarks">Benchmarks</a></h2>
<p>Typical performance on a modern system (Intel i7, 16GB RAM):</p>
<pre><code class="language-text">Algorithm | File Size | Comp. Time | Decomp. Time | Ratio | Throughput
----------|-----------|------------|--------------|-------|------------
Brotli    | 100 MB    | 8.2s       | 0.4s         | 82%   | 12 MB/s
Gzip      | 100 MB    | 1.5s       | 0.6s         | 75%   | 67 MB/s
Zstd      | 100 MB    | 0.8s       | 0.3s         | 78%   | 125 MB/s
LZ4       | 100 MB    | 0.2s       | 0.1s         | 60%   | 500 MB/s
</code></pre>
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<h3 id="choosing-the-right-algorithm"><a class="header" href="#choosing-the-right-algorithm">Choosing the Right Algorithm</a></h3>
<p><strong>Use Brotli when:</strong></p>
<ul>
<li>Storage space is critical</li>
<li>Compression time is not a concern</li>
<li>Data will be compressed once, decompressed many times (web assets)</li>
</ul>
<p><strong>Use Gzip when:</strong></p>
<ul>
<li>Compatibility with other systems is required</li>
<li>Balanced performance is needed</li>
<li>Working with legacy systems</li>
</ul>
<p><strong>Use Zstandard when:</strong></p>
<ul>
<li>Modern systems are available</li>
<li>Both speed and compression ratio matter</li>
<li>Real-time processing is needed</li>
</ul>
<p><strong>Use LZ4 when:</strong></p>
<ul>
<li>Speed is the top priority</li>
<li>Working with live data streams</li>
<li>Low latency is critical</li>
<li>Memory is limited</li>
</ul>
<h3 id="chunk-size-selection"><a class="header" href="#chunk-size-selection">Chunk Size Selection</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// For maximum compression
let chunk_size = 4 * 1024 * 1024;  // 4 MB

// For balanced performance
let chunk_size = 16 * 1024 * 1024; // 16 MB

// For maximum speed
let chunk_size = 64 * 1024 * 1024; // 64 MB
<span class="boring">}</span></code></pre></pre>
<h3 id="memory-considerations"><a class="header" href="#memory-considerations">Memory Considerations</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Estimate memory usage
fn estimate_memory_usage(
    chunk_size: usize,
    parallel_chunks: usize,
    algorithm: &amp;Algorithm,
) -&gt; usize {
    let per_chunk_overhead = match algorithm.name() {
        "brotli" =&gt; chunk_size * 2,  // Brotli uses ~2x for internal buffers
        "gzip" =&gt; chunk_size,         // Gzip uses ~1x
        "zstd" =&gt; chunk_size / 2,     // Zstd is efficient
        "lz4" =&gt; chunk_size / 4,      // LZ4 is very efficient
        _ =&gt; chunk_size,
    };

    per_chunk_overhead * parallel_chunks
}
<span class="boring">}</span></code></pre></pre>
<h2 id="next-steps-10"><a class="header" href="#next-steps-10">Next Steps</a></h2>
<p>Now that you understand compression implementation:</p>
<ul>
<li><a href="implementation/encryption.html">Encryption Implementation</a> - Data encryption details</li>
<li><a href="implementation/integrity.html">Integrity Verification</a> - Checksum implementation</li>
<li><a href="implementation/file-io.html">File I/O</a> - Efficient file operations</li>
<li><a href="implementation/../advanced/performance.html">Performance Tuning</a> - Optimization strategies</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="encryption-implementation"><a class="header" href="#encryption-implementation">Encryption Implementation</a></h1>
<p><strong>Version:</strong> 1.0
<strong>Date:</strong> 2025-01-04
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner, Claude Code
<strong>Status:</strong> Active</p>
<h2 id="overview-5"><a class="header" href="#overview-5">Overview</a></h2>
<p>The encryption service provides authenticated encryption with multiple algorithms, secure key management, and automatic integrity verification. It's implemented as an infrastructure adapter that implements the domain's <code>EncryptionService</code> trait.</p>
<p><strong>File:</strong> <code>pipeline/src/infrastructure/adapters/encryption_service_adapter.rs</code></p>
<h2 id="supported-algorithms-1"><a class="header" href="#supported-algorithms-1">Supported Algorithms</a></h2>
<h3 id="aes-256-gcm-advanced-encryption-standard"><a class="header" href="#aes-256-gcm-advanced-encryption-standard">AES-256-GCM (Advanced Encryption Standard)</a></h3>
<ul>
<li><strong>Key size:</strong> 256 bits (32 bytes)</li>
<li><strong>Nonce size:</strong> 96 bits (12 bytes)</li>
<li><strong>Security:</strong> Industry standard, FIPS 140-2 approved</li>
<li><strong>Performance:</strong> Excellent with AES-NI hardware acceleration</li>
<li><strong>Library:</strong> <code>aes-gcm</code> crate</li>
</ul>
<p><strong>Best for:</strong></p>
<ul>
<li>Compliance requirements (FIPS, government)</li>
<li>Systems with AES-NI support</li>
<li>Maximum security requirements</li>
<li>Long-term data protection</li>
</ul>
<p><strong>Performance characteristics:</strong></p>
<pre><code class="language-text">Operation   | With AES-NI | Without AES-NI
------------|-------------|----------------
Encryption  | 2-3 GB/s    | 100-200 MB/s
Decryption  | 2-3 GB/s    | 100-200 MB/s
Key setup   | &lt; 1 μs      | &lt; 1 μs
Memory      | Low         | Low
</code></pre>
<h3 id="chacha20-poly1305-stream-cipher"><a class="header" href="#chacha20-poly1305-stream-cipher">ChaCha20-Poly1305 (Stream Cipher)</a></h3>
<ul>
<li><strong>Key size:</strong> 256 bits (32 bytes)</li>
<li><strong>Nonce size:</strong> 96 bits (12 bytes)</li>
<li><strong>Security:</strong> Modern, constant-time implementation</li>
<li><strong>Performance:</strong> Consistent across all platforms</li>
<li><strong>Library:</strong> <code>chacha20poly1305</code> crate</li>
</ul>
<p><strong>Best for:</strong></p>
<ul>
<li>Systems without AES-NI</li>
<li>Mobile/embedded devices</li>
<li>Constant-time requirements</li>
<li>Side-channel attack resistance</li>
</ul>
<p><strong>Performance characteristics:</strong></p>
<pre><code class="language-text">Operation   | Any Platform
------------|-------------
Encryption  | 500-800 MB/s
Decryption  | 500-800 MB/s
Key setup   | &lt; 1 μs
Memory      | Low
</code></pre>
<h3 id="aes-128-gcm-faster-aes-variant"><a class="header" href="#aes-128-gcm-faster-aes-variant">AES-128-GCM (Faster AES Variant)</a></h3>
<ul>
<li><strong>Key size:</strong> 128 bits (16 bytes)</li>
<li><strong>Nonce size:</strong> 96 bits (12 bytes)</li>
<li><strong>Security:</strong> Very secure, faster than AES-256</li>
<li><strong>Performance:</strong> ~30% faster than AES-256</li>
<li><strong>Library:</strong> <code>aes-gcm</code> crate</li>
</ul>
<p><strong>Best for:</strong></p>
<ul>
<li>High-performance requirements</li>
<li>Short-term data protection</li>
<li>Real-time encryption</li>
<li>Bandwidth-constrained systems</li>
</ul>
<p><strong>Performance characteristics:</strong></p>
<pre><code class="language-text">Operation   | With AES-NI | Without AES-NI
------------|-------------|----------------
Encryption  | 3-4 GB/s    | 150-250 MB/s
Decryption  | 3-4 GB/s    | 150-250 MB/s
Key setup   | &lt; 1 μs      | &lt; 1 μs
Memory      | Low         | Low
</code></pre>
<h2 id="security-features"><a class="header" href="#security-features">Security Features</a></h2>
<h3 id="authenticated-encryption-aead"><a class="header" href="#authenticated-encryption-aead">Authenticated Encryption (AEAD)</a></h3>
<p>All algorithms provide Authenticated Encryption with Associated Data (AEAD):</p>
<pre><code class="language-text">Plaintext → Encrypt → Ciphertext + Authentication Tag
                ↓
            Detects tampering
</code></pre>
<p><strong>Properties:</strong></p>
<ul>
<li><strong>Confidentiality:</strong> Data is encrypted and unreadable</li>
<li><strong>Integrity:</strong> Any modification is detected</li>
<li><strong>Authentication:</strong> Verifies data origin</li>
</ul>
<h3 id="nonce-management"><a class="header" href="#nonce-management">Nonce Management</a></h3>
<p>Each encryption operation requires a unique nonce (number used once):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Nonces are automatically generated for each chunk
pub struct EncryptionContext {
    key: SecretKey,
    nonce_counter: AtomicU64,  // Incrementing counter
}

impl EncryptionContext {
    fn next_nonce(&amp;self) -&gt; Nonce {
        let counter = self.nonce_counter.fetch_add(1, Ordering::SeqCst);

        // Generate nonce from counter
        let mut nonce = [0u8; 12];
        nonce[0..8].copy_from_slice(&amp;counter.to_le_bytes());

        Nonce::from_slice(&amp;nonce)
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Important:</strong> Never reuse a nonce with the same key!</p>
<h3 id="key-derivation"><a class="header" href="#key-derivation">Key Derivation</a></h3>
<p>Derive encryption keys from passwords using secure KDFs:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub enum KeyDerivationFunction {
    Argon2,   // Memory-hard, GPU-resistant
    Scrypt,   // Memory-hard, tunable
    PBKDF2,   // Standard, widely supported
}

// Derive key from password
pub fn derive_key(
    password: &amp;[u8],
    salt: &amp;[u8],
    kdf: KeyDerivationFunction,
) -&gt; Result&lt;SecretKey, PipelineError&gt; {
    match kdf {
        KeyDerivationFunction::Argon2 =&gt; {
            // Memory: 64 MB, Iterations: 3, Parallelism: 4
            argon2::hash_raw(password, salt, &amp;argon2::Config::default())
        }
        KeyDerivationFunction::Scrypt =&gt; {
            // N=16384, r=8, p=1
            scrypt::scrypt(password, salt, &amp;scrypt::Params::new(14, 8, 1)?)
        }
        KeyDerivationFunction::PBKDF2 =&gt; {
            // 100,000 iterations
            pbkdf2::pbkdf2_hmac::&lt;sha2::Sha256&gt;(password, salt, 100_000)
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="architecture-2"><a class="header" href="#architecture-2">Architecture</a></h2>
<h3 id="service-interface-domain-layer-1"><a class="header" href="#service-interface-domain-layer-1">Service Interface (Domain Layer)</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// pipeline-domain/src/services/encryption_service.rs
use async_trait::async_trait;
use crate::value_objects::Algorithm;
use crate::error::PipelineError;

#[async_trait]
pub trait EncryptionService: Send + Sync {
    /// Encrypt data using the specified algorithm
    async fn encrypt(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
        key: &amp;EncryptionKey,
    ) -&gt; Result&lt;EncryptedData, PipelineError&gt;;

    /// Decrypt data using the specified algorithm
    async fn decrypt(
        &amp;self,
        encrypted: &amp;EncryptedData,
        algorithm: &amp;Algorithm,
        key: &amp;EncryptionKey,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt;;
}

/// Encrypted data with nonce and authentication tag
pub struct EncryptedData {
    pub ciphertext: Vec&lt;u8&gt;,
    pub nonce: Vec&lt;u8&gt;,        // 12 bytes
    pub tag: Vec&lt;u8&gt;,          // 16 bytes (authentication tag)
}
<span class="boring">}</span></code></pre></pre>
<h3 id="service-implementation-infrastructure-layer-1"><a class="header" href="#service-implementation-infrastructure-layer-1">Service Implementation (Infrastructure Layer)</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// pipeline/src/infrastructure/adapters/encryption_service_adapter.rs
use aes_gcm::{Aes256Gcm, Key, Nonce};
use aes_gcm::aead::{Aead, NewAead};
use chacha20poly1305::ChaCha20Poly1305;

pub struct EncryptionServiceAdapter {
    // Secure key storage
    key_store: Arc&lt;RwLock&lt;KeyStore&gt;&gt;,
}

#[async_trait]
impl EncryptionService for EncryptionServiceAdapter {
    async fn encrypt(
        &amp;self,
        data: &amp;[u8],
        algorithm: &amp;Algorithm,
        key: &amp;EncryptionKey,
    ) -&gt; Result&lt;EncryptedData, PipelineError&gt; {
        match algorithm.name() {
            "aes-256-gcm" =&gt; self.encrypt_aes_256_gcm(data, key),
            "chacha20-poly1305" =&gt; self.encrypt_chacha20(data, key),
            "aes-128-gcm" =&gt; self.encrypt_aes_128_gcm(data, key),
            _ =&gt; Err(PipelineError::UnsupportedAlgorithm(
                algorithm.name().to_string()
            )),
        }
    }

    async fn decrypt(
        &amp;self,
        encrypted: &amp;EncryptedData,
        algorithm: &amp;Algorithm,
        key: &amp;EncryptionKey,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        match algorithm.name() {
            "aes-256-gcm" =&gt; self.decrypt_aes_256_gcm(encrypted, key),
            "chacha20-poly1305" =&gt; self.decrypt_chacha20(encrypted, key),
            "aes-128-gcm" =&gt; self.decrypt_aes_128_gcm(encrypted, key),
            _ =&gt; Err(PipelineError::UnsupportedAlgorithm(
                algorithm.name().to_string()
            )),
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="algorithm-implementations-1"><a class="header" href="#algorithm-implementations-1">Algorithm Implementations</a></h2>
<h3 id="aes-256-gcm-implementation"><a class="header" href="#aes-256-gcm-implementation">AES-256-GCM Implementation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl EncryptionServiceAdapter {
    fn encrypt_aes_256_gcm(
        &amp;self,
        data: &amp;[u8],
        key: &amp;EncryptionKey,
    ) -&gt; Result&lt;EncryptedData, PipelineError&gt; {
        use aes_gcm::{Aes256Gcm, Key, Nonce};
        use aes_gcm::aead::{Aead, NewAead};

        // Create cipher from key
        let key = Key::from_slice(key.as_bytes());
        let cipher = Aes256Gcm::new(key);

        // Generate unique nonce
        let nonce = self.generate_nonce();
        let nonce_obj = Nonce::from_slice(&amp;nonce);

        // Encrypt with authentication
        let ciphertext = cipher
            .encrypt(nonce_obj, data)
            .map_err(|e| PipelineError::EncryptionError(e.to_string()))?;

        // Split ciphertext and tag
        let (ciphertext_bytes, tag) = ciphertext.split_at(ciphertext.len() - 16);

        Ok(EncryptedData {
            ciphertext: ciphertext_bytes.to_vec(),
            nonce: nonce.to_vec(),
            tag: tag.to_vec(),
        })
    }

    fn decrypt_aes_256_gcm(
        &amp;self,
        encrypted: &amp;EncryptedData,
        key: &amp;EncryptionKey,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        use aes_gcm::{Aes256Gcm, Key, Nonce};
        use aes_gcm::aead::{Aead, NewAead};

        // Create cipher from key
        let key = Key::from_slice(key.as_bytes());
        let cipher = Aes256Gcm::new(key);

        // Reconstruct nonce
        let nonce = Nonce::from_slice(&amp;encrypted.nonce);

        // Combine ciphertext and tag
        let mut combined = encrypted.ciphertext.clone();
        combined.extend_from_slice(&amp;encrypted.tag);

        // Decrypt and verify authentication
        let plaintext = cipher
            .decrypt(nonce, combined.as_slice())
            .map_err(|e| PipelineError::DecryptionError(
                format!("Decryption failed (possibly tampered): {}", e)
            ))?;

        Ok(plaintext)
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="chacha20-poly1305-implementation"><a class="header" href="#chacha20-poly1305-implementation">ChaCha20-Poly1305 Implementation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl EncryptionServiceAdapter {
    fn encrypt_chacha20(
        &amp;self,
        data: &amp;[u8],
        key: &amp;EncryptionKey,
    ) -&gt; Result&lt;EncryptedData, PipelineError&gt; {
        use chacha20poly1305::{ChaCha20Poly1305, Key, Nonce};
        use chacha20poly1305::aead::{Aead, NewAead};

        // Create cipher from key
        let key = Key::from_slice(key.as_bytes());
        let cipher = ChaCha20Poly1305::new(key);

        // Generate unique nonce
        let nonce = self.generate_nonce();
        let nonce_obj = Nonce::from_slice(&amp;nonce);

        // Encrypt with authentication
        let ciphertext = cipher
            .encrypt(nonce_obj, data)
            .map_err(|e| PipelineError::EncryptionError(e.to_string()))?;

        // Split ciphertext and tag
        let (ciphertext_bytes, tag) = ciphertext.split_at(ciphertext.len() - 16);

        Ok(EncryptedData {
            ciphertext: ciphertext_bytes.to_vec(),
            nonce: nonce.to_vec(),
            tag: tag.to_vec(),
        })
    }

    fn decrypt_chacha20(
        &amp;self,
        encrypted: &amp;EncryptedData,
        key: &amp;EncryptionKey,
    ) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
        use chacha20poly1305::{ChaCha20Poly1305, Key, Nonce};
        use chacha20poly1305::aead::{Aead, NewAead};

        // Create cipher from key
        let key = Key::from_slice(key.as_bytes());
        let cipher = ChaCha20Poly1305::new(key);

        // Reconstruct nonce
        let nonce = Nonce::from_slice(&amp;encrypted.nonce);

        // Combine ciphertext and tag
        let mut combined = encrypted.ciphertext.clone();
        combined.extend_from_slice(&amp;encrypted.tag);

        // Decrypt and verify authentication
        let plaintext = cipher
            .decrypt(nonce, combined.as_slice())
            .map_err(|e| PipelineError::DecryptionError(
                format!("Decryption failed (possibly tampered): {}", e)
            ))?;

        Ok(plaintext)
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="key-management"><a class="header" href="#key-management">Key Management</a></h2>
<h3 id="secure-key-storage"><a class="header" href="#secure-key-storage">Secure Key Storage</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use zeroize::Zeroize;

/// Secure key that zeroizes on drop
pub struct SecretKey {
    bytes: Vec&lt;u8&gt;,
}

impl SecretKey {
    pub fn new(bytes: Vec&lt;u8&gt;) -&gt; Self {
        Self { bytes }
    }

    pub fn as_bytes(&amp;self) -&gt; &amp;[u8] {
        &amp;self.bytes
    }

    /// Generate random key
    pub fn generate(size: usize) -&gt; Self {
        use rand::RngCore;
        let mut bytes = vec![0u8; size];
        rand::thread_rng().fill_bytes(&amp;mut bytes);
        Self::new(bytes)
    }
}

impl Drop for SecretKey {
    fn drop(&amp;mut self) {
        // Securely wipe key from memory
        self.bytes.zeroize();
    }
}

impl Zeroize for SecretKey {
    fn zeroize(&amp;mut self) {
        self.bytes.zeroize();
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="key-rotation"><a class="header" href="#key-rotation">Key Rotation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct KeyRotation {
    current_key: SecretKey,
    previous_key: Option&lt;SecretKey&gt;,
    rotation_interval: Duration,
    last_rotation: Instant,
}

impl KeyRotation {
    pub fn rotate(&amp;mut self) -&gt; Result&lt;(), PipelineError&gt; {
        // Save current key as previous
        let old_key = std::mem::replace(
            &amp;mut self.current_key,
            SecretKey::generate(32),
        );
        self.previous_key = Some(old_key);
        self.last_rotation = Instant::now();

        Ok(())
    }

    pub fn should_rotate(&amp;self) -&gt; bool {
        self.last_rotation.elapsed() &gt;= self.rotation_interval
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="performance-optimizations-1"><a class="header" href="#performance-optimizations-1">Performance Optimizations</a></h2>
<h3 id="parallel-chunk-encryption"><a class="header" href="#parallel-chunk-encryption">Parallel Chunk Encryption</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use rayon::prelude::*;

pub async fn encrypt_chunks(
    chunks: Vec&lt;FileChunk&gt;,
    algorithm: &amp;Algorithm,
    key: &amp;SecretKey,
    encryption_service: &amp;Arc&lt;dyn EncryptionService&gt;,
) -&gt; Result&lt;Vec&lt;EncryptedChunk&gt;, PipelineError&gt; {
    // Encrypt chunks in parallel
    chunks.par_iter()
        .map(|chunk| {
            let encrypted = encryption_service
                .encrypt(&amp;chunk.data, algorithm, key)?;

            Ok(EncryptedChunk {
                sequence: chunk.sequence,
                data: encrypted,
                original_size: chunk.data.len(),
            })
        })
        .collect()
}
<span class="boring">}</span></code></pre></pre>
<h3 id="hardware-acceleration"><a class="header" href="#hardware-acceleration">Hardware Acceleration</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Detect AES-NI support
pub fn has_aes_ni() -&gt; bool {
    #[cfg(target_arch = "x86_64")]
    {
        use std::arch::x86_64::*;
        is_x86_feature_detected!("aes")
    }
    #[cfg(not(target_arch = "x86_64"))]
    {
        false
    }
}

// Select algorithm based on hardware
pub fn select_algorithm() -&gt; Algorithm {
    if has_aes_ni() {
        Algorithm::aes_256_gcm()  // Fast with hardware support
    } else {
        Algorithm::chacha20_poly1305()  // Consistent without hardware
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="configuration-1"><a class="header" href="#configuration-1">Configuration</a></h2>
<h3 id="encryption-configuration"><a class="header" href="#encryption-configuration">Encryption Configuration</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct EncryptionConfig {
    pub algorithm: Algorithm,
    pub key_derivation: KeyDerivationFunction,
    pub key_rotation_interval: Duration,
    pub nonce_reuse_prevention: bool,
}

impl EncryptionConfig {
    pub fn maximum_security() -&gt; Self {
        Self {
            algorithm: Algorithm::aes_256_gcm(),
            key_derivation: KeyDerivationFunction::Argon2,
            key_rotation_interval: Duration::from_secs(86400), // 24 hours
            nonce_reuse_prevention: true,
        }
    }

    pub fn balanced() -&gt; Self {
        Self {
            algorithm: if has_aes_ni() {
                Algorithm::aes_256_gcm()
            } else {
                Algorithm::chacha20_poly1305()
            },
            key_derivation: KeyDerivationFunction::Scrypt,
            key_rotation_interval: Duration::from_secs(604800), // 7 days
            nonce_reuse_prevention: true,
        }
    }

    pub fn high_performance() -&gt; Self {
        Self {
            algorithm: if has_aes_ni() {
                Algorithm::aes_128_gcm()
            } else {
                Algorithm::chacha20_poly1305()
            },
            key_derivation: KeyDerivationFunction::PBKDF2,
            key_rotation_interval: Duration::from_secs(2592000), // 30 days
            nonce_reuse_prevention: true,
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="error-handling-1"><a class="header" href="#error-handling-1">Error Handling</a></h2>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Debug, thiserror::Error)]
pub enum EncryptionError {
    #[error("Encryption failed: {0}")]
    EncryptionFailed(String),

    #[error("Decryption failed: {0}")]
    DecryptionFailed(String),

    #[error("Authentication failed - data may be tampered")]
    AuthenticationFailed,

    #[error("Invalid key length: expected {expected}, got {actual}")]
    InvalidKeyLength { expected: usize, actual: usize },

    #[error("Nonce reuse detected")]
    NonceReuse,

    #[error("Key derivation failed: {0}")]
    KeyDerivationFailed(String),
}

impl From&lt;EncryptionError&gt; for PipelineError {
    fn from(err: EncryptionError) -&gt; Self {
        match err {
            EncryptionError::EncryptionFailed(msg) =&gt;
                PipelineError::EncryptionError(msg),
            EncryptionError::DecryptionFailed(msg) =&gt;
                PipelineError::DecryptionError(msg),
            EncryptionError::AuthenticationFailed =&gt;
                PipelineError::IntegrityError("Authentication failed".to_string()),
            _ =&gt; PipelineError::EncryptionError(err.to_string()),
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="usage-examples-1"><a class="header" href="#usage-examples-1">Usage Examples</a></h2>
<h3 id="basic-encryption"><a class="header" href="#basic-encryption">Basic Encryption</a></h3>
<pre><pre class="playground"><code class="language-rust">use pipeline::infrastructure::adapters::EncryptionServiceAdapter;
use pipeline_domain::services::EncryptionService;
use pipeline_domain::value_objects::Algorithm;

#[tokio::main]
async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    // Create encryption service
    let encryption = EncryptionServiceAdapter::new();

    // Generate encryption key
    let key = SecretKey::generate(32); // 256 bits

    // Encrypt data
    let data = b"Sensitive information";
    let encrypted = encryption.encrypt(
        data,
        &amp;Algorithm::aes_256_gcm(),
        &amp;key
    ).await?;

    println!("Original size: {} bytes", data.len());
    println!("Encrypted size: {} bytes", encrypted.ciphertext.len());
    println!("Nonce: {} bytes", encrypted.nonce.len());
    println!("Tag: {} bytes", encrypted.tag.len());

    // Decrypt data
    let decrypted = encryption.decrypt(
        &amp;encrypted,
        &amp;Algorithm::aes_256_gcm(),
        &amp;key
    ).await?;

    assert_eq!(data, decrypted.as_slice());
    println!("✓ Decryption successful");

    Ok(())
}</code></pre></pre>
<h3 id="password-based-encryption"><a class="header" href="#password-based-encryption">Password-Based Encryption</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn encrypt_with_password(
    data: &amp;[u8],
    password: &amp;str,
) -&gt; Result&lt;EncryptedData, PipelineError&gt; {
    // Generate random salt
    let salt = SecretKey::generate(16);

    // Derive key from password
    let key = derive_key(
        password.as_bytes(),
        salt.as_bytes(),
        KeyDerivationFunction::Argon2,
    )?;

    // Encrypt data
    let encryption = EncryptionServiceAdapter::new();
    let encrypted = encryption.encrypt(
        data,
        &amp;Algorithm::aes_256_gcm(),
        &amp;key,
    ).await?;

    // Store salt with encrypted data
    encrypted.salt = salt.as_bytes().to_vec();

    Ok(encrypted)
}
<span class="boring">}</span></code></pre></pre>
<h3 id="tamper-detection"><a class="header" href="#tamper-detection">Tamper Detection</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn decrypt_with_verification(
    encrypted: &amp;EncryptedData,
    key: &amp;SecretKey,
) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
    let encryption = EncryptionServiceAdapter::new();

    // Attempt decryption (will fail if tampered)
    match encryption.decrypt(encrypted, &amp;Algorithm::aes_256_gcm(), key).await {
        Ok(plaintext) =&gt; {
            println!("✓ Data is authentic and unmodified");
            Ok(plaintext)
        }
        Err(PipelineError::DecryptionError(_)) =&gt; {
            eprintln!("✗ Data has been tampered with!");
            Err(PipelineError::IntegrityError(
                "Authentication failed - data may be tampered".to_string()
            ))
        }
        Err(e) =&gt; Err(e),
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="benchmarks-1"><a class="header" href="#benchmarks-1">Benchmarks</a></h2>
<p>Typical performance on modern systems:</p>
<pre><code class="language-text">Algorithm          | File Size | Encrypt Time | Decrypt Time | Throughput
-------------------|-----------|--------------|--------------|------------
AES-256-GCM (NI)   | 100 MB    | 0.04s        | 0.04s        | 2.5 GB/s
AES-256-GCM (SW)   | 100 MB    | 0.8s         | 0.8s         | 125 MB/s
ChaCha20-Poly1305  | 100 MB    | 0.15s        | 0.15s        | 670 MB/s
AES-128-GCM (NI)   | 100 MB    | 0.03s        | 0.03s        | 3.3 GB/s
</code></pre>
<h2 id="best-practices-1"><a class="header" href="#best-practices-1">Best Practices</a></h2>
<h3 id="algorithm-selection"><a class="header" href="#algorithm-selection">Algorithm Selection</a></h3>
<p><strong>Use AES-256-GCM when:</strong></p>
<ul>
<li>Compliance requires FIPS-approved encryption</li>
<li>Long-term data protection is needed</li>
<li>Hardware has AES-NI support</li>
<li>Maximum security is required</li>
</ul>
<p><strong>Use ChaCha20-Poly1305 when:</strong></p>
<ul>
<li>Running on platforms without AES-NI</li>
<li>Constant-time execution is critical</li>
<li>Side-channel resistance is needed</li>
<li>Mobile/embedded deployment</li>
</ul>
<p><strong>Use AES-128-GCM when:</strong></p>
<ul>
<li>Maximum performance is required</li>
<li>Short-term data protection is sufficient</li>
<li>Hardware has AES-NI support</li>
</ul>
<h3 id="key-management-1"><a class="header" href="#key-management-1">Key Management</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Secure key handling
let key = SecretKey::generate(32);
let encrypted = encrypt(data, &amp;key)?;
// Key is automatically zeroized on drop

// ❌ BAD: Exposing key in logs
println!("Key: {:?}", key);  // Never log keys!

// ✅ GOOD: Key derivation from password
let key = derive_key(password, salt, KeyDerivationFunction::Argon2)?;

// ❌ BAD: Weak key derivation
let key = sha256(password);  // Not secure!
<span class="boring">}</span></code></pre></pre>
<h3 id="nonce-management-1"><a class="header" href="#nonce-management-1">Nonce Management</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Unique nonce per encryption
let nonce = generate_unique_nonce();

// ❌ BAD: Reusing nonces
let nonce = [0u8; 12];  // NEVER reuse nonces!

// ✅ GOOD: Counter-based nonces
let nonce_counter = AtomicU64::new(0);
let nonce = generate_nonce_from_counter(nonce_counter.fetch_add(1));
<span class="boring">}</span></code></pre></pre>
<h3 id="authentication-verification"><a class="header" href="#authentication-verification">Authentication Verification</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Always verify authentication
match decrypt(encrypted, key) {
    Ok(data) =&gt; process(data),
    Err(e) =&gt; {
        log::error!("Decryption failed - possible tampering");
        return Err(e);
    }
}

// ❌ BAD: Ignoring authentication failures
let data = decrypt(encrypted, key).unwrap_or_default();  // Dangerous!
<span class="boring">}</span></code></pre></pre>
<h2 id="security-considerations"><a class="header" href="#security-considerations">Security Considerations</a></h2>
<h3 id="nonce-uniqueness"><a class="header" href="#nonce-uniqueness">Nonce Uniqueness</a></h3>
<ul>
<li><strong>Critical:</strong> Never reuse a nonce with the same key</li>
<li>Use counter-based or random nonces</li>
<li>Rotate keys after 2^32 encryptions (GCM limit)</li>
</ul>
<h3 id="key-strength"><a class="header" href="#key-strength">Key Strength</a></h3>
<ul>
<li>Minimum 256 bits for long-term security</li>
<li>Use cryptographically secure random number generators</li>
<li>Derive keys properly from passwords (use Argon2)</li>
</ul>
<h3 id="memory-security"><a class="header" href="#memory-security">Memory Security</a></h3>
<ul>
<li>Keys are automatically zeroized on drop</li>
<li>Avoid cloning keys unnecessarily</li>
<li>Don't log or print keys</li>
</ul>
<h3 id="side-channel-attacks"><a class="header" href="#side-channel-attacks">Side-Channel Attacks</a></h3>
<ul>
<li>ChaCha20 provides constant-time execution</li>
<li>AES requires AES-NI for timing attack resistance</li>
<li>Validate all inputs before decryption</li>
</ul>
<h2 id="next-steps-11"><a class="header" href="#next-steps-11">Next Steps</a></h2>
<p>Now that you understand encryption implementation:</p>
<ul>
<li><a href="implementation/integrity.html">Integrity Verification</a> - Checksum and hashing</li>
<li><a href="implementation/../advanced/key-management.html">Key Management</a> - Advanced key handling</li>
<li><a href="implementation/../advanced/security.html">Security Best Practices</a> - Comprehensive security guide</li>
<li><a href="implementation/compression.html">Compression</a> - Data compression before encryption</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="integrity-verification"><a class="header" href="#integrity-verification">Integrity Verification</a></h1>
<p><strong>Version:</strong> 1.0
<strong>Date:</strong> 2025-01-04
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner, Claude Code
<strong>Status:</strong> Active</p>
<h2 id="overview-6"><a class="header" href="#overview-6">Overview</a></h2>
<p>Integrity verification ensures data hasn't been corrupted or tampered with during processing. The pipeline system uses cryptographic hash functions to calculate checksums at various stages, providing strong guarantees about data integrity.</p>
<p>The checksum service operates in two modes:</p>
<ul>
<li><strong>Calculate Mode</strong>: Generates checksums for data chunks</li>
<li><strong>Verify Mode</strong>: Validates existing checksums to detect tampering</li>
</ul>
<h2 id="supported-algorithms-2"><a class="header" href="#supported-algorithms-2">Supported Algorithms</a></h2>
<h3 id="sha-256-recommended"><a class="header" href="#sha-256-recommended">SHA-256 (Recommended)</a></h3>
<p><strong>Industry-standard cryptographic hash function</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::value_objects::Algorithm;

let algorithm = Algorithm::sha256();
<span class="boring">}</span></code></pre></pre>
<p><strong>Characteristics:</strong></p>
<ul>
<li><strong>Hash Size</strong>: 256 bits (32 bytes)</li>
<li><strong>Security</strong>: Cryptographically secure, collision-resistant</li>
<li><strong>Performance</strong>: ~500 MB/s (software), ~2 GB/s (hardware accelerated)</li>
<li><strong>Use Cases</strong>: General-purpose integrity verification</li>
</ul>
<p><strong>When to Use:</strong></p>
<ul>
<li>✅ General-purpose integrity verification</li>
<li>✅ Compliance requirements (FIPS 180-4)</li>
<li>✅ Cross-platform compatibility</li>
<li>✅ Hardware acceleration available (SHA extensions)</li>
</ul>
<h3 id="sha-512"><a class="header" href="#sha-512">SHA-512</a></h3>
<p><strong>Stronger variant of SHA-2 family</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let algorithm = Algorithm::sha512();
<span class="boring">}</span></code></pre></pre>
<p><strong>Characteristics:</strong></p>
<ul>
<li><strong>Hash Size</strong>: 512 bits (64 bytes)</li>
<li><strong>Security</strong>: Higher security margin than SHA-256</li>
<li><strong>Performance</strong>: ~400 MB/s (software), faster on 64-bit systems</li>
<li><strong>Use Cases</strong>: High-security requirements, 64-bit optimized systems</li>
</ul>
<p><strong>When to Use:</strong></p>
<ul>
<li>✅ Maximum security requirements</li>
<li>✅ 64-bit systems (better performance)</li>
<li>✅ Long-term archival (future-proof security)</li>
<li>❌ Resource-constrained systems (larger output)</li>
</ul>
<h3 id="blake3"><a class="header" href="#blake3">BLAKE3</a></h3>
<p><strong>Modern, high-performance cryptographic hash</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let algorithm = Algorithm::blake3();
<span class="boring">}</span></code></pre></pre>
<p><strong>Characteristics:</strong></p>
<ul>
<li><strong>Hash Size</strong>: 256 bits (32 bytes, configurable)</li>
<li><strong>Security</strong>: Based on BLAKE2, ChaCha stream cipher</li>
<li><strong>Performance</strong>: ~3 GB/s (parallelizable, SIMD-optimized)</li>
<li><strong>Use Cases</strong>: High-throughput processing, modern systems</li>
</ul>
<p><strong>When to Use:</strong></p>
<ul>
<li>✅ Maximum performance requirements</li>
<li>✅ Large file processing (highly parallelizable)</li>
<li>✅ Modern CPUs with SIMD support</li>
<li>✅ No regulatory compliance requirements</li>
<li>❌ FIPS compliance needed (not FIPS certified)</li>
</ul>
<h3 id="algorithm-comparison"><a class="header" href="#algorithm-comparison">Algorithm Comparison</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Algorithm</th><th>Hash Size</th><th>Throughput</th><th>Security</th><th>Hardware Accel</th><th>FIPS</th></tr></thead><tbody>
<tr><td>SHA-256</td><td>256 bits</td><td>500 MB/s</td><td>Strong</td><td>✅ (SHA-NI)</td><td>✅</td></tr>
<tr><td>SHA-512</td><td>512 bits</td><td>400 MB/s</td><td>Stronger</td><td>✅ (SHA-NI)</td><td>✅</td></tr>
<tr><td>BLAKE3</td><td>256 bits</td><td>3 GB/s</td><td>Strong</td><td>✅ (SIMD)</td><td>❌</td></tr>
</tbody></table>
</div>
<p><strong>Performance measured on Intel i7-10700K @ 3.8 GHz</strong></p>
<h2 id="architecture-3"><a class="header" href="#architecture-3">Architecture</a></h2>
<h3 id="service-interface"><a class="header" href="#service-interface">Service Interface</a></h3>
<p>The domain layer defines the checksum service interface:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::services::ChecksumService;
use pipeline_domain::entities::ProcessingContext;
use pipeline_domain::value_objects::FileChunk;
use pipeline_domain::PipelineError;

/// Domain service for integrity verification
pub trait ChecksumService: Send + Sync {
    /// Process a chunk and update the running checksum
    fn process_chunk(
        &amp;self,
        chunk: FileChunk,
        context: &amp;mut ProcessingContext,
        stage_name: &amp;str,
    ) -&gt; Result&lt;FileChunk, PipelineError&gt;;

    /// Get the final checksum value
    fn get_checksum(
        &amp;self,
        context: &amp;ProcessingContext,
        stage_name: &amp;str
    ) -&gt; Option&lt;String&gt;;
}
<span class="boring">}</span></code></pre></pre>
<h3 id="implementation-2"><a class="header" href="#implementation-2">Implementation</a></h3>
<p>The infrastructure layer provides concrete implementations:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::services::{ChecksumService, ChecksumProcessor};

/// Concrete checksum processor using SHA-256
pub struct ChecksumProcessor {
    pub algorithm: String,
    pub verify_existing: bool,
}

impl ChecksumProcessor {
    pub fn new(algorithm: String, verify_existing: bool) -&gt; Self {
        Self {
            algorithm,
            verify_existing,
        }
    }

    /// Creates a SHA-256 processor
    pub fn sha256_processor(verify_existing: bool) -&gt; Self {
        Self::new("SHA256".to_string(), verify_existing)
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="algorithm-implementations-2"><a class="header" href="#algorithm-implementations-2">Algorithm Implementations</a></h2>
<h3 id="sha-256-implementation"><a class="header" href="#sha-256-implementation">SHA-256 Implementation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use sha2::{Digest, Sha256};

impl ChecksumProcessor {
    /// Calculate SHA-256 checksum
    pub fn calculate_sha256(&amp;self, data: &amp;[u8]) -&gt; String {
        let mut hasher = Sha256::new();
        hasher.update(data);
        format!("{:x}", hasher.finalize())
    }

    /// Incremental SHA-256 hashing
    pub fn update_hash(&amp;self, hasher: &amp;mut Sha256, chunk: &amp;FileChunk) {
        hasher.update(chunk.data());
    }

    /// Finalize hash and return hex string
    pub fn finalize_hash(&amp;self, hasher: Sha256) -&gt; String {
        format!("{:x}", hasher.finalize())
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Key Features:</strong></p>
<ul>
<li>Incremental hashing for streaming large files</li>
<li>Memory-efficient (constant 32-byte state)</li>
<li>Hardware acceleration with SHA-NI instructions</li>
</ul>
<h3 id="sha-512-implementation"><a class="header" href="#sha-512-implementation">SHA-512 Implementation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use sha2::{Sha512};

impl ChecksumProcessor {
    /// Calculate SHA-512 checksum
    pub fn calculate_sha512(&amp;self, data: &amp;[u8]) -&gt; String {
        let mut hasher = Sha512::new();
        hasher.update(data);
        format!("{:x}", hasher.finalize())
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Key Features:</strong></p>
<ul>
<li>512-bit output for higher security margin</li>
<li>Optimized for 64-bit architectures</li>
<li>Suitable for long-term archival</li>
</ul>
<h3 id="blake3-implementation"><a class="header" href="#blake3-implementation">BLAKE3 Implementation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use blake3::Hasher;

impl ChecksumProcessor {
    /// Calculate BLAKE3 checksum
    pub fn calculate_blake3(&amp;self, data: &amp;[u8]) -&gt; String {
        let mut hasher = Hasher::new();
        hasher.update(data);
        hasher.finalize().to_hex().to_string()
    }

    /// Parallel BLAKE3 hashing
    pub fn calculate_blake3_parallel(&amp;self, chunks: &amp;[&amp;[u8]]) -&gt; String {
        let mut hasher = Hasher::new();
        for chunk in chunks {
            hasher.update(chunk);
        }
        hasher.finalize().to_hex().to_string()
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Key Features:</strong></p>
<ul>
<li>Highly parallelizable (uses Rayon internally)</li>
<li>SIMD-optimized for modern CPUs</li>
<li>Incremental and streaming support</li>
<li>Up to 6x faster than SHA-256</li>
</ul>
<h2 id="chunk-processing"><a class="header" href="#chunk-processing">Chunk Processing</a></h2>
<h3 id="chunkprocessor-trait"><a class="header" href="#chunkprocessor-trait">ChunkProcessor Trait</a></h3>
<p>The checksum service implements the <code>ChunkProcessor</code> trait for integration with the pipeline:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::services::file_processor_service::ChunkProcessor;

impl ChunkProcessor for ChecksumProcessor {
    /// Process chunk with checksum calculation/verification
    fn process_chunk(&amp;self, chunk: &amp;FileChunk) -&gt; Result&lt;FileChunk, PipelineError&gt; {
        // Step 1: Verify existing checksum if requested
        if self.verify_existing &amp;&amp; chunk.checksum().is_some() {
            let is_valid = chunk.verify_integrity()?;
            if !is_valid {
                return Err(PipelineError::IntegrityError(format!(
                    "Checksum verification failed for chunk {}",
                    chunk.sequence_number()
                )));
            }
        }

        // Step 2: Ensure chunk has checksum (calculate if missing)
        if chunk.checksum().is_none() {
            chunk.with_calculated_checksum()
        } else {
            Ok(chunk.clone())
        }
    }

    fn name(&amp;self) -&gt; &amp;str {
        "ChecksumProcessor"
    }

    fn modifies_data(&amp;self) -&gt; bool {
        false // Only modifies metadata
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="integrity-verification-1"><a class="header" href="#integrity-verification-1">Integrity Verification</a></h3>
<p>The <code>FileChunk</code> value object provides built-in integrity verification:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl FileChunk {
    /// Verify chunk integrity against stored checksum
    pub fn verify_integrity(&amp;self) -&gt; Result&lt;bool, PipelineError&gt; {
        match &amp;self.checksum {
            Some(stored_checksum) =&gt; {
                let calculated = Self::calculate_checksum(self.data());
                Ok(*stored_checksum == calculated)
            }
            None =&gt; Err(PipelineError::InvalidConfiguration(
                "No checksum to verify".to_string()
            )),
        }
    }

    /// Calculate checksum for chunk data
    fn calculate_checksum(data: &amp;[u8]) -&gt; String {
        use sha2::{Digest, Sha256};
        let mut hasher = Sha256::new();
        hasher.update(data);
        format!("{:x}", hasher.finalize())
    }

    /// Create new chunk with calculated checksum
    pub fn with_calculated_checksum(&amp;self) -&gt; Result&lt;FileChunk, PipelineError&gt; {
        let checksum = Self::calculate_checksum(self.data());
        Ok(FileChunk {
            sequence_number: self.sequence_number,
            data: self.data.clone(),
            checksum: Some(checksum),
            metadata: self.metadata.clone(),
        })
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="performance-optimizations-2"><a class="header" href="#performance-optimizations-2">Performance Optimizations</a></h2>
<h3 id="parallel-chunk-processing-2"><a class="header" href="#parallel-chunk-processing-2">Parallel Chunk Processing</a></h3>
<p>Process multiple chunks in parallel using Rayon:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use rayon::prelude::*;

impl ChecksumProcessor {
    /// Process chunks in parallel for maximum throughput
    pub fn process_chunks_parallel(
        &amp;self,
        chunks: &amp;[FileChunk]
    ) -&gt; Result&lt;Vec&lt;FileChunk&gt;, PipelineError&gt; {
        chunks
            .par_iter()
            .map(|chunk| self.process_chunk(chunk))
            .collect()
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Performance Benefits:</strong></p>
<ul>
<li><strong>Linear Scaling</strong>: Performance scales with CPU cores</li>
<li><strong>No Contention</strong>: Each chunk processed independently</li>
<li><strong>2-4x Speedup</strong>: On typical multi-core systems</li>
</ul>
<h3 id="hardware-acceleration-1"><a class="header" href="#hardware-acceleration-1">Hardware Acceleration</a></h3>
<p>Leverage CPU crypto extensions when available:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Check for SHA hardware acceleration
pub fn has_sha_extensions() -&gt; bool {
    #[cfg(target_arch = "x86_64")]
    {
        is_x86_feature_detected!("sha")
    }
    #[cfg(not(target_arch = "x86_64"))]
    {
        false
    }
}

/// Select optimal algorithm based on hardware
pub fn optimal_hash_algorithm() -&gt; Algorithm {
    if has_sha_extensions() {
        Algorithm::sha256() // Hardware accelerated
    } else {
        Algorithm::blake3() // Software optimized
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="memory-management-1"><a class="header" href="#memory-management-1">Memory Management</a></h3>
<p>Minimize allocations during hash calculation:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl ChecksumProcessor {
    /// Reuse buffer for hash calculations
    pub fn calculate_with_buffer(
        &amp;self,
        data: &amp;[u8],
        buffer: &amp;mut Vec&lt;u8&gt;
    ) -&gt; String {
        buffer.clear();
        buffer.extend_from_slice(data);
        self.calculate_sha256(buffer)
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="configuration-2"><a class="header" href="#configuration-2">Configuration</a></h2>
<h3 id="stage-configuration-1"><a class="header" href="#stage-configuration-1">Stage Configuration</a></h3>
<p>Configure integrity stages in your pipeline:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::entities::PipelineStage;
use pipeline_domain::value_objects::{Algorithm, StageType};

// Input integrity verification
let input_stage = PipelineStage::new(
    "input_checksum",
    StageType::Integrity,
    Algorithm::sha256(),
)?;

// Output integrity verification
let output_stage = PipelineStage::new(
    "output_checksum",
    StageType::Integrity,
    Algorithm::blake3(), // Faster for final verification
)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="verification-mode"><a class="header" href="#verification-mode">Verification Mode</a></h3>
<p>Enable checksum verification for existing data:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Calculate checksums only (default)
let processor = ChecksumProcessor::new("SHA256".to_string(), false);

// Verify existing checksums before processing
let verifying_processor = ChecksumProcessor::new("SHA256".to_string(), true);
<span class="boring">}</span></code></pre></pre>
<h3 id="algorithm-selection-1"><a class="header" href="#algorithm-selection-1">Algorithm Selection</a></h3>
<p>Choose algorithm based on requirements:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn select_hash_algorithm(
    security_level: SecurityLevel,
    performance_priority: bool,
) -&gt; Algorithm {
    match (security_level, performance_priority) {
        (SecurityLevel::Maximum, _) =&gt; Algorithm::sha512(),
        (SecurityLevel::High, false) =&gt; Algorithm::sha256(),
        (SecurityLevel::High, true) =&gt; Algorithm::blake3(),
        (SecurityLevel::Standard, _) =&gt; Algorithm::blake3(),
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="error-handling-2"><a class="header" href="#error-handling-2">Error Handling</a></h2>
<h3 id="error-types"><a class="header" href="#error-types">Error Types</a></h3>
<p>The service handles various error conditions:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub enum IntegrityError {
    /// Checksum verification failed
    ChecksumMismatch {
        expected: String,
        actual: String,
        chunk: u64,
    },

    /// Invalid algorithm specified
    UnsupportedAlgorithm(String),

    /// Hash calculation failed
    HashCalculationError(String),

    /// Chunk data corrupted
    CorruptedData {
        chunk: u64,
        reason: String,
    },
}
<span class="boring">}</span></code></pre></pre>
<h3 id="error-recovery"><a class="header" href="#error-recovery">Error Recovery</a></h3>
<p>Handle integrity errors gracefully:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl ChecksumProcessor {
    pub fn process_with_retry(
        &amp;self,
        chunk: &amp;FileChunk,
        max_retries: u32
    ) -&gt; Result&lt;FileChunk, PipelineError&gt; {
        let mut attempts = 0;

        loop {
            match self.process_chunk(chunk) {
                Ok(result) =&gt; return Ok(result),
                Err(PipelineError::IntegrityError(msg)) if attempts &lt; max_retries =&gt; {
                    attempts += 1;
                    eprintln!("Integrity check failed (attempt {}/{}): {}",
                        attempts, max_retries, msg);
                    continue;
                }
                Err(e) =&gt; return Err(e),
            }
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="usage-examples-2"><a class="header" href="#usage-examples-2">Usage Examples</a></h2>
<h3 id="basic-checksum-calculation"><a class="header" href="#basic-checksum-calculation">Basic Checksum Calculation</a></h3>
<p>Calculate SHA-256 checksums for data:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::services::ChecksumProcessor;

fn calculate_file_checksum(data: &amp;[u8]) -&gt; Result&lt;String, PipelineError&gt; {
    let processor = ChecksumProcessor::sha256_processor(false);
    let checksum = processor.calculate_sha256(data);
    Ok(checksum)
}

// Example usage
let data = b"Hello, world!";
let checksum = calculate_file_checksum(data)?;
println!("SHA-256: {}", checksum);
// Output: SHA-256: 315f5bdb76d078c43b8ac0064e4a0164612b1fce77c869345bfc94c75894edd3
<span class="boring">}</span></code></pre></pre>
<h3 id="integrity-verification-2"><a class="header" href="#integrity-verification-2">Integrity Verification</a></h3>
<p>Verify data hasn't been tampered with:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::value_objects::FileChunk;

fn verify_chunk_integrity(chunk: &amp;FileChunk) -&gt; Result&lt;bool, PipelineError&gt; {
    let processor = ChecksumProcessor::sha256_processor(true);

    // Process with verification enabled
    match processor.process_chunk(chunk) {
        Ok(_) =&gt; Ok(true),
        Err(PipelineError::IntegrityError(_)) =&gt; Ok(false),
        Err(e) =&gt; Err(e),
    }
}

// Example usage
let chunk = FileChunk::new(0, data.to_vec())?
    .with_calculated_checksum()?;

if verify_chunk_integrity(&amp;chunk)? {
    println!("✓ Chunk integrity verified");
} else {
    println!("✗ Chunk has been tampered with!");
}
<span class="boring">}</span></code></pre></pre>
<h3 id="pipeline-integration"><a class="header" href="#pipeline-integration">Pipeline Integration</a></h3>
<p>Integrate checksums into processing pipeline:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::entities::{Pipeline, PipelineStage};

fn create_verified_pipeline() -&gt; Result&lt;Pipeline, PipelineError&gt; {
    let stages = vec![
        // Input verification
        PipelineStage::new(
            "input_checksum",
            StageType::Integrity,
            Algorithm::sha256(),
        )?,

        // Processing stages...
        PipelineStage::new(
            "compression",
            StageType::Compression,
            Algorithm::zstd(),
        )?,

        // Output verification
        PipelineStage::new(
            "output_checksum",
            StageType::Integrity,
            Algorithm::sha256(),
        )?,
    ];

    Pipeline::new("verified-pipeline".to_string(), stages)
}
<span class="boring">}</span></code></pre></pre>
<h3 id="parallel-processing-1"><a class="header" href="#parallel-processing-1">Parallel Processing</a></h3>
<p>Process multiple chunks with maximum performance:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use rayon::prelude::*;

fn hash_large_file(chunks: Vec&lt;FileChunk&gt;) -&gt; Result&lt;Vec&lt;String&gt;, PipelineError&gt; {
    let processor = ChecksumProcessor::sha256_processor(false);

    chunks.par_iter()
        .map(|chunk| processor.calculate_sha256(chunk.data()))
        .collect()
}

// Example: Hash 1000 chunks in parallel
let checksums = hash_large_file(chunks)?;
println!("Processed {} chunks", checksums.len());
<span class="boring">}</span></code></pre></pre>
<h2 id="benchmarks-2"><a class="header" href="#benchmarks-2">Benchmarks</a></h2>
<h3 id="sha-256-performance"><a class="header" href="#sha-256-performance">SHA-256 Performance</a></h3>
<p><strong>File Size: 100 MB, Chunk Size: 1 MB</strong></p>
<div class="table-wrapper"><table><thead><tr><th>Configuration</th><th>Throughput</th><th>Total Time</th><th>CPU Usage</th></tr></thead><tbody>
<tr><td>Single-threaded</td><td>500 MB/s</td><td>200ms</td><td>100% (1 core)</td></tr>
<tr><td>Parallel (4 cores)</td><td>1.8 GB/s</td><td>56ms</td><td>400% (4 cores)</td></tr>
<tr><td>Hardware accel</td><td>2.0 GB/s</td><td>50ms</td><td>100% (1 core)</td></tr>
</tbody></table>
</div>
<h3 id="sha-512-performance"><a class="header" href="#sha-512-performance">SHA-512 Performance</a></h3>
<p><strong>File Size: 100 MB, Chunk Size: 1 MB</strong></p>
<div class="table-wrapper"><table><thead><tr><th>Configuration</th><th>Throughput</th><th>Total Time</th><th>CPU Usage</th></tr></thead><tbody>
<tr><td>Single-threaded</td><td>400 MB/s</td><td>250ms</td><td>100% (1 core)</td></tr>
<tr><td>Parallel (4 cores)</td><td>1.5 GB/s</td><td>67ms</td><td>400% (4 cores)</td></tr>
</tbody></table>
</div>
<h3 id="blake3-performance"><a class="header" href="#blake3-performance">BLAKE3 Performance</a></h3>
<p><strong>File Size: 100 MB, Chunk Size: 1 MB</strong></p>
<div class="table-wrapper"><table><thead><tr><th>Configuration</th><th>Throughput</th><th>Total Time</th><th>CPU Usage</th></tr></thead><tbody>
<tr><td>Single-threaded</td><td>1.2 GB/s</td><td>83ms</td><td>100% (1 core)</td></tr>
<tr><td>Parallel (4 cores)</td><td>3.2 GB/s</td><td>31ms</td><td>400% (4 cores)</td></tr>
<tr><td>SIMD optimized</td><td>3.5 GB/s</td><td>29ms</td><td>100% (1 core)</td></tr>
</tbody></table>
</div>
<p><strong>Test Environment:</strong> Intel i7-10700K @ 3.8 GHz, 32GB RAM, Ubuntu 22.04</p>
<h3 id="algorithm-recommendations-by-use-case"><a class="header" href="#algorithm-recommendations-by-use-case">Algorithm Recommendations by Use Case</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Use Case</th><th>Recommended Algorithm</th><th>Reason</th></tr></thead><tbody>
<tr><td>General integrity</td><td>SHA-256</td><td>Industry standard, FIPS certified</td></tr>
<tr><td>High security</td><td>SHA-512</td><td>Larger output, stronger security margin</td></tr>
<tr><td>High throughput</td><td>BLAKE3</td><td>3-6x faster, highly parallelizable</td></tr>
<tr><td>Compliance</td><td>SHA-256</td><td>FIPS 180-4 certified</td></tr>
<tr><td>Archival</td><td>SHA-512</td><td>Future-proof security</td></tr>
<tr><td>Real-time</td><td>BLAKE3</td><td>Lowest latency</td></tr>
</tbody></table>
</div>
<h2 id="best-practices-2"><a class="header" href="#best-practices-2">Best Practices</a></h2>
<h3 id="algorithm-selection-2"><a class="header" href="#algorithm-selection-2">Algorithm Selection</a></h3>
<p><strong>Choose the right algorithm for your requirements:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Compliance requirements
if needs_fips_compliance {
    Algorithm::sha256() // FIPS 180-4 certified
}
// Maximum security
else if security_level == SecurityLevel::Maximum {
    Algorithm::sha512() // Stronger security margin
}
// Performance critical
else if throughput_priority {
    Algorithm::blake3() // 3-6x faster
}
// Default
else {
    Algorithm::sha256() // Industry standard
}
<span class="boring">}</span></code></pre></pre>
<h3 id="verification-strategy"><a class="header" href="#verification-strategy">Verification Strategy</a></h3>
<p><strong>Implement defense-in-depth verification:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 1. Input verification (detect source corruption)
let input_checksum_stage = PipelineStage::new(
    "input_verify",
    StageType::Integrity,
    Algorithm::sha256(),
)?;

// 2. Processing stages...

// 3. Output verification (detect processing corruption)
let output_checksum_stage = PipelineStage::new(
    "output_verify",
    StageType::Integrity,
    Algorithm::sha256(),
)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="performance-optimization"><a class="header" href="#performance-optimization">Performance Optimization</a></h3>
<p><strong>Optimize for your workload:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Small files (&lt;10 MB): Use single-threaded
if file_size &lt; 10 * 1024 * 1024 {
    processor.calculate_sha256(data)
}
// Large files: Use parallel processing
else {
    processor.process_chunks_parallel(&amp;chunks)
}

// Hardware acceleration available: Use SHA-256
if has_sha_extensions() {
    Algorithm::sha256()
}
// No hardware acceleration: Use BLAKE3
else {
    Algorithm::blake3()
}
<span class="boring">}</span></code></pre></pre>
<h3 id="error-handling-3"><a class="header" href="#error-handling-3">Error Handling</a></h3>
<p><strong>Handle integrity failures appropriately:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>match processor.process_chunk(&amp;chunk) {
    Ok(verified_chunk) =&gt; {
        // Integrity verified, continue processing
        process_chunk(verified_chunk)
    }
    Err(PipelineError::IntegrityError(msg)) =&gt; {
        // Log error and attempt recovery
        eprintln!("Integrity failure: {}", msg);

        // Option 1: Retry from source
        let fresh_chunk = reload_chunk_from_source()?;
        processor.process_chunk(&amp;fresh_chunk)
    }
    Err(e) =&gt; return Err(e),
}
<span class="boring">}</span></code></pre></pre>
<h2 id="security-considerations-1"><a class="header" href="#security-considerations-1">Security Considerations</a></h2>
<h3 id="cryptographic-strength"><a class="header" href="#cryptographic-strength">Cryptographic Strength</a></h3>
<p><strong>All supported algorithms are cryptographically secure:</strong></p>
<ul>
<li><strong>SHA-256</strong>: 128-bit security level (2^128 operations for collision)</li>
<li><strong>SHA-512</strong>: 256-bit security level (2^256 operations for collision)</li>
<li><strong>BLAKE3</strong>: 128-bit security level (based on ChaCha20)</li>
</ul>
<h3 id="collision-resistance"><a class="header" href="#collision-resistance">Collision Resistance</a></h3>
<p><strong>Practical collision resistance:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// SHA-256 collision resistance: ~2^128 operations
// Effectively impossible with current technology
let sha256_security_bits = 128;

// SHA-512 collision resistance: ~2^256 operations
// Provides future-proof security margin
let sha512_security_bits = 256;
<span class="boring">}</span></code></pre></pre>
<h3 id="tampering-detection"><a class="header" href="#tampering-detection">Tampering Detection</a></h3>
<p><strong>Checksums detect any modification:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Even single-bit changes produce completely different hashes
let original = "Hello, World!";
let tampered = "Hello, world!"; // Changed 'W' to 'w'

let hash1 = processor.calculate_sha256(original.as_bytes());
let hash2 = processor.calculate_sha256(tampered.as_bytes());

assert_ne!(hash1, hash2); // Completely different hashes
<span class="boring">}</span></code></pre></pre>
<h3 id="not-for-authentication"><a class="header" href="#not-for-authentication">Not for Authentication</a></h3>
<p><strong>Important:</strong> Checksums alone don't provide authentication:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ❌ WRONG: Checksum alone doesn't prove authenticity
let checksum = calculate_sha256(data);
// Attacker can modify data AND update checksum

// ✅ CORRECT: Use HMAC for authentication
let hmac = calculate_hmac_sha256(data, secret_key);
// Attacker cannot forge HMAC without secret key
<span class="boring">}</span></code></pre></pre>
<p><strong>Use HMAC or digital signatures for authentication.</strong></p>
<h2 id="next-steps-12"><a class="header" href="#next-steps-12">Next Steps</a></h2>
<p>Now that you understand integrity verification:</p>
<ul>
<li><a href="implementation/repositories.html">Repositories</a> - Data persistence patterns</li>
<li><a href="implementation/binary-format.html">Binary Format</a> - File format with embedded checksums</li>
<li><a href="implementation/../advanced/error-handling.html">Error Handling</a> - Comprehensive error strategies</li>
<li><a href="implementation/../advanced/performance.html">Performance</a> - Advanced optimization techniques</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-persistence"><a class="header" href="#data-persistence">Data Persistence</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>How data is persisted to storage.</p>
<h2 id="storage-architecture"><a class="header" href="#storage-architecture">Storage Architecture</a></h2>
<p>TODO: Explain storage design</p>
<h2 id="database-choice"><a class="header" href="#database-choice">Database Choice</a></h2>
<p>TODO: Explain SQLite usage</p>
<h2 id="transaction-management"><a class="header" href="#transaction-management">Transaction Management</a></h2>
<p>TODO: Explain transactions</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="repository-implementation"><a class="header" href="#repository-implementation">Repository Implementation</a></h1>
<p><strong>Version:</strong> 1.0
<strong>Date:</strong> 2025-01-04
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner, Claude Code
<strong>Status:</strong> Active</p>
<h2 id="overview-7"><a class="header" href="#overview-7">Overview</a></h2>
<p>The repository pattern provides an abstraction layer between the domain and data persistence, enabling the application to work with domain entities without knowing about database details. This separation allows for flexible storage implementations and easier testing.</p>
<p><strong>Key Benefits:</strong></p>
<ul>
<li><strong>Domain Independence</strong>: Business logic stays free from persistence concerns</li>
<li><strong>Testability</strong>: Easy mocking with in-memory implementations</li>
<li><strong>Flexibility</strong>: Support for different storage backends (SQLite, PostgreSQL, etc.)</li>
<li><strong>Consistency</strong>: Standardized data access patterns</li>
</ul>
<h2 id="repository-interface"><a class="header" href="#repository-interface">Repository Interface</a></h2>
<h3 id="domain-defined-contract"><a class="header" href="#domain-defined-contract">Domain-Defined Contract</a></h3>
<p>The domain layer defines the repository interface:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::repositories::PipelineRepository;
use pipeline_domain::entities::Pipeline;
use pipeline_domain::value_objects::PipelineId;
use pipeline_domain::PipelineError;
use async_trait::async_trait;

#[async_trait]
pub trait PipelineRepository: Send + Sync {
    /// Saves a pipeline
    async fn save(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt;;

    /// Finds a pipeline by ID
    async fn find_by_id(&amp;self, id: PipelineId)
        -&gt; Result&lt;Option&lt;Pipeline&gt;, PipelineError&gt;;

    /// Finds a pipeline by name
    async fn find_by_name(&amp;self, name: &amp;str)
        -&gt; Result&lt;Option&lt;Pipeline&gt;, PipelineError&gt;;

    /// Lists all pipelines
    async fn list_all(&amp;self) -&gt; Result&lt;Vec&lt;Pipeline&gt;, PipelineError&gt;;

    /// Lists pipelines with pagination
    async fn list_paginated(&amp;self, offset: usize, limit: usize)
        -&gt; Result&lt;Vec&lt;Pipeline&gt;, PipelineError&gt;;

    /// Updates a pipeline
    async fn update(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt;;

    /// Deletes a pipeline by ID
    async fn delete(&amp;self, id: PipelineId) -&gt; Result&lt;bool, PipelineError&gt;;

    /// Checks if a pipeline exists
    async fn exists(&amp;self, id: PipelineId) -&gt; Result&lt;bool, PipelineError&gt;;

    /// Counts total pipelines
    async fn count(&amp;self) -&gt; Result&lt;usize, PipelineError&gt;;

    /// Finds pipelines by configuration parameter
    async fn find_by_config(&amp;self, key: &amp;str, value: &amp;str)
        -&gt; Result&lt;Vec&lt;Pipeline&gt;, PipelineError&gt;;

    /// Archives a pipeline (soft delete)
    async fn archive(&amp;self, id: PipelineId) -&gt; Result&lt;bool, PipelineError&gt;;

    /// Restores an archived pipeline
    async fn restore(&amp;self, id: PipelineId) -&gt; Result&lt;bool, PipelineError&gt;;

    /// Lists archived pipelines
    async fn list_archived(&amp;self) -&gt; Result&lt;Vec&lt;Pipeline&gt;, PipelineError&gt;;
}
<span class="boring">}</span></code></pre></pre>
<h3 id="thread-safety"><a class="header" href="#thread-safety">Thread Safety</a></h3>
<p>All repository implementations must be <code>Send + Sync</code> for concurrent access:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ CORRECT: Thread-safe repository
pub struct SqlitePipelineRepository {
    pool: SqlitePool, // SqlitePool is Send + Sync
}

// ❌ WRONG: Not thread-safe
pub struct UnsafeRepository {
    conn: Rc&lt;Connection&gt;, // Rc is not Send or Sync
}
<span class="boring">}</span></code></pre></pre>
<h2 id="sqlite-implementation"><a class="header" href="#sqlite-implementation">SQLite Implementation</a></h2>
<h3 id="architecture-4"><a class="header" href="#architecture-4">Architecture</a></h3>
<p>The SQLite repository implements the domain interface using sqlx for type-safe queries:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::repositories::PipelineRepository;
use sqlx::SqlitePool;

pub struct SqlitePipelineRepository {
    pool: SqlitePool,
}

impl SqlitePipelineRepository {
    pub async fn new(database_path: &amp;str) -&gt; Result&lt;Self, PipelineError&gt; {
        let database_url = format!("sqlite:{}", database_path);
        let pool = SqlitePool::connect(&amp;database_url)
            .await
            .map_err(|e| PipelineError::database_error(
                format!("Failed to connect: {}", e)
            ))?;

        Ok(Self { pool })
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="database-schema"><a class="header" href="#database-schema">Database Schema</a></h3>
<p>The repository uses a normalized relational schema:</p>
<h4 id="pipelines-table"><a class="header" href="#pipelines-table">Pipelines Table</a></h4>
<pre><code class="language-sql">CREATE TABLE pipelines (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL UNIQUE,
    description TEXT,
    archived BOOLEAN NOT NULL DEFAULT 0,
    created_at TEXT NOT NULL,
    updated_at TEXT NOT NULL
);

CREATE INDEX idx_pipelines_name ON pipelines(name);
CREATE INDEX idx_pipelines_archived ON pipelines(archived);
</code></pre>
<h4 id="pipeline-stages-table"><a class="header" href="#pipeline-stages-table">Pipeline Stages Table</a></h4>
<pre><code class="language-sql">CREATE TABLE pipeline_stages (
    id TEXT PRIMARY KEY,
    pipeline_id TEXT NOT NULL,
    name TEXT NOT NULL,
    stage_type TEXT NOT NULL,
    algorithm TEXT NOT NULL,
    enabled BOOLEAN NOT NULL DEFAULT 1,
    order_index INTEGER NOT NULL,
    parallel_processing BOOLEAN NOT NULL DEFAULT 0,
    chunk_size INTEGER,
    created_at TEXT NOT NULL,
    updated_at TEXT NOT NULL,
    FOREIGN KEY (pipeline_id) REFERENCES pipelines(id) ON DELETE CASCADE
);

CREATE INDEX idx_stages_pipeline ON pipeline_stages(pipeline_id);
CREATE INDEX idx_stages_order ON pipeline_stages(pipeline_id, order_index);
</code></pre>
<h4 id="pipeline-configuration-table"><a class="header" href="#pipeline-configuration-table">Pipeline Configuration Table</a></h4>
<pre><code class="language-sql">CREATE TABLE pipeline_configuration (
    pipeline_id TEXT NOT NULL,
    key TEXT NOT NULL,
    value TEXT NOT NULL,
    PRIMARY KEY (pipeline_id, key),
    FOREIGN KEY (pipeline_id) REFERENCES pipelines(id) ON DELETE CASCADE
);
</code></pre>
<h4 id="pipeline-metrics-table"><a class="header" href="#pipeline-metrics-table">Pipeline Metrics Table</a></h4>
<pre><code class="language-sql">CREATE TABLE pipeline_metrics (
    id TEXT PRIMARY KEY,
    pipeline_id TEXT NOT NULL,
    bytes_processed INTEGER NOT NULL DEFAULT 0,
    bytes_total INTEGER NOT NULL DEFAULT 0,
    chunks_processed INTEGER NOT NULL DEFAULT 0,
    chunks_total INTEGER NOT NULL DEFAULT 0,
    start_time TEXT,
    end_time TEXT,
    throughput_mbps REAL NOT NULL DEFAULT 0.0,
    compression_ratio REAL,
    error_count INTEGER NOT NULL DEFAULT 0,
    warning_count INTEGER NOT NULL DEFAULT 0,
    recorded_at TEXT NOT NULL,
    FOREIGN KEY (pipeline_id) REFERENCES pipelines(id) ON DELETE CASCADE
);

CREATE INDEX idx_metrics_pipeline ON pipeline_metrics(pipeline_id);
</code></pre>
<h2 id="crud-operations"><a class="header" href="#crud-operations">CRUD Operations</a></h2>
<h3 id="create-save"><a class="header" href="#create-save">Create (Save)</a></h3>
<p>Save a complete pipeline with all related data:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[async_trait]
impl PipelineRepository for SqlitePipelineRepository {
    async fn save(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt; {
        // Start transaction for atomicity
        let mut tx = self.pool.begin().await
            .map_err(|e| PipelineError::database_error(
                format!("Failed to start transaction: {}", e)
            ))?;

        // Insert pipeline
        sqlx::query(
            "INSERT INTO pipelines
             (id, name, description, archived, created_at, updated_at)
             VALUES (?, ?, ?, ?, ?, ?)"
        )
        .bind(pipeline.id().to_string())
        .bind(pipeline.name())
        .bind(pipeline.description())
        .bind(pipeline.archived())
        .bind(pipeline.created_at().to_rfc3339())
        .bind(pipeline.updated_at().to_rfc3339())
        .execute(&amp;mut *tx)
        .await
        .map_err(|e| PipelineError::database_error(
            format!("Failed to insert pipeline: {}", e)
        ))?;

        // Insert stages
        for (index, stage) in pipeline.stages().iter().enumerate() {
            sqlx::query(
                "INSERT INTO pipeline_stages
                 (id, pipeline_id, name, stage_type, algorithm, enabled,
                  order_index, parallel_processing, chunk_size,
                  created_at, updated_at)
                 VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)"
            )
            .bind(stage.id().to_string())
            .bind(pipeline.id().to_string())
            .bind(stage.name())
            .bind(stage.stage_type().to_string())
            .bind(stage.algorithm().name())
            .bind(stage.enabled())
            .bind(index as i64)
            .bind(stage.parallel_processing())
            .bind(stage.chunk_size().map(|cs| cs.as_u64() as i64))
            .bind(stage.created_at().to_rfc3339())
            .bind(stage.updated_at().to_rfc3339())
            .execute(&amp;mut *tx)
            .await
            .map_err(|e| PipelineError::database_error(
                format!("Failed to insert stage: {}", e)
            ))?;
        }

        // Insert configuration
        for (key, value) in pipeline.configuration() {
            sqlx::query(
                "INSERT INTO pipeline_configuration (pipeline_id, key, value)
                 VALUES (?, ?, ?)"
            )
            .bind(pipeline.id().to_string())
            .bind(key)
            .bind(value)
            .execute(&amp;mut *tx)
            .await
            .map_err(|e| PipelineError::database_error(
                format!("Failed to insert config: {}", e)
            ))?;
        }

        // Commit transaction
        tx.commit().await
            .map_err(|e| PipelineError::database_error(
                format!("Failed to commit: {}", e)
            ))?;

        Ok(())
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="read-find"><a class="header" href="#read-find">Read (Find)</a></h3>
<p>Retrieve pipelines with all related data:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl SqlitePipelineRepository {
    async fn find_by_id(&amp;self, id: PipelineId)
        -&gt; Result&lt;Option&lt;Pipeline&gt;, PipelineError&gt; {
        // Fetch pipeline
        let pipeline_row = sqlx::query(
            "SELECT id, name, description, archived, created_at, updated_at
             FROM pipelines WHERE id = ?"
        )
        .bind(id.to_string())
        .fetch_optional(&amp;self.pool)
        .await
        .map_err(|e| PipelineError::database_error(
            format!("Failed to fetch pipeline: {}", e)
        ))?;

        let Some(row) = pipeline_row else {
            return Ok(None);
        };

        // Fetch stages
        let stage_rows = sqlx::query(
            "SELECT id, name, stage_type, algorithm, enabled,
                    order_index, parallel_processing, chunk_size,
                    created_at, updated_at
             FROM pipeline_stages
             WHERE pipeline_id = ?
             ORDER BY order_index"
        )
        .bind(id.to_string())
        .fetch_all(&amp;self.pool)
        .await
        .map_err(|e| PipelineError::database_error(
            format!("Failed to fetch stages: {}", e)
        ))?;

        // Fetch configuration
        let config_rows = sqlx::query(
            "SELECT key, value FROM pipeline_configuration
             WHERE pipeline_id = ?"
        )
        .bind(id.to_string())
        .fetch_all(&amp;self.pool)
        .await
        .map_err(|e| PipelineError::database_error(
            format!("Failed to fetch config: {}", e)
        ))?;

        // Map rows to domain entities
        let pipeline = self.map_to_pipeline(row, stage_rows, config_rows)?;

        Ok(Some(pipeline))
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="update"><a class="header" href="#update">Update</a></h3>
<p>Update existing pipeline:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl SqlitePipelineRepository {
    async fn update(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt; {
        let mut tx = self.pool.begin().await
            .map_err(|e| PipelineError::database_error(
                format!("Failed to start transaction: {}", e)
            ))?;

        // Update pipeline
        sqlx::query(
            "UPDATE pipelines
             SET name = ?, description = ?, archived = ?, updated_at = ?
             WHERE id = ?"
        )
        .bind(pipeline.name())
        .bind(pipeline.description())
        .bind(pipeline.archived())
        .bind(pipeline.updated_at().to_rfc3339())
        .bind(pipeline.id().to_string())
        .execute(&amp;mut *tx)
        .await
        .map_err(|e| PipelineError::database_error(
            format!("Failed to update pipeline: {}", e)
        ))?;

        // Delete and re-insert stages (simpler than updating)
        sqlx::query("DELETE FROM pipeline_stages WHERE pipeline_id = ?")
            .bind(pipeline.id().to_string())
            .execute(&amp;mut *tx)
            .await?;

        // Insert updated stages
        for (index, stage) in pipeline.stages().iter().enumerate() {
            // ... (same as save operation)
        }

        tx.commit().await
            .map_err(|e| PipelineError::database_error(
                format!("Failed to commit: {}", e)
            ))?;

        Ok(())
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="delete"><a class="header" href="#delete">Delete</a></h3>
<p>Remove pipeline and all related data:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl SqlitePipelineRepository {
    async fn delete(&amp;self, id: PipelineId) -&gt; Result&lt;bool, PipelineError&gt; {
        let result = sqlx::query("DELETE FROM pipelines WHERE id = ?")
            .bind(id.to_string())
            .execute(&amp;self.pool)
            .await
            .map_err(|e| PipelineError::database_error(
                format!("Failed to delete: {}", e)
            ))?;

        // CASCADE will automatically delete related records
        Ok(result.rows_affected() &gt; 0)
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="advanced-queries"><a class="header" href="#advanced-queries">Advanced Queries</a></h2>
<h3 id="pagination"><a class="header" href="#pagination">Pagination</a></h3>
<p>Efficiently paginate large result sets:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl SqlitePipelineRepository {
    async fn list_paginated(&amp;self, offset: usize, limit: usize)
        -&gt; Result&lt;Vec&lt;Pipeline&gt;, PipelineError&gt; {
        let rows = sqlx::query(
            "SELECT id, name, description, archived, created_at, updated_at
             FROM pipelines
             ORDER BY created_at DESC
             LIMIT ? OFFSET ?"
        )
        .bind(limit as i64)
        .bind(offset as i64)
        .fetch_all(&amp;self.pool)
        .await?;

        // Load stages and config for each pipeline
        let mut pipelines = Vec::new();
        for row in rows {
            let id = PipelineId::parse(&amp;row.get::&lt;String, _&gt;("id"))?;
            if let Some(pipeline) = self.find_by_id(id).await? {
                pipelines.push(pipeline);
            }
        }

        Ok(pipelines)
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="configuration-search"><a class="header" href="#configuration-search">Configuration Search</a></h3>
<p>Find pipelines by configuration:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl SqlitePipelineRepository {
    async fn find_by_config(&amp;self, key: &amp;str, value: &amp;str)
        -&gt; Result&lt;Vec&lt;Pipeline&gt;, PipelineError&gt; {
        let rows = sqlx::query(
            "SELECT DISTINCT p.id
             FROM pipelines p
             JOIN pipeline_configuration pc ON p.id = pc.pipeline_id
             WHERE pc.key = ? AND pc.value = ?"
        )
        .bind(key)
        .bind(value)
        .fetch_all(&amp;self.pool)
        .await?;

        let mut pipelines = Vec::new();
        for row in rows {
            let id = PipelineId::parse(&amp;row.get::&lt;String, _&gt;("id"))?;
            if let Some(pipeline) = self.find_by_id(id).await? {
                pipelines.push(pipeline);
            }
        }

        Ok(pipelines)
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="archive-operations"><a class="header" href="#archive-operations">Archive Operations</a></h3>
<p>Soft delete with archive/restore:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl SqlitePipelineRepository {
    async fn archive(&amp;self, id: PipelineId) -&gt; Result&lt;bool, PipelineError&gt; {
        let result = sqlx::query(
            "UPDATE pipelines SET archived = 1, updated_at = ?
             WHERE id = ?"
        )
        .bind(chrono::Utc::now().to_rfc3339())
        .bind(id.to_string())
        .execute(&amp;self.pool)
        .await?;

        Ok(result.rows_affected() &gt; 0)
    }

    async fn restore(&amp;self, id: PipelineId) -&gt; Result&lt;bool, PipelineError&gt; {
        let result = sqlx::query(
            "UPDATE pipelines SET archived = 0, updated_at = ?
             WHERE id = ?"
        )
        .bind(chrono::Utc::now().to_rfc3339())
        .bind(id.to_string())
        .execute(&amp;self.pool)
        .await?;

        Ok(result.rows_affected() &gt; 0)
    }

    async fn list_archived(&amp;self) -&gt; Result&lt;Vec&lt;Pipeline&gt;, PipelineError&gt; {
        let rows = sqlx::query(
            "SELECT id, name, description, archived, created_at, updated_at
             FROM pipelines WHERE archived = 1"
        )
        .fetch_all(&amp;self.pool)
        .await?;

        // Load full pipelines
        let mut pipelines = Vec::new();
        for row in rows {
            let id = PipelineId::parse(&amp;row.get::&lt;String, _&gt;("id"))?;
            if let Some(pipeline) = self.find_by_id(id).await? {
                pipelines.push(pipeline);
            }
        }

        Ok(pipelines)
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="transaction-management-1"><a class="header" href="#transaction-management-1">Transaction Management</a></h2>
<h3 id="acid-guarantees"><a class="header" href="#acid-guarantees">ACID Guarantees</a></h3>
<p>Ensure data consistency with transactions:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl SqlitePipelineRepository {
    /// Execute multiple operations atomically
    async fn save_multiple(&amp;self, pipelines: &amp;[Pipeline])
        -&gt; Result&lt;(), PipelineError&gt; {
        let mut tx = self.pool.begin().await?;

        for pipeline in pipelines {
            // All operations use the same transaction
            self.save_in_transaction(&amp;mut tx, pipeline).await?;
        }

        // Commit all or rollback all
        tx.commit().await
            .map_err(|e| PipelineError::database_error(
                format!("Transaction commit failed: {}", e)
            ))?;

        Ok(())
    }

    async fn save_in_transaction(
        &amp;self,
        tx: &amp;mut Transaction&lt;'_, Sqlite&gt;,
        pipeline: &amp;Pipeline
    ) -&gt; Result&lt;(), PipelineError&gt; {
        // Insert using transaction
        sqlx::query("INSERT INTO pipelines ...")
            .execute(&amp;mut **tx)
            .await?;

        Ok(())
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="rollback-on-error"><a class="header" href="#rollback-on-error">Rollback on Error</a></h3>
<p>Automatic rollback ensures consistency:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn complex_operation(&amp;self, pipeline: &amp;Pipeline)
    -&gt; Result&lt;(), PipelineError&gt; {
    let mut tx = self.pool.begin().await?;

    // Step 1: Insert pipeline
    sqlx::query("INSERT INTO pipelines ...")
        .execute(&amp;mut *tx)
        .await?;

    // Step 2: Insert stages
    for stage in pipeline.stages() {
        sqlx::query("INSERT INTO pipeline_stages ...")
            .execute(&amp;mut *tx)
            .await?;
        // If this fails, Step 1 is automatically rolled back
    }

    // Commit only if all steps succeed
    tx.commit().await?;
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h2 id="error-handling-4"><a class="header" href="#error-handling-4">Error Handling</a></h2>
<h3 id="database-errors"><a class="header" href="#database-errors">Database Errors</a></h3>
<p>Handle various database error types:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl SqlitePipelineRepository {
    async fn save(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt; {
        match sqlx::query("INSERT INTO pipelines ...").execute(&amp;self.pool).await {
            Ok(_) =&gt; Ok(()),
            Err(sqlx::Error::Database(db_err)) =&gt; {
                if db_err.is_unique_violation() {
                    Err(PipelineError::AlreadyExists(pipeline.id().to_string()))
                } else if db_err.is_foreign_key_violation() {
                    Err(PipelineError::InvalidReference(
                        "Invalid foreign key".to_string()
                    ))
                } else {
                    Err(PipelineError::database_error(db_err.to_string()))
                }
            }
            Err(e) =&gt; Err(PipelineError::database_error(e.to_string())),
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="connection-failures"><a class="header" href="#connection-failures">Connection Failures</a></h3>
<p>Handle connection issues gracefully:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl SqlitePipelineRepository {
    async fn with_retry&lt;F, T&gt;(&amp;self, mut operation: F) -&gt; Result&lt;T, PipelineError&gt;
    where
        F: FnMut() -&gt; BoxFuture&lt;'_, Result&lt;T, PipelineError&gt;&gt;,
    {
        let max_retries = 3;
        let mut attempts = 0;

        loop {
            match operation().await {
                Ok(result) =&gt; return Ok(result),
                Err(PipelineError::DatabaseError(_)) if attempts &lt; max_retries =&gt; {
                    attempts += 1;
                    tokio::time::sleep(
                        Duration::from_millis(100 * 2_u64.pow(attempts))
                    ).await;
                    continue;
                }
                Err(e) =&gt; return Err(e),
            }
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="performance-optimizations-3"><a class="header" href="#performance-optimizations-3">Performance Optimizations</a></h2>
<h3 id="connection-pooling"><a class="header" href="#connection-pooling">Connection Pooling</a></h3>
<p>Configure optimal pool settings:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use sqlx::sqlite::SqlitePoolOptions;

impl SqlitePipelineRepository {
    pub async fn new_with_pool_config(
        database_path: &amp;str,
        max_connections: u32,
    ) -&gt; Result&lt;Self, PipelineError&gt; {
        let database_url = format!("sqlite:{}", database_path);

        let pool = SqlitePoolOptions::new()
            .max_connections(max_connections)
            .min_connections(5)
            .acquire_timeout(Duration::from_secs(10))
            .idle_timeout(Duration::from_secs(600))
            .max_lifetime(Duration::from_secs(1800))
            .connect(&amp;database_url)
            .await?;

        Ok(Self { pool })
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="batch-operations"><a class="header" href="#batch-operations">Batch Operations</a></h3>
<p>Optimize bulk inserts:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl SqlitePipelineRepository {
    async fn save_batch(&amp;self, pipelines: &amp;[Pipeline])
        -&gt; Result&lt;(), PipelineError&gt; {
        let mut tx = self.pool.begin().await?;

        // Build batch insert query
        let mut query_builder = sqlx::QueryBuilder::new(
            "INSERT INTO pipelines
             (id, name, description, archived, created_at, updated_at)"
        );

        query_builder.push_values(pipelines, |mut b, pipeline| {
            b.push_bind(pipeline.id().to_string())
             .push_bind(pipeline.name())
             .push_bind(pipeline.description())
             .push_bind(pipeline.archived())
             .push_bind(pipeline.created_at().to_rfc3339())
             .push_bind(pipeline.updated_at().to_rfc3339());
        });

        query_builder.build()
            .execute(&amp;mut *tx)
            .await?;

        tx.commit().await?;
        Ok(())
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="query-optimization"><a class="header" href="#query-optimization">Query Optimization</a></h3>
<p>Use indexes and optimized queries:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Uses index on pipeline_id
sqlx::query(
    "SELECT * FROM pipeline_stages
     WHERE pipeline_id = ?
     ORDER BY order_index"
)
.bind(id)
.fetch_all(&amp;pool)
.await?;

// ❌ BAD: Full table scan
sqlx::query(
    "SELECT * FROM pipeline_stages
     WHERE name LIKE '%test%'"
)
.fetch_all(&amp;pool)
.await?;

// ✅ BETTER: Use full-text search or specific index
sqlx::query(
    "SELECT * FROM pipeline_stages
     WHERE name = ?"
)
.bind("test")
.fetch_all(&amp;pool)
.await?;
<span class="boring">}</span></code></pre></pre>
<h2 id="testing-strategies"><a class="header" href="#testing-strategies">Testing Strategies</a></h2>
<h3 id="in-memory-repository"><a class="header" href="#in-memory-repository">In-Memory Repository</a></h3>
<p>Create test implementation:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::collections::HashMap;
use std::sync::{Arc, Mutex};

pub struct InMemoryPipelineRepository {
    pipelines: Arc&lt;Mutex&lt;HashMap&lt;PipelineId, Pipeline&gt;&gt;&gt;,
}

impl InMemoryPipelineRepository {
    pub fn new() -&gt; Self {
        Self {
            pipelines: Arc::new(Mutex::new(HashMap::new())),
        }
    }
}

#[async_trait]
impl PipelineRepository for InMemoryPipelineRepository {
    async fn save(&amp;self, pipeline: &amp;Pipeline) -&gt; Result&lt;(), PipelineError&gt; {
        let mut pipelines = self.pipelines.lock().unwrap();

        if pipelines.contains_key(pipeline.id()) {
            return Err(PipelineError::AlreadyExists(
                pipeline.id().to_string()
            ));
        }

        pipelines.insert(pipeline.id().clone(), pipeline.clone());
        Ok(())
    }

    async fn find_by_id(&amp;self, id: PipelineId)
        -&gt; Result&lt;Option&lt;Pipeline&gt;, PipelineError&gt; {
        let pipelines = self.pipelines.lock().unwrap();
        Ok(pipelines.get(&amp;id).cloned())
    }

    // ... implement other methods
}
<span class="boring">}</span></code></pre></pre>
<h3 id="unit-tests"><a class="header" href="#unit-tests">Unit Tests</a></h3>
<p>Test repository operations:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_save_and_find() {
        let repo = InMemoryPipelineRepository::new();
        let pipeline = Pipeline::new("test".to_string(), vec![])?;

        // Save
        repo.save(&amp;pipeline).await.unwrap();

        // Find
        let found = repo.find_by_id(pipeline.id().clone())
            .await
            .unwrap()
            .unwrap();

        assert_eq!(found.id(), pipeline.id());
        assert_eq!(found.name(), pipeline.name());
    }

    #[tokio::test]
    async fn test_duplicate_save_fails() {
        let repo = InMemoryPipelineRepository::new();
        let pipeline = Pipeline::new("test".to_string(), vec![])?;

        repo.save(&amp;pipeline).await.unwrap();

        let result = repo.save(&amp;pipeline).await;
        assert!(matches!(result, Err(PipelineError::AlreadyExists(_))));
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="integration-tests"><a class="header" href="#integration-tests">Integration Tests</a></h3>
<p>Test with real database:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(test)]
mod integration_tests {
    use super::*;

    async fn create_test_db() -&gt; SqlitePipelineRepository {
        SqlitePipelineRepository::new(":memory:").await.unwrap()
    }

    #[tokio::test]
    async fn test_transaction_rollback() {
        let repo = create_test_db().await;
        let pipeline = Pipeline::new("test".to_string(), vec![])?;

        // Start transaction
        let mut tx = repo.pool.begin().await.unwrap();

        // Insert pipeline
        sqlx::query("INSERT INTO pipelines ...")
            .execute(&amp;mut *tx)
            .await
            .unwrap();

        // Rollback
        tx.rollback().await.unwrap();

        // Verify pipeline was not saved
        let found = repo.find_by_id(pipeline.id().clone()).await.unwrap();
        assert!(found.is_none());
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="best-practices-3"><a class="header" href="#best-practices-3">Best Practices</a></h2>
<h3 id="use-parameterized-queries"><a class="header" href="#use-parameterized-queries">Use Parameterized Queries</a></h3>
<p>Prevent SQL injection:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Parameterized query
sqlx::query("SELECT * FROM pipelines WHERE name = ?")
    .bind(name)
    .fetch_one(&amp;pool)
    .await?;

// ❌ BAD: String concatenation (SQL injection risk!)
let query = format!("SELECT * FROM pipelines WHERE name = '{}'", name);
sqlx::query(&amp;query).fetch_one(&amp;pool).await?;
<span class="boring">}</span></code></pre></pre>
<h3 id="handle-null-values"><a class="header" href="#handle-null-values">Handle NULL Values</a></h3>
<p>Properly handle nullable columns:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let description: Option&lt;String&gt; = row.try_get("description")?;
let chunk_size: Option&lt;i64&gt; = row.try_get("chunk_size")?;

let pipeline = Pipeline {
    description: description.unwrap_or_default(),
    chunk_size: chunk_size.map(|cs| ChunkSize::new(cs as u64)?),
    // ...
};
<span class="boring">}</span></code></pre></pre>
<h3 id="use-foreign-keys"><a class="header" href="#use-foreign-keys">Use Foreign Keys</a></h3>
<p>Maintain referential integrity:</p>
<pre><code class="language-sql">CREATE TABLE pipeline_stages (
    id TEXT PRIMARY KEY,
    pipeline_id TEXT NOT NULL,
    -- ... other columns
    FOREIGN KEY (pipeline_id)
        REFERENCES pipelines(id)
        ON DELETE CASCADE
);
</code></pre>
<h3 id="index-strategic-columns"><a class="header" href="#index-strategic-columns">Index Strategic Columns</a></h3>
<p>Optimize query performance:</p>
<pre><code class="language-sql">-- Primary lookups
CREATE INDEX idx_pipelines_id ON pipelines(id);
CREATE INDEX idx_pipelines_name ON pipelines(name);

-- Filtering
CREATE INDEX idx_pipelines_archived ON pipelines(archived);

-- Foreign keys
CREATE INDEX idx_stages_pipeline ON pipeline_stages(pipeline_id);

-- Sorting
CREATE INDEX idx_stages_order
    ON pipeline_stages(pipeline_id, order_index);
</code></pre>
<h2 id="next-steps-13"><a class="header" href="#next-steps-13">Next Steps</a></h2>
<p>Now that you understand repository implementation:</p>
<ul>
<li><a href="implementation/schema.html">Schema Management</a> - Database migrations and versioning</li>
<li><a href="implementation/binary-format.html">Binary Format</a> - File persistence patterns</li>
<li><a href="implementation/observability.html">Observability</a> - Monitoring and metrics</li>
<li><a href="implementation/../advanced/testing.html">Testing</a> - Comprehensive testing strategies</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="schema-management"><a class="header" href="#schema-management">Schema Management</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Database schema and migrations.</p>
<h2 id="schema-design"><a class="header" href="#schema-design">Schema Design</a></h2>
<p>TODO: Show database schema</p>
<h2 id="migrations"><a class="header" href="#migrations">Migrations</a></h2>
<p>TODO: Explain sqlx migrations</p>
<h2 id="version-management"><a class="header" href="#version-management">Version Management</a></h2>
<p>TODO: Explain versioning</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="file-io"><a class="header" href="#file-io">File I/O</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>File input/output operations.</p>
<h2 id="file-reading"><a class="header" href="#file-reading">File Reading</a></h2>
<p>TODO: Extract from file_io_service.rs</p>
<h2 id="file-writing"><a class="header" href="#file-writing">File Writing</a></h2>
<p>TODO: Show writing patterns</p>
<h2 id="error-handling-5"><a class="header" href="#error-handling-5">Error Handling</a></h2>
<p>TODO: Show error handling</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chunking-strategy"><a class="header" href="#chunking-strategy">Chunking Strategy</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>How files are split into chunks for processing.</p>
<h2 id="chunk-size-1"><a class="header" href="#chunk-size-1">Chunk Size</a></h2>
<p>TODO: Explain chunk sizing</p>
<h2 id="chunk-processing-1"><a class="header" href="#chunk-processing-1">Chunk Processing</a></h2>
<p>TODO: Show chunk processing</p>
<h2 id="memory-management-2"><a class="header" href="#memory-management-2">Memory Management</a></h2>
<p>TODO: Explain memory efficiency</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="binary-file-format"><a class="header" href="#binary-file-format">Binary File Format</a></h1>
<p><strong>Version:</strong> 1.0
<strong>Date:</strong> 2025-01-04
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner, Claude Code
<strong>Status:</strong> Active</p>
<h2 id="overview-8"><a class="header" href="#overview-8">Overview</a></h2>
<p>The Adaptive Pipeline uses a custom binary file format (<code>.adapipe</code>) to store processed files with complete recovery metadata and integrity verification. This format enables perfect restoration of original files while maintaining processing history and security.</p>
<p><strong>Key Features:</strong></p>
<ul>
<li><strong>Complete Recovery</strong>: All metadata needed to restore original files</li>
<li><strong>Integrity Verification</strong>: SHA-256 checksums for both input and output</li>
<li><strong>Processing History</strong>: Complete record of all processing steps</li>
<li><strong>Format Versioning</strong>: Backward compatibility through version management</li>
<li><strong>Security</strong>: Supports encryption with nonce management</li>
</ul>
<h2 id="file-format-specification"><a class="header" href="#file-format-specification">File Format Specification</a></h2>
<h3 id="binary-layout"><a class="header" href="#binary-layout">Binary Layout</a></h3>
<p>The <code>.adapipe</code> format uses a reverse-header design for efficient processing:</p>
<pre><code class="language-text">┌─────────────────────────────────────────────┐
│          PROCESSED CHUNK DATA               │
│         (variable length)                   │
│  - Compressed and/or encrypted chunks       │
│  - Each chunk: [NONCE][LENGTH][DATA]        │
└─────────────────────────────────────────────┘
┌─────────────────────────────────────────────┐
│          JSON HEADER                        │
│         (variable length)                   │
│  - Processing metadata                      │
│  - Recovery information                     │
│  - Checksums                                │
└─────────────────────────────────────────────┘
┌─────────────────────────────────────────────┐
│      HEADER_LENGTH (4 bytes, u32 LE)        │
│  - Length of JSON header in bytes           │
└─────────────────────────────────────────────┘
┌─────────────────────────────────────────────┐
│    FORMAT_VERSION (2 bytes, u16 LE)         │
│  - Current version: 1                       │
└─────────────────────────────────────────────┘
┌─────────────────────────────────────────────┐
│      MAGIC_BYTES (8 bytes)                  │
│  - "ADAPIPE\0" (0x4144415049504500)         │
└─────────────────────────────────────────────┘
</code></pre>
<p><strong>Why Reverse Header?</strong></p>
<ul>
<li><strong>Efficient Reading</strong>: Read magic bytes and version first</li>
<li><strong>Validation</strong>: Quickly validate format without reading entire file</li>
<li><strong>Streaming</strong>: Process chunk data while reading header</li>
<li><strong>Metadata Location</strong>: Header location calculated from end of file</li>
</ul>
<h3 id="magic-bytes"><a class="header" href="#magic-bytes">Magic Bytes</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub const MAGIC_BYTES: [u8; 8] = [
    0x41, 0x44, 0x41, 0x50, // "ADAP"
    0x49, 0x50, 0x45, 0x00  // "IPE\0"
];
<span class="boring">}</span></code></pre></pre>
<p><strong>Purpose:</strong></p>
<ul>
<li>Identify files in <code>.adapipe</code> format</li>
<li>Prevent accidental processing of wrong file types</li>
<li>Enable format detection tools</li>
</ul>
<h3 id="format-version"><a class="header" href="#format-version">Format Version</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub const CURRENT_FORMAT_VERSION: u16 = 1;
<span class="boring">}</span></code></pre></pre>
<p><strong>Version History:</strong></p>
<ul>
<li><strong>Version 1</strong>: Initial format with compression, encryption, checksum support</li>
</ul>
<p><strong>Future Versions:</strong></p>
<ul>
<li>Version 2: Enhanced metadata, additional algorithms</li>
<li>Version 3: Streaming optimizations, compression improvements</li>
</ul>
<h2 id="file-header-structure"><a class="header" href="#file-header-structure">File Header Structure</a></h2>
<h3 id="header-fields"><a class="header" href="#header-fields">Header Fields</a></h3>
<p>The JSON header contains comprehensive metadata:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct FileHeader {
    /// Application version (e.g., "0.1.0")
    pub app_version: String,

    /// File format version (1)
    pub format_version: u16,

    /// Original input filename
    pub original_filename: String,

    /// Original file size in bytes
    pub original_size: u64,

    /// SHA-256 checksum of original file
    pub original_checksum: String,

    /// SHA-256 checksum of processed file
    pub output_checksum: String,

    /// Processing steps applied (in order)
    pub processing_steps: Vec&lt;ProcessingStep&gt;,

    /// Chunk size used (bytes)
    pub chunk_size: u32,

    /// Number of chunks
    pub chunk_count: u32,

    /// Processing timestamp (RFC3339)
    pub processed_at: DateTime&lt;Utc&gt;,

    /// Pipeline ID
    pub pipeline_id: String,

    /// Additional metadata
    pub metadata: HashMap&lt;String, String&gt;,
}
<span class="boring">}</span></code></pre></pre>
<h3 id="processing-steps"><a class="header" href="#processing-steps">Processing Steps</a></h3>
<p>Each processing step records transformation details:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct ProcessingStep {
    /// Step type (compression, encryption, etc.)
    pub step_type: ProcessingStepType,

    /// Algorithm used (e.g., "brotli", "aes-256-gcm")
    pub algorithm: String,

    /// Algorithm-specific parameters
    pub parameters: HashMap&lt;String, String&gt;,

    /// Application order (0-based)
    pub order: u32,
}

pub enum ProcessingStepType {
    Compression,
    Encryption,
    Checksum,
    PassThrough,
    Custom(String),
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Example Processing Steps:</strong></p>
<pre><code class="language-json">{
  "processing_steps": [
    {
      "step_type": "Compression",
      "algorithm": "brotli",
      "parameters": {
        "level": "6"
      },
      "order": 0
    },
    {
      "step_type": "Encryption",
      "algorithm": "aes-256-gcm",
      "parameters": {
        "key_derivation": "argon2"
      },
      "order": 1
    },
    {
      "step_type": "Checksum",
      "algorithm": "sha256",
      "parameters": {},
      "order": 2
    }
  ]
}
</code></pre>
<h2 id="chunk-format"><a class="header" href="#chunk-format">Chunk Format</a></h2>
<h3 id="chunk-structure"><a class="header" href="#chunk-structure">Chunk Structure</a></h3>
<p>Each chunk in the processed data section follows this format:</p>
<pre><code class="language-text">┌────────────────────────────────────┐
│   NONCE (12 bytes)                 │
│  - Unique for each chunk           │
│  - Used for encryption IV          │
└────────────────────────────────────┘
┌────────────────────────────────────┐
│   DATA_LENGTH (4 bytes, u32 LE)    │
│  - Length of encrypted data        │
└────────────────────────────────────┘
┌────────────────────────────────────┐
│   ENCRYPTED_DATA (variable)        │
│  - Compressed and encrypted        │
│  - Includes authentication tag     │
└────────────────────────────────────┘
</code></pre>
<p><strong>Rust Structure:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct ChunkFormat {
    /// Encryption nonce (12 bytes for AES-GCM)
    pub nonce: [u8; 12],

    /// Length of encrypted data
    pub data_length: u32,

    /// Encrypted (and possibly compressed) chunk data
    pub encrypted_data: Vec&lt;u8&gt;,
}
<span class="boring">}</span></code></pre></pre>
<h3 id="chunk-processing-2"><a class="header" href="#chunk-processing-2">Chunk Processing</a></h3>
<p><strong>Forward Processing (Compress → Encrypt):</strong></p>
<pre><code class="language-text">1. Read original chunk
2. Compress chunk data
3. Generate unique nonce
4. Encrypt compressed data
5. Write: [NONCE][LENGTH][ENCRYPTED_DATA]
</code></pre>
<p><strong>Reverse Processing (Decrypt → Decompress):</strong></p>
<pre><code class="language-text">1. Read: [NONCE][LENGTH][ENCRYPTED_DATA]
2. Decrypt using nonce
3. Decompress decrypted data
4. Verify checksum
5. Write original chunk
</code></pre>
<h2 id="creating-binary-files"><a class="header" href="#creating-binary-files">Creating Binary Files</a></h2>
<h3 id="basic-file-creation"><a class="header" href="#basic-file-creation">Basic File Creation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::value_objects::{FileHeader, ProcessingStep};
use std::fs::File;
use std::io::Write;

fn create_adapipe_file(
    input_data: &amp;[u8],
    output_path: &amp;str,
    processing_steps: Vec&lt;ProcessingStep&gt;,
) -&gt; Result&lt;(), PipelineError&gt; {
    // Create header
    let original_checksum = calculate_sha256(input_data);
    let mut header = FileHeader::new(
        "input.txt".to_string(),
        input_data.len() as u64,
        original_checksum,
    );

    // Add processing steps
    header.processing_steps = processing_steps;
    header.chunk_count = calculate_chunk_count(input_data.len(), header.chunk_size);

    // Process chunks
    let processed_data = process_chunks(input_data, &amp;header.processing_steps)?;

    // Calculate output checksum
    header.output_checksum = calculate_sha256(&amp;processed_data);

    // Serialize header to JSON
    let json_header = serde_json::to_vec(&amp;header)?;
    let header_length = json_header.len() as u32;

    // Write file in reverse order
    let mut file = File::create(output_path)?;

    // 1. Write processed data
    file.write_all(&amp;processed_data)?;

    // 2. Write JSON header
    file.write_all(&amp;json_header)?;

    // 3. Write header length
    file.write_all(&amp;header_length.to_le_bytes())?;

    // 4. Write format version
    file.write_all(&amp;CURRENT_FORMAT_VERSION.to_le_bytes())?;

    // 5. Write magic bytes
    file.write_all(&amp;MAGIC_BYTES)?;

    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h3 id="adding-processing-steps"><a class="header" href="#adding-processing-steps">Adding Processing Steps</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl FileHeader {
    /// Add compression step
    pub fn add_compression_step(mut self, algorithm: &amp;str, level: u32) -&gt; Self {
        let mut parameters = HashMap::new();
        parameters.insert("level".to_string(), level.to_string());

        self.processing_steps.push(ProcessingStep {
            step_type: ProcessingStepType::Compression,
            algorithm: algorithm.to_string(),
            parameters,
            order: self.processing_steps.len() as u32,
        });

        self
    }

    /// Add encryption step
    pub fn add_encryption_step(
        mut self,
        algorithm: &amp;str,
        key_derivation: &amp;str
    ) -&gt; Self {
        let mut parameters = HashMap::new();
        parameters.insert("key_derivation".to_string(), key_derivation.to_string());

        self.processing_steps.push(ProcessingStep {
            step_type: ProcessingStepType::Encryption,
            algorithm: algorithm.to_string(),
            parameters,
            order: self.processing_steps.len() as u32,
        });

        self
    }

    /// Add checksum step
    pub fn add_checksum_step(mut self, algorithm: &amp;str) -&gt; Self {
        self.processing_steps.push(ProcessingStep {
            step_type: ProcessingStepType::Checksum,
            algorithm: algorithm.to_string(),
            parameters: HashMap::new(),
            order: self.processing_steps.len() as u32,
        });

        self
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="reading-binary-files"><a class="header" href="#reading-binary-files">Reading Binary Files</a></h2>
<h3 id="basic-file-reading"><a class="header" href="#basic-file-reading">Basic File Reading</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::fs::File;
use std::io::{Read, Seek, SeekFrom};

fn read_adapipe_file(path: &amp;str) -&gt; Result&lt;FileHeader, PipelineError&gt; {
    let mut file = File::open(path)?;

    // Read from end of file (reverse header)
    file.seek(SeekFrom::End(-8))?;

    // 1. Read and validate magic bytes
    let mut magic = [0u8; 8];
    file.read_exact(&amp;mut magic)?;

    if magic != MAGIC_BYTES {
        return Err(PipelineError::InvalidFormat(
            "Not an .adapipe file".to_string()
        ));
    }

    // 2. Read format version
    file.seek(SeekFrom::End(-10))?;
    let mut version_bytes = [0u8; 2];
    file.read_exact(&amp;mut version_bytes)?;
    let version = u16::from_le_bytes(version_bytes);

    if version &gt; CURRENT_FORMAT_VERSION {
        return Err(PipelineError::UnsupportedVersion(version));
    }

    // 3. Read header length
    file.seek(SeekFrom::End(-14))?;
    let mut length_bytes = [0u8; 4];
    file.read_exact(&amp;mut length_bytes)?;
    let header_length = u32::from_le_bytes(length_bytes) as usize;

    // 4. Read JSON header
    file.seek(SeekFrom::End(-(14 + header_length as i64)))?;
    let mut json_data = vec![0u8; header_length];
    file.read_exact(&amp;mut json_data)?;

    // 5. Deserialize header
    let header: FileHeader = serde_json::from_slice(&amp;json_data)?;

    Ok(header)
}
<span class="boring">}</span></code></pre></pre>
<h3 id="reading-chunk-data"><a class="header" href="#reading-chunk-data">Reading Chunk Data</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn read_chunks(
    file: &amp;mut File,
    header: &amp;FileHeader
) -&gt; Result&lt;Vec&lt;ChunkFormat&gt;, PipelineError&gt; {
    let mut chunks = Vec::with_capacity(header.chunk_count as usize);

    // Seek to start of chunk data
    file.seek(SeekFrom::Start(0))?;

    for _ in 0..header.chunk_count {
        // Read nonce
        let mut nonce = [0u8; 12];
        file.read_exact(&amp;mut nonce)?;

        // Read data length
        let mut length_bytes = [0u8; 4];
        file.read_exact(&amp;mut length_bytes)?;
        let data_length = u32::from_le_bytes(length_bytes);

        // Read encrypted data
        let mut encrypted_data = vec![0u8; data_length as usize];
        file.read_exact(&amp;mut encrypted_data)?;

        chunks.push(ChunkFormat {
            nonce,
            data_length,
            encrypted_data,
        });
    }

    Ok(chunks)
}
<span class="boring">}</span></code></pre></pre>
<h2 id="file-recovery"><a class="header" href="#file-recovery">File Recovery</a></h2>
<h3 id="complete-recovery-process"><a class="header" href="#complete-recovery-process">Complete Recovery Process</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn restore_original_file(
    input_path: &amp;str,
    output_path: &amp;str,
    password: Option&lt;&amp;str&gt;,
) -&gt; Result&lt;(), PipelineError&gt; {
    // 1. Read header
    let header = read_adapipe_file(input_path)?;

    // 2. Read chunks
    let mut file = File::open(input_path)?;
    let chunks = read_chunks(&amp;mut file, &amp;header)?;

    // 3. Process chunks in reverse order
    let mut restored_data = Vec::new();

    for chunk in chunks {
        let mut chunk_data = chunk.encrypted_data;

        // Reverse processing steps
        for step in header.processing_steps.iter().rev() {
            chunk_data = match step.step_type {
                ProcessingStepType::Encryption =&gt; {
                    decrypt_chunk(chunk_data, &amp;chunk.nonce, &amp;step, password)?
                }
                ProcessingStepType::Compression =&gt; {
                    decompress_chunk(chunk_data, &amp;step)?
                }
                ProcessingStepType::Checksum =&gt; {
                    verify_chunk_checksum(&amp;chunk_data, &amp;step)?;
                    chunk_data
                }
                _ =&gt; chunk_data,
            };
        }

        restored_data.extend_from_slice(&amp;chunk_data);
    }

    // 4. Verify restored data
    let restored_checksum = calculate_sha256(&amp;restored_data);
    if restored_checksum != header.original_checksum {
        return Err(PipelineError::IntegrityError(
            "Restored data checksum mismatch".to_string()
        ));
    }

    // 5. Write restored file
    let mut output = File::create(output_path)?;
    output.write_all(&amp;restored_data)?;

    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h3 id="processing-step-reversal"><a class="header" href="#processing-step-reversal">Processing Step Reversal</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn reverse_processing_step(
    data: Vec&lt;u8&gt;,
    step: &amp;ProcessingStep,
    password: Option&lt;&amp;str&gt;,
) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
    match step.step_type {
        ProcessingStepType::Compression =&gt; {
            // Decompress
            match step.algorithm.as_str() {
                "brotli" =&gt; decompress_brotli(data),
                "gzip" =&gt; decompress_gzip(data),
                "zstd" =&gt; decompress_zstd(data),
                "lz4" =&gt; decompress_lz4(data),
                _ =&gt; Err(PipelineError::UnsupportedAlgorithm(
                    step.algorithm.clone()
                )),
            }
        }
        ProcessingStepType::Encryption =&gt; {
            // Decrypt
            let password = password.ok_or(PipelineError::MissingPassword)?;
            match step.algorithm.as_str() {
                "aes-256-gcm" =&gt; decrypt_aes_256_gcm(data, password, step),
                "chacha20-poly1305" =&gt; decrypt_chacha20(data, password, step),
                _ =&gt; Err(PipelineError::UnsupportedAlgorithm(
                    step.algorithm.clone()
                )),
            }
        }
        ProcessingStepType::Checksum =&gt; {
            // Verify checksum (no transformation)
            verify_checksum(&amp;data, step)?;
            Ok(data)
        }
        _ =&gt; Ok(data),
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="integrity-verification-3"><a class="header" href="#integrity-verification-3">Integrity Verification</a></h2>
<h3 id="file-validation"><a class="header" href="#file-validation">File Validation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn validate_adapipe_file(path: &amp;str) -&gt; Result&lt;ValidationReport, PipelineError&gt; {
    let mut report = ValidationReport::new();

    // 1. Read and validate header
    let header = match read_adapipe_file(path) {
        Ok(h) =&gt; {
            report.add_check("Header format", true, "Valid");
            h
        }
        Err(e) =&gt; {
            report.add_check("Header format", false, &amp;e.to_string());
            return Ok(report);
        }
    };

    // 2. Validate format version
    if header.format_version &lt;= CURRENT_FORMAT_VERSION {
        report.add_check("Format version", true, &amp;format!("v{}", header.format_version));
    } else {
        report.add_check(
            "Format version",
            false,
            &amp;format!("Unsupported: v{}", header.format_version)
        );
    }

    // 3. Validate processing steps
    for (i, step) in header.processing_steps.iter().enumerate() {
        let is_supported = match step.step_type {
            ProcessingStepType::Compression =&gt; {
                matches!(step.algorithm.as_str(), "brotli" | "gzip" | "zstd" | "lz4")
            }
            ProcessingStepType::Encryption =&gt; {
                matches!(step.algorithm.as_str(), "aes-256-gcm" | "chacha20-poly1305")
            }
            _ =&gt; true,
        };

        report.add_check(
            &amp;format!("Step {} ({:?})", i, step.step_type),
            is_supported,
            &amp;step.algorithm
        );
    }

    // 4. Verify output checksum
    let mut file = File::open(path)?;
    let data_length = file.metadata()?.len() - 14 - header.json_size() as u64;
    let mut processed_data = vec![0u8; data_length as usize];
    file.read_exact(&amp;mut processed_data)?;

    let calculated_checksum = calculate_sha256(&amp;processed_data);
    let checksums_match = calculated_checksum == header.output_checksum;

    report.add_check(
        "Output checksum",
        checksums_match,
        if checksums_match { "Valid" } else { "Mismatch" }
    );

    Ok(report)
}

pub struct ValidationReport {
    checks: Vec&lt;(String, bool, String)&gt;,
}

impl ValidationReport {
    pub fn new() -&gt; Self {
        Self { checks: Vec::new() }
    }

    pub fn add_check(&amp;mut self, name: &amp;str, passed: bool, message: &amp;str) {
        self.checks.push((name.to_string(), passed, message.to_string()));
    }

    pub fn is_valid(&amp;self) -&gt; bool {
        self.checks.iter().all(|(_, passed, _)| *passed)
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="checksum-verification"><a class="header" href="#checksum-verification">Checksum Verification</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn verify_file_integrity(path: &amp;str) -&gt; Result&lt;bool, PipelineError&gt; {
    let header = read_adapipe_file(path)?;

    // Calculate actual checksum
    let mut file = File::open(path)?;
    let data_length = file.metadata()?.len() - 14 - header.json_size() as u64;
    let mut data = vec![0u8; data_length as usize];
    file.read_exact(&amp;mut data)?;

    let calculated = calculate_sha256(&amp;data);

    // Compare with stored checksum
    Ok(calculated == header.output_checksum)
}

fn calculate_sha256(data: &amp;[u8]) -&gt; String {
    let mut hasher = Sha256::new();
    hasher.update(data);
    format!("{:x}", hasher.finalize())
}
<span class="boring">}</span></code></pre></pre>
<h2 id="version-management-1"><a class="header" href="#version-management-1">Version Management</a></h2>
<h3 id="format-versioning"><a class="header" href="#format-versioning">Format Versioning</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn check_format_compatibility(version: u16) -&gt; Result&lt;(), PipelineError&gt; {
    match version {
        1 =&gt; Ok(()), // Current version
        v if v &lt; CURRENT_FORMAT_VERSION =&gt; {
            // Older version - attempt migration
            migrate_format(v, CURRENT_FORMAT_VERSION)
        }
        v =&gt; Err(PipelineError::UnsupportedVersion(v)),
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="format-migration"><a class="header" href="#format-migration">Format Migration</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn migrate_format(from: u16, to: u16) -&gt; Result&lt;(), PipelineError&gt; {
    match (from, to) {
        (1, 2) =&gt; {
            // Migration from v1 to v2
            // Add new fields with defaults
            Ok(())
        }
        _ =&gt; Err(PipelineError::MigrationUnsupported(from, to)),
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="backward-compatibility"><a class="header" href="#backward-compatibility">Backward Compatibility</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn read_any_version(path: &amp;str) -&gt; Result&lt;FileHeader, PipelineError&gt; {
    let version = read_format_version(path)?;

    match version {
        1 =&gt; read_v1_format(path),
        2 =&gt; read_v2_format(path),
        v =&gt; Err(PipelineError::UnsupportedVersion(v)),
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="best-practices-4"><a class="header" href="#best-practices-4">Best Practices</a></h2>
<h3 id="file-creation"><a class="header" href="#file-creation">File Creation</a></h3>
<p><strong>Always set checksums:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Set both checksums
let original_checksum = calculate_sha256(&amp;input_data);
let header = FileHeader::new(filename, size, original_checksum);
// ... process data ...
header.output_checksum = calculate_sha256(&amp;processed_data);
<span class="boring">}</span></code></pre></pre>
<p><strong>Record all processing steps:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Record every transformation
header = header
    .add_compression_step("brotli", 6)
    .add_encryption_step("aes-256-gcm", "argon2")
    .add_checksum_step("sha256");
<span class="boring">}</span></code></pre></pre>
<h3 id="file-reading-1"><a class="header" href="#file-reading-1">File Reading</a></h3>
<p><strong>Always validate format:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Validate before processing
let header = read_adapipe_file(path)?;

if header.format_version &gt; CURRENT_FORMAT_VERSION {
    return Err(PipelineError::UnsupportedVersion(
        header.format_version
    ));
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Verify checksums:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Verify integrity
let restored_checksum = calculate_sha256(&amp;restored_data);
if restored_checksum != header.original_checksum {
    return Err(PipelineError::IntegrityError(
        "Checksum mismatch".to_string()
    ));
}
<span class="boring">}</span></code></pre></pre>
<h3 id="error-handling-6"><a class="header" href="#error-handling-6">Error Handling</a></h3>
<p><strong>Handle all error cases:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>match read_adapipe_file(path) {
    Ok(header) =&gt; process_file(header),
    Err(PipelineError::InvalidFormat(msg)) =&gt; {
        eprintln!("Not a valid .adapipe file: {}", msg);
    }
    Err(PipelineError::UnsupportedVersion(v)) =&gt; {
        eprintln!("Unsupported format version: {}", v);
    }
    Err(e) =&gt; {
        eprintln!("Error reading file: {}", e);
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="security-considerations-2"><a class="header" href="#security-considerations-2">Security Considerations</a></h2>
<h3 id="nonce-management-2"><a class="header" href="#nonce-management-2">Nonce Management</a></h3>
<p><strong>Never reuse nonces:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Generate unique nonce per chunk
fn generate_nonce() -&gt; [u8; 12] {
    let mut nonce = [0u8; 12];
    use rand::RngCore;
    rand::thread_rng().fill_bytes(&amp;mut nonce);
    nonce
}
<span class="boring">}</span></code></pre></pre>
<h3 id="key-derivation-1"><a class="header" href="#key-derivation-1">Key Derivation</a></h3>
<p><strong>Use strong key derivation:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Argon2 for password-based encryption
fn derive_key(password: &amp;str, salt: &amp;[u8]) -&gt; Vec&lt;u8&gt; {
    use argon2::{Argon2, PasswordHasher};

    let argon2 = Argon2::default();
    let hash = argon2.hash_password(password.as_bytes(), salt)
        .unwrap();

    hash.hash.unwrap().as_bytes().to_vec()
}
<span class="boring">}</span></code></pre></pre>
<h3 id="integrity-protection"><a class="header" href="#integrity-protection">Integrity Protection</a></h3>
<p><strong>Verify at every step:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Verify after each transformation
fn process_with_verification(
    data: Vec&lt;u8&gt;,
    step: &amp;ProcessingStep
) -&gt; Result&lt;Vec&lt;u8&gt;, PipelineError&gt; {
    let processed = apply_transformation(data, step)?;
    verify_transformation(&amp;processed, step)?;
    Ok(processed)
}
<span class="boring">}</span></code></pre></pre>
<h2 id="next-steps-14"><a class="header" href="#next-steps-14">Next Steps</a></h2>
<p>Now that you understand the binary file format:</p>
<ul>
<li><a href="implementation/chunking.html">Chunking Strategy</a> - Efficient chunk processing</li>
<li><a href="implementation/file-io.html">File I/O</a> - File reading and writing patterns</li>
<li><a href="implementation/integrity.html">Integrity Verification</a> - Checksum algorithms</li>
<li><a href="implementation/encryption.html">Encryption</a> - Encryption implementation details</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="observability-overview"><a class="header" href="#observability-overview">Observability Overview</a></h1>
<p><strong>Version</strong>: 1.0
<strong>Date</strong>: 2025-10-04
<strong>License</strong>: BSD-3-Clause
<strong>Copyright</strong>: (c) 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors</strong>: Michael Gardner
<strong>Status</strong>: Active</p>
<hr />
<h2 id="overview-9"><a class="header" href="#overview-9">Overview</a></h2>
<p>Observability is the ability to understand the internal state of a system by examining its external outputs. The Optimized Adaptive Pipeline implements a comprehensive observability strategy that combines <strong>metrics</strong>, <strong>logging</strong>, and <strong>health monitoring</strong> to provide complete system visibility.</p>
<h3 id="key-principles-1"><a class="header" href="#key-principles-1">Key Principles</a></h3>
<ul>
<li><strong>Three Pillars</strong>: Metrics, Logs, and Traces (health monitoring)</li>
<li><strong>Comprehensive Coverage</strong>: Monitor all aspects of system operation</li>
<li><strong>Real-Time Insights</strong>: Live performance tracking and alerting</li>
<li><strong>Low Overhead</strong>: Minimal performance impact on pipeline processing</li>
<li><strong>Integration Ready</strong>: Compatible with external monitoring systems (Prometheus, Grafana)</li>
<li><strong>Actionable</strong>: Designed to support debugging, optimization, and operations</li>
</ul>
<hr />
<h2 id="the-three-pillars"><a class="header" href="#the-three-pillars">The Three Pillars</a></h2>
<h3 id="1-metrics---quantitative-measurements"><a class="header" href="#1-metrics---quantitative-measurements">1. Metrics - Quantitative Measurements</a></h3>
<p><strong>What</strong>: Numerical measurements aggregated over time</p>
<p><strong>Purpose</strong>: Track system performance, identify trends, detect anomalies</p>
<p><strong>Implementation</strong>:</p>
<ul>
<li>Domain layer: <code>ProcessingMetrics</code> entity</li>
<li>Infrastructure layer: <code>MetricsService</code> with Prometheus integration</li>
<li>HTTP <code>/metrics</code> endpoint for scraping</li>
</ul>
<p><strong>Key Metrics</strong>:</p>
<ul>
<li><strong>Counters</strong>: Total pipelines processed, bytes processed, errors</li>
<li><strong>Gauges</strong>: Active pipelines, current throughput, memory usage</li>
<li><strong>Histograms</strong>: Processing duration, latency distribution</li>
</ul>
<p><strong>See</strong>: <a href="implementation/metrics.html">Metrics Collection</a></p>
<h3 id="2-logging---contextual-events"><a class="header" href="#2-logging---contextual-events">2. Logging - Contextual Events</a></h3>
<p><strong>What</strong>: Timestamped records of discrete events with structured context</p>
<p><strong>Purpose</strong>: Understand what happened, when, and why</p>
<p><strong>Implementation</strong>:</p>
<ul>
<li>Bootstrap phase: <code>BootstrapLogger</code> trait</li>
<li>Application phase: <code>tracing</code> crate with structured logging</li>
<li>Multiple log levels: ERROR, WARN, INFO, DEBUG, TRACE</li>
</ul>
<p><strong>Key Features</strong>:</p>
<ul>
<li>Structured fields for filtering and analysis</li>
<li>Correlation IDs for request tracing</li>
<li>Integration with ObservabilityService for alerts</li>
</ul>
<p><strong>See</strong>: <a href="implementation/logging.html">Logging Implementation</a></p>
<h3 id="3-health-monitoring---system-status"><a class="header" href="#3-health-monitoring---system-status">3. Health Monitoring - System Status</a></h3>
<p><strong>What</strong>: Aggregated health scores and status indicators</p>
<p><strong>Purpose</strong>: Quickly assess system health and detect degradation</p>
<p><strong>Implementation</strong>:</p>
<ul>
<li><code>ObservabilityService</code> with real-time health scoring</li>
<li><code>SystemHealth</code> status reporting</li>
<li>Alert generation for threshold violations</li>
</ul>
<p><strong>Key Components</strong>:</p>
<ul>
<li>Performance health (throughput, latency)</li>
<li>Error health (error rates, failure patterns)</li>
<li>Resource health (CPU, memory, I/O)</li>
<li>Overall health score (weighted composite)</li>
</ul>
<hr />
<h2 id="architecture-5"><a class="header" href="#architecture-5">Architecture</a></h2>
<h3 id="layered-observability"><a class="header" href="#layered-observability">Layered Observability</a></h3>
<pre><code class="language-text">┌─────────────────────────────────────────────────────────────┐
│                    Application Layer                         │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  ┌─────────────────────────────────────────────────────┐   │
│  │           ObservabilityService                      │   │
│  │  (Orchestrates monitoring, alerting, health)        │   │
│  └──────────┬────────────────┬──────────────┬──────────┘   │
│             │                │              │               │
│             ▼                ▼              ▼               │
│  ┌──────────────┐  ┌─────────────┐  ┌─────────────┐       │
│  │ Performance  │  │   Alert     │  │   Health    │       │
│  │   Tracker    │  │  Manager    │  │  Monitor    │       │
│  └──────────────┘  └─────────────┘  └─────────────┘       │
│                                                              │
└─────────────────────────────────────────────────────────────┘
                           │
                           │ Uses
                           ▼
┌─────────────────────────────────────────────────────────────┐
│                  Infrastructure Layer                        │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  ┌──────────────────┐              ┌──────────────────┐    │
│  │ MetricsService   │              │ Logging (tracing)│    │
│  │ (Prometheus)     │              │ (Structured logs)│    │
│  └──────────────────┘              └──────────────────┘    │
│           │                                 │               │
│           │                                 │               │
│           ▼                                 ▼               │
│  ┌──────────────────┐              ┌──────────────────┐    │
│  │ /metrics HTTP    │              │ Log Subscribers  │    │
│  │ endpoint         │              │ (console, file)  │    │
│  └──────────────────┘              └──────────────────┘    │
│                                                              │
└─────────────────────────────────────────────────────────────┘
                           │
                           │ Exposes
                           ▼
┌─────────────────────────────────────────────────────────────┐
│                    External Systems                          │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐ │
│  │  Prometheus  │    │    Grafana   │    │ Log Analysis │ │
│  │   (Scraper)  │    │ (Dashboards) │    │    Tools     │ │
│  └──────────────┘    └──────────────┘    └──────────────┘ │
│                                                              │
└─────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="component-integration"><a class="header" href="#component-integration">Component Integration</a></h3>
<p>The observability components are tightly integrated:</p>
<ol>
<li><strong>ObservabilityService</strong> orchestrates monitoring</li>
<li><strong>MetricsService</strong> records quantitative data</li>
<li><strong>Logging</strong> records contextual events</li>
<li><strong>PerformanceTracker</strong> maintains real-time state</li>
<li><strong>AlertManager</strong> checks thresholds and generates alerts</li>
<li><strong>HealthMonitor</strong> computes system health scores</li>
</ol>
<hr />
<h2 id="observabilityservice"><a class="header" href="#observabilityservice">ObservabilityService</a></h2>
<h3 id="core-responsibilities"><a class="header" href="#core-responsibilities">Core Responsibilities</a></h3>
<p>The <code>ObservabilityService</code> is the central orchestrator for monitoring:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct ObservabilityService {
    metrics_service: Arc&lt;MetricsService&gt;,
    performance_tracker: Arc&lt;RwLock&lt;PerformanceTracker&gt;&gt;,
    alert_thresholds: AlertThresholds,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Key Methods</strong>:</p>
<ul>
<li><code>start_operation()</code> - Begin tracking an operation</li>
<li><code>complete_operation()</code> - End tracking with metrics</li>
<li><code>get_system_health()</code> - Get current health status</li>
<li><code>record_processing_metrics()</code> - Record pipeline metrics</li>
<li><code>check_alerts()</code> - Evaluate alert conditions</li>
</ul>
<h3 id="performancetracker"><a class="header" href="#performancetracker">PerformanceTracker</a></h3>
<p>Maintains real-time performance state:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct PerformanceTracker {
    pub active_operations: u32,
    pub total_operations: u64,
    pub average_throughput_mbps: f64,
    pub peak_throughput_mbps: f64,
    pub error_rate_percent: f64,
    pub system_health_score: f64,
    pub last_update: Instant,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Tracked Metrics</strong>:</p>
<ul>
<li>Active operation count</li>
<li>Total operation count</li>
<li>Average and peak throughput</li>
<li>Error rate percentage</li>
<li>Overall health score</li>
<li>Last update timestamp</li>
</ul>
<h3 id="operationtracker"><a class="header" href="#operationtracker">OperationTracker</a></h3>
<p>Automatic operation lifecycle tracking:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct OperationTracker {
    operation_name: String,
    start_time: Instant,
    observability_service: ObservabilityService,
    completed: AtomicBool,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Lifecycle</strong>:</p>
<ol>
<li>Created via <code>start_operation()</code></li>
<li>Increments active operation count</li>
<li>Logs operation start</li>
<li>On completion: Records duration, throughput, success/failure</li>
<li>On drop (if not completed): Marks as failed</li>
</ol>
<p><strong>Drop Safety</strong>: If the tracker is dropped without explicit completion (e.g., due to panic), it automatically marks the operation as failed.</p>
<hr />
<h2 id="health-monitoring"><a class="header" href="#health-monitoring">Health Monitoring</a></h2>
<h3 id="systemhealth-structure"><a class="header" href="#systemhealth-structure">SystemHealth Structure</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct SystemHealth {
    pub status: HealthStatus,
    pub score: f64,
    pub active_operations: u32,
    pub throughput_mbps: f64,
    pub error_rate_percent: f64,
    pub uptime_seconds: u64,
    pub alerts: Vec&lt;Alert&gt;,
}

pub enum HealthStatus {
    Healthy,   // Score &gt;= 90.0
    Warning,   // Score &gt;= 70.0 &amp;&amp; &lt; 90.0
    Critical,  // Score &lt; 70.0
    Unknown,   // Unable to determine health
}
<span class="boring">}</span></code></pre></pre>
<h3 id="health-score-calculation"><a class="header" href="#health-score-calculation">Health Score Calculation</a></h3>
<p>The health score starts at 100 and deductions are applied:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut score = 100.0;

// Deduct for high error rate
if error_rate_percent &gt; max_error_rate_percent {
    score -= 30.0;  // Error rate is critical
}

// Deduct for low throughput
if average_throughput_mbps &lt; min_throughput_mbps {
    score -= 20.0;  // Performance degradation
}

// Additional deductions for other factors...
<span class="boring">}</span></code></pre></pre>
<p><strong>Health Score Ranges</strong>:</p>
<ul>
<li><strong>100-90</strong>: Healthy - System operating normally</li>
<li><strong>89-70</strong>: Warning - Degraded performance, investigation needed</li>
<li><strong>69-0</strong>: Critical - System in distress, immediate action required</li>
</ul>
<h3 id="alert-structure"><a class="header" href="#alert-structure">Alert Structure</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct Alert {
    pub severity: AlertSeverity,
    pub message: String,
    pub timestamp: String,
    pub metric_name: String,
    pub current_value: f64,
    pub threshold: f64,
}

pub enum AlertSeverity {
    Info,
    Warning,
    Critical,
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="alert-thresholds"><a class="header" href="#alert-thresholds">Alert Thresholds</a></h2>
<h3 id="configuration-3"><a class="header" href="#configuration-3">Configuration</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct AlertThresholds {
    pub max_error_rate_percent: f64,
    pub min_throughput_mbps: f64,
    pub max_processing_duration_seconds: f64,
    pub max_memory_usage_mb: f64,
}

impl Default for AlertThresholds {
    fn default() -&gt; Self {
        Self {
            max_error_rate_percent: 5.0,
            min_throughput_mbps: 1.0,
            max_processing_duration_seconds: 300.0,
            max_memory_usage_mb: 1024.0,
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="alert-generation"><a class="header" href="#alert-generation">Alert Generation</a></h3>
<p>Alerts are generated when thresholds are violated:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn check_alerts(&amp;self, tracker: &amp;PerformanceTracker) {
    // High error rate alert
    if tracker.error_rate_percent &gt; self.alert_thresholds.max_error_rate_percent {
        warn!(
            "🚨 Alert: High error rate {:.1}% (threshold: {:.1}%)",
            tracker.error_rate_percent,
            self.alert_thresholds.max_error_rate_percent
        );
    }

    // Low throughput alert
    if tracker.average_throughput_mbps &lt; self.alert_thresholds.min_throughput_mbps {
        warn!(
            "🚨 Alert: Low throughput {:.2} MB/s (threshold: {:.2} MB/s)",
            tracker.average_throughput_mbps,
            self.alert_thresholds.min_throughput_mbps
        );
    }

    // High concurrent operations alert
    if tracker.active_operations &gt; 10 {
        warn!("🚨 Alert: High concurrent operations: {}", tracker.active_operations);
    }
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="usage-patterns"><a class="header" href="#usage-patterns">Usage Patterns</a></h2>
<h3 id="basic-operation-tracking"><a class="header" href="#basic-operation-tracking">Basic Operation Tracking</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Start operation tracking
let tracker = observability_service
    .start_operation("file_processing")
    .await;

// Do work
let result = process_file(&amp;input_path).await?;

// Complete tracking with success/failure
tracker.complete(true, result.bytes_processed).await;
<span class="boring">}</span></code></pre></pre>
<h3 id="automatic-tracking-with-drop-safety"><a class="header" href="#automatic-tracking-with-drop-safety">Automatic Tracking with Drop Safety</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn process_pipeline(id: &amp;PipelineId) -&gt; Result&lt;()&gt; {
    // Tracker automatically handles failure if function panics or returns Err
    let tracker = observability_service
        .start_operation("pipeline_execution")
        .await;

    // If this fails, tracker is dropped and marks operation as failed
    let result = execute_stages(id).await?;

    // Explicit success
    tracker.complete(true, result.bytes_processed).await;
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h3 id="recording-pipeline-metrics"><a class="header" href="#recording-pipeline-metrics">Recording Pipeline Metrics</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// After pipeline completion
let metrics = pipeline.processing_metrics();

// Record to both Prometheus and performance tracker
observability_service
    .record_processing_metrics(&amp;metrics)
    .await;

// This automatically:
// - Updates Prometheus counters/gauges/histograms
// - Updates PerformanceTracker state
// - Checks alert thresholds
// - Logs completion with metrics
<span class="boring">}</span></code></pre></pre>
<h3 id="health-check-endpoint"><a class="header" href="#health-check-endpoint">Health Check Endpoint</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn health_check() -&gt; Result&lt;SystemHealth&gt; {
    let health = observability_service.get_system_health().await;

    match health.status {
        HealthStatus::Healthy =&gt; {
            info!("System health: HEALTHY (score: {:.1})", health.score);
        }
        HealthStatus::Warning =&gt; {
            warn!(
                "System health: WARNING (score: {:.1}, {} alerts)",
                health.score,
                health.alerts.len()
            );
        }
        HealthStatus::Critical =&gt; {
            error!(
                "System health: CRITICAL (score: {:.1}, {} alerts)",
                health.score,
                health.alerts.len()
            );
        }
        HealthStatus::Unknown =&gt; {
            warn!("System health: UNKNOWN");
        }
    }

    Ok(health)
}
<span class="boring">}</span></code></pre></pre>
<h3 id="performance-summary"><a class="header" href="#performance-summary">Performance Summary</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Get human-readable performance summary
let summary = observability_service
    .get_performance_summary()
    .await;

println!("{}", summary);
<span class="boring">}</span></code></pre></pre>
<p><strong>Output</strong>:</p>
<pre><code class="language-text">📊 Performance Summary:
Active Operations: 3
Total Operations: 1247
Average Throughput: 45.67 MB/s
Peak Throughput: 89.23 MB/s
Error Rate: 2.1%
System Health: 88.5/100 (Warning)
Alerts: 1
</code></pre>
<hr />
<h2 id="integration-with-external-systems"><a class="header" href="#integration-with-external-systems">Integration with External Systems</a></h2>
<h3 id="prometheus-integration"><a class="header" href="#prometheus-integration">Prometheus Integration</a></h3>
<p>The system exposes metrics via HTTP endpoint:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// HTTP /metrics endpoint
use axum::{routing::get, Router};

let app = Router::new()
    .route("/metrics", get(metrics_handler));

async fn metrics_handler() -&gt; String {
    metrics_service.get_metrics()
        .unwrap_or_else(|_| "# Error generating metrics\n".to_string())
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Prometheus Configuration</strong>:</p>
<pre><code class="language-yaml">scrape_configs:
  - job_name: 'pipeline'
    static_configs:
      - targets: ['localhost:9090']
    scrape_interval: 15s
    scrape_timeout: 10s
</code></pre>
<h3 id="grafana-dashboards"><a class="header" href="#grafana-dashboards">Grafana Dashboards</a></h3>
<p>Create dashboards to visualize:</p>
<ul>
<li><strong>Pipeline Throughput</strong>: Line graph of MB/s over time</li>
<li><strong>Active Operations</strong>: Gauge of current active count</li>
<li><strong>Error Rate</strong>: Line graph of error percentage</li>
<li><strong>Processing Duration</strong>: Histogram of completion times</li>
<li><strong>System Health</strong>: Gauge with color thresholds</li>
</ul>
<p><strong>Example PromQL Queries</strong>:</p>
<pre><code class="language-promql"># Average throughput over 5 minutes
rate(pipeline_bytes_processed_total[5m]) / 1024 / 1024

# Error rate percentage
100 * (
  rate(pipeline_errors_total[5m]) /
  rate(pipeline_processed_total[5m])
)

# P99 processing duration
histogram_quantile(0.99, pipeline_processing_duration_seconds_bucket)
</code></pre>
<h3 id="log-aggregation"><a class="header" href="#log-aggregation">Log Aggregation</a></h3>
<p>Send logs to external systems:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tracing_subscriber::{fmt, layer::SubscriberExt, EnvFilter, Registry};
use tracing_appender::{non_blocking, rolling};

// JSON logs for shipping to ELK/Splunk
let file_appender = rolling::daily("./logs", "pipeline.json");
let (non_blocking_appender, _guard) = non_blocking(file_appender);

let file_layer = fmt::layer()
    .with_writer(non_blocking_appender)
    .json()
    .with_target(true)
    .with_thread_ids(true);

let subscriber = Registry::default()
    .with(EnvFilter::new("info"))
    .with(file_layer);

tracing::subscriber::set_global_default(subscriber)?;
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="performance-considerations"><a class="header" href="#performance-considerations">Performance Considerations</a></h2>
<h3 id="low-overhead-design"><a class="header" href="#low-overhead-design">Low Overhead Design</a></h3>
<p><strong>Atomic Operations</strong>: Metrics use atomic types to avoid locks:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct MetricsService {
    pipelines_processed: Arc&lt;AtomicU64&gt;,
    bytes_processed: Arc&lt;AtomicU64&gt;,
    // ...
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Async RwLock</strong>: PerformanceTracker uses async RwLock for concurrent reads:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>performance_tracker: Arc&lt;RwLock&lt;PerformanceTracker&gt;&gt;
<span class="boring">}</span></code></pre></pre>
<p><strong>Lazy Evaluation</strong>: Expensive calculations only performed when health is queried</p>
<p><strong>Compile-Time Filtering</strong>: Debug/trace logs have zero overhead in release builds</p>
<h3 id="benchmark-results"><a class="header" href="#benchmark-results">Benchmark Results</a></h3>
<p>Observability overhead on Intel i7-10700K @ 3.8 GHz:</p>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Time</th><th>Overhead</th></tr></thead><tbody>
<tr><td><code>start_operation()</code></td><td>~500 ns</td><td>Negligible</td></tr>
<tr><td><code>complete_operation()</code></td><td>~1.2 μs</td><td>Minimal</td></tr>
<tr><td><code>record_processing_metrics()</code></td><td>~2.5 μs</td><td>Low</td></tr>
<tr><td><code>get_system_health()</code></td><td>~8 μs</td><td>Moderate (infrequent)</td></tr>
<tr><td><code>info!()</code> log</td><td>~80 ns</td><td>Negligible</td></tr>
<tr><td><code>debug!()</code> log (disabled)</td><td>~0 ns</td><td>Zero</td></tr>
</tbody></table>
</div>
<p><strong>Total overhead</strong>: &lt; 0.1% of pipeline processing time</p>
<hr />
<h2 id="best-practices-5"><a class="header" href="#best-practices-5">Best Practices</a></h2>
<h3 id="-do"><a class="header" href="#-do">✅ DO</a></h3>
<p><strong>Track all significant operations</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let tracker = observability.start_operation("file_compression").await;
let result = compress_file(&amp;path).await?;
tracker.complete(true, result.compressed_size).await;
<span class="boring">}</span></code></pre></pre>
<p><strong>Use structured logging</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>info!(
    pipeline_id = %id,
    bytes = total_bytes,
    duration_ms = elapsed.as_millis(),
    "Pipeline completed"
);
<span class="boring">}</span></code></pre></pre>
<p><strong>Record domain metrics</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>observability.record_processing_metrics(&amp;pipeline.metrics()).await;
<span class="boring">}</span></code></pre></pre>
<p><strong>Check health regularly</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// In health check endpoint
let health = observability.get_system_health().await;
<span class="boring">}</span></code></pre></pre>
<p><strong>Configure thresholds appropriately</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let observability = ObservabilityService::new_with_config(metrics_service).await;
<span class="boring">}</span></code></pre></pre>
<h3 id="-dont"><a class="header" href="#-dont">❌ DON'T</a></h3>
<p><strong>Don't track trivial operations</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// BAD: Too fine-grained
let tracker = observability.start_operation("allocate_vec").await;
let vec = Vec::with_capacity(100);
tracker.complete(true, 0).await; // Overhead &gt; value
<span class="boring">}</span></code></pre></pre>
<p><strong>Don't log in hot loops without rate limiting</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// BAD: Excessive logging
for chunk in chunks {
    debug!("Processing chunk {}", chunk.id); // Called millions of times!
}

// GOOD: Log summary
debug!(chunk_count = chunks.len(), "Processing chunks");
info!(chunks_processed = chunks.len(), "Chunk processing complete");
<span class="boring">}</span></code></pre></pre>
<p><strong>Don't forget to complete trackers</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// BAD: Leaks active operation count
let tracker = observability.start_operation("process").await;
process().await?;
// Forgot to call tracker.complete()!

// GOOD: Explicit completion
let tracker = observability.start_operation("process").await;
let result = process().await?;
tracker.complete(true, result.bytes).await;
<span class="boring">}</span></code></pre></pre>
<p><strong>Don't block on observability operations</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// BAD: Blocking in async context
tokio::task::block_in_place(|| {
    observability.get_system_health().await // Won't compile anyway!
});

// GOOD: Await directly
let health = observability.get_system_health().await;
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="testing-strategies-1"><a class="header" href="#testing-strategies-1">Testing Strategies</a></h2>
<h3 id="unit-testing-observabilityservice"><a class="header" href="#unit-testing-observabilityservice">Unit Testing ObservabilityService</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_operation_tracking() {
    let metrics_service = Arc::new(MetricsService::new().unwrap());
    let observability = ObservabilityService::new(metrics_service);

    // Start operation
    let tracker = observability.start_operation("test").await;

    // Check active count increased
    let health = observability.get_system_health().await;
    assert_eq!(health.active_operations, 1);

    // Complete operation
    tracker.complete(true, 1000).await;

    // Check active count decreased
    let health = observability.get_system_health().await;
    assert_eq!(health.active_operations, 0);
}
<span class="boring">}</span></code></pre></pre>
<h3 id="testing-alert-generation"><a class="header" href="#testing-alert-generation">Testing Alert Generation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_high_error_rate_alert() {
    let metrics_service = Arc::new(MetricsService::new().unwrap());
    let mut observability = ObservabilityService::new(metrics_service);

    // Set low threshold
    observability.alert_thresholds.max_error_rate_percent = 1.0;

    // Simulate high error rate
    for _ in 0..10 {
        let tracker = observability.start_operation("test").await;
        tracker.complete(false, 0).await; // All failures
    }

    // Check health has alerts
    let health = observability.get_system_health().await;
    assert!(!health.alerts.is_empty());
    assert_eq!(health.status, HealthStatus::Critical);
}
<span class="boring">}</span></code></pre></pre>
<h3 id="integration-testing"><a class="header" href="#integration-testing">Integration Testing</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test]
async fn test_end_to_end_observability() {
    // Setup
    let metrics_service = Arc::new(MetricsService::new().unwrap());
    let observability = Arc::new(ObservabilityService::new(metrics_service.clone()));

    // Run pipeline with tracking
    let tracker = observability.start_operation("pipeline").await;
    let result = run_test_pipeline().await.unwrap();
    tracker.complete(true, result.bytes_processed).await;

    // Verify metrics recorded
    let metrics_output = metrics_service.get_metrics().unwrap();
    assert!(metrics_output.contains("pipeline_processed_total"));

    // Verify health is good
    let health = observability.get_system_health().await;
    assert_eq!(health.status, HealthStatus::Healthy);
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="common-issues-and-solutions"><a class="header" href="#common-issues-and-solutions">Common Issues and Solutions</a></h2>
<h3 id="issue-active-operations-count-stuck"><a class="header" href="#issue-active-operations-count-stuck">Issue: Active operations count stuck</a></h3>
<p><strong>Symptom</strong>: <code>active_operations</code> never decreases</p>
<p><strong>Cause</strong>: <code>OperationTracker</code> not completed or dropped</p>
<p><strong>Solution</strong>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Ensure tracker is completed in all code paths
let tracker = observability.start_operation("op").await;
let result = match dangerous_operation().await {
    Ok(r) =&gt; {
        tracker.complete(true, r.bytes).await;
        Ok(r)
    }
    Err(e) =&gt; {
        tracker.complete(false, 0).await;
        Err(e)
    }
};
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-health-score-always-100"><a class="header" href="#issue-health-score-always-100">Issue: Health score always 100</a></h3>
<p><strong>Symptom</strong>: Health never degrades despite errors</p>
<p><strong>Cause</strong>: Metrics not being recorded</p>
<p><strong>Solution</strong>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Always record processing metrics
observability.record_processing_metrics(&amp;metrics).await;
<span class="boring">}</span></code></pre></pre>
<h3 id="issue-alerts-not-firing"><a class="header" href="#issue-alerts-not-firing">Issue: Alerts not firing</a></h3>
<p><strong>Symptom</strong>: Thresholds violated but no alerts logged</p>
<p><strong>Cause</strong>: Log level filtering out WARN messages</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># Enable WARN level
export RUST_LOG=warn

# Or per-module
export RUST_LOG=pipeline::infrastructure::logging=warn
</code></pre>
<hr />
<h2 id="next-steps-15"><a class="header" href="#next-steps-15">Next Steps</a></h2>
<ul>
<li><strong><a href="implementation/metrics.html">Metrics Collection</a></strong>: Deep dive into Prometheus metrics</li>
<li><strong><a href="implementation/logging.html">Logging Implementation</a></strong>: Structured logging with tracing</li>
<li><strong><a href="implementation/configuration.html">Configuration</a></strong>: Configure alert thresholds and settings</li>
<li><strong><a href="implementation/../testing/integration-tests.html">Testing</a></strong>: Integration testing strategies</li>
</ul>
<hr />
<h2 id="references"><a class="header" href="#references">References</a></h2>
<ul>
<li>Source: <code>pipeline/src/infrastructure/logging/observability_service.rs</code> (lines 1-716)</li>
<li><a href="https://prometheus.io/docs/">Prometheus Documentation</a></li>
<li><a href="https://grafana.com/docs/grafana/latest/dashboards/">Grafana Dashboards</a></li>
<li><a href="https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/ch04.html">The Three Pillars of Observability</a></li>
<li><a href="https://sre.google/sre-book/monitoring-distributed-systems/">Site Reliability Engineering</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="metrics-collection"><a class="header" href="#metrics-collection">Metrics Collection</a></h1>
<p><strong>Version:</strong> 1.0
<strong>Date:</strong> 2025-01-04
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner, Claude Code
<strong>Status:</strong> Active</p>
<h2 id="overview-10"><a class="header" href="#overview-10">Overview</a></h2>
<p>The pipeline system implements comprehensive metrics collection for monitoring, observability, and performance analysis. Metrics are collected in real-time, exported in Prometheus format, and can be visualized using Grafana dashboards.</p>
<p><strong>Key Features:</strong></p>
<ul>
<li><strong>Prometheus Integration</strong>: Native Prometheus metrics export</li>
<li><strong>Real-Time Collection</strong>: Low-overhead metric updates</li>
<li><strong>Comprehensive Coverage</strong>: Performance, system, and business metrics</li>
<li><strong>Dimensional Data</strong>: Labels for multi-dimensional analysis</li>
<li><strong>Thread-Safe</strong>: Safe concurrent metric updates</li>
</ul>
<h2 id="metrics-architecture"><a class="header" href="#metrics-architecture">Metrics Architecture</a></h2>
<h3 id="two-level-metrics-system"><a class="header" href="#two-level-metrics-system">Two-Level Metrics System</a></h3>
<p>The system uses a two-level metrics architecture:</p>
<pre><code class="language-text">┌─────────────────────────────────────────────┐
│         Application Layer                   │
│  - ProcessingMetrics (domain entity)        │
│  - Per-pipeline performance tracking        │
│  - Business metrics and analytics           │
└─────────────────┬───────────────────────────┘
                  │
                  ↓
┌─────────────────────────────────────────────┐
│       Infrastructure Layer                  │
│  - MetricsService (Prometheus)              │
│  - System-wide aggregation                  │
│  - HTTP export endpoint                     │
└─────────────────────────────────────────────┘
</code></pre>
<p><strong>Domain Metrics (ProcessingMetrics):</strong></p>
<ul>
<li>Attached to processing context</li>
<li>Track individual pipeline execution</li>
<li>Support detailed analytics</li>
<li>Persist to database</li>
</ul>
<p><strong>Infrastructure Metrics (MetricsService):</strong></p>
<ul>
<li>System-wide aggregation</li>
<li>Prometheus counters, gauges, histograms</li>
<li>HTTP /metrics endpoint</li>
<li>Real-time monitoring</li>
</ul>
<h2 id="domain-metrics"><a class="header" href="#domain-metrics">Domain Metrics</a></h2>
<h3 id="processingmetrics-entity"><a class="header" href="#processingmetrics-entity">ProcessingMetrics Entity</a></h3>
<p>Tracks performance data for individual pipeline executions:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline_domain::entities::ProcessingMetrics;
use std::time::{Duration, Instant};
use std::collections::HashMap;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProcessingMetrics {
    // Progress tracking
    bytes_processed: u64,
    bytes_total: u64,
    chunks_processed: u64,
    chunks_total: u64,

    // Timing (high-resolution internal, RFC3339 for export)
    #[serde(skip)]
    start_time: Option&lt;Instant&gt;,
    #[serde(skip)]
    end_time: Option&lt;Instant&gt;,
    start_time_rfc3339: Option&lt;String&gt;,
    end_time_rfc3339: Option&lt;String&gt;,
    processing_duration: Option&lt;Duration&gt;,

    // Performance metrics
    throughput_bytes_per_second: f64,
    compression_ratio: Option&lt;f64&gt;,

    // Error tracking
    error_count: u64,
    warning_count: u64,

    // File information
    input_file_size_bytes: u64,
    output_file_size_bytes: u64,
    input_file_checksum: Option&lt;String&gt;,
    output_file_checksum: Option&lt;String&gt;,

    // Per-stage metrics
    stage_metrics: HashMap&lt;String, StageMetrics&gt;,
}
<span class="boring">}</span></code></pre></pre>
<h3 id="creating-and-using-processingmetrics"><a class="header" href="#creating-and-using-processingmetrics">Creating and Using ProcessingMetrics</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl ProcessingMetrics {
    /// Create new metrics for pipeline execution
    pub fn new(total_bytes: u64, total_chunks: u64) -&gt; Self {
        Self {
            bytes_processed: 0,
            bytes_total: total_bytes,
            chunks_processed: 0,
            chunks_total: total_chunks,
            start_time: Some(Instant::now()),
            start_time_rfc3339: Some(Utc::now().to_rfc3339()),
            end_time: None,
            end_time_rfc3339: None,
            processing_duration: None,
            throughput_bytes_per_second: 0.0,
            compression_ratio: None,
            error_count: 0,
            warning_count: 0,
            input_file_size_bytes: total_bytes,
            output_file_size_bytes: 0,
            input_file_checksum: None,
            output_file_checksum: None,
            stage_metrics: HashMap::new(),
        }
    }

    /// Update progress
    pub fn update_progress(&amp;mut self, bytes: u64, chunks: u64) {
        self.bytes_processed += bytes;
        self.chunks_processed += chunks;
        self.update_throughput();
    }

    /// Calculate throughput
    fn update_throughput(&amp;mut self) {
        if let Some(start) = self.start_time {
            let elapsed = start.elapsed().as_secs_f64();
            if elapsed &gt; 0.0 {
                self.throughput_bytes_per_second =
                    self.bytes_processed as f64 / elapsed;
            }
        }
    }

    /// Complete processing
    pub fn complete(&amp;mut self) {
        self.end_time = Some(Instant::now());
        self.end_time_rfc3339 = Some(Utc::now().to_rfc3339());

        if let (Some(start), Some(end)) = (self.start_time, self.end_time) {
            self.processing_duration = Some(end - start);
        }

        self.update_throughput();
        self.calculate_compression_ratio();
    }

    /// Calculate compression ratio
    fn calculate_compression_ratio(&amp;mut self) {
        if self.output_file_size_bytes &gt; 0 &amp;&amp; self.input_file_size_bytes &gt; 0 {
            self.compression_ratio = Some(
                self.output_file_size_bytes as f64 /
                self.input_file_size_bytes as f64
            );
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="stagemetrics"><a class="header" href="#stagemetrics">StageMetrics</a></h3>
<p>Track performance for individual pipeline stages:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StageMetrics {
    /// Stage name
    stage_name: String,

    /// Bytes processed by this stage
    bytes_processed: u64,

    /// Processing time for this stage
    #[serde(skip)]
    processing_time: Option&lt;Duration&gt;,

    /// Throughput (bytes per second)
    throughput_bps: f64,

    /// Error count for this stage
    error_count: u64,

    /// Memory usage (optional)
    memory_usage_bytes: Option&lt;u64&gt;,
}

impl ProcessingMetrics {
    /// Record stage metrics
    pub fn record_stage_metrics(
        &amp;mut self,
        stage_name: String,
        bytes: u64,
        duration: Duration,
    ) {
        let throughput = if duration.as_secs_f64() &gt; 0.0 {
            bytes as f64 / duration.as_secs_f64()
        } else {
            0.0
        };

        let stage_metrics = StageMetrics {
            stage_name: stage_name.clone(),
            bytes_processed: bytes,
            processing_time: Some(duration),
            throughput_bps: throughput,
            error_count: 0,
            memory_usage_bytes: None,
        };

        self.stage_metrics.insert(stage_name, stage_metrics);
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="prometheus-metrics"><a class="header" href="#prometheus-metrics">Prometheus Metrics</a></h2>
<h3 id="metricsservice"><a class="header" href="#metricsservice">MetricsService</a></h3>
<p>Infrastructure service for Prometheus metrics:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use prometheus::{
    IntCounter, IntGauge, Gauge, Histogram,
    HistogramOpts, Opts, Registry
};
use std::sync::Arc;

#[derive(Clone)]
pub struct MetricsService {
    registry: Arc&lt;Registry&gt;,

    // Pipeline execution metrics
    pipelines_processed_total: IntCounter,
    pipeline_processing_duration: Histogram,
    pipeline_bytes_processed_total: IntCounter,
    pipeline_chunks_processed_total: IntCounter,
    pipeline_errors_total: IntCounter,
    pipeline_warnings_total: IntCounter,

    // Performance metrics
    throughput_mbps: Gauge,
    compression_ratio: Gauge,
    active_pipelines: IntGauge,

    // System metrics
    memory_usage_bytes: IntGauge,
    cpu_utilization_percent: Gauge,
}

impl MetricsService {
    pub fn new() -&gt; Result&lt;Self, PipelineError&gt; {
        let registry = Arc::new(Registry::new());

        // Create counters
        let pipelines_processed_total = IntCounter::new(
            "pipeline_processed_total",
            "Total number of pipelines processed"
        )?;

        let pipeline_bytes_processed_total = IntCounter::new(
            "pipeline_bytes_processed_total",
            "Total bytes processed by pipelines"
        )?;

        let pipeline_errors_total = IntCounter::new(
            "pipeline_errors_total",
            "Total number of processing errors"
        )?;

        // Create histograms
        let pipeline_processing_duration = Histogram::with_opts(
            HistogramOpts::new(
                "pipeline_processing_duration_seconds",
                "Pipeline processing duration in seconds"
            )
            .buckets(vec![0.1, 0.5, 1.0, 5.0, 10.0, 30.0, 60.0])
        )?;

        // Create gauges
        let throughput_mbps = Gauge::new(
            "pipeline_throughput_mbps",
            "Current processing throughput in MB/s"
        )?;

        let compression_ratio = Gauge::new(
            "pipeline_compression_ratio",
            "Current compression ratio"
        )?;

        let active_pipelines = IntGauge::new(
            "pipeline_active_count",
            "Number of currently active pipelines"
        )?;

        // Register all metrics
        registry.register(Box::new(pipelines_processed_total.clone()))?;
        registry.register(Box::new(pipeline_bytes_processed_total.clone()))?;
        registry.register(Box::new(pipeline_errors_total.clone()))?;
        registry.register(Box::new(pipeline_processing_duration.clone()))?;
        registry.register(Box::new(throughput_mbps.clone()))?;
        registry.register(Box::new(compression_ratio.clone()))?;
        registry.register(Box::new(active_pipelines.clone()))?;

        Ok(Self {
            registry,
            pipelines_processed_total,
            pipeline_processing_duration,
            pipeline_bytes_processed_total,
            pipeline_chunks_processed_total: /* ... */,
            pipeline_errors_total,
            pipeline_warnings_total: /* ... */,
            throughput_mbps,
            compression_ratio,
            active_pipelines,
            memory_usage_bytes: /* ... */,
            cpu_utilization_percent: /* ... */,
        })
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="recording-metrics"><a class="header" href="#recording-metrics">Recording Metrics</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl MetricsService {
    /// Record pipeline completion
    pub fn record_pipeline_completion(
        &amp;self,
        metrics: &amp;ProcessingMetrics
    ) -&gt; Result&lt;(), PipelineError&gt; {
        // Increment counters
        self.pipelines_processed_total.inc();
        self.pipeline_bytes_processed_total
            .inc_by(metrics.bytes_processed());
        self.pipeline_chunks_processed_total
            .inc_by(metrics.chunks_processed());

        // Record duration
        if let Some(duration) = metrics.processing_duration() {
            self.pipeline_processing_duration
                .observe(duration.as_secs_f64());
        }

        // Update gauges
        self.throughput_mbps.set(
            metrics.throughput_bytes_per_second() / 1_000_000.0
        );

        if let Some(ratio) = metrics.compression_ratio() {
            self.compression_ratio.set(ratio);
        }

        // Record errors
        if metrics.error_count() &gt; 0 {
            self.pipeline_errors_total
                .inc_by(metrics.error_count());
        }

        Ok(())
    }

    /// Record pipeline start
    pub fn record_pipeline_start(&amp;self) {
        self.active_pipelines.inc();
    }

    /// Record pipeline end
    pub fn record_pipeline_end(&amp;self) {
        self.active_pipelines.dec();
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="available-metrics"><a class="header" href="#available-metrics">Available Metrics</a></h2>
<h3 id="counter-metrics"><a class="header" href="#counter-metrics">Counter Metrics</a></h3>
<p>Monotonically increasing values:</p>
<div class="table-wrapper"><table><thead><tr><th>Metric Name</th><th>Type</th><th>Description</th></tr></thead><tbody>
<tr><td><code>pipeline_processed_total</code></td><td>Counter</td><td>Total pipelines processed</td></tr>
<tr><td><code>pipeline_bytes_processed_total</code></td><td>Counter</td><td>Total bytes processed</td></tr>
<tr><td><code>pipeline_chunks_processed_total</code></td><td>Counter</td><td>Total chunks processed</td></tr>
<tr><td><code>pipeline_errors_total</code></td><td>Counter</td><td>Total processing errors</td></tr>
<tr><td><code>pipeline_warnings_total</code></td><td>Counter</td><td>Total warnings</td></tr>
</tbody></table>
</div>
<h3 id="gauge-metrics"><a class="header" href="#gauge-metrics">Gauge Metrics</a></h3>
<p>Values that can increase or decrease:</p>
<div class="table-wrapper"><table><thead><tr><th>Metric Name</th><th>Type</th><th>Description</th></tr></thead><tbody>
<tr><td><code>pipeline_active_count</code></td><td>Gauge</td><td>Currently active pipelines</td></tr>
<tr><td><code>pipeline_throughput_mbps</code></td><td>Gauge</td><td>Current throughput (MB/s)</td></tr>
<tr><td><code>pipeline_compression_ratio</code></td><td>Gauge</td><td>Current compression ratio</td></tr>
<tr><td><code>pipeline_memory_usage_bytes</code></td><td>Gauge</td><td>Memory usage in bytes</td></tr>
<tr><td><code>pipeline_cpu_utilization_percent</code></td><td>Gauge</td><td>CPU utilization percentage</td></tr>
</tbody></table>
</div>
<h3 id="histogram-metrics"><a class="header" href="#histogram-metrics">Histogram Metrics</a></h3>
<p>Distribution of values with buckets:</p>
<div class="table-wrapper"><table><thead><tr><th>Metric Name</th><th>Type</th><th>Buckets</th><th>Description</th></tr></thead><tbody>
<tr><td><code>pipeline_processing_duration_seconds</code></td><td>Histogram</td><td>0.1, 0.5, 1, 5, 10, 30, 60</td><td>Processing duration</td></tr>
<tr><td><code>pipeline_chunk_size_bytes</code></td><td>Histogram</td><td>1K, 10K, 100K, 1M, 10M</td><td>Chunk size distribution</td></tr>
<tr><td><code>pipeline_stage_duration_seconds</code></td><td>Histogram</td><td>0.01, 0.1, 0.5, 1, 5</td><td>Stage processing time</td></tr>
</tbody></table>
</div>
<h2 id="metrics-export"><a class="header" href="#metrics-export">Metrics Export</a></h2>
<h3 id="http-endpoint"><a class="header" href="#http-endpoint">HTTP Endpoint</a></h3>
<p>Export metrics via HTTP for Prometheus scraping:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use warp::Filter;

pub fn metrics_endpoint(
    metrics_service: Arc&lt;MetricsService&gt;
) -&gt; impl Filter&lt;Extract = impl warp::Reply, Error = warp::Rejection&gt; + Clone {
    warp::path!("metrics")
        .and(warp::get())
        .map(move || {
            let encoder = prometheus::TextEncoder::new();
            let metric_families = metrics_service.registry.gather();
            let mut buffer = Vec::new();

            encoder.encode(&amp;metric_families, &amp;mut buffer)
                .unwrap_or_else(|e| {
                    eprintln!("Failed to encode metrics: {}", e);
                });

            warp::reply::with_header(
                buffer,
                "Content-Type",
                encoder.format_type()
            )
        })
}
<span class="boring">}</span></code></pre></pre>
<h3 id="prometheus-configuration"><a class="header" href="#prometheus-configuration">Prometheus Configuration</a></h3>
<p>Configure Prometheus to scrape metrics:</p>
<pre><code class="language-yaml"># prometheus.yml
scrape_configs:
  - job_name: 'pipeline'
    scrape_interval: 15s
    static_configs:
      - targets: ['localhost:9090']
    metrics_path: '/metrics'
</code></pre>
<h3 id="example-metrics-output"><a class="header" href="#example-metrics-output">Example Metrics Output</a></h3>
<pre><code class="language-text"># HELP pipeline_processed_total Total number of pipelines processed
# TYPE pipeline_processed_total counter
pipeline_processed_total 1234

# HELP pipeline_bytes_processed_total Total bytes processed
# TYPE pipeline_bytes_processed_total counter
pipeline_bytes_processed_total 1073741824

# HELP pipeline_processing_duration_seconds Pipeline processing duration
# TYPE pipeline_processing_duration_seconds histogram
pipeline_processing_duration_seconds_bucket{le="0.1"} 45
pipeline_processing_duration_seconds_bucket{le="0.5"} 120
pipeline_processing_duration_seconds_bucket{le="1.0"} 280
pipeline_processing_duration_seconds_bucket{le="5.0"} 450
pipeline_processing_duration_seconds_bucket{le="10.0"} 500
pipeline_processing_duration_seconds_bucket{le="+Inf"} 520
pipeline_processing_duration_seconds_sum 2340.5
pipeline_processing_duration_seconds_count 520

# HELP pipeline_throughput_mbps Current processing throughput
# TYPE pipeline_throughput_mbps gauge
pipeline_throughput_mbps 125.7

# HELP pipeline_compression_ratio Current compression ratio
# TYPE pipeline_compression_ratio gauge
pipeline_compression_ratio 0.35
</code></pre>
<h2 id="integration-with-processing"><a class="header" href="#integration-with-processing">Integration with Processing</a></h2>
<h3 id="automatic-metric-collection"><a class="header" href="#automatic-metric-collection">Automatic Metric Collection</a></h3>
<p>Metrics are automatically collected during processing:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pipeline::MetricsService;

async fn process_file_with_metrics(
    input_path: &amp;str,
    output_path: &amp;str,
    metrics_service: Arc&lt;MetricsService&gt;,
) -&gt; Result&lt;(), PipelineError&gt; {
    // Create domain metrics
    let mut metrics = ProcessingMetrics::new(
        file_size,
        chunk_count
    );

    // Record start
    metrics_service.record_pipeline_start();

    // Process file
    for chunk in chunks {
        let start = Instant::now();

        let processed = process_chunk(chunk)?;

        metrics.update_progress(
            processed.len() as u64,
            1
        );

        metrics.record_stage_metrics(
            "compression".to_string(),
            processed.len() as u64,
            start.elapsed()
        );
    }

    // Complete metrics
    metrics.complete();

    // Export to Prometheus
    metrics_service.record_pipeline_completion(&amp;metrics)?;
    metrics_service.record_pipeline_end();

    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h3 id="observer-pattern-1"><a class="header" href="#observer-pattern-1">Observer Pattern</a></h3>
<p>Use observer pattern for automatic metric updates:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct MetricsObserver {
    metrics_service: Arc&lt;MetricsService&gt;,
}

impl MetricsObserver {
    pub fn observe_processing(
        &amp;self,
        event: ProcessingEvent
    ) {
        match event {
            ProcessingEvent::PipelineStarted =&gt; {
                self.metrics_service.record_pipeline_start();
            }
            ProcessingEvent::ChunkProcessed { bytes, duration } =&gt; {
                self.metrics_service.pipeline_bytes_processed_total
                    .inc_by(bytes);
                self.metrics_service.pipeline_processing_duration
                    .observe(duration.as_secs_f64());
            }
            ProcessingEvent::PipelineCompleted { metrics } =&gt; {
                self.metrics_service
                    .record_pipeline_completion(&amp;metrics)
                    .ok();
                self.metrics_service.record_pipeline_end();
            }
            ProcessingEvent::Error =&gt; {
                self.metrics_service.pipeline_errors_total.inc();
            }
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="visualization-with-grafana"><a class="header" href="#visualization-with-grafana">Visualization with Grafana</a></h2>
<h3 id="dashboard-configuration"><a class="header" href="#dashboard-configuration">Dashboard Configuration</a></h3>
<p>Create Grafana dashboard for visualization:</p>
<pre><code class="language-json">{
  "dashboard": {
    "title": "Pipeline Metrics",
    "panels": [
      {
        "title": "Throughput",
        "targets": [{
          "expr": "rate(pipeline_bytes_processed_total[5m]) / 1000000",
          "legendFormat": "Throughput (MB/s)"
        }]
      },
      {
        "title": "Processing Duration (P95)",
        "targets": [{
          "expr": "histogram_quantile(0.95, rate(pipeline_processing_duration_seconds_bucket[5m]))",
          "legendFormat": "P95 Duration"
        }]
      },
      {
        "title": "Active Pipelines",
        "targets": [{
          "expr": "pipeline_active_count",
          "legendFormat": "Active"
        }]
      },
      {
        "title": "Error Rate",
        "targets": [{
          "expr": "rate(pipeline_errors_total[5m])",
          "legendFormat": "Errors/sec"
        }]
      }
    ]
  }
}
</code></pre>
<h3 id="common-queries"><a class="header" href="#common-queries">Common Queries</a></h3>
<p><strong>Throughput over time:</strong></p>
<pre><code class="language-promql">rate(pipeline_bytes_processed_total[5m]) / 1000000
</code></pre>
<p><strong>Average processing duration:</strong></p>
<pre><code class="language-promql">rate(pipeline_processing_duration_seconds_sum[5m]) /
rate(pipeline_processing_duration_seconds_count[5m])
</code></pre>
<p><strong>P99 latency:</strong></p>
<pre><code class="language-promql">histogram_quantile(0.99,
  rate(pipeline_processing_duration_seconds_bucket[5m])
)
</code></pre>
<p><strong>Error rate:</strong></p>
<pre><code class="language-promql">rate(pipeline_errors_total[5m])
</code></pre>
<p><strong>Compression effectiveness:</strong></p>
<pre><code class="language-promql">avg(pipeline_compression_ratio)
</code></pre>
<h2 id="performance-considerations-1"><a class="header" href="#performance-considerations-1">Performance Considerations</a></h2>
<h3 id="low-overhead-updates"><a class="header" href="#low-overhead-updates">Low-Overhead Updates</a></h3>
<p>Metrics use atomic operations for minimal overhead:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Atomic increment
self.counter.inc();

// ❌ BAD: Locking for simple increment
let mut guard = self.counter.lock().unwrap();
*guard += 1;
<span class="boring">}</span></code></pre></pre>
<h3 id="batch-updates"><a class="header" href="#batch-updates">Batch Updates</a></h3>
<p>Batch metric updates when possible:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Batch update
self.pipeline_bytes_processed_total.inc_by(total_bytes);

// ❌ BAD: Multiple individual updates
for _ in 0..total_bytes {
    self.pipeline_bytes_processed_total.inc();
}
<span class="boring">}</span></code></pre></pre>
<h3 id="efficient-labels"><a class="header" href="#efficient-labels">Efficient Labels</a></h3>
<p>Use labels judiciously to avoid cardinality explosion:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Limited cardinality
let counter = register_int_counter_vec!(
    "pipeline_processed_total",
    "Total pipelines processed",
    &amp;["algorithm", "stage"]  // ~10 algorithms × ~5 stages = 50 series
)?;

// ❌ BAD: High cardinality
let counter = register_int_counter_vec!(
    "pipeline_processed_total",
    "Total pipelines processed",
    &amp;["pipeline_id", "user_id"]  // Could be millions of series!
)?;
<span class="boring">}</span></code></pre></pre>
<h2 id="alerting"><a class="header" href="#alerting">Alerting</a></h2>
<h3 id="alert-rules"><a class="header" href="#alert-rules">Alert Rules</a></h3>
<p>Define Prometheus alert rules:</p>
<pre><code class="language-yaml"># alerts.yml
groups:
  - name: pipeline_alerts
    rules:
      - alert: HighErrorRate
        expr: rate(pipeline_errors_total[5m]) &gt; 0.1
        for: 5m
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} errors/sec"

      - alert: LowThroughput
        expr: rate(pipeline_bytes_processed_total[5m]) &lt; 1000000
        for: 10m
        annotations:
          summary: "Low throughput detected"
          description: "Throughput is {{ $value }} bytes/sec"

      - alert: HighLatency
        expr: |
          histogram_quantile(0.95,
            rate(pipeline_processing_duration_seconds_bucket[5m])
          ) &gt; 60
        for: 5m
        annotations:
          summary: "High P95 latency"
          description: "P95 latency is {{ $value }}s"
</code></pre>
<h2 id="best-practices-6"><a class="header" href="#best-practices-6">Best Practices</a></h2>
<h3 id="metric-naming"><a class="header" href="#metric-naming">Metric Naming</a></h3>
<p>Follow Prometheus naming conventions:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Clear, consistent names
pipeline_bytes_processed_total      // Counter with _total suffix
pipeline_processing_duration_seconds // Time in base unit (seconds)
pipeline_active_count               // Gauge without suffix

// ❌ BAD: Inconsistent naming
processed_bytes                     // Missing namespace
duration_ms                        // Wrong unit (use seconds)
active                             // Too vague
<span class="boring">}</span></code></pre></pre>
<h3 id="unit-consistency"><a class="header" href="#unit-consistency">Unit Consistency</a></h3>
<p>Always use base units:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Base units
duration_seconds: f64              // Seconds, not milliseconds
size_bytes: u64                    // Bytes, not KB/MB
ratio: f64                         // Unitless ratio 0.0-1.0

// ❌ BAD: Non-standard units
duration_ms: u64
size_mb: f64
percentage: u8
<span class="boring">}</span></code></pre></pre>
<h3 id="documentation"><a class="header" href="#documentation">Documentation</a></h3>
<p>Document all metrics:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Well documented
let counter = IntCounter::with_opts(
    Opts::new(
        "pipeline_processed_total",
        "Total number of pipelines processed successfully. \
         Incremented on completion of each pipeline execution."
    )
)?;
<span class="boring">}</span></code></pre></pre>
<h2 id="next-steps-16"><a class="header" href="#next-steps-16">Next Steps</a></h2>
<p>Now that you understand metrics collection:</p>
<ul>
<li><a href="implementation/logging.html">Logging</a> - Structured logging implementation</li>
<li><a href="implementation/observability.html">Observability</a> - Complete observability strategy</li>
<li><a href="implementation/../advanced/performance.html">Performance</a> - Performance optimization</li>
<li><a href="implementation/../advanced/monitoring.html">Monitoring</a> - Production monitoring setup</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="logging-implementation"><a class="header" href="#logging-implementation">Logging Implementation</a></h1>
<p><strong>Version</strong>: 1.0
<strong>Date</strong>: 2025-10-04
<strong>License</strong>: BSD-3-Clause
<strong>Copyright</strong>: (c) 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors</strong>: Michael Gardner
<strong>Status</strong>: Active</p>
<hr />
<h2 id="overview-11"><a class="header" href="#overview-11">Overview</a></h2>
<p>The Optimized Adaptive Pipeline uses <strong>structured logging</strong> via the <a href="https://docs.rs/tracing">tracing</a> crate to provide comprehensive observability throughout the system. This chapter details the logging architecture, implementation patterns, and best practices.</p>
<h3 id="key-features"><a class="header" href="#key-features">Key Features</a></h3>
<ul>
<li><strong>Structured Logging</strong>: Rich, structured log events with contextual metadata</li>
<li><strong>Two-Phase Architecture</strong>: Separate bootstrap and application logging</li>
<li><strong>Hierarchical Levels</strong>: Traditional log levels (ERROR, WARN, INFO, DEBUG, TRACE)</li>
<li><strong>Targeted Filtering</strong>: Fine-grained control via log targets</li>
<li><strong>Integration</strong>: Seamless integration with ObservabilityService and metrics</li>
<li><strong>Performance</strong>: Low-overhead logging with compile-time filtering</li>
<li><strong>Testability</strong>: Trait-based abstractions for testing</li>
</ul>
<hr />
<h2 id="architecture-6"><a class="header" href="#architecture-6">Architecture</a></h2>
<h3 id="two-phase-logging-system"><a class="header" href="#two-phase-logging-system">Two-Phase Logging System</a></h3>
<p>The system employs a two-phase logging approach to handle different initialization stages:</p>
<pre><code class="language-text">┌─────────────────────────────────────────────────────────────┐
│                    Application Lifecycle                     │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  Phase 1: Bootstrap                Phase 2: Application     │
│  ┌───────────────────┐             ┌──────────────────────┐ │
│  │ BootstrapLogger   │────────────&gt;│ Tracing Subscriber   │ │
│  │ (Early init)      │             │ (Full featured)      │ │
│  ├───────────────────┤             ├──────────────────────┤ │
│  │ - Simple API      │             │ - Structured events  │ │
│  │ - No dependencies │             │ - Span tracking      │ │
│  │ - Testable        │             │ - Context propagation│ │
│  │ - Minimal overhead│             │ - External outputs   │ │
│  └───────────────────┘             └──────────────────────┘ │
│                                                              │
└─────────────────────────────────────────────────────────────┘
</code></pre>
<h4 id="phase-1-bootstrap-logger"><a class="header" href="#phase-1-bootstrap-logger">Phase 1: Bootstrap Logger</a></h4>
<p>Located in <code>bootstrap/src/logger.rs</code>, the bootstrap logger provides minimal logging during early initialization:</p>
<ul>
<li><strong>Minimal dependencies</strong>: No heavy tracing infrastructure</li>
<li><strong>Trait-based abstraction</strong>: <code>BootstrapLogger</code> trait for testability</li>
<li><strong>Simple API</strong>: Only 4 log levels (error, warn, info, debug)</li>
<li><strong>Early availability</strong>: Available before tracing subscriber initialization</li>
</ul>
<h4 id="phase-2-application-logging"><a class="header" href="#phase-2-application-logging">Phase 2: Application Logging</a></h4>
<p>Once the application is fully initialized, the full tracing infrastructure is used:</p>
<ul>
<li><strong>Rich structured logging</strong>: Fields, spans, and events</li>
<li><strong>Multiple subscribers</strong>: Console, file, JSON, external systems</li>
<li><strong>Performance tracking</strong>: Integration with ObservabilityService</li>
<li><strong>Distributed tracing</strong>: Context propagation for async operations</li>
</ul>
<hr />
<h2 id="log-levels"><a class="header" href="#log-levels">Log Levels</a></h2>
<p>The system uses five hierarchical log levels, from most to least severe:</p>
<div class="table-wrapper"><table><thead><tr><th>Level</th><th>Macro</th><th>Use Case</th><th>Example</th></tr></thead><tbody>
<tr><td><strong>ERROR</strong></td><td><code>error!()</code></td><td>Fatal errors, unrecoverable failures</td><td>Database connection failure, file corruption</td></tr>
<tr><td><strong>WARN</strong></td><td><code>warn!()</code></td><td>Non-fatal issues, degraded performance</td><td>High error rate alert, configuration warning</td></tr>
<tr><td><strong>INFO</strong></td><td><code>info!()</code></td><td>Normal operations, key milestones</td><td>Pipeline started, file processed successfully</td></tr>
<tr><td><strong>DEBUG</strong></td><td><code>debug!()</code></td><td>Detailed diagnostic information</td><td>Stage execution details, chunk processing</td></tr>
<tr><td><strong>TRACE</strong></td><td><code>trace!()</code></td><td>Very verbose debugging</td><td>Function entry/exit, detailed state dumps</td></tr>
</tbody></table>
</div>
<h3 id="level-guidelines"><a class="header" href="#level-guidelines">Level Guidelines</a></h3>
<p><strong>ERROR</strong>: Use sparingly for genuine failures that require attention</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>error!("Failed to connect to database: {}", err);
error!(pipeline_id = %id, "Pipeline execution failed: {}", err);
<span class="boring">}</span></code></pre></pre>
<p><strong>WARN</strong>: Use for concerning situations that don't prevent operation</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>warn!("High error rate: {:.1}% (threshold: {:.1}%)", rate, threshold);
warn!(stage = %name, "Stage processing slower than expected");
<span class="boring">}</span></code></pre></pre>
<p><strong>INFO</strong>: Use for important operational events</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>info!("Started pipeline processing: {}", pipeline_name);
info!(bytes = %bytes_processed, duration = ?elapsed, "File processing completed");
<span class="boring">}</span></code></pre></pre>
<p><strong>DEBUG</strong>: Use for detailed diagnostic information during development</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>debug!("Preparing stage: {}", stage.name());
debug!(chunk_count = chunks, size = bytes, "Processing chunk batch");
<span class="boring">}</span></code></pre></pre>
<p><strong>TRACE</strong>: Use for extremely detailed debugging (usually disabled in production)</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>trace!("Entering function with args: {:?}", args);
trace!(state = ?current_state, "State transition complete");
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="bootstrap-logger"><a class="header" href="#bootstrap-logger">Bootstrap Logger</a></h2>
<h3 id="bootstraplogger-trait"><a class="header" href="#bootstraplogger-trait">BootstrapLogger Trait</a></h3>
<p>The bootstrap logger abstraction allows for different implementations:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Bootstrap logging abstraction
pub trait BootstrapLogger: Send + Sync {
    /// Log an error message (fatal errors during bootstrap)
    fn error(&amp;self, message: &amp;str);

    /// Log a warning message (non-fatal issues)
    fn warn(&amp;self, message: &amp;str);

    /// Log an info message (normal bootstrap progress)
    fn info(&amp;self, message: &amp;str);

    /// Log a debug message (detailed diagnostic information)
    fn debug(&amp;self, message: &amp;str);
}
<span class="boring">}</span></code></pre></pre>
<h3 id="consolelogger-implementation"><a class="header" href="#consolelogger-implementation">ConsoleLogger Implementation</a></h3>
<p>The production implementation wraps the tracing crate:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Console logger implementation using tracing
pub struct ConsoleLogger {
    prefix: String,
}

impl ConsoleLogger {
    /// Create a new console logger with default prefix
    pub fn new() -&gt; Self {
        Self::with_prefix("bootstrap")
    }

    /// Create a new console logger with custom prefix
    pub fn with_prefix(prefix: impl Into&lt;String&gt;) -&gt; Self {
        Self { prefix: prefix.into() }
    }
}

impl BootstrapLogger for ConsoleLogger {
    fn error(&amp;self, message: &amp;str) {
        tracing::error!(target: "bootstrap", "[{}] {}", self.prefix, message);
    }

    fn warn(&amp;self, message: &amp;str) {
        tracing::warn!(target: "bootstrap", "[{}] {}", self.prefix, message);
    }

    fn info(&amp;self, message: &amp;str) {
        tracing::info!(target: "bootstrap", "[{}] {}", self.prefix, message);
    }

    fn debug(&amp;self, message: &amp;str) {
        tracing::debug!(target: "bootstrap", "[{}] {}", self.prefix, message);
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="nooplogger-for-testing"><a class="header" href="#nooplogger-for-testing">NoOpLogger for Testing</a></h3>
<p>A no-op implementation for testing scenarios where logging should be silent:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// No-op logger for testing
pub struct NoOpLogger;

impl NoOpLogger {
    pub fn new() -&gt; Self {
        Self
    }
}

impl BootstrapLogger for NoOpLogger {
    fn error(&amp;self, _message: &amp;str) {}
    fn warn(&amp;self, _message: &amp;str) {}
    fn info(&amp;self, _message: &amp;str) {}
    fn debug(&amp;self, _message: &amp;str) {}
}
<span class="boring">}</span></code></pre></pre>
<h3 id="usage-example-1"><a class="header" href="#usage-example-1">Usage Example</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use bootstrap::logger::{BootstrapLogger, ConsoleLogger};

fn bootstrap_application() -&gt; Result&lt;()&gt; {
    let logger = ConsoleLogger::new();

    logger.info("Starting application bootstrap");
    logger.debug("Parsing command line arguments");

    match parse_config() {
        Ok(config) =&gt; {
            logger.info("Configuration loaded successfully");
            Ok(())
        }
        Err(e) =&gt; {
            logger.error(&amp;format!("Failed to load configuration: {}", e));
            Err(e)
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="application-logging-with-tracing"><a class="header" href="#application-logging-with-tracing">Application Logging with Tracing</a></h2>
<h3 id="basic-logging-macros"><a class="header" href="#basic-logging-macros">Basic Logging Macros</a></h3>
<p>Once the tracing subscriber is initialized, use the standard tracing macros:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tracing::{trace, debug, info, warn, error};

fn process_pipeline(pipeline_id: &amp;str) -&gt; Result&lt;()&gt; {
    info!("Starting pipeline processing: {}", pipeline_id);

    debug!(pipeline_id = %pipeline_id, "Loading pipeline configuration");

    match execute_pipeline(pipeline_id) {
        Ok(result) =&gt; {
            info!(
                pipeline_id = %pipeline_id,
                bytes_processed = result.bytes,
                duration = ?result.duration,
                "Pipeline completed successfully"
            );
            Ok(result)
        }
        Err(e) =&gt; {
            error!(
                pipeline_id = %pipeline_id,
                error = %e,
                "Pipeline execution failed"
            );
            Err(e)
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="structured-fields"><a class="header" href="#structured-fields">Structured Fields</a></h3>
<p>Add structured fields to log events for better searchability and filtering:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Simple field
info!(stage = "compression", "Stage started");

// Display formatting with %
info!(pipeline_id = %id, "Processing pipeline");

// Debug formatting with ?
debug!(config = ?pipeline_config, "Loaded configuration");

// Multiple fields
info!(
    pipeline_id = %id,
    stage = "encryption",
    bytes_processed = total_bytes,
    duration_ms = elapsed.as_millis(),
    "Stage completed"
);
<span class="boring">}</span></code></pre></pre>
<h3 id="log-targets"><a class="header" href="#log-targets">Log Targets</a></h3>
<p>Use targets to categorize and filter log events:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Bootstrap logs
tracing::info!(target: "bootstrap", "Application starting");

// Domain events
tracing::debug!(target: "domain::pipeline", "Creating pipeline entity");

// Infrastructure events
tracing::debug!(target: "infrastructure::database", "Executing query");

// Metrics events
tracing::info!(target: "metrics", "Recording pipeline completion");
<span class="boring">}</span></code></pre></pre>
<h3 id="example-from-codebase"><a class="header" href="#example-from-codebase">Example from Codebase</a></h3>
<p>From <code>pipeline/src/infrastructure/repositories/stage_executor.rs</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>tracing::debug!(
    "Processing {} chunks with {} workers",
    chunks.len(),
    worker_count
);

tracing::info!(
    "Processed {} bytes in {:.2}s ({:.2} MB/s)",
    total_bytes,
    duration.as_secs_f64(),
    throughput_mbps
);

tracing::debug!(
    "Stage {} processed {} chunks successfully",
    stage_name,
    chunk_count
);
<span class="boring">}</span></code></pre></pre>
<p>From <code>bootstrap/src/signals.rs</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>tracing::info!("Received SIGTERM, initiating graceful shutdown");
tracing::info!("Received SIGINT (Ctrl+C), initiating graceful shutdown");
tracing::error!("Failed to register SIGTERM handler: {}", e);
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="integration-with-observabilityservice"><a class="header" href="#integration-with-observabilityservice">Integration with ObservabilityService</a></h2>
<p>The logging system integrates with the ObservabilityService for comprehensive monitoring.</p>
<h3 id="automatic-logging-in-operations"><a class="header" href="#automatic-logging-in-operations">Automatic Logging in Operations</a></h3>
<p>From <code>pipeline/src/infrastructure/logging/observability_service.rs</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl ObservabilityService {
    /// Complete operation tracking
    pub async fn complete_operation(
        &amp;self,
        operation_name: &amp;str,
        duration: Duration,
        success: bool,
        throughput_mbps: f64,
    ) {
        // ... update metrics ...

        info!(
            "Completed operation: {} in {:.2}s (throughput: {:.2} MB/s, success: {})",
            operation_name,
            duration.as_secs_f64(),
            throughput_mbps,
            success
        );

        // Check for alerts
        self.check_alerts(&amp;tracker).await;
    }

    /// Check for alerts based on current metrics
    async fn check_alerts(&amp;self, tracker: &amp;PerformanceTracker) {
        if tracker.error_rate_percent &gt; self.alert_thresholds.max_error_rate_percent {
            warn!(
                "🚨 Alert: High error rate {:.1}% (threshold: {:.1}%)",
                tracker.error_rate_percent,
                self.alert_thresholds.max_error_rate_percent
            );
        }

        if tracker.average_throughput_mbps &lt; self.alert_thresholds.min_throughput_mbps {
            warn!(
                "🚨 Alert: Low throughput {:.2} MB/s (threshold: {:.2} MB/s)",
                tracker.average_throughput_mbps,
                self.alert_thresholds.min_throughput_mbps
            );
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="operationtracker-with-logging"><a class="header" href="#operationtracker-with-logging">OperationTracker with Logging</a></h3>
<p>The <code>OperationTracker</code> automatically logs operation lifecycle:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct OperationTracker {
    operation_name: String,
    start_time: Instant,
    observability_service: ObservabilityService,
    completed: std::sync::atomic::AtomicBool,
}

impl OperationTracker {
    pub async fn complete(self, success: bool, bytes_processed: u64) {
        self.completed.store(true, std::sync::atomic::Ordering::Relaxed);

        let duration = self.start_time.elapsed();
        let throughput_mbps = calculate_throughput(bytes_processed, duration);

        // Logs completion via ObservabilityService.complete_operation()
        self.observability_service
            .complete_operation(&amp;self.operation_name, duration, success, throughput_mbps)
            .await;
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="usage-pattern"><a class="header" href="#usage-pattern">Usage Pattern</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Start operation (increments active count, logs start)
let tracker = observability_service
    .start_operation("file_processing")
    .await;

// Do work...
let result = process_file(&amp;input_path).await?;

// Complete operation (logs completion with metrics)
tracker.complete(true, result.bytes_processed).await;
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="logging-configuration"><a class="header" href="#logging-configuration">Logging Configuration</a></h2>
<h3 id="environment-variables-1"><a class="header" href="#environment-variables-1">Environment Variables</a></h3>
<p>Control logging behavior via environment variables:</p>
<pre><code class="language-bash"># Set log level (error, warn, info, debug, trace)
export RUST_LOG=info

# Set level per module
export RUST_LOG=pipeline=debug,bootstrap=info

# Set level per target
export RUST_LOG=infrastructure::database=debug

# Complex filtering
export RUST_LOG="info,pipeline::domain=debug,sqlx=warn"
</code></pre>
<h3 id="subscriber-initialization"><a class="header" href="#subscriber-initialization">Subscriber Initialization</a></h3>
<p>In <code>main.rs</code> or bootstrap code:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tracing_subscriber::{fmt, EnvFilter};

fn init_logging() -&gt; Result&lt;()&gt; {
    // Initialize tracing subscriber with environment filter
    tracing_subscriber::fmt()
        .with_env_filter(
            EnvFilter::try_from_default_env()
                .unwrap_or_else(|_| EnvFilter::new("info"))
        )
        .with_target(true)
        .with_thread_ids(true)
        .with_line_number(true)
        .init();

    info!("Logging initialized");
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h3 id="advanced-subscriber-configuration"><a class="header" href="#advanced-subscriber-configuration">Advanced Subscriber Configuration</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tracing_subscriber::{fmt, layer::SubscriberExt, EnvFilter, Registry};
use tracing_appender::{non_blocking, rolling};

fn init_advanced_logging() -&gt; Result&lt;()&gt; {
    // Console output
    let console_layer = fmt::layer()
        .with_target(true)
        .with_thread_ids(true);

    // File output with daily rotation
    let file_appender = rolling::daily("./logs", "pipeline.log");
    let (non_blocking_appender, _guard) = non_blocking(file_appender);
    let file_layer = fmt::layer()
        .with_writer(non_blocking_appender)
        .with_ansi(false)
        .json();

    // Combine layers
    let subscriber = Registry::default()
        .with(EnvFilter::try_from_default_env().unwrap_or_else(|_| EnvFilter::new("info")))
        .with(console_layer)
        .with(file_layer);

    tracing::subscriber::set_global_default(subscriber)?;

    info!("Advanced logging initialized");
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="performance-considerations-2"><a class="header" href="#performance-considerations-2">Performance Considerations</a></h2>
<h3 id="compile-time-filtering"><a class="header" href="#compile-time-filtering">Compile-Time Filtering</a></h3>
<p>Tracing macros are compile-time filtered at the <code>trace!</code> and <code>debug!</code> levels in release builds:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// This has ZERO overhead in release builds if max_level_debug is not set
debug!("Expensive computation result: {:?}", expensive_calculation());
<span class="boring">}</span></code></pre></pre>
<p>To enable debug/trace in release builds, add to <code>Cargo.toml</code>:</p>
<pre><code class="language-toml">[dependencies]
tracing = { version = "0.1", features = ["max_level_debug"] }
</code></pre>
<h3 id="lazy-evaluation"><a class="header" href="#lazy-evaluation">Lazy Evaluation</a></h3>
<p>Use closures for expensive operations:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// BAD: Always evaluates format_large_struct() even if debug is disabled
debug!("Large struct: {}", format_large_struct(&amp;data));

// GOOD: Only evaluates if debug level is enabled
debug!(data = ?data, "Processing large struct");
<span class="boring">}</span></code></pre></pre>
<h3 id="structured-vs-formatted"><a class="header" href="#structured-vs-formatted">Structured vs. Formatted</a></h3>
<p>Prefer structured fields over formatted strings:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Less efficient: String formatting always happens
info!("Processed {} bytes in {}ms", bytes, duration_ms);

// More efficient: Fields recorded directly
info!(bytes = bytes, duration_ms = duration_ms, "Processed data");
<span class="boring">}</span></code></pre></pre>
<h3 id="async-performance"><a class="header" href="#async-performance">Async Performance</a></h3>
<p>In async contexts, avoid blocking operations in log statements:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// BAD: Blocking call in log statement
info!("Config: {}", read_config_file_sync()); // Blocks async task!

// GOOD: Log after async operation completes
let config = read_config_file().await?;
info!(config = ?config, "Configuration loaded");
<span class="boring">}</span></code></pre></pre>
<h3 id="benchmark-results-1"><a class="header" href="#benchmark-results-1">Benchmark Results</a></h3>
<p>Logging overhead on Intel i7-10700K @ 3.8 GHz:</p>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Time</th><th>Overhead</th></tr></thead><tbody>
<tr><td><code>info!()</code> with 3 fields</td><td>~80 ns</td><td>Negligible</td></tr>
<tr><td><code>debug!()</code> (disabled)</td><td>~0 ns</td><td>Zero</td></tr>
<tr><td><code>debug!()</code> (enabled) with 5 fields</td><td>~120 ns</td><td>Minimal</td></tr>
<tr><td>JSON serialization</td><td>~500 ns</td><td>Low</td></tr>
<tr><td>File I/O (async)</td><td>~10 μs</td><td>Moderate</td></tr>
</tbody></table>
</div>
<p><strong>Recommendation</strong>: Log freely at <code>info!</code> and above. Use <code>debug!</code> and <code>trace!</code> judiciously.</p>
<hr />
<h2 id="best-practices-7"><a class="header" href="#best-practices-7">Best Practices</a></h2>
<h3 id="-do-1"><a class="header" href="#-do-1">✅ DO</a></h3>
<p><strong>Use structured fields for important data</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>info!(
    pipeline_id = %id,
    bytes = total_bytes,
    duration_ms = elapsed.as_millis(),
    "Pipeline completed"
);
<span class="boring">}</span></code></pre></pre>
<p><strong>Use display formatting (%) for user-facing types</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>info!(pipeline_id = %id, stage = %stage_name, "Processing stage");
<span class="boring">}</span></code></pre></pre>
<p><strong>Use debug formatting (?) for complex types</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>debug!(config = ?pipeline_config, "Loaded configuration");
<span class="boring">}</span></code></pre></pre>
<p><strong>Log errors with context</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>error!(
    pipeline_id = %id,
    stage = "encryption",
    error = %err,
    "Stage execution failed"
);
<span class="boring">}</span></code></pre></pre>
<p><strong>Use targets for filtering</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>tracing::debug!(target: "infrastructure::database", "Executing query: {}", sql);
<span class="boring">}</span></code></pre></pre>
<p><strong>Log at appropriate levels</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Error: Genuine failures
error!("Database connection lost: {}", err);

// Warn: Concerning but recoverable
warn!("Retry attempt {} of {}", attempt, max_attempts);

// Info: Important operational events
info!("Pipeline started successfully");

// Debug: Detailed diagnostic information
debug!("Stage prepared with {} workers", worker_count);
<span class="boring">}</span></code></pre></pre>
<h3 id="-dont-1"><a class="header" href="#-dont-1">❌ DON'T</a></h3>
<p><strong>Don't log sensitive information</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// BAD: Logging encryption keys
debug!("Encryption key: {}", key); // SECURITY RISK!

// GOOD: Log that operation happened without exposing secrets
debug!("Encryption key loaded from configuration");
<span class="boring">}</span></code></pre></pre>
<p><strong>Don't use println! or eprintln! in production code</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// BAD
println!("Processing file: {}", path);

// GOOD
info!(path = %path, "Processing file");
<span class="boring">}</span></code></pre></pre>
<p><strong>Don't log in hot loops without rate limiting</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// BAD: Logs millions of times
for chunk in chunks {
    debug!("Processing chunk {}", chunk.id); // Too verbose!
}

// GOOD: Log summary
debug!(chunk_count = chunks.len(), "Processing chunks");
// ... process chunks ...
info!(chunks_processed = chunks.len(), "Chunk processing complete");
<span class="boring">}</span></code></pre></pre>
<p><strong>Don't perform expensive operations in log statements</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// BAD
debug!("Data: {}", expensive_serialization(&amp;data));

// GOOD: Use debug formatting for lazy evaluation
debug!(data = ?data, "Processing data");
<span class="boring">}</span></code></pre></pre>
<p><strong>Don't duplicate metrics in logs</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// BAD: Metrics already tracked separately
info!("Throughput: {} MB/s", throughput); // Redundant with MetricsService!

// GOOD: Log events, not metrics
info!("File processing completed successfully");
// MetricsService already tracks throughput
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="testing-strategies-2"><a class="header" href="#testing-strategies-2">Testing Strategies</a></h2>
<h3 id="capturinglogger-for-tests"><a class="header" href="#capturinglogger-for-tests">CapturingLogger for Tests</a></h3>
<p>The bootstrap layer provides a capturing logger for tests:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(test)]
pub struct CapturingLogger {
    messages: Arc&lt;Mutex&lt;Vec&lt;LogMessage&gt;&gt;&gt;,
}

#[cfg(test)]
impl CapturingLogger {
    pub fn new() -&gt; Self {
        Self {
            messages: Arc::new(Mutex::new(Vec::new())),
        }
    }

    pub fn messages(&amp;self) -&gt; Vec&lt;LogMessage&gt; {
        self.messages.lock().unwrap().clone()
    }

    pub fn clear(&amp;self) {
        self.messages.lock().unwrap().clear();
    }
}

#[cfg(test)]
impl BootstrapLogger for CapturingLogger {
    fn error(&amp;self, message: &amp;str) {
        self.log(LogLevel::Error, message);
    }

    fn warn(&amp;self, message: &amp;str) {
        self.log(LogLevel::Warn, message);
    }

    fn info(&amp;self, message: &amp;str) {
        self.log(LogLevel::Info, message);
    }

    fn debug(&amp;self, message: &amp;str) {
        self.log(LogLevel::Debug, message);
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="test-usage"><a class="header" href="#test-usage">Test Usage</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[test]
fn test_bootstrap_logging() {
    let logger = CapturingLogger::new();

    logger.info("Starting operation");
    logger.debug("Detailed diagnostic");
    logger.error("Operation failed");

    let messages = logger.messages();
    assert_eq!(messages.len(), 3);
    assert_eq!(messages[0].level, LogLevel::Info);
    assert_eq!(messages[0].message, "Starting operation");
    assert_eq!(messages[2].level, LogLevel::Error);
}
<span class="boring">}</span></code></pre></pre>
<h3 id="testing-with-tracing-subscriber"><a class="header" href="#testing-with-tracing-subscriber">Testing with tracing-subscriber</a></h3>
<p>For testing application-level logging:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tracing_subscriber::{fmt, EnvFilter};

#[test]
fn test_application_logging() {
    // Initialize test subscriber
    let subscriber = fmt()
        .with_test_writer()
        .with_env_filter(EnvFilter::new("debug"))
        .finish();

    tracing::subscriber::with_default(subscriber, || {
        // Test code that produces logs
        process_pipeline("test-pipeline");
    });
}
<span class="boring">}</span></code></pre></pre>
<h3 id="noop-logger-for-silent-tests"><a class="header" href="#noop-logger-for-silent-tests">NoOp Logger for Silent Tests</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[test]
fn test_without_logging() {
    let logger = NoOpLogger::new();

    // Run tests without any log output
    let result = bootstrap_with_logger(&amp;logger);

    assert!(result.is_ok());
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="common-patterns"><a class="header" href="#common-patterns">Common Patterns</a></h2>
<h3 id="requestresponse-logging"><a class="header" href="#requestresponse-logging">Request/Response Logging</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub async fn process_file(path: &amp;Path) -&gt; Result&lt;ProcessingResult&gt; {
    let request_id = Uuid::new_v4();

    info!(
        request_id = %request_id,
        path = %path.display(),
        "Processing file"
    );

    match do_processing(path).await {
        Ok(result) =&gt; {
            info!(
                request_id = %request_id,
                bytes_processed = result.bytes,
                duration_ms = result.duration.as_millis(),
                "File processed successfully"
            );
            Ok(result)
        }
        Err(e) =&gt; {
            error!(
                request_id = %request_id,
                error = %e,
                "File processing failed"
            );
            Err(e)
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="progress-logging"><a class="header" href="#progress-logging">Progress Logging</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub async fn process_chunks(chunks: Vec&lt;Chunk&gt;) -&gt; Result&lt;()&gt; {
    let total = chunks.len();

    info!(total_chunks = total, "Starting chunk processing");

    for (i, chunk) in chunks.iter().enumerate() {
        process_chunk(chunk).await?;

        // Log progress every 10%
        if (i + 1) % (total / 10).max(1) == 0 {
            let percent = ((i + 1) * 100) / total;
            info!(
                processed = i + 1,
                total = total,
                percent = percent,
                "Chunk processing progress"
            );
        }
    }

    info!(total_chunks = total, "Chunk processing completed");
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h3 id="error-context-logging"><a class="header" href="#error-context-logging">Error Context Logging</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub async fn execute_pipeline(id: &amp;PipelineId) -&gt; Result&lt;()&gt; {
    debug!(pipeline_id = %id, "Loading pipeline from database");

    let pipeline = repository.find_by_id(id).await
        .map_err(|e| {
            error!(
                pipeline_id = %id,
                error = %e,
                "Failed to load pipeline from database"
            );
            e
        })?;

    debug!(
        pipeline_id = %id,
        stage_count = pipeline.stages().len(),
        "Pipeline loaded successfully"
    );

    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h3 id="conditional-logging"><a class="header" href="#conditional-logging">Conditional Logging</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn process_with_validation(data: &amp;Data) -&gt; Result&lt;()&gt; {
    if let Err(e) = validate(data) {
        warn!(
            validation_error = %e,
            "Data validation failed, attempting recovery"
        );

        return recover_from_validation_error(data, e);
    }

    debug!("Data validation passed");
    process(data)
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h2 id="next-steps-17"><a class="header" href="#next-steps-17">Next Steps</a></h2>
<ul>
<li><strong><a href="implementation/observability.html">Observability Overview</a></strong>: Complete observability strategy</li>
<li><strong><a href="implementation/metrics.html">Metrics Collection</a></strong>: Prometheus metrics integration</li>
<li><strong><a href="implementation/../architecture/error-handling.html">Error Handling</a></strong>: Error handling patterns</li>
<li><strong><a href="implementation/../testing/unit-tests.html">Testing</a></strong>: Testing strategies and practices</li>
</ul>
<hr />
<h2 id="references-1"><a class="header" href="#references-1">References</a></h2>
<ul>
<li><a href="https://docs.rs/tracing">tracing Documentation</a></li>
<li><a href="https://docs.rs/tracing-subscriber">tracing-subscriber Documentation</a></li>
<li><a href="https://www.honeycomb.io/blog/structured-logging-and-your-team">Structured Logging Best Practices</a></li>
<li>Source: <code>bootstrap/src/logger.rs</code> (lines 1-292)</li>
<li>Source: <code>pipeline/src/infrastructure/logging/observability_service.rs</code> (lines 1-716)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="concurrency-model"><a class="header" href="#concurrency-model">Concurrency Model</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Understanding the concurrency model.</p>
<h2 id="asyncawait"><a class="header" href="#asyncawait">Async/Await</a></h2>
<p>TODO: Explain async usage</p>
<h2 id="tokio-runtime"><a class="header" href="#tokio-runtime">Tokio Runtime</a></h2>
<p>TODO: Explain Tokio integration</p>
<h2 id="concurrency-patterns"><a class="header" href="#concurrency-patterns">Concurrency Patterns</a></h2>
<p>TODO: Show patterns</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="thread-pooling"><a class="header" href="#thread-pooling">Thread Pooling</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Thread pool configuration and tuning.</p>
<h2 id="thread-pool-architecture"><a class="header" href="#thread-pool-architecture">Thread Pool Architecture</a></h2>
<p>TODO: Explain thread pooling</p>
<h2 id="configuration-4"><a class="header" href="#configuration-4">Configuration</a></h2>
<p>TODO: Show configuration</p>
<h2 id="tuning-guidelines"><a class="header" href="#tuning-guidelines">Tuning Guidelines</a></h2>
<p>TODO: Add tuning guidance</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="resource-management"><a class="header" href="#resource-management">Resource Management</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Managing system resources efficiently.</p>
<h2 id="memory-management-3"><a class="header" href="#memory-management-3">Memory Management</a></h2>
<p>TODO: Explain memory usage</p>
<h2 id="file-descriptors"><a class="header" href="#file-descriptors">File Descriptors</a></h2>
<p>TODO: Explain FD management</p>
<h2 id="resource-limits"><a class="header" href="#resource-limits">Resource Limits</a></h2>
<p>TODO: Show limits and tuning</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="performance-optimization-1"><a class="header" href="#performance-optimization-1">Performance Optimization</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Performance optimization techniques.</p>
<h2 id="optimization-strategies"><a class="header" href="#optimization-strategies">Optimization Strategies</a></h2>
<p>TODO: List strategies</p>
<h2 id="common-bottlenecks"><a class="header" href="#common-bottlenecks">Common Bottlenecks</a></h2>
<p>TODO: Identify bottlenecks</p>
<h2 id="tuning-parameters"><a class="header" href="#tuning-parameters">Tuning Parameters</a></h2>
<p>TODO: Show tuning options</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="benchmarking"><a class="header" href="#benchmarking">Benchmarking</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Benchmarking the pipeline.</p>
<h2 id="benchmark-suite"><a class="header" href="#benchmark-suite">Benchmark Suite</a></h2>
<p>TODO: Show benchmark suite</p>
<h2 id="running-benchmarks"><a class="header" href="#running-benchmarks">Running Benchmarks</a></h2>
<p>TODO: Show how to run</p>
<h2 id="interpreting-results"><a class="header" href="#interpreting-results">Interpreting Results</a></h2>
<p>TODO: Explain results</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="profiling"><a class="header" href="#profiling">Profiling</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Profiling tools and techniques.</p>
<h2 id="profiling-tools"><a class="header" href="#profiling-tools">Profiling Tools</a></h2>
<p>TODO: List tools (flamegraph, perf, etc.)</p>
<h2 id="cpu-profiling"><a class="header" href="#cpu-profiling">CPU Profiling</a></h2>
<p>TODO: Show CPU profiling</p>
<h2 id="memory-profiling"><a class="header" href="#memory-profiling">Memory Profiling</a></h2>
<p>TODO: Show memory profiling</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="extending-the-pipeline"><a class="header" href="#extending-the-pipeline">Extending the Pipeline</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>How to extend the pipeline with custom functionality.</p>
<h2 id="extension-points"><a class="header" href="#extension-points">Extension Points</a></h2>
<p>TODO: List extension points</p>
<h2 id="plugin-architecture"><a class="header" href="#plugin-architecture">Plugin Architecture</a></h2>
<p>TODO: Explain plugin system (if any)</p>
<h2 id="best-practices-8"><a class="header" href="#best-practices-8">Best Practices</a></h2>
<p>TODO: Add guidance</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="custom-stages"><a class="header" href="#custom-stages">Custom Stages</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Creating custom pipeline stages.</p>
<h2 id="stage-interface-1"><a class="header" href="#stage-interface-1">Stage Interface</a></h2>
<p>TODO: Show stage trait</p>
<h2 id="implementation-example"><a class="header" href="#implementation-example">Implementation Example</a></h2>
<p>TODO: Show complete example</p>
<h2 id="testing-custom-stages"><a class="header" href="#testing-custom-stages">Testing Custom Stages</a></h2>
<p>TODO: Show testing approach</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="custom-algorithms"><a class="header" href="#custom-algorithms">Custom Algorithms</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Adding custom compression or encryption algorithms.</p>
<h2 id="algorithm-interface"><a class="header" href="#algorithm-interface">Algorithm Interface</a></h2>
<p>TODO: Show algorithm trait</p>
<h2 id="compression-example"><a class="header" href="#compression-example">Compression Example</a></h2>
<p>TODO: Show compression algorithm</p>
<h2 id="encryption-example"><a class="header" href="#encryption-example">Encryption Example</a></h2>
<p>TODO: Show encryption algorithm</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="software-requirements-specification"><a class="header" href="#software-requirements-specification">Software Requirements Specification</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Comprehensive software requirements specification.</p>
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<p>TODO: Add SRS introduction</p>
<h2 id="functional-requirements"><a class="header" href="#functional-requirements">Functional Requirements</a></h2>
<p>TODO: List functional requirements</p>
<h2 id="non-functional-requirements"><a class="header" href="#non-functional-requirements">Non-Functional Requirements</a></h2>
<p>TODO: List non-functional requirements</p>
<h2 id="system-requirements"><a class="header" href="#system-requirements">System Requirements</a></h2>
<p>TODO: List system requirements</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="software-design-document"><a class="header" href="#software-design-document">Software Design Document</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Comprehensive software design document.</p>
<h2 id="introduction-1"><a class="header" href="#introduction-1">Introduction</a></h2>
<p>TODO: Add SDD introduction</p>
<h2 id="system-architecture"><a class="header" href="#system-architecture">System Architecture</a></h2>
<p>TODO: Add detailed architecture</p>
<h2 id="component-design"><a class="header" href="#component-design">Component Design</a></h2>
<p>TODO: Add component designs</p>
<h2 id="data-design"><a class="header" href="#data-design">Data Design</a></h2>
<p>TODO: Add data models</p>
<h2 id="interface-design"><a class="header" href="#interface-design">Interface Design</a></h2>
<p>TODO: Add interface specifications</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="software-test-plan"><a class="header" href="#software-test-plan">Software Test Plan</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Software test plan and strategy.</p>
<h2 id="introduction-2"><a class="header" href="#introduction-2">Introduction</a></h2>
<p>TODO: Add STP introduction</p>
<h2 id="test-strategy"><a class="header" href="#test-strategy">Test Strategy</a></h2>
<p>TODO: Add test strategy</p>
<h2 id="test-levels"><a class="header" href="#test-levels">Test Levels</a></h2>
<p>TODO: List test levels (unit, integration, system)</p>
<h2 id="test-cases"><a class="header" href="#test-cases">Test Cases</a></h2>
<p>TODO: Add key test cases</p>
<h2 id="test-environment"><a class="header" href="#test-environment">Test Environment</a></h2>
<p>TODO: Describe test environment</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="public-api-reference"><a class="header" href="#public-api-reference">Public API Reference</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Public API documentation and examples.</p>
<h2 id="api-overview"><a class="header" href="#api-overview">API Overview</a></h2>
<p>TODO: Add API overview</p>
<h2 id="core-types"><a class="header" href="#core-types">Core Types</a></h2>
<p>TODO: List core public types</p>
<h2 id="examples-1"><a class="header" href="#examples-1">Examples</a></h2>
<p>TODO: Add usage examples</p>
<h2 id="generated-documentation"><a class="header" href="#generated-documentation">Generated Documentation</a></h2>
<p>See <a href="api/../../../target/doc/pipeline/index.html">rustdoc API documentation</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="internal-apis"><a class="header" href="#internal-apis">Internal APIs</a></h1>
<p><strong>Version:</strong> 0.1.0
<strong>Date:</strong> October 2025
<strong>SPDX-License-Identifier:</strong> BSD-3-Clause
<strong>License File:</strong> See the LICENSE file in the project root.
<strong>Copyright:</strong> © 2025 Michael Gardner, A Bit of Help, Inc.
<strong>Authors:</strong> Michael Gardner
<strong>Status:</strong> Draft</p>
<p>Internal API documentation for contributors.</p>
<h2 id="internal-architecture"><a class="header" href="#internal-architecture">Internal Architecture</a></h2>
<p>TODO: Add internal API overview</p>
<h2 id="module-organization"><a class="header" href="#module-organization">Module Organization</a></h2>
<p>TODO: Explain module structure</p>
<h2 id="extension-points-1"><a class="header" href="#extension-points-1">Extension Points</a></h2>
<p>TODO: List internal extension points</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
